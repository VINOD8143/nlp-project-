authors,abstract,title
"['S. Ben Jabra', 'Ezzeddine Zagrouba']","In this paper, a robust 3D triangular mesh watermarking algorithm based on 3D segmentation is proposed. In this algorithm three classes of watermarking are combined. First, we segment the original image to many different regions. Then we mark every type of region with the corresponding algorithm based on their curvature value. The experiments show that our watermarking is robust against numerous attacks including RST transformations, smoothing, additive random noise, cropping, simplification and remeshing.",A new approach of 3D watermarking based on image segmentation
"['Joaqu??n J. Torres', 'Jes?ßs M. Cort??s', 'Joaqu??n Marro', 'Hilbert J. Kappen']","We studied an autoassociative neural network with dynamic synapses which include a facilitating mechanism. We have developed a general mean-field framework to study the relevance of the different parameters defining the dynamics of the synapses and their influence on the collective properties of the network. Depending on these parameters, the network shows different types of behaviour including a retrieval phase, an oscillatory regime, and a non-retrieval phase. In the oscillatory phase, the network activity continously jumps between the stored patterns. Compared with other activity-dependent mechanisms such as synaptic depression, synaptic facilitation enhances the network ability to switch among the stored patterns and, therefore, its adaptation to external stimuli. A detailed analysis of our system reflects an efficient-more rapid and with lesser errors-network access to the stored information with stronger facilitation. We also present a set of Monte Carlo simulations confirming our analytical results.",Attractor neural networks with activity-dependent synapses: The role of synaptic facilitation
"['Genevi eve Paquin', 'Laurent Vuillon']","It is well-known that Sturmian sequences are the non ultimately periodic sequences that are balanced over a 2-letter alphabet. They are also characterized by their complexity: they have exactly $(n+1)$ distinct factors of length $n$. A natural generalization of Sturmian sequences is the set of infinite episturmian sequences. These sequences are not necessarily balanced over a $k$-letter alphabet, nor are they necessarily aperiodic. In this paper, we characterize balanced episturmian sequences, periodic or not, and prove Fraenkel's conjecture for the special case of episturmian sequences. It appears that balanced episturmian sequences are all ultimately periodic and they can be classified in 3 families.",A characterization of balanced episturmian sequences
"['Yaser Sheikh', 'Mumtaz Sheikh', 'Mubarak Shah']","One of the fundamental challenges of recognizing actions is accounting for the variability that arises when arbitrary cameras capture humans performing actions. In this paper, we explicitly identify three important sources of variability: (1) viewpoint, (2) execution rate, and (3) anthropometry of actors, and propose a model of human actions that allows us to investigate all three. Our hypothesis is that the variability associated with the execution of an action can be closely approximated by a linear combination of action bases in joint spatio-temporal space. We demonstrate that such a model bounds the rank of a matrix of image measurements and that this bound can be used to achieve recognition of actions based only on imaged data. A test employing principal angles between subspaces that is robust to statistical fluctuations in measurement data is presented to find the membership of an instance of an action. The algorithm is applied to recognize several actions, and promising results have been obtained.",Exploring the space of a human action
"['Efraim Laksman', 'H?ùkan Lennerstad', 'Magnus Nilsson']","This paper generalizes previous optimal upper bounds on the minimum Euclidean distance for phase shift keying (PSK) block codes, that are explicit in three parameters: alphabet size, block length a ...",Generalized upper bounds on the minimum distance of PSK block codes
"['Simonetta Balsamo', 'GianÉ??Luca Dei Rossi', 'Andrea Marin']","Queueing networks with multiple classes of customers play a fundamental role for evaluating the performance of both software and hardware architectures. The main strength of productÉ??form models, in particular of BCMP queueing networks, is that they combine a flexible formalism with efficient analysis techniques and solution algorithms. In this paper we provide an algorithm that starting from a highÉ??level description of a system, and from the definition of its components in terms of interacting subÉ??systems, computes a multipleÉ??class and multipleÉ??chain BCMP queueing network. We believe that the strength of this approach is twofold. First, the modeller deals with simplified models, which are defined in a modular and hierarchical way. Hence, we can carry on sensitivity analysis that may easily include structural changes (and not only on the time parameters). Second, maintaining the productÉ??form property allows one to derive the average system performance indices very efficiently. The paper also discusses the ...",Applying BCMP multi-class queueing networks for the performance evaluation of hierarchical and modular software systems
"['Andrea Mazzanti', 'Pietro Andreani']","A CMOS oscillator employing differential transistor pairs working in Class-C in push-pull configuration is presented. The oscillator exhibits the same advantages enjoyed by complementary topologies on oscillators based on a single differential pair, while yielding a substantial power consumption reduction thanks to the Class-C operation. The phase-noise performance and the fundamental conditions required to keep the transistors working in Class-C are analyzed in detail. It is shown that, for an optimal performance, both nMOS and pMOS transistors should not be pushed into the deep triode region by the instantaneous resonator voltage, and a simple circuit solution is proposed to accommodate a large oscillation swing. A 0.18- ?¨m CMOS prototype of the (voltage-controlled) oscillator displays an oscillation frequency from 6.09 to 7.50 GHz. The phase noise at 2-MHz offset is below -120 dBc/Hz with a power dissipation of 2.2 mW, for a state-of-the-art figure-of-merit ranging from 189 to 191 dBc/Hz.",A PushÉ??Pull Class-C CMOS VCO
['Daniil Ryabko'],"In statistical setting of the pattern recognition problem the number of examples required to approximate an unknown labelling function is linear in the VC dimension of the target learning class. In this work we consider the question whether such bounds exist if consider only computable pattern recognition methods, assuming that the unknown labelling function is also computable. We find that in this case the number of examples required for a computable method to approximate the labelling function not only is not linear, but grows faster (in the VC dimension of the class) than any computable function. No time or space constraints are put on the predictors or target functions; the only resource we consider is the training examples.#R##N##R##N#The task of pattern recognition is considered in conjunction with another learning problem É?? data compression. An impossibility result for the task of data compression allows us to estimate the sample complexity for pattern recognition.",On computability of pattern recognition problems
"['Maria Chiara Carrozza', 'Paolo Dario', 'Arianna Menciassi', 'A. Fenu']","We first discuss some general aspects of micromanipulation and possible different approaches. Then, we present new results in the micromanipulation of mechanical and biological objects. The apparatus we use is a purposely developed workstation comprising macro- and micro-manipulators. The most innovative component of the workstation is a micro-gripper fabricated using LIGA technology and actuated by piezoelectric actuators. We describe the design, fabrication and performance of a few prototypes of LIGA micro-grippers. Results are presented which demonstrate the ability of the system to manipulate effectively both micro-mechanical and biological micro-objects.",Manipulating biological and mechanical micro-objects using LIGA-microfabricated end-effectors
['Koji Chinen'],"In 1999, Iwan Duursma defined the zeta function for a linear code as a generating function of its Hamming weight enumerator. It can also be defined for other homogeneous polynomials not corresponding to existing codes. If the homogeneous polynomial is invariant under the MacWilliams transform, then its zeta function satisfies a functional equation and we can formulate an analogue of the Riemann hypothesis. As far as existing codes are concerned, the Riemann hypothesis is believed to be closely related to the extremal property. In this article, we show there are abundant polynomials invariant by the MacWilliams transform which satisfy the Riemann hypothesis. The proof is carried out by explicit construction of such polynomials. To prove the Riemann hypothesis for a certain class of invariant polynomials, we establish an analogue of the Enestrom-Kakeya theorem.",An abundance of invariant polynomials satisfying the Riemann hypothesis
"['Steven Euijong Whang', 'David Menestrina', 'Georgia Koutrika', 'Martin Theobald', 'Hector Garcia-Molina']","Entity Resolution (ER) is the problem of identifying which records in a database refer to the same real-world entity. An exhaustive ER process involves computing the similarities between pairs of records, which can be very expensive for large datasets. Various blocking techniques can be used to enhance the performance of ER by dividing the records into blocks in multiple ways and only comparing records within the same block. However, most blocking techniques process blocks separately and do not exploit the results of other blocks. In this paper, we propose an  iterative blocking framework  where the ER results of blocks are reflected to subsequently processed blocks. Blocks are now iteratively processed until no block contains any more matching records. Compared to simple blocking, iterative blocking may achieve higher accuracy because reflecting the ER results of blocks to other blocks may generate additional record matches. Iterative blocking may also be more efficient because processing a block now saves the processing time for other blocks. We implement a scalable iterative blocking system and demonstrate that iterative blocking can be more accurate and efficient than blocking for large datasets.",Entity resolution with iterative blocking
"['Todd Mytkowicz', 'Amer Diwan', 'Matthias Hauswirth', 'Peter F. Sweeney']","Performance analysts profile their programs to find methods that are worth optimizing: the ""hot"" methods. This paper shows that four commonly-used Java profilers ( xprof , hprof , jprofile, and yourkit ) often disagree on the identity of the hot methods. If two profilers disagree, at least one must be incorrect. Thus, there is a good chance that a profiler will mislead a performance analyst into wasting time optimizing a cold method with little or no performance improvement.   This paper uses causality analysis to evaluate profilers and to gain insight into the source of their incorrectness. It shows that these profilers all violate a fundamental requirement for sampling based profilers: to be correct, a sampling-based profilermust collect samples randomly.   We show that a proof-of-concept profiler, which collects samples randomly, does not suffer from the above problems. Specifically, we show, using a number of case studies, that our profiler correctly identifies methods that are important to optimize; in some cases other profilers report that these methods are cold and thus not worth optimizing.",Evaluating the accuracy of Java profilers
"['Tao Fang', 'Lap-Pui Chau']","In motion-compensated video-coding schemes, such as MPEG, an I frame is normally followed by several P frames and possibly B frames in a group-of-picture (GOP). In error-prone environments, errors happening in the previous frames in a GOP may propagate to all the following frames until the next I frame, which is the beginning of the next GOP. In this paper, we propose a novel GOP structure for robust transmission of MPEG video bitstream. By selecting the optimal position of the I frame in a GOP, robustness can be achieved without reducing any coding efficiency. Experimental results demonstrate the robustness of the proposed GOP structure.",Robust group-of-picture architecture for video transmission over error-prone channels
['David R. Stoutemyer'],"Most of us have taken the exact rational and approximate numbers in our computer algebra systems for granted for a long time, not thinking to ask if they could be significantly better. With exact rational arithmetic and adjustable-precision floating-point arithmetic to precision limited only by the total computer memory or our patience, what more could we want for such numbers? It turns out that there is much more that can be done that permits us to obtain exact results more often, more intelligible results, approximate results guaranteed to have requested error bounds, and recovery of exact results from approximate ones.",Useful computations need useful numbers
"['Awadhesh Srivastava', 'Kanad K. Biswas']","This paper addresses the problem of retargeting, namely adapting large source images for effective viewing at a smaller size with possible applications to PDAs, or dynamic page layouts. Instead of extracting regions of interest for retargeting, the uninteresting parts are removed from the scene in Shai Avidan and Shamir, A. (2007). This is done by computing the RGB variance within non-overlapping 3times3 blocks and removing the block path with minimal variance cost using dynamic programming. It is shown that transformation to CIELAB space is more effective for visual interpretation of image content. The implementations are shown to be much faster than the seam carving approach of Shai Avidan and Shamir, A. (2007). Schemes are also presented for speeding up the seam carving scheme itself.",Fast Content Aware Image Retargeting
"['Ashish Goel', 'Cyrus Shahabi', 'Shu-Yuen Didi Yao', 'Roger Zimmermann']","Scalable storage architectures allow for the addition of disks to increase storage capacity and/or bandwidth. In its general form, disk scaling also refers to disk removals when either capacity needs to be conserved or old disk drives are retired. Assuming random placement of blocks on multiple nodes of a continuous media server, our optimization objective is to redistribute a minimum number of media blocks after disk scaling. This objective should be met under two restrictions. First, uniform distribution and hence a balanced load should be ensured after redistribution. Second, the redistributed blocks should be retrieved at the normal mode of operation in one disk access and through low complexity computation. We propose a technique that meets the objective, while we prove that it also satisfies both restrictions. The SCADDAR approach is based on using a series of REMAP functions which can derive the location of a new block using only its original location as a basis.",SCADDAR: an efficient randomized technique to reorganize continuous media blocks
['Werner Sandmann'],"Rare event simulation for stochastic models of complex systems is still a great challenge even for Markovian models. We review results in importance sampling for Markov chains, provide new viewpoints and insights, and we pose some future research directions.",Importance sampling in Markovian settings
"['Angelica Lindl??f', 'Marcus Br??utigam', 'Aakash Chawade', 'Bj??rn Olsson', 'Olof Olsson']","Freezing tolerance in plants is obtained during a period of low nonfreezing temperatures before the winter sets on, through a biological process known as cold acclimation. Cold is one of the major stress factors that limits the growth, productivity and distribution of plants, and understanding the mechanism of cold tolerance is therefore important for crop improvement. Expressed sequence tags (EST) analysis is a powerful, economical and time-efficient way of assembling information on the transcriptome. To date, several EST sets have been generated from cold-induced cDNA libraries from several different plant species. In this study we utilize the variation in the frequency of ESTs sampled from different cold-stressed plant libraries, in order to identify genes preferentially expressed in cold in comparison to a number of control sets. The species included in the comparative study are oat (Avena sativa), barley (Hordeum vulgare), wheat (Triticum aestivum), rice (Oryza sativa) and Arabidopsis thaliana. However, in order to get comparable gene expression estimates across multiple species and data sets, we choose to compare the expression of tentative ortholog groups (TOGs) instead of single genes, as in the normal procedure. We consider TOGs as preferentially expressed if they are detected as differentially expressed by a test statistic and up-regulated in comparison to all control sets, and/or uniquely expressed during cold stress, i.e., not present in any of the control sets. The result of this analysis revealed a diverse representation of genes in the different species. In addition, the derived TOGs mainly represent genes that are long-term highly or moderately expressed in response to cold and/or other stresses.",Identification of cold-induced genes in cereal crops and arabidopsis through comparative analysis of multiple EST sets
"['Alan Burns', 'Neil C. Audsley', 'Andy J. Wellings']","This position paper concerns itself with real-time safety critical distributed systems. It presents a computational model that is appropriate for this type of application and architecture. It then defines a resource allocations scheme based upon fixed priority scheduling. Such a scheme has the advantage (over purely static schedules) of supporting greater levels of flexibility and non-determinism, whilst still providing static guarantees of necessary timing behaviour (i.e. end-to-end deadlines through the systems). Priority based communication protocols are investigated, with possible future techniques reviewed.",Real-time distributed computing
"['Frances E. Pereira', 'Amparo Galindo', 'George Jackson', 'Claire S. Adjiman']","a b s t r a c t The constant pressureÉ??temperature (PÉ??T) flash plays an important role in the modelling of fluid-phase behaviour, and its solution is especially challenging for equations of state in which the volume is expressed as an implicit function of the pressure. We explore the relative merits of solving the PÉ??T flash in two ensembles: mole numbers, pressure and temperature, in which each free-energy evaluation requires the use of a numerical solver; and mole numbers, volume and temperature, in which a direct evaluation of the free-energy is possible. We examine the performance of two algorithms, HELD (Helmholtz free energy Lagrangian dual), introduced in Pereira et al. (2012), and GILD (Gibbs free energy Lagrangian dual), introduced here, for the fluid-phase equilibria of 8 mixtures comprising up to 10 components, using two equations of state. While the reliability of both algorithms is comparable, the computational cost of HELD is consistently lower; this difference becomes increasingly pronounced as the number of components is increased. ?? 2014 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY license",On the impact of using volume as an independent variable for the solution of P-T fluid-phase equilibrium with equations of state
"['Anastasia Karanastasi', 'Stavros Christodoulakis']","The OntoNL Framework provides an architecture and re-usable components for automating as much as possible the building of natural language interfaces to information systems. In addition to the syntactic analysis components, OntoNL has semantic analysis components which exploit domain ontologies to provide better disambiguation of the user input. We present in this paper the algorithms used for semantic processing of the natural language queries, as well as an ontology-driven semantic relatedness measure developed for this purpose. We also present extensive evaluation results with different ontologies using human subjects.",Semantic Processing of Natural Language Queries in the OntoNL Framework
"['Kil Soo Suh', 'A. Milton Jenkins']","This study compares a linear keyword language interface and a restricted natural language interface for data retrieval by a novice user. The comparison focuses on the effect of different data base interfaces on user performance as measured by query correctness and query writing time in a query writing task across varying query types and training levels. To accomplish this objective, a laboratory experiment was conducted using a split-plot factorial design using two between-subjects factors and one within-subjects factor. The results indicate that the restricted natural language subjects performed significantly better than the linear keyword language subjects in terms of both query correctness and query writing time.",A Comparison of Linear Keyword and Restricted Natural Language Data Base Interfaces for Novice Users
"['Abdalkarim Awad', 'Andreas Mitschele-Thiel', 'Falko Dressler']","Virtual position-based routing protocols have many attractive characteristics for wireless sensor networks. Typically, such protocols use a proactive scheme for updating routing tables. Because sensor networks can have very low data rate, sending periodic beacons to update routing tables can be very expensive. Instead, reactive approaches might be more appropriate in such scenarios. MANET-inspired reactive routing protocols do not scale well because of the effort in the order of O(n) for each routing information update. In this paper, we present Reactive Virtual Cord Protocol (RVCP), a data-centric reactive virtual position based routing protocol for use in sensor networks. Route discovery is directed towards the destination and hence there is no need to flood the entire network to discover a route. Our approach is based on Virtual Cord Protocol (VCP), an efficient, virtual relative position based routing protocol that also provides support for data management as known from typical Distributed Hash Table (DHT) services. To minimize the end-to-end delay and energy consumption, we used adaptive techniques for the development of RVCP.",Reactive Virtual Position-Based Routing in Wireless Sensor Networks
"['Jacek Blazewicz', 'Klaus Ecker', 'Tam?≠s Kis', 'Michal Tanas']","This paper considers a problem of coupled task scheduling on one processor, where all processing times are equal to 1, the gap has exact length h, precedence constraints are strict and the criterion is to minimise the schedule length. This problem is introduced e.g. in systems controlling radar operations. We show that the general problem is NP-hard.",A note on the complexity of scheduling coupled tasks on a single processor
"['Huaguang Zhang', 'Zhenwei Liu', 'Guang-Bin Huang', 'Zhanshan Wang']","In this paper, a weighting-delay-based method is developed for the study of the stability problem of a class of recurrent neural networks (RNNs) with time-varying delay. Different from previous results, the delay interval [0, d(t)] is divided into some variable subintervals by employing weighting delays. Thus, new delay-dependent stability criteria for RNNs with time-varying delay are derived by applying this weighting-delay method, which are less conservative than previous results. The proposed stability criteria depend on the positions of weighting delays in the interval [0, d(t)], which can be denoted by the weighting-delay parameters. Different weighting-delay parameters lead to different stability margins for a given system. Thus, a solution based on optimization methods is further given to calculate the optimal weighting-delay parameters. Several examples are provided to verify the effectiveness of the proposed criteria.",Novel Weighting-Delay-Based Stability Criteria for Recurrent Neural Networks With Time-Varying Delay
['Jian-Yun Nie'],"Search for information is no longer exclusively limited within the native language of the user, but is more and more extended to other languages. This gives rise to the problem of cross-language information retrieval (CLIR), whose goal is to find relevant information written in a different language to a query. In addition to the problems of monolingual information retrieval (IR), translation is the key problem in CLIR: one should translate either the query or the documents from a language to another. However, this translation problem is not identical to full-text machine translation (MT): the goal is not to produce a human-readable translation, but a translation suitable for finding relevant documents. Specific translation methods are thus required. The goal of this book is to provide a comprehensive description of the specifi c problems arising in CLIR, the solutions proposed in this area, as well as the remaining problems. The book starts with a general description of the monolingual IR and CLIR problems. Different classes of approaches to translation are then presented: approaches using an MT system, dictionary-based translation and approaches based on parallel and comparable corpora. In addition, the typical retrieval effectiveness using different approaches is compared. It will be shown that translation approaches specifically designed for CLIR can rival and outperform high-quality MT systems. Finally, the book offers a look into the future that draws a strong parallel between query expansion in monolingual IR and query translation in CLIR, suggesting that many approaches developed in monolingual IR can be adapted to CLIR. The book can be used as an introduction to CLIR. Advanced readers can also find more technical details and discussions about the remaining research challenges in the future. It is suitable to new researchers who intend to carry out research on CLIR.",Cross-Language Information Retrieval
['Hannes Frey'],"I consider the known localized multicast protocol MSTEAM and derive the energy consumed by the multicast tree constructed by this protocol in the best case. Moreover, I show that the length of multicast links connecting into a multicast branch can not be bounded from above. For typical wireless networks where links have a limited communication range, however, I can show that asymptotically the relation between the derived best case energy consumption of MSTEAM and a known lower bound on multicast energy consumption is limited by a factor of 2.",Best case energy analysis of localized euclidean minimum spanning tree based multicasting in ad hoc and sensor networks
"['Terje O. Espelid', 'K. J. Overholt']",We describe an automatic quadrature routine which is specifically designed for real functions having a certain type of infinite oscillating tails. The algorithm is designed to integrate a vector function over an infinite interval. A FORTRAN implementation of the algorithm is included.,DQAINF: an algorithm for automatic integration of infinite oscillating tails
"['A. van Overeem', 'J. Witters', 'Vassilios Peristeras']","Interoperability between public administrations receives nowadays a lot of attention. Also in the European Union interworking is high on the priority list, but the challenges to achieve the European administrative space is enormous. Many research projects are undertaken, especially in the domain of semantic interoperability. Many of these efforts seem to start from a technical solution rather than from an actual business problem. By taking a narrow view on the problem space, they only promise limited support for the many challenges in the domain of interoperability and innovation of e-government services. In this paper we present a business driven approach that looks promising in enabling entire classes of interoperability solutions",An Interoperability Framework for Pan-European E-Government Services (PEGS)
"['Ahmed Bader', 'Eylem Ekici']","The performance of a multihop wireless network is typically affected by the interference caused by transmissions in the same network. In a statistical fading environment, the interference effects become harder to predict. Information sources in a multihop wireless network can improve throughput and delay performance of data streams by implementing interference-aware packet injection mechanisms. Forcing packets to wait at the head of queues and coordinating packet injections among different sources enable effective control of copacket interference. In this paper, throughput and delay performance in interference-limited multihop networks is analyzed. Using nonlinear probabilistic hopping models, waiting times which jointly optimize throughput and delay performances are derived. Optimal coordinated injection strategies are also investigated as functions of the number of information sources and their separations. The resulting analysis demonstrates the interaction of performance constraints and achievable capacity in a wireless multihop network.",Performance optimization of interference-limited multihop networks
"['Tony Jebara', 'Alex Pentland']","We propose Action-Reaction Learning as an approach for analyzing and synthesizing human behaviour. This paradigm uncovers causal mappings between past and future events or between an action and its reaction by observing time sequences. We apply this method to analyze human interaction and to subsequently synthesize human behaviour. Using a time series of perceptual measurements, a system automatically discovers correlations between past gestures from one human participant (action) and a subsequent gesture (reaction) from another participant. A probabilistic model is trained from data of the human interaction using a novel estimation technique, Conditional Expectation Maximization (CEM). The estimation uses general bounding and maximization to monotonically find the maximum conditional likelihood solution. The learning system drives a graphical interactive character which probabilistically predicts a likely response to a user's behaviour and performs it interactively. Thus, after analyzing human interaction in a pair of participants, the system is able to replace one of them and interact with a single remaining user.",Action Reaction Learning: Automatic Visual Analysis and Synthesis of Interactive Behaviour
"['William K. C. Lam', 'Robert K. Brayton', 'Alberto L. Sangiovanni-Vincentelli']","We propose a general circuit delay model that unifies all previous delay models, e.g. floating, viability, and transition delays, and models introduced in this paper, e.g. delays by sequences of vectors and minimum delays. Then, we formulate the computation of the exact circuit delays, under both bounded and unbounded gate delay models, as a mixed Boolean linear programming using a new formulation technique, called Timed Boolean Function. Next, we compute the exact delays of combinational circuits for transition delay and delay by sequences of vectors. We show that delays by sequences of vectors and floating (or viability) delays are invariant under both bounded and unbounded gate delay models. Finally, we address the effect of gate delay lower bounds on delays of circuits. We demonstrate the effectiveness of the method by giving exact delay results for all ISCAS benchmark circuits (except C6188).",Circuit Delay Models and Their Exact Computation Using Timed Boolean Functions
['Olov Marklund'],"In this paper, a new method to reconstruct piecewise continuous phase estimates using inphase and quadrature components acquired from interferometry measurements is derived and discussed. The method, based on the concept of anisotropic evolution formulations, is shown to be far less noise sensitive than similar methods operating on modulo-mapped data (i.e., traditional phase unwrapping methods). The method is able to produce reliable phase estimates from data containing complex sheared structures in combination with high noise content without relying on user-defined weights.",An anisotropic evolution formulation applied in 2-D unwrapping of discontinuous phase surfaces
"['Eun-Kyoung Kim', 'Woo-Jin Han', 'Yung-Hwan Oh']","Two-band speech model which assumes lower band is a quasi-periodic component and upper band is a non-periodic component is widely used due to its natural and simple framework. In this paper, a score function is defined for splitting lower and upper band of two-band speech model and estimation method of band-splitting frequency which is the boundary of the two bands is proposed. The score function is calculated for each harmonic frequency using the normalized autocorrelation function of the time signal corresponding to the each sub-band divided by the given frequency. By using the score function, tracking technique is applied to the band-splitting frequency estimation procedure to reflect the continuity between neighboring frames. Experimental tests confirm that the proposed score function is effective for estimation of the band-splitting frequency and produces better results compared with the previous other methods.",A score function of splitting band for two-band speech model
"['Xin Zhang', 'Jianwu Zhang', 'Qingliang Zeng', 'Guoshun Sun']","In allusion to the reliability, which exists in the parametric design and optimizing process of the single hydraulic prop, this paper presents the new method in comparison with the traditional calculating and checking method for the reliability. The geometry model of the hydraulic prop is built firstly based on the 3D software, then analyzed and optimized by the finite element software-ANSYS. Results show that the method presented for the reliability is efficient and accurate.",The Reliability Study of the Single Hydraulic Prop Based on Finite Element Analysis
"['Yan-Kui Liu', 'Xiao-Liang Zhu']","Based on credibility theory, a new class of two-stage minimum risk location-allocation model is first proposed. Then we deal with the approximation of the location and allocation problem after that, a hybrid algorithm, which integrates the approximation approach, neural network and simulated annealing, is designed to solve the proposed location-allocation problem, and a numerical example is provided to test the effectiveness of the hybrid algorithm",Fuzzy Capacitated Location-allocation Problem with Minimum Risk Criteria
"['Xuanzhe Liu', 'Gang Huang', 'Hong Mei']","The popularity of service oriented computing (SOC) brings a large number of distributed, well-encapsulated and reusable services all over Internet, and makes it possible to create value-added services by means of service composition. Current composition styles are too professional to those end users when building their own applications. Actually, the end user would prefer rapidly discovering the best-of-breed services to assemble as well as visually personalizing the presentation to enjoy rich experiences. We propose an end user service composition approach for reducing the composition complexity and difficulty from the end user perspective. In our approach, similar candidate services are aggregated together as a unified resource, whose wide QoS spectrum can be easily manipulated by the end users to satisfy their requirements. Then they can personalize the services and, the composition occurs only at the presentation layer. The main contributions of the approach are: (i) enabling the end users to personalize the composite application with more powerful presentation; (ii) supporting the end users to dynamically customize the service composition in terms of QoS; (iii) alleviating the end users from the time-consuming task of selecting service to compose.",Towards End User Service Composition
"['Archana K. Singh', 'Hideki Asoh', 'Steven Phillips']","article i nfo Computing phase-locking values between EEG signals is a popular method for quantifying functional connectivity. However, this method involves large-scale, high-resolution datasets, which impose a serious multiple testing problem. Standard multiple testing methods fail to exploit the information from the complex dependence structure that varies across hypotheses in spectral, temporal, and spatial dimensions and result in a severe loss of power. They tend to control the false positives at the cost of hiding true positives. We introduce a new approach, called optimal discovery procedure (ODP) for identifying synchrony that is statistically significant. ODP maximizes the number of true positives for a given number of false positives, and thus offers a theoretical optimum for detecting significant synchrony in a multiple testing situation. We demonstrate the utility of this method with PLV data obtained from a visual search study. We also present simulation analysis to confirm the validity and relevance of using ODP in comparison with the standard FDR method for given configurations of true synchrony. We also compare the effectiveness of ODP with our previously published investigation of hierarchical FDR method (Singh and Phillips, 2010).",Optimal detection of functional connectivity from high-dimensional EEG synchrony data.
"['Jun-Hao Li', 'Shun Liu', 'Hui Zhou', 'Liang-Hu Qu', 'Jian-Hua Yang']","Although microRNAs (miRNAs), other non-coding RNAs (ncRNAs) (e.g. lncRNAs, pseudogenes and circRNAs) and competing endogenous RNAs (ceRNAs) have been implicated in cell-fate determination and in various human diseases, surprisingly little is known about the regulatory interaction networks among the multiple classes of RNAs. In this study, we developed starBase v2.0 (http://starbase.sysu. edu.cn/) to systematically identify the RNAÉ??RNA and proteinÉ??RNA interaction networks from 108 CLIP-Seq (PAR-CLIP, HITS-CLIP, iCLIP, CLASH) data sets generated by 37 independent studies. By analyzing millions of RNA-binding protein binding sites, we identified ã®´ 9000 miRNA-circRNA, 16 000 miRNApseudogene and 285 000 proteinÉ??RNA regulatory relationships. Moreover, starBase v2.0 has been updated to provide the most comprehensive CLIP-Seq experimentally supported miRNA-mRNA and miRNAlncRNA interaction networks to date. We identified ã®´ 10 000 ceRNA pairs from CLIP-supported miRNA target sites. By combining 13 functional genomic annotations, we developed miRFunction and ceRNAFunction web servers to predict the function of miRNAs and other ncRNAs from the miRNAmediated regulatory networks. Finally, we developed interactive web implementations to provide visualization, analysis and downloading of the aforementioned large-scale data sets. This study will greatly expand our understanding of ncRNA functions and their coordinated regulatory networks.","starBase v2.0: decoding miRNA-ceRNA, miRNA-ncRNA and proteinÉ??RNA interaction networks from large-scale CLIP-Seq data"
"['Krishnakumar Sundaresan', 'Gavin K. Ho', 'Siavash Pourkamali', 'Farrokh Ayazi']",The paper presents a temperature compensated 100MHz reference oscillator based on a capacitive silicon Bulk Acoustic Wave (BAW) resonator interfaced with a CMOS amplifier. The resonator is optimized for high quality factor (92000) and low impedance. The CMOS IC comprises of a trans-impedance amplifier to sustain oscillations and an oven control mechanism for temperature control. A phase noise floor of ?136dBc/Hz was measured for the oscillator and the temperature drift of frequency was measured to be 56ppm over 100?¯C.,A Low Phase Noise 100MHz Silicon BAW Reference Oscillator
"['Jocelyn Y. K. Aulin', 'Djordje Jeremic']","For OFDM systems in fast fading, it is difficult to obtain a closed form expression for the OFDM symbol error probability. Thus, tight analytical bounds on actual performance are extremely useful for performance prediction and verification. Additionally, the structure of the bound provides insight into system behavior. A new expurgated 2-dimensional union bound is proposed and applied to an OFDM system in fast fading. This bound becomes extremely tight as the rate of fading L increases. Performance comparison of the new bound to a lower bound on a known expurgated union bound demonstrates a gain of up to 1.5 dB at P(e) = 10 2 for channel implicit diversity order of L = 4. A new simulated upper bound that uses a reduced state vector in a modified trellis search algorithm and a simulated lower bound that assumes limited knowledge of the ICI are also presented. Both bounds are derived from a ""time-varying"" finite state machine model of the received signal. Performance results for these bounds are extremely tight for small to large values of N, where N is the number of OFDM signal tones. Also, the reduction in computational complexity achieved for N = 512 is from 2 512  to 2 6  for both bounds.",Performance Bounds for MLSD Reception of OFDM Signals in Fast Fading
['Abdulkader A. Murad'],"Spatial interaction models are used today in facilities planning research for predicting and for allocating flows of demand between origin and destination areas based on the attractiveness of each facility and based on the distance between facilities and demand areas. These models have been adapted to a wide range of application areas including predicting flows of people to shops, offices, schools, and hospitals. The aim of this paper is to use GIS for producing spatial interaction models for two retail centres Jeddah City, Saudi Arabia. These models are created using ArcGIS software and using the interaction function which is available within the network analysis module. To produce these models, detailed geo-database was created that covers location of retail centres, the capacity of each centre, the size of centres demand at the study area, and road network coverage for Jeddah City. The created models can be used by city planners for identifying areas of the city that are poorly served by existing retail centres. In addition, these models can be used to define the impacts of expanding retail supply and or retail demand at the study area.",Creating GIS-based spatial interaction models for retail centres in Jeddah City
"['Petr Felkel', 'Rainer Wegenkittl', 'Katja B?¨hler']","This paper describes a new method for generating surfaces of branching tubular structures with given center-lines and radii. As the centerlines are not straight lines, the cross-sections are not parallel and well-known algorithms for surface tiling from parallel cross-sections cannot be used. Nonparallel cross-sections can be tiled by means of the maximal-disc interpolation method; special methods for branching-structures modeling by means of convolution surfaces produce excellent results, but these methods are more complex than our approach. The proposed method tiles nonparallel circular cross-sections and constructs a topologically-correct surface mesh. The method is not artifact-free, but it is fast and simple. The surface mesh serves as a data representation of a vessel tree suitable for real-time virtual reality operation planning and operation support within a medical application. Proposed method extracts a ""classical"" polygonal representation, which can be used in common surface-oriented graphic accelerators",Surface models of tube trees
"['Antoine Bossard', 'Keiichi Kaneko', 'Shietung Peng']","The metacube interconnection network introduced a few years ago has some very interesting properties: it has a short diameter similar to the hypercube, and its degree is much lower than that of a hypercube of the same size. In this paper, we describe an efficient algorithm for finding disjoint paths between one source node and at most m+k target nodes in a metacube MC(k, m) excluding MC(*,1), MC(2,2), MC(3,2) and MC(3,3). We show that we can find m+k disjoint paths between the source node and the m+k targets of length at most metacube diameter plus (k+4) with time complexity of order of metacube degree times its diameter.",Node-to-Set Disjoint-path Routing in Metacube
"['Charles B. Walter', 'Stephan P. Swinnen', 'Natalia Dounskaia', 'H. Van Langendonk']","Current views of the control of complex, purposeful movements acknowledge that organizational processes must reconcile multiple concerns. The central priority is of course accomplishing the actorÉ??s goal. But in specifying the manner in which this occurs, the action plan must accommodate such factors as the interaction of mechanical forces associated with the motion of a multilinked system (classical mechanics) and, in many cases, intrinsic bias toward preferred movement patterns, characterized by so-called É??coordination dynamics.É?ù The most familiar example of the latter is the symmetry constraint, where spatial trajectories and/or temporal landmarks (e.g., reversal points) of concurrentlymoving body segments (limbs, digits, etc.) exhibit mutual attraction. The natural coordination tendencies that emerge through these constraints can facilitate or hinder motor control, depending on the degree of congruency with the desired movement pattern. Motor control theorists have long recognized the role of classical mechanics in theories of movement organization, but an appreciation of the importance of intrinsic interlimb bias has been gained only recently. Although detailed descriptions of temporal coordination dynamics have been provided, systematic attempts to identify additional salient dimensions of interlimb constraint have been lacking. We develop and implement here a novel method for examining this problem by exploiting two robust principles of psychomotor behavior, the symmetry constraint and the Two-Thirds Power Law. Empirical evidence is provided that the relative spatial patterns of concurrently moving limbs are naturally constrained in much the same manner as previously identified temporal constraints and, further, that apparent velocity interference is an indirect, secondary consequence of primary spatial assimilation. The theoretical implications of spatial interference are elaborated with respect to movement organization and motor learning. The need to carefully consider the appropriate dimensions with which to characterize coordination dynamics is also discussed. ?? 2001 Cognitive Science",Systematic error in the organization of physical action
"['E. Yeguas', 'Robert Joan-Arinyo', 'M. V. Luz??n']","The availability of a model to measure the performance of evolutionary algorithms is very important, especially when these algorithms are applied to solve problems with high computational requirements. That model would compute an index of the quality of the solution reached by the algorithm as a function of run-time. Conversely, if we fix an index of quality for the solution, the model would give the number of iterations to be expected. In this work, we develop a statistical model to describe the performance of PBIL and CHC evolutionary algorithms applied to solve the root identification problem. This problem is basic in constraint-based, geometric parametric modeling, as an instance of general constraint-satisfaction problems. The performance model is empirically validated over a benchmark with very large search spaces.",Modeling the performance of evolutionary algorithms on the root identification problem: A case study with pbil and chc algorithms
"['Si Liu', 'Yongxin Zhao', 'Huibiao Zhu', 'Qin Li']","In this paper we present a probabilistic calculus for formally modeling and reasoning about Mobile Ad Hoc Networks (MANETs) with unreliable connections and mobility of nodes. In our calculus, a MANET node can locally broadcast messages to a group of nodes within its physical transmission range. The group probability is also introduced since two distinct nodes within different groups should receive messages from the same sender with different possibilities. Our calculus naturally captures essential features of MANETs, i.e., local broadcast, mobility and probability. Moreover, we give a formal operational semantics of the calculus in terms of the labeled transition system and define the notion of open bisimulation. Finally, we illustrate our calculus with a toy example.",Towards a Probabilistic Calculus for Mobile Ad Hoc Networks
"['Daniele D. Giusto', 'Maurizio Murroni', 'Giulio Soro']","Slow motion replay is a special effect used in the video entertainment field. It consists in a presentation of a video scene at a rate display lower than the original. Already consolidated as a commercial feature of analog video players, today slow motion is likely to be extended to the digital environment. Purpose of this paper is to present a technique combining fractals (I. F. S.) and wavelets to obtain a subjectively pleasant zoom and slow motion of digital video sequences. Active scene detection and post processing techniques are used to reduce computational cost and improve visual quality respectively. This study shows that the proposed technique produces better results than the state of the art techniques based either on data replication or classical interpolation.",Slow motion replay of video sequences using fractal zooming
"['Yonghong Zeng', 'Abdul Rahim Leyman', 'Tung-Sang Ng']","A semiblind method is proposed for simultaneously estimating the carrier frequency offsets (CFOs) and channels of an uplink multiuser multiple-input multiple-output orthogonal frequency-division multiplexing (MIMO-OFDM) system. By incorporating the CFOs into the transmitted symbols and channels, the MIMO-OFDM with CFO is remodeled into an MIMO-OFDM without CFO. The known blind method for channel estimation (Zeng and Ng in 2004) (Y. H. Zeng and T. S. Ng, ldquoA semi-blind channel estimation method for multi-user multi-antenna OFDM systems,rdquo IEEE Trans. Signal Process., vol. 52, no. 5, pp. 1419-1429, May 2004.) is then directly used for the remodeled system to obtain the shaped channels with an ambiguity matrix. A pilot OFDM block for each user is then exploited to resolve the CFOs and the ambiguity matrix. Two dedicated pilot designs, periodical and consecutive pilots, are discussed. Based on each pilot design and the estimated shaped channels, two methods are proposed to estimate the CFOs. As a result, based on the second-order statistics (SOS) of the received signal and one pilot OFDM block, the CFOs and channels are found simultaneously. Finally, a fast equalization method is given to recover the signals corrupted by the CFOs.",Joint Semiblind Frequency Offset and Channel Estimation for Multiuser MIMOÉ??OFDM Uplink
['Christian Bettstetter'],"Given is a wireless multihop network whose nodes are randomly distributed according to a homogeneous Poisson point process of density /spl rho/ (in nodes per unit area). The network employs Basagni's distributed mobility-adaptive clustering (DMAC) algorithm to achieve a self-organizing network structure. We show that the cluster density, i.e., the expected number of cluster- heads per unit area, is /spl rho//sub c/= /spl rho//spl divide/(1+/spl mu//spl divide/2), where /spl mu/ denotes the expected number of neighbors of a node. Consequently, a clusterhead is expected to incorporate half of its neighboring nodes into its cluster. This result also holds in a scenario with mobile nodes and serves as a bound for inhomogeneous spatial node distributions.",The cluster density of a distributed clustering algorithm in ad hoc networks
"['Sun-Yuan Hsieh', 'Gen-Huey Chen']","The arrangement graph A/sub n,k/, which is a generalization of the star graph (n-t=1), presents more flexibility than the star graph in adjusting the major design parameters: number of nodes, degree, and diameter. Previously the arrangement graph has proven hamiltonian. In this paper we further show that the arrangement graph remains hamiltonian even if it is faulty. Let |F/sub e/| and |F/sub v/| denote the numbers of edge faults and vertex faults, respectively. We show that A/sub n,k/ is hamiltonian when (1) (k=2 and n-k/spl ges/4, or k/spl ges/3 and n-k/spl ges/4+[k/2]), and |F/sub e/|/spl les/k(n-k-2)-1, or (2) k/spl ges/2, n-k/spl ges/2+[k/2], and |F/sub e/|/spl les/k(n-k-3)-1, or (3) k/spl ges/2, n-k/spl ges/3, and |F/sub 3/|/spl les/k.",Fault-tolerant ring embedding in faulty arrangement graphs
['Brian Curtin'],"In this paper, we consider a bipartite distance-regular graph ?= (X, E) with diameter d? 3. We investigate the local structure of? , focusing on those vertices with distance at most 2 from a given vertex x. To do this, we consider a subalgebra R=R(x) ofMat0307a0x.gif X(C), where 0307a1x.gifX denotes the set of vertices in X at distance 2 from x. R is generated by matrices A, 0307a2x.gif J, and 0307a3x.gif D defined as follows. For all y, z? 0307a4x.gif X, the (y,z )-entry of A is 1 if y, z are at distance 2, and 0 otherwise. The (y, z)-entry of 0307a5x.gif J equals 1, and the (y,z )-entry of 0307a6x.gif D equals the number of vertices of X adjacent to each ofx , y, and z. We show that R is commutative and semisimple, with dimension at least 2. We assume thatdimR is one of 2, 3, or 4, and explore the combinatorial implications of this. We are motivated by the fact that if ? has a Q-polynomial structure, thendimR? 4.",The Local Structure of a Bipartite Distance-regular Graph
"['Justin C. Sanchez', 'Jose C. Principe']","Recent advancements in the neuroscience and engineering of Brain-Machine Interfaces are providing a blueprint for how new co-adaptive designs based on reinforcement learning change the nature of a user's ability to accomplish tasks that were not possible using static methodologies. By designing adaptive controls and artificial intelligence into the neural interface, computers can become active assistants in goal-directed behavior and further enhance human performance. This paper presents a set of minimal prerequisites that enable a cooperative symbiosis and dialogue between biological and artificial systems.",Prerequesites for symbiotic brain-machine interfaces
"['Qiao Lin', 'Joel W. Burdick', 'Elon Rimon']","This paper computes and analyzes the natural compliance of fixturing and grasping arrangements. Traditionally, linear-spring contact models have been used to determine the natural compliance of multiple contact arrangements. However, these models are not supported by experiments or elasticity theory. We derive a closed-form formula for the stiffness matrix of multiple contact arrangements that admits a variety of nonlinear contact models, including the well-justified Hertz model. The stiffness matrix formula depends on the geometrical and material properties of the contacting bodies and on the initial loading at the contacts. We use the formula to analyze the relative influence of first- and second-order geometrical effects on the stability of multiple contact arrangements. Second-order effects, i.e., curvature effects, are often practically beneficial and sometimes lead to significant grasp stabilization. However, in some contact arrangements, curvature has a dominant destabilizing influence. Such contact arrangements are deemed stable under an all-rigid body model but, in fact, are unstable when the natural compliance of the contacting bodies is taken into account. We also consider the combined influence of curvature and contact preloading on stability. Contrary to conventional wisdom, under certain curvature conditions, higher preloading can increase rather than decrease grasp stability. Finally, we use the stiffness matrix formula to investigate the impact of different choices of contact model on the assessment of the stability of multiple contact arrangements. While the linear-spring model and the more realistic Hertz model usually lead to the same stability conclusions, in some cases, the two models lead to different stability results.",Computation and analysis of natural compliance in fixturing and grasping arrangements
"['Christian Bauckhage', 'Christian Thurau']","We consider the problem of finding points of interest along local curves of binary images. Information theoretic vector quantization is a clustering algorithm that shifts cluster centers towards the modes of principal curves of a data set. Its runtime characteristics, however, do not allow for efficient processing of many data points. In this paper, we show how to solve this problem when dealing with data on a 2D lattice. Borrowing concepts from signal processing, we adapt information theoretic clustering to the quantization of binary images and gain significant speedup.",Adapting Information Theoretic Clustering to Binary Images
"['Hiro Takahashi', 'Takeshi Kobayashi', 'Hiroyuki Honda']","Motivation: For establishing prognostic predictors of various diseases using DNA microarray analysis technology, it is desired to find selectively significant genes for constructing the prognostic model and it is also necessary to eliminate non-specific genes or genes with error before constructing the model.#R##N##R##N#Results: We applied projective adaptive resonance theory (PART) to gene screening for DNA microarray data. Genes selected by PART were subjected to our FNN-SWEEP modeling method for the construction of a cancer class prediction model. The model performance was evaluated through comparison with a conventional screening signal-to-noise (S2N) method or nearest shrunken centroids (NSC) method. The FNN-SWEEP predictor with PART screening could discriminate classes of acute leukemia in blinded data with 97.1% accuracy and classes of lung cancer with 90.0% accuracy, while the predictor with S2N was only 85.3 and 70.0% or the predictor with NSC was 88.2 and 90.0%, respectively. The results have proven that PART was superior for gene screening.#R##N##R##N#Availability: The software is available upon request from the authors.#R##N##R##N#Contact: honda@nubio.nagoya-u.ac.jp",Construction of robust prognostic predictors by using projective adaptive resonance theory as a gene filtering method
"['Marc A. Peters', 'Pablo A. Iglesias']","The connection between minimum entropy control and risk-sensitive control for linear time-varying systems is investigated. For time-invariant systems, the entropy functional and the linear exponential quadratic Gaussian cost are the same. In this paper, it is shown that this is not true for general time varying systems. It does hold, however, when the system admits a state-space representation.",The relationship between minimum entropy control and risk-sensitive control for time-varying systems
"['S. Das', 'Sunil P. Khatri']","Two-operand binary addition is the most widely used arithmetic operation in modern datapath designs. To improve the efficiency of this operation, it is desirable to use an adder with good performance and area tradeoff characteristics. This paper presents an efficient carry-lookahead adder architecture based on the parallel-prefix computation graph. In our proposed method, we define the notion of triple-carry-operator, which computes the generate and propagate signals for a merged block which combines three adjacent blocks. We use this in conjunction with the classic approach of the carry-operator to compute the generate and propagate signals for a merged block combining two adjacent blocks. The timing-driven nature of the proposed design reduces the depth of the adder. In addition, we use a ripple-carry type of structure in the nontiming critical portion of the parallel-prefix computation network. These techniques help produce a good timing-area tradeoff characteristic. The experimental results indicate that our proposed adder is significantly faster than the popular Brent-Kung adder with some area overhead. On the adder hand, the proposed adder also shows marginally faster performance than the fast Kogge-Stone adder with significant area savings.",A Novel Hybrid Parallel-Prefix Adder Architecture With Efficient Timing-Area Characteristic
"['Faa-Jeng Lin', 'Jonq-Chin Hwang', 'Kuang-Hsiung Tan', 'Zong-Han Lu', 'Yung-Ruei Chang']","An intelligent control stand-alone doubly-fed induction generator (DFIG) system using proportional-integral-derivative neural network (PIDNN) is proposed in this study. This system can be applied as a stand-alone power supply system or as the emergency power system when the electricity grid fails for all sub-synchronous, synchronous and super-synchronous conditions. The rotor side converter is controlled using the field-oriented control to produce three-phase stator voltages with constant magnitude and frequency at different rotor speeds. Moreover, the stator side converter, which is also controlled using field-oriented control, is primarily implemented to maintain the magnitude of the DC-link voltage. Furthermore, the intelligent PIDNN controller is proposed for both the rotor and stator side converters to improve the transient and steady-state responses of the DFIG system for different operating conditions. Both the network structure and on-line learning algorithm are introduced in detail. Finally, the feasibility of the proposed control scheme is verified through experimentation.",Control of Doubly-Fed Induction Generator System Using PIDNNs
"['Guomin Li', 'Dake He', 'Wei Guo']","A tripartite authenticated key agreement protocol is designed for three entities to communicate securely over an open network particularly with a shared key. Password-authenticated key exchange (PAKE) allows the participants to share a session key using a human memorable password only. In this paper, A password-based authenticated tripartite key exchange protocol(3-PAKE) is presented in the standard model. The security of the protocol is reduced to theDecisional Bilinear Diffie-Hellman (DBDH) problem, and the protocol provides not only the properties of forward secrecy, but also resistance against known key attacks. The proposed protocol is more efficient than the similar protocols in terms of both communication and computation.",Password-based tripartite key exchange protocol with forward secrecy
['Jeremy Straub'],The utility of high-altitude balloons as a learning technology to facilitate education in several disciplines is considered in this paper. The role that a high-altitude balloon can play as a learning technology is discussed and its utility in this role is considered. The need for a formal design framework for high-altitude ballooning is also discussed. A framework for the assessment of high-altitude ballooning in supporting an undergraduate-level course is evaluated and an assessment using this framework is conducted. The paper concludes with a discussion of techniques that can be used to broaden access to high-altitude ballooning in education.,Evaluation of high-altitude balloons as a learning technology
"['Daniel Weinland', 'R??mi Ronfard', 'Edmond Boyer']","We present a new method for segmenting actions into primitives and classifying them into a hierarchy of action classes. Our scheme learns action classes in an unsupervised manner using examples recorded by multiple cameras. Segmentation and clustering of action classes is based on a recently proposed motion descriptor which can be extracted efficiently from reconstructed volume sequences. Because our representation is independent of viewpoint, it results in segmentation and classification methods which are surprisingly efficient and robust. Our new method can be used as the first step in a semi-supervised action recognition system that will automatically break down training examples of people performing sequences of actions into primitive actions that can be discriminatingly classified and assembled into high-level recognizers.",Automatic Discovery of Action Taxonomies from Multiple Views
"['Xin Zhao', 'Qi Liu', 'Qingqing Cai', 'Yanyun Li', 'Congjian Xu', 'Yixue Li', 'Zuofeng Li', 'Xiaoyan Zhang']","Viral integration plays an important role in the development of malignant diseases. Viruses differ in preferred integration site and flanking sequence. Viral integration sites (VIS) have been found next to oncogenes and common fragile sites. Understanding the typical DNA features near VIS is useful for the identification of potential oncogenes, prediction of malignant disease development and assessing the probability of malignant transformation in gene therapy. Therefore, we have built a database of human disease-related VIS (Dr.VIS, http://www.scbit.org/dbmi/drvis) to collect and maintain human disease-related VIS data, including characteristics of the malignant disease, chromosome region, genomic position and viralÉ??host junction sequence. The current build of Dr.VIS covers about 600 natural VIS of 5 oncogenic viruses representing 11 diseases. Among them, about 200 VIS have viralÉ??host junction sequence.",Dr.VIS: a database of human disease-related viral integration sites
"['Jian Liu', 'Yanqing Wang']","The present models about Internet marketing, to a certain extent, have some limits, which cannot systematically reveals the process and characteristics of Internet marketing. Based on the theories of traditional marketing and Internet marketing, the paper builds the model of Internet marketing relationship that describes consumer's purchase decision process and firm's Internet marketing process, and correlation among consumer, firm, and bank and logistics firm. Through systematically analyzing the two processes, the model reveals intrinsic characteristics and essences of Internet marketing that are all different from traditional marketing. The model provides a researchful platform for the researchers, and a fundamental basis for further researching Internet marketing.",Research on Internet marketing relationship model
"['Michael S. Quayle', 'Jon A. Solworth']","A data structure derived from corner stitching which allows efficient representation of VLSI layouts is presented. While each entry in the expanded rectangle database is larger than the corresponding corner-stitched entry, generally fewer entries are required to represent the same VLSI layout. The data structure has two important features: first, the VLSI design is represented as a slicing structure in which each slice contains a portion of the solid material; and second, corner stitches are used to provide two-dimensional nearness information. Initial measurements indicate that expanded rectangles is a viable data structure for use in a complete VLSI layout system. >",Expanded rectangles: a new VLSI data structure
"['Agapito Ledezma', 'Ricardo Aler', 'Araceli Sanchis', 'Daniel Borrajo']","In competitive domains, the knowledge about the opponent can give players a clear advantage. This idea lead us in the past to propose an approach to acquire models of opponents, based only on the observation of their input-output behavior. If opponent outputs could be accessed directly, a model can be constructed by feeding a machine learning method with traces of the opponent. However, that is not the case in the Robocup domain. To overcome this problem, in this paper we present a three phases approach to model low-level behavior of individual opponent agents. First, we build a classifier to label opponent actions based on observation. Second, our agent observes an opponent and labels its actions using the previous classifier. From these observations, a model is constructed to predict the opponent actions. Finally, the agent uses the model to anticipate opponent reactions. In this paper, we have presented a proof-of-principle of our approach, termed OMBO (Opponent Modeling Based on Observation), so that a striker agent can anticipate a goalie. Results show that scores are significantly higher using the acquired opponent's model of actions.",Predicting opponent actions by observation
"['Joonho Hyun', 'Doo-Jin Choi', 'Sukil Kim']","This paper introduces Korean web accessibility activities, such as relational laws, ordinances, policies, guidelines. It also presents analytical result of the investigation on web-contents accessibilities of the 39 Korean government agencies. The result shows that only one agency provides web contents satisfying all the minimum requirements, while 97% of the agencies does not satisfy all the minimum requirements. Unfortunately, 6 agencies do not satisfy any.",Web accessibility compliance of government web sites in Korea
"['Philippe Hanhart', 'Emilie Bosc', 'Patrick Le Callet', 'Touradj Ebrahimi']","Free-viewpoint television is expected to create a more natural and interactive viewing experience by providing the ability to interactively change the viewpoint to enjoy a 3D scene. To render new virtual viewpoints, free-viewpoint systems rely on view synthesis. However, it is known that most objective metrics fail at predicting perceived quality of synthesized views. Therefore, it is legitimate to question the reliability of commonly used objective metrics to assess the quality of free-viewpoint video (FVV) sequences. In this paper, we analyze the performance of several commonly used objective quality metrics on FVV sequences, which were synthesized from decompressed depth data, using subjective scores as ground truth. Statistical analyses showed that commonly used metrics were not reliable predictors of perceived image quality when different contents and distortions were considered. However, the correlation improved when considering individual conditions, which indicates that the artifacts produced by some view synthesis algorithms might not be correctly handled by current metrics.",Free-Viewpoint Video Sequences: a New Challenge for Objective Quality Metrics
"['Fabrice M??rillion', 'Gilles Muller']","Writing code that talks to hardware is a crucial part of any embedded project. Both productivity and quality are needed, but some flaws in the traditional development process make these requirements difficult to meet.  We have recently introduced a new approach of dealing with hardware, based on the Devil language. Devil allows to write a high-level, formal definition of the programming interface of a peripheral circuit. A compiler automatically checks the consistency of a Devil specification, from which it generates the low-level, hardware-operating code.  In our original framework, the generated code is dependent of the host architecture (CPU, buses, and bridges). Consequently, any variation in the hardware environment requires a specific tuning of the compiler. Considering the variability of embedded architectures, this is a serious drawback. In addition, this prevents from mixing different buses in the same circuit interface.  In this paper, we remove those limitations by improving our framework in two ways. (i) We propose a better isolation between the Devil compiler and the host architecture. (ii) We introduce Trident, a language extension aimed at mapping one or several buses to each peripheral circuit.",Dealing with Hardware in Embedded Software: A General Framework Based on the Devil Language
"['Leonardo Mangeruca', 'Alberto Ferrari', 'Alberto L. Sangiovanni-Vincentelli']","In this paper we present a novel approach to the constrained scheduling problem, while addressing a more general class of constraints that arise from the timing requirements on real-time embedded controllers and from the implementation of mixed data-flow/event-driven real-time systems. We provide general necessary and sufficient conditions for scheduling under precedence constraints and derive sufficient conditions for two well-known scheduling policies. We define mathematical problems that provide optimum priority and deadline assignments, while ensuring both precedence constraints and system??s schedulability.We show how these problems can be relaxed to corresponding ILP formulations leveraging on available solvers.",Uniprocessor Scheduling Under Precedence Constraints
"['Tibor Cinkler', 'P??ter Hegyi', 'G??za Geleji', 'J?≠nos Szigeti', '?Åkos Lad?≠nyi']","In this paper we propose a new traffic engineering scheme to be used jointly with protection in multi-layer, grooming-capable, optical-beared networks. To make the working and protection paths of demands better adapt to changing traffic and network conditions we propose the adaptive multi-layer traffic engineering (AMLTE) scheme that ""tailors"" i.e., fragments and de-fragments wavelength paths in a fully automatic distributed way.",Adaptive Multi-Layer Traffic Engineering with Shared Risk Group Protection
"['Elisabeth Andr??', 'Matthias Rehm', 'Wolfgang Minker', 'Dirk B?¨hler']","While most dialogue systems restrict themselves to the adjustment of the propositional contents, our work concentrates on the generation of stylistic va- riations in order to improve the user's perception of the interaction. To accomplish this goal, our approach integrates a social theory of politeness with a cognitive theory of emotions. We propose a hierarchical selection process for politeness behaviors in order to enable the refinement of decisions in case additional context information becomes available.",Endowing Spoken Language Dialogue Systems with Emotional Intelligence
"['Gun Akkor', 'John S. Baras', 'Michael H. Hadjitheodosiou']","In this paper, we present a feedback implosion suppression (FIS) algorithm that reduces the volume of feedback information transmitted through the network without relying on any collaboration between users, or on any infrastructure other than the satellite network. Next generation satellite systems that utilize the Ka frequency band are likely to rely on various fade mitigation techniques, in order to guarantee a service quality that is comparable to other broadband technologies. User feedback would be a valuable input for a number of such components, however, collecting periodic feedback from a large number of users would result in the well-known feedback implosion problem. Feedback implosion is identified as a major problem when a large number of users try to transmit their feedback messages through the network, holding up a significant portion of the uplink resources and clogging the shared uplink medium. In this paper, we look at a system where uplink channel access is organized in time-slots. The goal of the FIS algorithm is to reduce the number of uplink time-slots hold up for the purpose of feedback transmission. Our analysis show that the FIS algorithm effectively suppresses the feedback messages of 95% of all active users, but still achieves acceptable performance results when the ratio of available time-slots to number of users is equal to or higher than 5%",A multiple subset sum formulation for feedback implosion suppression over satellite networks
['Chang Wu Yu'],"Random geometric graphs (RGG) contain vertices whose points are uniformly distributed in a given plane and an edge between two distinct nodes exists when their distance is less than a given positive value. RGGs are appropriate for modeling ad hoc networks consisting of n mobile devices that are independently and uniformly distributed randomly in an area. To the best of our knowledge, this work presents the first paradigm to compute the subgraph probability of RGGs in a systematical way. In contrast to previous asymptotic bounds or approximation, which always assume that the number of nodes in the network tends to infinity, the closed-form formulas we derived herein are fairly accurate and of practical value. Moreover, computing exact subgraph probability in RGGs is shown to be a useful tool for counting the number of induced subgraphs, which explores fairly accurate quantitative property on topology of ad hoc networks.",Computing subgraph probability of random geometric graphs with applications in quantitative analysis of ad hoc networks
"['Chih-Ming Chen', 'Chia-Wen Lin', 'Yung-Chang Chen']","In this paper, we propose a content-aware retry limit adaptation scheme for video streaming over IEEE 802.11 wireless LANs (WLANs). Video packets of different importance are unequally protected with different retry limits at the MAC layer. The loss impact of each packet is estimated to guide the selection of its retry limit. More retry numbers are allocated to packets of higher loss impact to achieve unequal error protection. Experimental results show that the proposed adaptation scheme can effectively mitigate the error propagation due to packet loss and assure the on-time arrival of packets for presentation, thereby improving video quality significantly.",Unequal Error Protection for Video Streaming Over Wireless LANs using Content-Aware Packet Retry Limit
"['J?≠n Maè?uch', 'Murray Patterson', 'Roland Wittler', 'Cedric Chauve', 'Eric Tannier']","Background#R##N#Recovering the structure of ancestral genomes can be formalized in terms of properties of binary matrices such as the Consecutive-Ones Property (C1P). The Linearization Problem asks to extract, from a given binary matrix, a maximum weight subset of rows that satisfies such a property. This problem is in general intractable, and in particular if the ancestral genome is expected to contain only linear chromosomes or a unique circular chromosome. In the present work, we consider a relaxation of this problem, which allows ancestral genomes that can contain several chromosomes, each either linear or circular.",Linearization of ancestral multichromosomal genomes.
"['Andrew Meads', 'Ian Warren']","Today's computationally able mobile devices are capable of acting as service providers as opposed to their traditional role as consumers. To address the challenges associated with the development of these mobile services, we have developed Odin, a middleware which masks complexity, allowing rapid development of mobile services. Odin, however, does not allow cross-platform development, which is an important concern with today's wide variety of mobile devices. To solve this problem, we have designed Odin Tools - a model-driven toolkit for cross-platform development of mobile services. Leveraging appropriate metamodels, a prototype has been implemented in Eclipse and Marama that allows developers to model mobile services in a platform-independent manner. We are currently working on transformations between levels of the model hierarchy which will allow full Odin-based service implementations to be generated automatically.",OdinTools--Model-Driven Development of Intelligent Mobile Services
"['Diana Hoogeveen', 'Hans Oppelland']","In recent years many studies have been published on the assessment of payoffs from investments in IT. The research has produced mainly mixed results. Different explanations can be given for these mixed results. One possible explanation might be the dominance of the rational perspective in previous research. The consequence of this overemphasis on rationality is that the social political nature of the IT investment process has largely been neglected in previous research on IT business value. This omission produces an incomplete picture and might contribute to the conflicting empirical results. In this research we studied whether and how the socio political perspective can be used to explain the mixed results. Case study research was used to test whether the attitude towards the value of IT, destructive conflict, and a low level of trust influence the relationship between IT investments and business performance.",A socio political model of the relationship between IT investments and business performance
"['Kasilingam Periyasamy', 'X. Liu']","Software metrics proposed and used for procedural paradigm have been found inadequate for object oriented software products, mainly because of the distinguishing features of the object oriented paradigm such as inheritance and polymorphism. Several object oriented software metrics have been described in the literature. These metrics are goal driven in the sense that they are targeted towards specific software qualities. We propose a new set of metrics for object oriented programs; this set is targeted towards estimating the testing efforts for these programs. The definitions of these metrics are based on the concepts of object orientation and hence are independent of the object oriented programming languages. The new metrics set has been critically compared with three other metrics sets published in the literature.",A new metrics set for evaluating testing efforts for object-oriented programs
"['Marios D. Dikaiakos', 'Artemakis Artemiou', 'George Tsouloupas']","In this paper, we present the design and implementation of Ovid, a browser for grid-related information. The key goal of Ovid is to support the seamless navigation of users in the grid information space. Key aspects of Ovid are: (i) a set of navigational primitives, which are designed to cope with problems such as network disorientation and information overloading; (ii) a small set of Ovid views, which present the end-user with high-level, visual abstractions of grid information; these abstractions correspond to simple models that capture essential aspects of a grid infrastructure; (iii) support for embedding and implementing hyperlinks that connect related entities represented within different information views; (iv) a plug-in mechanism, which enables the seamless integration with Ovid of third-party software that retrieves and displays data from various grid information sources; and (v) a modular software design, which allows the easy integration of different visualization algorithms that support the graphical representation of large amounts of grid-related information in the context of Ovid's views.",Towards a universal client for grid monitoring systems: design and implementation of the Ovid browser
"['Don Sylwester', 'Sharad C. Seth']","Column segmentation logically precedes OCR in the document analysis process. The trainable algorithm XYCUT relies on horizontal and vertical binary profiles to produce an XY-tree representing the column structure of a page of a technical document in a single pass through the bit image. Training against ground truth adjusts a single, resolution independent, parameter using only local information and guided by an edit distance function. The algorithm correctly segments the page image for a (fairly) wide range of parameter values, although small, local and repairable errors may be made, an effect measured by a repair cost function.","A trainable, single-pass algorithm for column segmentation"
"['Cheng-Yeh Wang', 'Chih-Bin Kuo', 'Jing-Yang Jou']","Quickly and accurately predicting the performance based on the requirements for IP-based system implementations optimizes the design and reduces the design time and overall cost. This study describes a novel hybrid method for the word-length optimization of pipelined FFT processors that is the arithmetic kernel of OFDM-based systems. This methodology utilizes the rapid computing of statistical analysis and the accurate evaluation of simulation-based analysis to investigate a speedy optimization flow. A statistical error model for varying word-lengths of PE stages of an FFT processor was developed to support this optimization flow. Experimental results designate that the word-length optimization employing the speedy flow reduces the percentage of the total area of the FFT processor that increases with an increasing FFT length. Finally, the proposed hybrid method requires a shorter prediction time than the absolute simulation-based method does and achieves more accurate outcomes than a statistical calculation does.",Hybrid Wordlength Optimization Methods of Pipelined FFT Processors
"['Takayuki Itoh', 'Kazunori Miyata', 'Kenji Shimada']",This article presents a method for generating organic textures by tessellating a region into a set of pseudo-Voronoi polygons using a particle model and then generating the detailed geometry each of the polygons with fractal noise.,Generating organic textures with controlled anisotropy and directionality
"['Russell A. Poldrack', 'Paul C. Fletcher', 'Richard N. A. Henson', 'Keith J. Worsley', 'Matthew Brett', 'Thomas E. Nichols']","In this editorial, we outline a set of guidelines for the reporting of methods and results in functional magnetic resonance imaging studies and provide a checklist to assist authors in preparing manuscripts that meet these guidelines.",Guidelines for reporting an fMRI study
"['Manzur Ashraf', 'Aruna Jayasuriya', 'Sylvie Perreau']","In this paper, we derive throughput of a threshold-based transmission policy, namely load-regulated CSMA, taking into account the propagation delay of the medium and the offered load at different probability of the fading channel. In case of the saturated load regulated CSMA, a trivial relationship between deterministic offered load to the channel at a particular fading channel condition and the maximum possible offered load has been shown. We further extend the load regulation concept into multi-channel domain. Both single and multi-channel load regulated CSMA improves the throughput of the system compared to the existing CSMA system which does not consider channel fading to control the packet transmissions.",On Load Regulated CSMA
"['Alexander Wold', 'Dirk Koch', 'Jim Torresen']","Constraint satisfaction modeling is both an efficient, and an elegant approach to model and solve many real world problems. In this paper, we present a constraint solver targeting module placement in static and partial run-time reconfigurable systems. We use the constraint solver to compute feasible placement positions. Our placement model incorporates communication, implementation variants and device configuration granularity. In addition, we model heterogeneous resources such as embedded memory, multipliers and logic. Furthermore, we take into account that logic resources consist of different types including logic only LUTs, arithmetic LUTs with carry chains, and LUTs with distributed memory. Our work targets state of the art field-programmable gate arrays (FPGAs) in both design-time and run-time applications. In order to evaluate our placement model and module placer implementation, we have implemented a repository containing 200 fully functional, placed and routed relocatable modules. The modules are used to implement complete systems. This validates the feasibility of both the model and the module placer. Furthermore, we present simulated results for run-time applications, and compare this to other state of the art research. In run-time applications, the results point to improved resource utilization. This is a result of using a finer tile grid and complex module shapes.",Component based design using constraint programming for module placement on FPGAs
"['Luigi Lancieri', 'Anne Lavallard', 'Patrice Manson']","The purpose of this article is to present a methodology and tools allowing the use of online multiple-choice questionnaires to enhance collaborative work. The first goal is to allow the questionnaires generation and setting with a simple and ergonomic manner, but also to let questioned people making comments and proposing new questions to other contributors. The developed system provides a visualization of a synthesis of the questionnaire results that is also accessible by the mean of external applications through standard Web services. These principles were developed and tested on a sample of users.",E-BRAINSTORMING: OPTIMIZATION OF COLLABORATIVE LEARNING THANKS TO ONLINE QUESTIONNAIRES.
"['Arredondo V. Tom?≠s', 'Seeger P. Michael', 'Lioubov Dombrovskaia', 'Avarias A. Jorge', 'Calder??n B. Felipe', 'Candel C. Diego', 'Mu?Òoz R. Freddy', 'Latorre R. Valeria', 'Loreine Agull??', 'Cordova H. Macarena', 'Luis Revello G??mez']","A vast amount of bioinformatics information is continuously being introduced to different databases around the world. Handling the various applications used to study this information present a major data management and analysis challenge to researchers. The present work investigates the problem of integrating heterogeneous applications and databases towards providing a more efficient data-mining environment for bioinformatics research. A framework is proposed and GeXpert, an application using the framework towards metabolic pathway determination is introduced. Some sample implementation results are also presented.",Bioinformatics integration framework for metabolic pathway data-mining
"['Iain Bate', 'Alan Burns']","For a number of years, work has been performed in collaboration with industry to establish improved techniques for achieving and proving the system timing constraints. The specific requirements encountered during the course of this work for both uniprocessor and distributed systems indicate a need for an efficient mechanism for handling the timing analysis of task sets which feature offsets. Little research has been performed on this subject. The paper describes a new technique tailored to a set of real world problems so that the results are effective and the complexity is manageable.",Schedulability analysis of fixed priority real-time systems with offsets
"['Marcin Zalewski', 'Sibylle Schupp']","Since the Standard Template Library (STL), generic libraries in C++ rely on concepts to precisely specify the requirements of generic algorithms (function templates) on their parameters (template arguments). Modifying the definition of a concept even slightly, can have a potentially large impact on the (interfaces of the) entire library. In particular the non-local effects of a change, however, make its impact difficult to determine by hand. In this paper we propose a conceptual change impact analysis (CCIA), which determines the impact of changes of the conceptual specification of a generic library. The analysis is organized in a pipe-and-filter manner, where the first stage finds any kind of impact, the second stage various specific kinds of impact. Both stages describe reachability algorithms, which operate on a conceptual dependence graph. In a case study, we apply CCIA to a new proposal for STL iterator concepts, which is under review by the C++ standardization committee. The analysis shows a number of unexpected incompatibilities and, for certain STL algorithms, a loss of genericity.",Change Impact Analysis for Generic Libraries
"['Mehmet Murat FadéÒlog?ülu', 'Oya Ekin Karasan', 'Mustafa ??. PéÒnar']","In the rapidly changing environment of Fast Moving Consumer Goods sector where new product launches are frequent, retail channels need to reallocate their shelf spaces intelligently while keeping up their total profit margins, and to simultaneously avoid product pollution. In this paper we propose an optimization model which yields the optimal product mix on the shelf in terms of profitability, and thus helps the retailers to use their shelves more effectively. The model is applied to the shampoo product class at two regional supermarket chains. The results reveal not only a computationally viable model, but also substantial potential increases in the profitability after the reorganization of the product list.",A model and case study for efficient shelf usage and assortment analysis
"['Jinpeng Wei', 'Calton Pu']","Attacks exploiting race conditions have been considered rare and ""low risk"". However, the increasing popularity of multiprocessors has changed this situation: instead of waiting for the victim process to be suspended to carry out an attack, the attacker can now run on a dedicated processor and actively seek attack opportunities. This change from fortuitous encountering to active exploiting may greatly increase the success probability of race condition attacks. This point is exemplified by studying the TOCTTOU (Time-of- Check-to-Time-of-Use) race condition attacks in this paper. We first propose a probabilistic model for predicting TOCTTOU attack success rate on both uniprocessors and multiprocessors. Then we confirm the applicability of this model by carrying out TOCTTOU attacks against two widely used utility programs: vi and gedit. The success probability of attacking vi increases from low single digit percentage on a uniprocessor to almost 100% on a multiprocessor. Similarly, the success rate of attacking gedit jumps from almost zero to 83%. These case studies suggest that our model captures the sharply increased risks, and hence the decreased dependability of our systems, represented by race condition attacks such as TOCTTOU on the next generation multiprocessors.",Multiprocessors May Reduce System Dependability under File-Based Race Condition Attacks
"['David Diamondstone', 'Bj??rn Kjos-Hanssen']","The members of Martin-Lof random closed sets under a distribution studied by Barmpalias et al. are exactly the infinite paths through Martin-Lof random Galton-Watson trees with survival parameter $\frac{2}{3}$. To be such a member, a sufficient condition is to have effective Hausdorff dimension strictly greater than $\gamma=\log_2 \frac{3}{2}$, and a necessary condition is to have effective Hausdorff dimension greater than or equal to  ***  .",Members of Random Closed Sets
"['Lemin Xiao', 'Kaizhi Tang', 'Xiong Liu', 'Hui Yang', 'Zheng Chen', 'Roger Xu']","High-quality experimental data are important when developing predictive models for studying nanomaterial environmental impact (NEI). Given that raw data from experimental laboratories and manufacturing workplaces are usually proprietary and small-scaled, extracting information from publications is an attractive alternative for collecting data. We developed an information extraction system that can extract useful information from full-text nanotoxicity related publications. This information extraction system consists of five components: raw data transformation into machine readable format, data preprocessing, ontology-based named entity recognition, rule-based numerical attribute extraction from both tables and unstructured text, and relation extraction among entities and attributes. The information extraction system is applied on a dataset made of 94 publications, and results in an acceptable accuracy. By storing extracted data into a table according to relations among the data, a dataset that can be used to predict nanomaterial environmental impact is obtained. Such a system is unique in current nanomaterial community, and can help nanomaterial scientists and practitioners quickly locate useful information they need without spending lots of time reading articles.",Information extraction from nanotoxicity related publications
"['Carmine Gianni', 'Giuseppe Scotti', 'Alessandro Trifiletti', 'Salvatore Pennisi']","A cascode current mirror with auxiliary body-driven feedback loop is proposed. Main performance parameters are analytically evaluated and compared to those of a conventional high-swing cascode and of a recently-proposed body-driven topology. Simulations are also provided confirming improvements in the achievable output resistance (important for short channel technologies), DC accuracy, and input dynamic range. Linearity, bandwidth, noise and voltage requirements are substantially the same of the conventional high-swing cascode solution.",CMOS body-enhanced cascode current mirror
"['Torsten Braun', 'Marc Heissenb?¨ttel', 'Tobias Roth']","The beacon-less routing protocol (BLR) is a position-based routing protocol for mobile ad-hoc networks that makes use of location information to reduce routing overhead. Unlike other position-based routing protocols, BLR does not require nodes to periodically broadcast hello messages. This avoids drawbacks such as extensive use of scarce battery-power, interferences with regular data transmission, and outdated position information in case of high mobility. This paper discusses the behavior and performance of BLR in realistic scenarios, in particular with irregular transmission ranges. BLR has been implemented using appropriate simulation models and in an out-door test-bed consisting of GNU/Linux laptops with wireless LAN network interfaces and GPS receivers.",Performance of the beacon-less routing protocol in realistic scenarios
"['Rumen Andonov', 'Hafid Bourzoufi', 'Sanjay V. Rajopadhye']","In pipelined parallel computations the inner loops are often implemented in a block fashion. In such programs, an important compiler optimization involves the need to statically determine the grain size. This paper presents extensions and experimental validation of the previous results of Andonov and Rajopadhye (1994) on optimal grain size determination.",Two-dimensional orthogonal tiling: from theory to practice
"['Fumiaki Hirose', 'Masahiko Fukuhara', 'Tomoya Hatano', 'Hiroshi Shigeno', 'Kenichi Okada']","Many versions of TCP have been proposed for transmitting data, and among them, TCP Reno is most widely used today. However, the problem, that it is difficult to use the wide bandwidth efficiently with TCP Reno, has been pointed out. HSTCP is one of the several new versions of TCP that are proposed to address this problem, but when its flows compete with TCP Reno flows at the same link, HSTCP gains most of the bandwidth and it is impossible to conduct fair transfer. In order to address this problem, we propose a two-level ECN marking to increase the frequency of congestion controls of HSTCP flows, holding its throughput. We evaluate our proposal through computer simulations, and the results show that our proposal mitigates bandwidth allocation to HSTCP, promoting fair transfer with TCP Reno.",A two-level ECN marking for fair bandwidth allocation between HSTCP and TCP Reno
"['H. Jedidi', 'B??chir el Ayeb']","MSS, a computer-based monitoring system with integrated cooperative objects is proposed. MSS uses an object-based framework to interface with the user to guide a specific system evolution. MSS espouses a blackboard architecture and runs according a cooperating objects model. To achieve monitoring tasks, MSS selects the appropriate technique(s) within a set of a high performance algorithms. From the user viewpoint, MSS has been developed as a control assistant featuring different levels of interactivity, a hierarchical design style and fully embedded algorithmic tools. Virtually, MSS is able to design a monitoring board for any dynamic system.",A distributed simulation based monitoring
"['Aswath Oruganti', 'Nagarajan Ranganathan']","The noise sensitivity of low power circuits is rapidly increasing with the increasing levels of process variability and uncertainty. In this work, we study the problem of leakage power minimization in dual-Vdd and dual-Vth designs in the presence of significant Vth variation. The impact of the uncertainty in Vth on leakage power and timing are studied through probabilistic analytical models. We develop probabilistic models for timing slack and leakage power considering threshold variations with the objective of achieving an optimal selection of Vth. An analysis of the models indicate that, in the presence of variability, the value of the second Vth must be about 30mV higher than the Vth value obtained without considering variability. We show that our proposed method for the selection of Vth yields the lowest leakage power ratio of the dual-Vdd and dual-Vth versus the single-Vdd and single-Vth designs. In addition, the proposed models can be used to determine the ideal values for the second Vdd and Vth values in the context of variability for a variety of process conditions.",Leakage power reduction in dual-Vdd and dual-Vth designs through probabilistic analysis of Vth variation
"['Shandong Wu', 'Youfu Li', 'Jianwei Zhang']","In this paper, a novel 3-D motion trajectory signature is introduced to serve as an effective description to the raw trajectory. More importantly, based on the trajectory signature, a probabilistic model-based cluster signature is further developed for modeling a motion class. The cluster signature is a mixture model-based motion description that is useful for motion class perception, recognition and to benefit a generalized robot task representation. The signature modeling process is supported by integrating the EM and IPRA algorithms. The conducted experiments verified the cluster signature's effectiveness.",Probabilistic Cluster Signature for Modeling Motion Classes
['Tom Pfeifer'],"Positioning, as one of the prime components of context, has been a driving factor in the development of ubiquitous computing applications throughout the past two decades. Based on the redundant positioning architecture, this paper discusses the issues of exchanging positioning data between applications and between different administrative domains, with a focus on implementing security and privacy concepts in a self-learning and self-adapting autonomic systems environment",Secure cross-domain positioning architecture for autonomic systems
['Li-Min Fu'],"One important issue in designing medical knowledge-based systems is the management of uncertainty. Among the schemes that have been developed for this purpose, probability and CF (certainty factor) are the most widely used. If rules are organized according to a connectionist model, then neural network learning suggests a promising solution to this problem. When most rules are correct, semantically incorrect rules can be recognized if their associated certainty factors are weakened or change signs after training with correct samples. The techniques for rule base refinement are examined under this approach. The concept has been implemented and tested in an actual medical expert system. >",Refinement of medical knowledge bases: a neural network approach
"['Mahendra Kumar', 'Richard E. Newman', 'Jos?? A. B. Fortes', 'Dennis R. Durbin', 'Flaura Koplin Winston']","This paper describes the architecture and implementation of a Java-based appliance for collaborative review of crashes involving injured children in order to determine mechanisms of injury. The multidisciplinary expertise needed for such reviews is not available at any one institution, resulting in the need for remote collaboration, while the sensitive nature of the information requires secure transmission and controlled access of data. The intended users of the appliance are researchers, engineers, medical doctors, government regulators, automobile and restraint manufacturers, insurance company representatives, and others who are interested in understanding the types and causes of injuries to children involved in motor vehicle crashes. The ultimate goal is to devise engineering solutions that prevent similar injuries from occurring in the future. The collaboration appliance (called Telecenter) enables the following activities: (1) the distributed asynchronous collection of digital content needed for each crash case review under a scheme that consistently organizes content across multiple cases; (2) the secure, Web-based remote participation of users in case-review meetings that involve viewing of case-specific content, live communication (written or verbal), multimedia access and sharing (slide presentations/ images), and use of Web resources; and (3) archival and post-review access of case reviews for follow-up activities and other functions (e.g., statistics, search, and networking). The Telecenter design supports audio conferencing, remote delivery and viewing of slide presentations, and other collaboration features also available in commercial and public-domain collaboration middleware products. However, it goes beyond existing solutions by also embedding a specific workflow and content organization suited for traffic injury reviews, supporting spatio-temporal role-based access control, distributed management of content and seamless integration of existing services. The current status and experience from using an early prototype of the Telecenter in actual case reviews are discussed, along with planned extensions to its functionality.",An IT appliance for remote collaborative review of mechanisms of injury to children in motor vehicle crashes
"['LiYing Cheng', 'Decheng Wen', 'Zhongrui Sun']","In order to make the strategy for research and development R&D purchase system better serve the personalization of material requirements in R&D process, the authors propose to develop a strategy set which will satisfy the internal as well as external constraints simultaneously. Social network analysis is used to analyze the vertical and horizontal relationships among the project, department and enterprise layers. Through a case study, the authors display the regulatory relationship of participants under given organization pattern and supply chain configuration. To disclose the restricted equilibrium mechanism of participants involved, changes under different strategies are compared, which can assist enterprises to enforce the decision making and to improve the R&D purchase system ability. The authors outline some of the managerial implications arising from the research findings at the end of this paper.",Strategic Design of the Purchase System Toward R&D Supply Chain Based on SNA
"['Maria Bina', 'George M. Giaglis']","In this paper we study the nature of factors that facilitate mobile data services use, as well as the characteristics of early adopters, to shed light into diffusion patterns and inform predictions for future growth. We advocate that the use of mobile data services can be associated with one's level of satisfaction with his/her life. Based on the findings of a questionnaire-based survey (N=388), we have found that users satisfied with their personal life use information, mobile e-mail, and stock broking services more frequently than dissatisfied ones, while users satisfied with their professional life tend to use financial, information, and mobile e-mail services more heavily. Furthermore, we identify early adopters' profiles in terms of their demographic characteristics (gender, age, education, and income) to inform the design of effective target marketing strategies.",Exploring early usage patterns of mobile data services
"['Xue Jiang', 'Wen-Jun Zeng', 'En Cheng']","Channels with a long but sparse impulse response arise in a variety of wireless communication applications, such as high definition television (HDTV) terrestrial transmission and underwater acoustic communications. By adopting the $\ell_1$-norm as the sparsity metric of the channel response, the channel estimation is formulated as a complex-valued convex optimization problem. A fast fixed point iteration algorithm is developed to solve the resultant complex-valued $\ell_1$-minimization problem. The proposed fast channel estimation algorithm is easy to implement and has a low computational complexity of $O(N\log N)$ per iteration with $N$ the signal length. Simulation results are provided to demonstrate the performance of the proposed fixed point algorithm.",A Fast Fixed Point Iteration Algorithm for Sparse Channel Estimation
"['Tieniu Tan', 'Keith D. Baker', 'Geoffrey D. Sullivan']",A novel algorithm is presented for determining the orientation of road vehicles in traffic scenes using video images. The algorithm requires no specific 3-D vehicle models and only uses local image gradient values. It may easily be implemented in real-time. Experimental results with a variety of vehicles in routine traffic scenes are included to demonstrate the effectiveness of the algorithm.,Model-independent recovery of object orientations
"['Hao Jiang', 'Stella X. Yu', 'David R. Martin']","Matching visual patterns that appear scaled, rotated, and deformed with respect to each other is a challenging problem. We propose a linear formulation that simultaneously matches feature points and estimates global geometrical transformation in a constrained linear space. The linear scheme enables search space reduction based on the lower convex hull property so that the problem size is largely decoupled from the original hard combinatorial problem. Our method therefore can be used to solve large scale problems that involve a very large number of candidate feature points. Without using prepruning in the search, this method is more robust in dealing with weak features and clutter. We apply the proposed method to action detection and image matching. Our results on a variety of images and videos demonstrate that our method is accurate, efficient, and robust.",Linear Scale and Rotation Invariant Matching
"['Scott Chiu', 'Christos A. Papachristou']","A built-in self-test (BIST) hardware insertion technique is addressed. Applying to register transfer level designs, this technique utilizes not only the circuit structure but also the module functionality in reducing test hardware overhead. Experimental results have shown up to 38% reduction in area overhead over other system level BIST techniques. >",A built-in self-testing approach for minimizing hardware overhead
"['Dae-Woon Lim', 'Hyung-Suk Noh', 'Hyun-Bae Jeon', 'Jong-Seon No', 'Dong-Joon Shin']","In the tone reservation (TR) scheme of the orthogonal frequency division multiplexing (OFDM) systems, there exists a trade-off between the peak to average power ratio (PAPR) reduction performance and the peak reduction tone (PRT) set size. In this paper, we propose a multi-stage TR scheme for PAPR reduction, which adaptively selects one of several PRT sets according to the PAPR of OFDM signal while the PRT set is fixed for the conventional TR scheme. It is shown that the PAPR reduction performance of the proposed scheme is better than that of the conventional TR scheme when the tone reservation rate (TRR) is the same.",Multi-Stage TR Scheme for PAPR Reduction in OFDM Signals
"['Haowen Chan', 'Adrian Perrig', 'Dawn Xiaodong Song']","Key establishment in sensor networks is a challenging problem because asymmetric key cryptosystems are unsuitable for use in resource constrained sensor nodes, and also because the nodes could be physically compromised by an adversary. We present three new mechanisms for key establishment using the framework of pre-distributing a random set of keys to each node. First, in the q-composite keys scheme, we trade off the unlikeliness of a large-scale network attack in order to significantly strengthen random key predistribution's strength against smaller-scale attacks. Second, in the multipath-reinforcement scheme, we show how to strengthen the security between any two nodes by leveraging the security of other links. Finally, we present the random-pairwise keys scheme, which perfectly preserves the secrecy of the rest of the network when any node is captured, and also enables node-to-node authentication and quorum-based revocation.",Random key predistribution schemes for sensor networks
"['Kuniharu Imai', 'Mitsuru Ikeda', 'Yukihiro Enchi', 'Takanaga Niimi']","Abstract  The purpose of our study is to quantitatively assess the effects of  z -axis automatic tube current modulation technique on image noise and streak artifact, by comparing with fixed tube current technique. Standard deviation of CT-values was employed as a physical index for evaluating image noise, and streak artifact was quantitatively evaluated using our devised Gumbel evaluation method.  z -Axis automatic tube current modulation technique will improve image noise and streak artifact, compared with fixed tube current technique, and will make it possible to significantly reduce radiation doses at lung levels while maintaining the same image quality as fixed tube current technique.",Quantitative assessment of image noise and streak artifact on CT image: comparison of z-axis automatic tube current modulation technique with fixed tube current technique.
['Tien-Yu Lin'],"Unlike the traditional integrated supplier-buyer coordination model, this research incorporates overlapped delivery and imperfect items into the production-distribution model. This model improves the observable fact that the system might experience shortage during the screening duration and also takes quantity discount into account. This approach has not been discussed in previous integrated supplier-buyer coordination models. The expected annual integrated total cost function is derived and properties and theorems are explored to help develop an algorithm. A solution procedure, free from the convexity associated with an algorithm is established to find the optimal solution. A numerical example is given to illustrate the proposed procedure and algorithm. A sensitivity analysis is made to investigate the effects of five important parameters (the inspect rate, the annual demand, the defective rate, the holding cost, and the receiving cost) on the optimal solution. Managerial insights are also discussed.",Coordination policy for a two-stage supply chain considering quantity discounts and overlapped delivery with imperfect quality
"['Michael J. McShane', 'Sohi Rastegar', 'Michael V. Pishko', 'Gerard L. Cote']","A Monte Carlo simulation of photon propagation through human skin and interaction with a subcutaneous fluorescent sensing layer is presented. The algorithm will facilitate design of an optical probe for an implantable fluorescent sensor, which holds potential for monitoring many parameters of biomedical interest. Results are analyzed with respect to output light intensity as a function of radial distance from source, angle of exit for escaping photons, and sensor fluorescence (SF) relative to tissue autofluorescence (AF). A sensitivity study was performed to elucidate the effects on the output due to changes in optical properties, thickness of tissue layers, thickness of the sensor layer, and both tissue and sensor quantum yields. The optical properties as well as the thickness of the stratum corneum, epidermis, (tissue layers through which photons must pass to reach the sensor) and the papillary dermis (tissue distal to sensor) are highly influential. The spatial emission profile of the SF is broad compared that of the tissue fluorescence and the ratio of sensor to tissue fluorescence increases with distance from the source. The angular distribution of escaping photons is more concentrated around the normal for SF than for tissue AF. The information gained from these simulations will he helpful in designing appropriate optics for collection of the signal of interest.",Monte Carlo modeling for implantable fluorescent analyte sensors
"['Vishwas Chavan', 'Peter Ingwersen']","Background: Currently primary scientific data, especially that dealing with biodiversity, is neither easily discoverable nor accessible. Amongst several impediments, one is a lack of professional recognition of scientific data publishing efforts. A possible solution is establishment of a É??Data Publishing FrameworkÉ?? which would encourage and recognise investments and efforts by institutions and individuals towards management, and publishing of primary scientific data potentially on a par with recognitions received for scholarly publications. Discussion: This paper reviews the state-of-the-art of primary biodiversity data publishing, and conceptualises a É??Data Publishing FrameworkÉ?? that would help incentivise efforts and investments by institutions and individuals in facilitating free and open access to biodiversity data. It further postulates the institutionalisation of a É??Data Usage Index (DUI)É??, that would attribute due recognition to multiple players in the data collection/creation, management and publishing cycle. Conclusion: We believe that institutionalisation of such a É??Data Publishing FrameworkÉ?? that offers socio-cultural, legal, technical, economic and policy environment conducive for data publishing will facilitate expedited discovery and mobilisation of an exponential increase in quantity of É??fit-for-useÉ?? primary biodiversity data, much of which is currently invisible.",Towards a data publishing framework for primary biodiversity data: challenges and potentials for the biodiversity informatics community
"['Jos?? Manuel Cazeaux', 'Daniele Rossi', 'Martin Omana', 'Cecilia Metra', 'Abhijit Chatterjee']","In this paper we present a detailed analysis on how the critical charge (Q/sub crit/) of a circuit node, usually employed to evaluate the probability of transient fault (TF) occurrence as a consequence of a particle hit, depends on transistors' sizing. We derive an analytical model allowing us to calculate a node's Q/sub crit/ given the size of the node's driving gate and fan-out gate(s), thus avoiding time costly electrical level simulations. We verified that such a model features an accuracy of the 97% with respect to electrical level simulations performed by HSPICE. Our proposed model shows that Q/sub crit/ depends much more on the strength (conductance) of the gate driving the node, than on the node total capacitance. We also evaluated the impact of increasing the conductance of the driving gate on TFs' propagation, hence on soft error susceptibility (SES). We found that such a conductance increase not only improves the TF robustness of the hardened node, but also that of the whole circuit.",On transistor level gate sizing for increased robustness to transient faults
"['Arnold P. O. S. Vermeeren', 'Virpi Roto', 'Kaisa V????n??nen']","Since the third wave in humanÉ??computer interaction HCI, research on user experience UX has gained momentum within the HCI community. The focus has shifted from systematic usability requirements and measures towards guidance on designing for experiences. This is a big change, since design has traditionally not played a large role in HCI research. Yet, the literature addressing this shift in focus is very limited. We believe that the field of UX research can learn from a field where design and experiential aspects have always been important: design research. In this article, we discuss why design is needed in UX research and how research that includes design as a part of research can support and advance UX design practice. We do this by investigating types of design-inclusive UX research and by learning from real-life cases of UX-related design research. We report the results of an interview study with 41 researchers in three academic research units where design research meets UX research. Based on our interview findings, and building on existing literature, we describe the different roles design can play in research projects. We also report how design research results can inform designing for experience methodologically or by providing new knowledge on UX. The results are presented in a structured palette that can help UX researchers reflect and focus more on design in their research projects, thereby tackling experience design challenges in their own research.",Design-inclusive UX research: design as a part of doing user experience research
"['Mike Tao Zhang', 'Sophia Niu', 'Marky Mai', 'Qi Li']","To enable the huge saving of the kit-breakdown, we developed MaxIt v1.2 to generate an optimal capacity plan at the kit component level for the mid-range build plan in multi-factory environment. We describe the MILP (mixed integer linear programming) model and system architecture of MaxIt v1.2. We also conduct detailed sensitivity analysis on parameter setting and objective prioritizing. With the implementation in the Intel Shanghai and Manila sites, we have significantly improved data integrity and enabled a -US/spl ges/ cost savings.",Multi-factory optimization enables kit reconfiguration in semiconductor manufacturing
['Steve Mann'],"This paper presents a framework for wearable computing, based on the principle that it be unobtrusive, and that it be integrated into ordinary clothing. This design philosophy, called 'eudaemonic computing' (named in honor of the group of physicists who designed the first truly unobtrusive wearable computers with vibrotactile displays) is reduced to practice through the 'underwearable computer' ('underwearable' for short). The 'underwearable' is a computer system that is meant to be worn within or under ordinary clothing. The first 'underwearables' were built in the early 1980s, and have evolved into a form that very much resembles a tank-top. There were three reasons for the tank structure: (1) weight is evenly and comfortably distributed over the body, and bulk is distributed unobtrusively; (2) it provides privacy by situating the apparatus within the corporeal boundary we consider our own (personal) space, and others also so-regard; and (3) proximity to the body affords capability to both sense biological signal quantities (such as respiration and heart signals which are both accessible to a vest-based device), as well as produce output that we can sense, unobtrusively. The vibrotactile output modality (VibraVest) was explored as a means of assisting the visually challenged (to avoid bumping into objects through an ability to 'feel' objects at a distance). The success of VibraVest suggests other possibilities for similar unobtrusive devices that can be worn over an extended period of time, in all facets of day-to-day life.",Eudaemonic computing ('underwearables')
['M. Six Silberman'],"This paper discusses what kinds of computer information systems might be of broad social value in the context of the increasingly severe ecological and social consequences of economic growth, and how they might be built and maintained. The paper has two parts. The first offers a particular understanding of the ecological and social É??limitsÉ?ù to economic growth. The second considers how this understanding can inform computer information systems design and operation and characterizes good É??limits-awareÉ?ù computing research.",Information systems for the age of consequences
"['Ricardo Bastos Cavalcante Prud?¶ncio', 'Teresa Bernarda Ludermir']","Meta-learning has been used to relate the performance of algorithms and the features of the problems being tackled. The knowledge in meta-learning is acquired from a set of meta-examples which are generated from the empirical evaluation of the algorithms on problems in the past. In this work, active learning is used to reduce the number of meta-examples needed for meta-learning. The motivation is to select only the most relevant problems for meta-example generation, and consequently to reduce the number of empirical evaluations of the candidate algorithms. Experiments were performed in two different case studies, yielding promising results.",Active Selection of Training Examples for Meta-Learning
"['G.J.P.M. Houben', 'Barna P', 'R Richard Vdovj?≠k', 'Flavius Frasincar', 'Geert-Jan Houben', 'P Peter Barna']","Web Information Systems (WIS) use the Web paradigm and technologies to retrieve information from sources connected to the Web, and present the information in a web or hypermedia presentation to the user. Hera is a design methodology that supports the design of WIS. It is a model -driven method that distinguishes integration, data gathering, and presentation generation. In this paper we address the Hera methodology and specifically explain the integration model that covers the different aspects of integration, and the adaptation model, that specifies how the generated presentations are adaptable (e.g. device capabilities, user preferences). The Hera software framework provides a set of transformations that allow a WIS to go from integration to presentation generation. These transformations are based on RDF(S), and we show how RDF(S) has proven its value in combining all relevant aspects of WIS design. In this way, RDF(S) being the foundation of the Semantic Web, Hera allows the engineering of Semantic Web Information Systems (SWIS).",Engineering Semantic Web Information Systems
"['Jason Cong', 'Lei He']","The optimal wiresizing problem for nets with multiple sources is studied under the distributed Elmore delay model. We decompose such a net into a source subtree (SST) and a set of loading subtrees (LSTs), and show the optimal wiresizing solution satisfies a number of interesting properties, including: the LST separability, the LST monotone property, the SST local monotone property and the general dominance property. Furthermore, we study the optimal wiresizing problem using a variable grid and reveal the bundled refinement property. These properties lead to efficient algorithms to compute the lower and upper bounds of the optimal solutions. Experiment results on nets from an Intel processor layout show an interconnect delay reduction of up to 35.9\% when compared to the minimum-width solution. In addition, the algorithm based on a variable grid yields a speedup of two orders of magnitude without loss of accuracy, when compared with the fixed grid based methods.",Optimal wiresizing for interconnects with multiple sources
"['Nuria Do?Òamayor', 'M. Ariel Schoenfeld', 'Thomas F. M?¨nte']","article i nfo Article history: Accepted 19 April 2012 Available online 26 April 2012 The monetary incentive delay task was used to characterize reward anticipation and delivery with concur- rently acquired evoked magnetic fields, EEG potentials and EEG/MEG oscillatory responses, obtaining a pre- cise portrayal of their spatiotemporal evolution. In the anticipation phase, differential activity was most prominent over midline electrodes and parieto-occipital sensors. Differences between non-reward- and reward-predicting cues were localized in the cuneus and later in the dorsal PCC, suggesting a modulation by potential reward information during early visual processing, followed by a coarse emotional evaluation of the cues. Oscillatory analysis revealed increased theta power after non-reward cues over fronto-central sites. In the beta range, power decreased with the magnitude of the potential reward and increased with re- action time, probably reflecting the influence of the striatal response to potential reward on the sensorimotor cortex. At reward delivery, negative prediction errors led to a larger mediofrontal negativity. The spatiotem- poral evolution of reward processing was modulated by prediction error: whereas differences were located in PCC and putamen in the prediction error comparison, in the case of expected outcomes they were located in PCC, ACC and parahippocampal gyrus. In the oscillatory realm, theta power was largest following rewards and, in the case of non-rewards, was largest when these were unexpected. Higher beta activity following re- wards was also observed in both modalities, but MEG additionally showed a significant power decrease for this condition over parieto-occipital sensors. Our results show how visual, limbic and striatal structures are involved in the different stages of reward anticipation and delivery, and how theta and beta oscillations have a prominent role in the processing of these stimuli.",Magneto- and electroencephalographic manifestations of reward anticipation and delivery
"['Songxiang Cen', 'Li Han', 'Jian Ma']","In this paper, we analyze peopleÉ??s reading and commenting behaviors in blogspace and proposed an algorithm for blog ranking. Upon two selected communities, AI and Medical, we show how comments, reading records, active browsing and multi time browsing can help to construct the weblog graph and reflect a blogÉ??s popularity. Based on these analysis, we propose cRank, a graph based algorithm, to rank blog among community members. Finally, we divide our dataset temporally and present how the proposed algorithm can make prediction on blogsÉ?? rankings. The experiment shows that cRank has a better performance upon several baseline systems.",Ranking Weblogs by Analyzing Reading and Commenting Activities
"['Shilin Wang', 'Wing Hong Lau', 'Alan Wee-Chung Liew', 'Shu Hung Leung']","Robust and accurate lip region segmentation is of vital importance for lip image analysis. However, most of the current techniques break down in the presence of mustaches and beards. With mustaches and beards, the background region becomes complex and inhomogeneous. We propose in this paper a novel multi-class, shape-guided FCM (MS-FCM) clustering algorithm to solve this problem. For this new approach, one cluster is set for the object, i.e. the lip region, and a combination of multiple clusters for the background which generally includes the skin region, lip shadow or beards. The proper number of background clusters is derived automatically which maximizes a cluster validity index. A spatial penalty term considering the spatial location information is introduced and incorporated into the objective function such that pixels having similar color but located in different regions can be differentiated. This facilitates the separation of lip and background pixels that otherwise are inseparable due to the similarity in color. Experimental results show that the proposed algorithm provides accurate lip-background partition even for the images with complex background features like mustaches and beards.",Robust lip region segmentation for lip images with complex background
['Bart de Boer'],"The value of the first formant of high back and high front vowels (/u/ and /i/) has been determined for near minimal pairs in a 30-language sample. It is found that for 29 out of 30 languages the average of the first formant is higher for high back vowels than for high front vowels, and that for 26 out of 28 languages the majority of minimal pairs has a high back vowel with a higher first formant than that of the high front vowel. A trend towards smaller differences was found in women, but this is not significant in the present data set.#R##N##R##N#Two factors may explain this observation. Firstly, the human vocal tract can only vary the position of gradual (and not abrupt) transitions of cross-sectional area. Secondly, there is a narrow tube just above the glottis (the epilarynx tube). Both factors cause the first formant of high back vowels to be raised, but neither is sufficiently important to explain the observed differences on its own.",First formant difference for /i/ and /u/: A cross-linguistic study and an explanation
"['Shinobu Hasegawa', 'Akihiro Kashihara', ""Jun'ichi Toyoca""]","The main topic addressed in this paper is how to help learners select some instructive hypermedia-based learning resources according to their learning contexts from the Web. Our approach is to provide a digital library for web-based learning called e-Learning Library, which includes learning resource repository, local indexing, and adaptive navigation support. This aims to promote their learning with diverse learning resources involving a certain topic.",An e-Learning Library on the Web
"['Youfeng Su', 'Jie Huang']","In this paper, we first establish a stability result for a class of linear switching systems involving Kronecker product. The problem is intriguing in that the system matrix does not have to be Hurwitz in any time instant. We have established the main result by a combination of the Lyapunov stability analysis and a generalized Barbalat's Lemma applicable to piecewise continuous linear systems. As applications of this stability result, we study both the leaderless consensus problem and the leader-following consensus problem for general marginally stable linear multi-agent systems under switching network topology. In contrast with many existing results, our result only assume that the dynamic graph is uniformly connected.",Stability of a class of linear switching systems with applications to two consensus problems
"['Ben Recht', 'Christopher R??', 'Joel A. Tropp', 'Victor Bittorf']","This paper describes a new approach, based on linear programming, for computing nonnegative matrix factorizations (NMFs). The key idea is a data-driven model for the factorization where the most salient features in the data are used to express the remaining features. More precisely, given a data matrix X, the algorithm identifies a matrix C that satisfies X É?? CX and some linear constraints. The constraints are chosen to ensure that the matrix C selects features; these features can then be used to find a low-rank NMF of X. A theoretical analysis demonstrates that this approach has guarantees similar to those of the recent NMF algorithm of Arora et al. (2012). In contrast with this earlier work, the proposed method extends to more general noise models and leads to efficient, scalable algorithms. Experiments with synthetic and real datasets provide evidence that the new approach is also superior in practice. An optimized C++ implementation can factor a multigigabyte matrix in a matter of minutes.",Factoring nonnegative matrices with linear programs
"['Christine I. Podilchuk', 'Nikil Jayant', 'Nariman Farvardin']",We describe and show the results of video coding based on a three-dimensional (3-D) spatio-temporal subband decomposition. The results include a 1-Mbps coder based on a new adaptive differential pulse code modulation scheme (ADPCM) and adaptive bit allocation. This rate is useful for video storage on CD-ROM. Coding results are also shown for a 384-kbps rate that are based on ADPCM for the lowest frequency band and a new form of vector quantization (geometric vector quantization (GVQ)) for the data in the higher frequency bands. GVQ takes advantage of the inherent structure and sparseness of the data in the higher bands. Results are also shown for a 128-kbps coder that is based on an unbalanced tree-structured vector quantizer (UTSVQ) for the lowest frequency band and GVQ for the higher frequency bands. The results are competitive with traditional video coding techniques and provide the motivation for investigating the 3-D subband framework for different coding schemes and various applications. >,Three-dimensional subband coding of video
"['Wladimir Schamai', 'Peter Fritzson', 'Christiaan J. J. Paredis']","ModelicaML is a UML profile that enables modeling and simulation of systems and their dynamic behavior. ModelicaML combines the power of the OMG UML standardized graphical notation for systems and software modeling, and the simulation power of Modelica. This addresses the increasing need for precise and integrated modeling of products containing both software and hardware. This article discusses the usage of executable UML state machines for system modeling, i.e. usage of the same formalism for describing the state-based dynamic behavior of physical system components and software. Moreover, it points out that the usage of Modelica as an action language enables an integrated simulation of continuous-time and reactive/event-based system dynamics. The main purpose of this article is however to highlight issues that are identified regarding the UML specification which are experienced with typical executable implementations of UML state machines. The issues identified are resolved and rationales for the taken design decisions are provided.",Translation of UML state machines to Modelica: Handling semantic issues
"['Thomas Arildsen', 'Torben Larsen']","Existing convex relaxation-based approaches to reconstruction in compressed sensing assume that noise in the measurements is independent of the signal of interest. We consider the case of noise being linearly correlated with the signal and introduce a simple technique for improving compressed sensing reconstruction from such measurements. The technique is based on a linear model of the correlation of additive noise with the signal. The modification of the reconstruction algorithm based on this model is very simple and has negligible additional computational cost compared to standard reconstruction algorithms, but is not known in existing literature. The proposed technique reduces reconstruction error considerably in the case of linearly correlated measurements and noise. Numerical experiments confirm the efficacy of the technique. The technique is demonstrated with application to low-rate quantization of compressed measurements, which is known to introduce correlated noise, and improvements in reconstruction error compared to ordinary Basis Pursuit De-Noising of up to approximately 7dB are observed for 1bit/sample quantization. Furthermore, the proposed method is compared to Binary Iterative Hard Thresholding which it is demonstrated to outperform in terms of reconstruction error for sparse signals with a number of non-zero coefficients greater than approximately 1/10th of the number of compressed measurements.",Compressed sensing with linear correlation between signal and measurement noise
"['Damien Chablat', 'Philippe Wenger']","This paper addresses the architecture optimization of a three-degree-of-freedom translational parallel mechanism designed for machining applications. The design optimization is conducted on the basis of a prescribed Cartesian workspace with prescribed kinetostatic performances. The resulting machine, the Orthoglide, features three fixed parallel linear joints which are mounted orthogonally, and a mobile platform which moves in the Cartesian x-y-z space with fixed orientation. The interesting features of the Orthoglide are a regular Cartesian workspace shape, uniform performances in all directions, and good compactness. A small-scale prototype of the Orthoglide under development is presented at the end of this paper.","Architecture optimization of a 3-DOF translational parallel mechanism for machining applications, the orthoglide"
"['Xiao-Bo Jin', 'Cheng-Lin Liu', 'Xinwen Hou']","The classification performance of nearest prototype classifiers largely relies on the prototype learning algorithms, such as the learning vector quantization (LVQ) and the minimum classification error (MCE). This paper proposes a new prototype learning algorithm based on the minimization of a conditional log-likelihood loss (CLL), called log-likelihood of margin (LOGM). A regularization term is added to avoid over-fitting in training. The CLL loss in LOGM is a convex function of margin, and so, gives better convergence than the MCE algorithm. Our empirical study on a large suite of benchmark datasets demonstrates that the proposed algorithm yields higher accuracies than the MCE, the generalized LVQ (GLVQ), and the soft nearest prototype classifier (SNPC).",Prototype learning with margin-based conditional log-likelihood loss
"['Yulei Weng', 'Alex Doboli']",A system level design methodology is applied to the embedded system design for a typical sensor network application: face detection for security purpose. The tradeoff analysis is performed for hardware and software implementations of the tasks in this application. The best system design is achieved with limited hardware resources.,Smart sensor architecture customized for image processing applications
"['Bo Wang', 'Haiyan Zhao', 'Wei Zhang', 'Zhi Jin', 'Hong Mei']","In the software development, most stakeholders cannot clearly and objectively express their needs for the envisioned software systems. In this paper, we propose a problem-driven collaborative requirements elicitation approach, with the purpose of helping identify and extract the requirements of the Internetwares (a complex and new software paradigm). The basic idea of our approach is that the requirements of the software systems should be stated by stakeholders in an objective way (i.e. problem-identifying-solving way). That is, first identify the problems existed in the as-is problem domain, and then find the solutions to the problems. The solutions to the problems are the requirements of the envisioned software systems. To this end, we propose the structure of problems and a collaborative process for achieving the solutions.",A problem-driven collaborative approach to eliciting requirements of internetwares
"['Rustam M. Vahidov', 'Fei Ji']","In this paper we propose a method for supporting consumer buying decisions in e-commerce. We are advocating the diversity-driven approach to generating alternatives for infrequently purchased products (i.e., computers, vehicles, etc.). Our method is based upon the well-known ''divergence/convergence'' principle of problem solving. The paper discusses the method based on fuzzy weighted-sum model and cluster analysis, the architecture and the operation of the decision support system for generating product alternatives. The preliminary experiments with the prototype for notebook selection provide some support in favor of our approach over the catalog-based systems.",A diversity-based method for infrequent purchase decision support in e-commerce
"['Lars M??nch', 'Hari Balasubramanian', 'John W. Fowler', 'Michele E. Pfund']","This research is motivated by a scheduling problem found in the diffusion and oxidation areas of semiconductor wafer fabrication, where the machines can be modeled as parallel batch processors. We attempt to minimize total weighted tardiness on parallel batch machines with incompatible job families and unequal ready times of the jobs. Given that the problem is NP-hard, we propose two different decomposition approaches. The first approach forms fixed batches, then assigns these batches to the machines using a genetic algorithm (GA), and finally sequences the batches on individual machines. The second approach first assigns jobs to machines using a GA, then forms batches on each machine for the jobs assigned to it, and finally sequences these batches. Dispatching and scheduling rules are used for the batching phase and the sequencing phase of the two approaches. In addition, as part of the second decomposition approach, we develop variations of a time window heuristic based on a decision theory approach for forming and sequencing the batches on a single machine.",Heuristic scheduling of jobs on parallel batch machines with incompatible job families and unequal ready times
"['Mei-Yu Hsiao', 'Chien-Chung Chen', 'Jyh-Horng Chen']","Associating fMRI image datasets with the available literature is crucial for the analysis and interpretation of fMRI data. Here, we present a human brain function mapping knowledge-base system (BrainKnowledge) that associates fMRI data analysis and literature search functions. BrainKnowledge not only contains indexed literature, but also provides the ability to compare experimental data with those derived from the literature. BrainKnowledge provides three major functions: (1) to search for brain activation models by selecting a particular brain function; (2) to query functions by brain structure; (3) to compare the fMRI data with data extracted from the literature. All these functions are based on our literature extraction and mining module developed earlier (Hsiao, Chen, Chen. Journal of Biomedical Informatics 42, 912É??922, 2009), which automatically downloads and extracts information from a vast amount of fMRI literature and generates co-occurrence models and brain association patterns to illustrate the relevance of brain structures and functions. BrainKnowledge currently provides three co-occurrence models: (1) a structure-to-function co-occurrence model; (2) a function-to-structure co-occurrence model; and (3) a brain structure co-occurrence model. Each model has been generated from over 15,000 extracted Medline abstracts. In this study, we illustrate the capabilities of BrainKnowledge and provide an application example with the studies of affect. BrainKnowledge, which combines fMRI experimental results with Medline abstracts, may be of great assistance to scientists not only by freeing up resources and valuable time, but also by providing a powerful tool that collects and organizes over ten thousand abstracts into readily usable and relevant sources of information for researchers.",BrainKnowledge: A Human Brain Function Mapping Knowledge-Base System
"['Slawomir Pietrzyk', 'Gerard J. M. Janssen']","In this paper we address the problem of radio resource allocation for QoS support in the downlink of a cellular OFDMA system. The major impairments considered are cochannel interference (CCI) and frequency selective fading. The allocation problem involves assignment of base stations and subcarriers, bit loading, and power control, for multiple users. We propose a three-stage, low-complexity, heuristic algorithm to distribute radio resources among multiple users according to their individual QoS requirements, while at the same time maintaining the QoS of already established links in all the cochannel cells. The allocation objective is to minimize the total transmit power, which adds to reducing CCI. Simulation results show a superior performance of the proposed method when compared to classical radio resource management techniques. Our scheme allows us to achieve almost 6 times higher capacity (sum data rate) than the method based on FDMA with power control, at a blocking probability of 0.02.",Radio resource allocation for cellular networks based on OFDMA with QoS guarantees
['Daniel R. Berger'],"In this sketch we present a new method for rendering large-scale, high-resolution, non-repetitive textures in real-time using multi layer texturing. The basic idea of spectral texturing is to construct the nal texture by multiple texture layers where each layer provides a certain range of the spectrum of the textureÉ??s spatial frequencies. Alpha channels are used to introduce statistical dependencies between the frequency bands. This approach extends a method called detail texturing, which does not use alpha channels to model higher statistical properties of the resulting texture. Other approaches to generate textures with specied statistical properties, like [DeBonet 1997], are not suitable for real-time use and would require storage of the generated texture. Spectral texturing is very easy to implement, runs on all contemporary 3d graphics cards, and is especially suitable for naturalistic textures in real-time applications which are viewed from a large range of distances.",Spectral texturing for real-time applications
"['Rene Mueller', 'Jens Teubner', 'Gustavo Alonso']","Taking advantage of many-core, heterogeneous hardware for data processing tasks is a difficult problem. In this paper, we consider the use of FPGAs for data stream processing as coprocessors in many-core architectures. We present Glacier, a component library and compositional compiler that transforms continuous queries into logic circuits by composing library components on an operator-level basis. In the paper we consider selection, aggregation, grouping, as well as windowing operators, and discuss their design as modular elements.#R##N##R##N#We also show how significant performance improvements can be achieved by inserting the FPGA into the system's data path (e.g., between the network interface and the host CPU). Our experiments show that queries on the FPGA can process streams at more than one million tuples per second and that they can do this directly from the network, removing much of the overhead of transferring the data to a conventional CPU.",Streams on wires: a query compiler for FPGAs
"['Hong S. He', 'David R. Larsen', 'David J. Mladenoff']","Forest management issues are increasingly required to be addressed in a spatial context, which has led to the development of spatially explicit forest landscape models. The numerous processes, complex spatial interactions, and diverse applications in spatial modeling make the development of forest landscape models difficult for any single research group. New developments in componentbased modeling approaches provide a viable solution. Component-based modeling breaks a monolithic model into small, interchangeable, and binary components. They have these advantages compared to the traditional modeling work: 1) developing a component is a much smaller task than developing a whole model, 2) a component can be developed using most programming languages, since the interface format is binary, and 3) new components can replace the existing ones under the same model framework; this reduces the duplication and allows the modeling community to focus resources on the common products, and to compare results. In this paper, we explore the design of a spatially explicit forest landscape model in a component-based modeling framework, based on our work on object-oriented forest landscape modeling. We examine the representation of the major components and the interactions between them. Our goal is to facilitate the use of the component-based modeling approach at the early stage of spatially explicit landscape modeling. ã?? 2002 Elsevier Science Ltd. All rights reserved.",Exploring component-based approaches in forest landscape modeling
"['Hideo Bannai', 'Yoshinori Tamada', 'Osamu Maruyama', 'Kenta Nakai', 'Satoru Miyano']","Motivation: The prediction of localization sites of various proteins is an important and challenging problem in the field of molecular biology. TargetP, by Emanuelsson et al. (J. Mol. Biol., 300, 1005É?ê1016, 2000) is a neural network based system which is currently the best predictor in the literature for N-terminal sorting signals. One drawback of neural networks, however, is that it is generally difficult to understand and interpret how and why they make such predictions. In this paper, we aim to generate simple and interpretable rules as predictors, and still achieve a practical prediction accuracy. We adopt an approach which consists of an extensive search for simple rules and various attributes which is partially guided by human intuition. Results: We have succeeded in finding rules whose prediction accuracies come close to that of TargetP, while still retaining a very simple and interpretable form. We also discuss and interpret the discovered rules. Availability: An (experimental) web service using rules obtained by our method is provided at http:",Extensive feature detection of N-terminal protein sorting signals
['Johji Tajima'],"Abstract#R##N##R##N#For high-fidelity color reproduction with digital color platemaking systems, it is the most important to determine the color masking matrix which converts the red, green and blue intensities of the monitor to cyan, magenta, yellow and black inks halftone dot area rates. With regard to this determination process, the author previously developed a color difference least square method by which the optimum matrix is determined, using a simulation with the Neugebauer equation and virtual color samples. First, this paper evaluates the method with actual images. Next, the method is extended for adaptive masking matrix optimization. By adapting the matrix to each color image using black ink, it was shown that the average color differences of reproduced images could be reduced to below six when black ink is used as much as possible, i.e., in the case of achromatic printing. This masking matrix adaption is a new concept which was impossible by conventional color scanner processing, but became possible by using virtual color samples.","Optimum color masking matrix determination for digital color platemaking, using virtual color samples"
"['Tuan Anh Le', 'Mohammad Reza Nakhai']","In this paper, we propose a downlink transmission strategy based on intercell interference pricing and a distributed algorithm that enables each base station (BS) to design locally its own beamforming vectors without relying on downlink channel state information of links from other BSs to the users. This algorithm is the solution to an optimization problem that minimizes a linear combination of data transmission power and the resulting weighted intercell interference with pricing factors at each BS and maintains the required signal-to-interference-plus-noise ratios (SINR) at user terminals. We provide a convergence analysis for the proposed distributed algorithm and derive conditions for its existence. We characterize the impact of the pricing factors in expanding the operational range of SINR targets at user terminals in a power-efficient manner. Simulation results confirm that the proposed algorithm converges to a network-wide equilibrium point by balancing and stabilizing the intercell interference levels and assigning power optimal beamforming vectors to the BSs. The results also show the effectiveness of the proposed algorithm in closely following the performance limits of its centralized coordinated beamforming counterpart.",Downlink Optimization with Interference Pricing and Statistical CSI
"['Niladri B. Puhan', 'Anthony T. S. Ho']","Authentication watermarking schemes using block-wise watermarks for tamper localization are vulnerable to the Holliman---Memon attack. In this paper, we propose a novel method based on the Wong's localization scheme (Proceedings of the IS&T PIC, Portland) to resist this attack. A unique image index scheme is used for computing the authentication signature that is embedded in the least significant bit-plane of the block. The informed detector estimates the correct image index by using the side information about the watermarked image. The image index estimation from the fake image can definitely be an alternative to keeping a directory of image indices. So it is not necessary to manage the database of image indices for the verification purpose. The authenticity measure is defined to quantify the attack severity by taking the connectivity among possible authentic blocks into consideration. There are more blocks verified as authentic when this measure is high for a fake image constructed using this attack. As such, the blocks for a fake image can be chosen from a reduced number of database images. The blocks from any such image are to be connected with each other to maximize the authenticity measure. Thus, the attacker's task to generate a fake image of reasonable perceptual quality becomes increasingly difficult. With the proposed method there is no loss or ambiguity in localization after the Holliman---Memon attack and content tampering in an image. The localization accuracy in the proposed method is demonstrated by the simulation results and is equal to the chosen block size, similar to the Wong's scheme.",Secure authentication watermarking for localization against the Holliman---Memon attack
"['Jorge Cruz-Gonz?≠lez', 'Pedro L??pez-S?≠ez', 'Jos?? Emilio Navas-L??pez', 'Miriam Delgado-Verde']","Purpose É?? The aim of the paper is to identify the different directions of external knowledge search and to investigate their individual effect on performance at the firm level. Design/methodology/approach É?? The empirical study is based on survey data gathered from two distinct informants of 248 large- and medium-sized high-tech manufacturing Spanish firms. In dealing with concerns on simultaneity and reverse causality, perceived time-lags among dependent and independent variables were introduced. Quantitative methods based on questionnaire answers were used. Findings É?? Findings reveal six distinct external search patterns and indicate that, while market sources such as customers and competitors are positively associated with performance, knowledge acquired from general information sources, other firms beyond the core business and patents and databases have no significant effect. Moreover, knowledge obtained from science and technology organizations and from suppliers displays an inversed U-shaped effect o...",Directions of external knowledge search: investigating their different impact on firm performance in high-technology industries
"['Jing Su', 'Ashvin Goel', 'E. de Lara']","Radio equipped mobile devices have enjoyed tremendous growth in the past few years. We observe that in the near future it might be possible to build a network that routes delay-tolerant packets by harnessing user mobility and the pervasive availability of wireless devices. Such a delay-tolerant network could be used to supplement wireless infrastructure or provide service where none is available. Since mobile devices in a delay-tolerant network forward packets to nearby users, the devices can use short-range radio, which potentially reduces device power consumption and radio contention. The design of a user mobility based delay-tolerant network raises two key challenges: determining the connectivity of such a network, and determining the latency characteristics and replication requirements of routing algorithms in such a network. To determine realistic contact patterns, we collected user mobility data by conducting two user studies. We outfitted groups of students with instrumented wireless-enabled PDAs that logged pairwise contacts between study participants over a period of several weeks. Experiments conducted on these traces show that it is possible to form a delay-tolerant network based on human mobility. The network has good connectivity, so that routes exist between almost all study participants via some multi-hop path. Moreover, it is possible to effectively route packets with modest replication.",An Empirical Evaluation of the Student-Net Delay Tolerant Network
"['Oliver Friedmann', 'Martin Lange']","Theproblemofsolvinga paritygameis atthecore ofmanyproblemsin modelchecking,satisfiability checking and program synthesis. Some of the best algorithms for solving parity game are strategy improvement algorithms. These are global in nature since they require the entire parity game to be present at the beginning. This is a distinct disadvantagebecause in many applications one only needs to know which winning region a particular node belongs to, an daw itnessing winning strategy may cover only a fractional part of the entire game graph. We present a local strategy improvement algorithm which explores the game graph on-the-fly whilst performing the improvement steps. We also compare it empirically with existing global strategy improvementalgorithms and the currently only other local algorithm for solving parity games. It turnsout that local strategy improvementcan outperformthese othersby severalordersof magnitude.",Local Strategy Improvement for Parity Game Solving
"['Derek K. Jones', 'Thomas R. Kn??sche', 'Robert Turner']","Diffusion-weighted MRI (DW-MRI) has been increasingly used in imaging neuroscience over the last decade. An early form of this technique, diffusion tensor imaging (DTI) was rapidly implemented by major MRI scanner companies as a scanner selling point. Due to the ease of use of such implementations, and the plausibility of some of their results, DTI was leapt on by imaging neuroscientists who saw it as a powerful and unique new tool for exploring the structural connectivity of human brain. However, DTI is a rather approximate technique, and its results have frequently been given implausible interpretations that have escaped proper critique and have appeared misleadingly in journals of high reputation. In order to encourage the use of improved DW-MRI methods, which have a better chance of characterizing the actual fiber structure of white matter, and to warn against the misuse and misinterpretation of DTI, we review the physics of DW-MRI, indicate currently preferred methodology, and explain the limits of interpretation of its results. We conclude with a list of É??Do's and Don'tsÉ?? which define good practice in this expanding area of imaging neuroscience.","White matter integrity, fiber count, and other fallacies: The do's and don'ts of diffusion MRI"
"['Alun D. Preece', 'Winston R. Sieck']","In May 2006, the US Army Research Laboratory and UK Ministry of Defense created the international technology alliance. The consortium of 26 partners including the ARL and MoD offers an open research environment in which leading US and UK companies and universities can collaborate (see table 1). It will also fuse the best aspects of the US Army's Collaborative Technology Alliances and UK MoD's Defense Technology Centers on an international scale. The ITA aims to develop flexible, distributed, and secure decision-making procedures to improve networked coalition operations. Network science is a young discipline we have limited information models and network theories to describe the behavior and scaling of large, complex mobile ad hoc networks.1 moreover, you can't understand a coalition network's performance without understanding its cognitive and sociocultural aspects and physical characteristics. A key ITA goal is to perform basic research in network-centric coalition decision making across four technical areas: network theory, security across a system of systems, sensor information processing and delivery, and distributed coalition planning and decision making, 2. we focus on the last area because this is where intelligent systems will play the biggest role.",The International Technology Alliance in Network and Information Sciences
['Hedley J. Hansen'],"The millimeter (MM) wave and sub-MM wave (30-600 GHz) frequency band contains fundamental rotational and vibrational resonances of many molecular gases composed of carbon, nitrogen, oxygen, and sulphur. The high specificity of rotational spectra to organic molecules affords MM wave spectroscopy having potential use in remotely sensing atmospheric pollutants and the detection of airborne chemicals is important for arms control treaty verification, intelligence collection, and environmental monitoring. This paper considers the sensitivity requirements of radiofrequency receiver systems for measuring MM wave absorption/emission signatures. The significance of receiver sensitivity and material optical depth to sensing is highlighted. A background to the technology needed for sensing at MM and sub-MM wavelengths then provides the basis for a review of MM wave spectroscopy and its role on profiling the concentrations of trace polar molecules and ionized radicals in the high altitude atmosphere. The application of the MM wave spectroscopic technique in ambient conditions is then reviewed and the issues associated with developing the technique for standoff remote sensing is discussed.",Standoff Detection Using Millimeter and Submillimeter Wave Spectroscopy
"['Matthias Rarey', 'Adrian Kolodzik', 'Sascha Urbaczek']","Defining the ring topology of a molecule belongs to the central and elementary problems of cheminformatics. Questions like 'How many rings does a molecule contain?' or 'In how many rings is a specific atom involved?' are based on such a definition. Obviously, the ring topology must be unique, i.e. it does not depend on atom order, chemical meaningful, and of reasonable size, at most polynomial in the number of atoms. For a long period, the smallest set of smallest rings (SSSR) was used in cheminformatics applications ignoring a very critical flaw, namely that it is not unique even for simple structures. Other definitions like the set of relevant cycles heal this flaw; however they are sometimes chemically not meaningful and can become exponential in size. Among all attempts made, none fulfils all three criteria at the same time [1].#R##N##R##N#Recently, we developed a ring definition named 'unique ring family' (URF) and a corresponding algorithm for calculating it in polynomial time [2]. URFs match the common chemical sense of a molecule's ring topology. The definition results in a unique ring description consisting of a number of ring prototypes which is at most quadratic in the number of atoms. In this talk, we will present the algorithm, benchmarks as well as several examples demonstrating the usefulness of URFs.",Let's talk about rings
"['Y. Zimmels', 'Leonid G. Fel']","The capillary instability of a magnetically anisotropic liquid cylinder and jets, such as Nematic liquid crystals (LC), in magnetic fields, is considered using an energy approach. The boundary problem is solved in the linear approximation of the anisotropy /spl chi//sub a/ of the magnetic susceptibility /spl chi/. The effect of the anisotropy, in the region 1 |/spl chi/|>| /spl chi//sub a/| /spl chi//sup 2/, can be strong enough to counteract and even reverse the tendency of the field to enhance stabilization by increasing the cut-off wave number k/sub s/, beyond the conventional one set by Rayleigh. It is shown that the elastic effect, which is typical of LC, is significant on the scale of nano-jets, where it prevails over the magnetic effect. The jet instability is determined by surface tension, elasticity, and magnetic permeability and anisotropy. The relative influence of the elasticity and permeability on the jet stability depends on its radius. This is particularly true on the nano-scale.",Instability of submicron anisotropic liquid cylinders and jets in magnetic field
"['Xiaoqiang Xiao', 'Jasha Droppo', 'Alex Acero']","In this paper, we use information retrieval (IR) techniques to improve a speech recognition (ASR) system. The potential benefits include improved speed, accuracy, and scalability. Where conventional HMM-based speech recognition systems decode words directly, our IR-based system first decodes subword units. These are then mapped to a target word by the IR system. In this decoupled system, the IR serves as a lightweight, data-driven pronunciation model. Our proposed method is evaluated in the Windows Live Search for Mobile (WLS4M) task, and our best system has 12% fewer errors than a comparable HMM classifier. We show that even using an inexpensive IR weighting scheme (TF-IDF) yields a 3% relative error rate reduction while maintaining all of the advantages of the IR approach.",Information retrieval methods for automatic speech recognition
"['T. K. Das', 'D. P. Acharjya']","In modern era of computing, there is a need of development in data analysis and decision making. Most of our tools are crisp, deterministic and precise in character. But general real life situations contains uncertainties. To handle such uncertainties many theories are developed such as fuzzy set, rough set, rough set on fuzzy approximation spaces etc. But all these theories have their own limitations. To overcome the limitations, the concept of soft set is introduced. But, soft set also fails if the attributes in the information system are almost identical rather exactly identical. In this paper, we propose a decision making model that consists of two processes such as preprocess and postprocess to mine decisions. In preprocess we use rough set on fuzzy approximation spaces to get the almost equivalence classes whereas in postprocess we use soft set techniques to obtain decisions. The proposed model is tested over an institutional dataset and the results show practical viability of the proposed research.",A decision making model using soft set and rough set on fuzzy approximation spaces
"['Patrick Chin Hooi Soh', 'John P. Charlton', 'K. W. Chew']","The impact of parental and peer attachment on four Internet usage motives and Internet addiction was compared using path modelling of survey data from 1,577 adolescent Malaysian school students. The model accounted for 31 percent of Internet addiction score variance. Lesser parental attachment was associated with greater Internet addiction risk. Psychological escape motives were more strongly related to Internet addiction than other motives, and had the largest mediating effect upon the parental attachmentÉ??addiction relationship. Peer attachment was unrelated to addiction risk, its main influence on Internet usage motives being encouragement of use for social interaction. It is concluded that dysfunctional parental attachment has a greater influence than peer attachment upon the likelihood of adolescents becoming addicted to InternetÉ??related activities. It is also concluded that the need to relieve dysphoria resulting from poor adolescentÉ??parent relationships may be a major reason for Internet addiction, and that parentsÉ?? fostering of strong bonds with their children should reduce addiction risk.",The influence of parental and peer attachment on Internet usage motives and addiction
"['H Rik Eshuis', 'Akhil Kumar']","Workflow analysis is indispensable to capture modeling errors in workflow designs. While several workflow analysis approaches have been defined previously, these approaches do not give precise feedback, thus making it hard for a designer to pinpoint the exact cause of modeling errors. In this paper we introduce a novel approach for analyzing and diagnosing workflows based on integer programming (IP). Each workflow model is translated into a set of IP constraints. Faulty control flow connectors can be easily detected using the approach by relaxing the corresponding constraints. We have implemented this diagnosis approach in a tool called DiagFlow which reads and diagnoses XPDL models using an existing open source IP solver as a backend. We show that the diagnosis approach is correct and illustrate it with realistic examples. Moreover, the approach is flexible and can be extended to handle a variety of new constraints, as well as to support new workflow patterns. Results of testing on large process models show that DiagFlow outperforms a state of the art tool like Woflan in terms of the solution time.",An integer programming based approach for verification and diagnosis of workflows
"['Takahiro Fujiwara', 'Noboru Iida', 'Takashi Watanabe']","This paper proposes a hybrid wireless network scheme enhanced with ad hoc networking for disaster damage assessment and emergency communications. The network aims to maintain the connection between a base station (BS) and nodes by way of multihopping. In the event that a direct link between BS and a node is disconnected, the node switches modes from cellular to ad hoc in order to access BS via neighboring nodes. A routing protocol proposed in this paper is capable of building a route using unicast-based route discovery process without route request flooding. A proposed MAC protocol satisfies the requirement of maintaining accessibility and a short delay even in emergency circumstances. We discuss an analytical model based on a Markov process. Experimental results are shown regarding reachability, throughput and delay.",A hybrid wireless network enhanced with multihopping for emergency communications
['Elena L. Glassman'],"This paper describes the development and testing of a wavelet-like filter, named the SNAP, created from a neural activity simulation and used, in place of a wavelet, in a wavelet transform for improving EEG wavelet analysis, intended for brain-computer interfaces. The hypothesis is that an optimal wavelet can be approximated by deriving it from underlying components of the EEG. The SNAP was compared to standard wavelets by measuring Support Vector Machine-based EEG classification accuracy when using different wavelets/filters for EEG analysis. When classifying P300 evoked potentials, the error, as a function of the wavelet/filter used, ranged from 6.92% to 11.99%, almost twofold. Classification using the SNAP was more accurate than that with any of the six standard wavelets tested. Similarly, when differentiating between preparation for left- or right-hand movements, classification using the SNAP was more accurate (10.03% error) than for four out of five of the standard wavelets (9.54% to 12.00% error) and internationally competitive (7% error) on the 2001 NIPS competition test set. Phenomena shown only in maps of discriminatory EEG activity may explain why the SNAP appears to have promise for improving EEG wavelet analysis. It represents the initial exploration of a potential family of EEG-specific wavelets.",A wavelet-like filter based on neuron action potentials for analysis of human scalp electroencephalographs
"['Aruna Govada', 'Bhavul Gauri', 'Sanjay Kumar Sahay']",Data mining algorithms are originally designed by assuming the data is available at one centralized site. These algorithms also assume that the whole data is fit into main memory while running the algorithm. But in today's scenario the data has to be handled is distributed even geographically. Bringing the data into a centralized site is a bottleneck in terms of the bandwidth when compared with the size of the data. In this paper for multiclass SVM we propose an algorithm which builds a global SVM model by merging the local SVMs using a distributed approach(DSVM). And the global SVM will be communicated to each site and made it available for further classification. The experimental analysis has shown promising results with better accuracy when compared with both the centralized and ensemble method. The time complexity is also reduced drastically because of the parallel construction of local SVMs. The experiments are conducted by considering the data sets of size 100s to hundred of 100s which also addresses the issue of scalability.,Distributed Multi Class SVM for Large Data Sets
"['Christopher A. Le Dantec', 'W. Keith Edwards']","The use of ICTs in the public sector has long been touted for its potential to transform the institutions that govern and provide social services. The focus, however, has largely been on systems that are used within particular scales of the public sector, such as at the scale of state or national government, the scale of regional or municipal entity, or at the scale of local service providers. The work presented here takes aim at examining ICT use that crosses these scales of influence and accountability. We report on a year long ethnographic investigation conducted at a variety of social service outlets to understand how a shared information system crosses the boundaries of these very distinct organizations. We put forward that such systems are central to the work done in the public sector and represent a class of collaborative work that has gone understudied.",Across boundaries of influence and accountability: the multiple scales of public sector information systems
"['Elena Celledoni', 'David Cohen', 'Brynjulf Owren']","In this article, we derive and study symmetric exponential integrators. Numerical experiments are performed for the cubic Schrodinger equation and comparisons with classical exponential integrators and other geometric methods are also given. Some of the proposed methods preserve the L 2-norm and/or the energy of the system.",Symmetric Exponential Integrators with an Application to the Cubic Schr??dinger Equation
"['Rainer Spang', 'Martin Vingron']","Motivation: Noise in database searches resulting from random sequence similarities increases as the databases expand rapidly. The noise problems are not a technical shortcoming of the database search programs, but a logical consequence of the idea of homology searches. The effect can be observed in simulation experiments. Results: We have investigated noise levels in pairwise alignment based database searches. The noise levels of 38 releases of the SwissProt database, display perfect logarithmic growth with the total length of the databases. Clustering of real biological sequences reduces noise levels, but the effect is marginal.",Limits of homology detection by pairwise sequence comparison
"['Iman Poernomo', 'Timur Umarov']","Middleware support for business process Management BPM has met some of the challenges with respect to encoding, performance and maintenance of workflows. A remaining challenge is complexity: business processes are becoming widely distributed, interoperating across a range of inter- and intra-organizational behaviours, vocabularies and semantics. It is important that this semantic complexity is checked and analyzed for optimality and trustworthiness prior to deployment. Petri nets are a formal method that successfully provides behavioural analysis. A shortcoming of Petri nets is that the data exchanged between business activities abstract too far away from the importance of data in actual business processes. This paper addresses this abstraction gap via additional semantic enrichment, through a two stage, model-driven approach.",Business Process Development in Semantically-Enriched Environment
"['Theodoros Salonidis', 'Leandros Tassiulas']","We present a framework for the provision of deterministic end-to-end bandwidth guarantees in wireless ad hoc networks. Guided by a set of local feasibility conditions, multi-hop sessions are dynamically offered allocations, further translated to link demands. Using a distributed Time Division Multiple Access (TDMA) protocol nodes adapt to the demand changes on their adjacent links by local, conflict-free slot reassignments. As soon as the demand changes stabilize, the nodes must incrementally converge to a TDMA schedule that realizes the global link (and session) demand allocation.We first derive sufficient local feasibility conditions for certain topology classes and show that trees can be maximally utilized.We then introduce a converging distributed link scheduling algorithm that exploits the logical tree structure that arises in several ad hoc network applications.Decoupling bandwidth allocation to multi-hop sessions from link scheduling allows support of various end-to-end Quality of Service (QoS) objectives. We focus on the max-min fairness (MMF) objective and design an end-to-end asynchronous distributed algorithm for the computation of the session MMF rates. Once the end-to-end algorithm converges, the link scheduling algorithm converges to a TDMA schedule that realizes these rates.We demonstrate the applicability of this framework through an implementation over an existing wireless technology. This implementation is free of restrictive assumptions of previous TDMA approaches: it does not require any a-priori knowledge on the number of nodes in the network nor even network-wide slot synchronization.",Distributed dynamic scheduling for end-to-end rate guarantees in wireless ad hoc networks
"['Christos Strydis', 'Georgi Gaydadjiev']","This paper evaluates various branch-prediction schemes under different cache configurations in terms of performance, power, energy and area on suitably selected biomedical workloads. The benchmark suite used consists of compression, encryption and data-integrity algorithms as well as real implant applications, all executed on realistic biomedical input datasets. Results are used to drive the (micro)architectural design of a novel microprocessor targeting microelectronic implants. Our profiling study has revealed that, under strict or relaxed area constraints and regardless of cache size, the ALWAYS TAKEN and ALWAYS NOT-TAKEN static prediction schemes are, in almost all cases, the most suitable choices for the envisioned implant processor. It is further shown that bimodal predictors with small Branch-Target-Buffer (BTB) tables are suboptimal yet also attractive solutions when processor I/D-cache sizes are up to 1024KB/512KB, respectively.",Evaluating Various Branch-Prediction Schemes for Biomedical-Implant Processors
"['Mirang Park', 'Naonobu Okazaki', 'Yoshimasa Baba']","For constructing a ubiquitous network, the highspeed wireless LAN (WLAN) attracts attention as an infrastructure for global access. However, some issues are impeding further adoption of the technology, in particular, security problems including user authentication, message compromising, password theft, connection hijacking, etc. In this paper, we discuss a fast authentication method of mobile ubiquitous terminals in WLAN. To achieve an efficient access control between Access Points (APs) and mobile terminals and sharing of a session key between terminals, we propose a new user secure authentication method and a session key distribution protocol based on service ticket issuing system.",A New User Authentication Protocol for Mobile Terminals in Wireless Network
"['Dakuo He', 'Lin Xu', 'Jianhui Wang', 'Junxin Wu', 'Yan Zheng']","Learning theory develops in the constant process of transcendence and integration. Not only from knowledge to people, but also its theory approaches are all beyond each other and integration. Therefore, the emergence of blended e_learning is inevitable, which is based on the theoretical study?s transcendence and integration, and as a learning concepts and theories, blended e_learning is also bound to each other than with the integration. Such mutual transcendence and integration of learning theory is just an important basis for the starting point.",On the Basis of the Generated Foundation of Blended E-learning - Transcendence and Integration
"['Raylin Tso', 'Takeshi Okamoto', 'Eiji Okamoto']","Signcryption is a new cryptographic primitive which simultaneously provides both confidentiality and authenticity. This paper proposes an improved signcryption scheme and a variant scheme providing message recovery. The first scheme is revised from an authenticated encryption scheme which has been found to have a security-flaw. Our scheme solves the security-flaw and provides an additional property called the public verifiability of the signature. The second scheme is a message recovery type. It surpasses most of the current signcryption schemes on the size of the signcrypted ciphertext. That is, in our second scheme, we require only two parameters, (r, s), with r epsi Z p  and s epsi Z  q  while most signcryption schemes require three parameters (c, r, s) with the additional parameter c epsi Z p . This second scheme is modified from an authenticated encryption scheme with message recovery and surpasses the based authenticated encryption scheme on the property of non-repudiation of the origin",An Improved Signcryption Scheme and Its Variation
"['Shiann-Tsong Sheu', 'Meng-Hong Chen']","In the hybrid fiber/coax (HFC) architecture, over several hundreds subscribers in CATV (community antenna TV) network may cause serious collisions. In this paper, we propose a new network architecture which using an intelligent node (IN) to stand for a group of subscribers to request the demand resources. The IN has the ability to reduce the collision probability as well as the collision resolving period. The simulation results show that the proposed architecture in terms of throughput, buffer delay, and fairness outperforms the standard architecture.",A new network architecture with intelligent node (IN) to enhance IEEE 802.14 HFC networks
"['Edward Suh', 'Bhagirath Narahari', 'Rahul Simha']","This paper presents an experimental study of dynamic load balancing methods for a parallelized solution to a well-known problem in computational molecular biology: computing the accessible surface areas (ASA) of proteins. The main contribution is a better understanding of how certain techniques for load estimation and redistribution must be combined carefully for effectiveness and how these combinations need to change during the course of a computation. In particular, the Shrake-Rupley ASA algorithm is implemented and three aspects of dynamic load balancing are studied: how to estimate load imbalance (the estimation problem); when to invoke load redistribution (the invocation problem); and how to load balance (the mapping problem). The results in this paper show that a dynamically-selected mix of algorithms in each category that adapts to changing structure within the protein works better than a static periodic application of a static mix of algorithms.",Dynamic load balancing schemes for computing accessible surface area of protein molecules
"['Norman C. Beaulieu', 'Xiaofei Dong']","The average level crossing rate and average fade duration of the output signal of a maximal ratio combiner (MRC) and equal gain combiner (EGC), operating on independent Ricean fading input branch signals, are derived. Exact, closed-form results are obtained for MRC diversity, while precise expressions for EGC diversity are presented with an infinite series method. The results are valid for an arbitrary number of independent, identically distributed diversity branches, isotropic scattering, and a specular component perpendicular to the line of motion of the mobile.",Level crossing rate and average fade duration of MRC and EGC diversity in Ricean fading
"['Wen-Ting Wu', 'Jyh-Cheng Chen', 'Kai-Hsiu Chen', 'Kuo-Pao Fan']","This paper presents the design and implementation of WIRE Diameter. The WIRE Diameter is an open source implementation of Diameter Based Protocol and Diameter EAP application developed by the Wireless Internet Research & Engineering (WIRE) Laboratory. Research has shown that traditional RADIUS protocol may suffer performance degradation and data loss in a large system. Diameter, thus, was proposed to address the deficiencies in RADIUS. Both 3GPP and 3GPP2 have adopted Diameter as their AAA protocol. The WIRE Diameter could be used to authenticate and authorize 802.1x supplicant. It provides various authentication schemes, including EAP-MD5, EAP-TLS, EAP-TTLS, and PEAP. The WIRE Diameter is developed to be independent of OS as much as possible. Currently it supports Linux, FreeBSD and various versions of MS Windows. It is believed that the WIRE Diameter is the first open source implementation of Diameter EAP Application in the world. The source code can be downloaded freely. The WIRE Diameter should be useful for the research community. This paper demonstrates the design and implementation of the WIRE Diameter.",Design and implementation of WIRE Diameter
"['Zhi-Hong Mao', 'Eric Feron', 'Karl D. Bilimoria']","This paper considers the problem of two intersecting aircraft flows under decentralized conflict resolution rules. Considering aircraft flowing through a fixed control volume, new air traffic control models and scenarios are defined that enable the study of long-term aircraft flow stability. For a class of two intersecting aircraft flows, this paper considers conflict scenarios involving arbitrary encounter angles. It is shown that aircraft flow stability, defined both in terms of safety and performance, is preserved under the decentralized conflict resolution algorithm considered. It is shown that the lateral deviations experienced by aircraft in each flow are bounded.",Stability and performance of intersecting aircraft flows under decentralized conflict avoidance rules
['Robert M. Leve'],"Abstract#R##N##R##N#Life forms must organize information into cognitive models reflecting the outside environment, and in a complex and changing environment a life form must constantly select and organize this mass of information to avoid slipping into a chaotic cognitive state. The task of developing and maintaining adaptive cognitive models can be understood through two processes, crucial to regulating the interconnections between environmental elements. The inclusion and exclusion of information follows a process designated by P and the process by which cognitive models change is designated by K. Higher order concepts are created by reducing the interconnections between elements to a minimal number to avoid cognitive chaos. ?? 2004 Wiley Periodicals, Inc. Complexity 9:31É??37, 2004",Informational acquisition and cognitive models
"['Yadong Mu', 'Shuicheng Yan', 'Thomas S. Huang', 'Bingfeng Zhou']","In this work, we propose a general method for computing distance between video frames or sequences. Unlike conventional appearance-based methods, we first extract motion fields from original videos. To avoid the huge memory requirement demanded by the previous approaches, we utilize the É??bag of motion vectorsÉ?ù model, and select Gaussian mixture model as compact representation. Thus, estimating distance between two frames is equivalent to calculating the distance between their corresponding Gaussian mixture models, which is solved via earth mover distance (EMD) in this paper. On the basis of the inter-frame distance, we further develop the distance measures for both full video sequences.#R##N##R##N#Our main contribution is four-fold. Firstly, we operate on a tangent vector field of spatio-temporal 2D surface manifold generated by video motions, rather than the intensity gradient space. Here we argue that the former space is more fundamental. Secondly, the correlations between frames are explicitly exploited using a generative model named dynamic conditional random fields (DCRF). Under this framework, motion fields are estimated by Markov volumetric regression, which is more robust and may avoid the rank deficiency problem. Thirdly, our definition for video distance is in accord with human intuition and makes a better tradeoff between frame dissimilarity and chronological ordering. Lastly, our definition for frame distance allows for partial distance.",Contextual motion field-based distance for video analysis
"['Arati S. Deo', 'Ian D. Walker']","This paper investigates the use of an infinity norm in formulating the optimization measures for computing the inverse kinematics of redundant arms. The infinity norm of a vector is its maximum absolute value component and hence its minimization implies the determination of a minimum effort solution as opposed to the minimum-energy criterion associated with the Euclidean norm. In applications where individual magnitudes of the vector components are of concern, this norm represents the physical requirements more closely than does the Euclidean norm. We first study the minimization of the infinity-norm of the joint velocity vector itself, and discuss its physical interpretation. Next, a new method of optimizing a subtask criterion, defined using the infinity-norm, to perform additional tasks such as obstacle avoidance or joint limit avoidance is introduced. Simulations illustrating these methods and comparing the results with the Euclidean norm solutions are presented.",Minimum effort inverse kinematics for redundant manipulators
"['Aharon Abadi', 'Mordechai Nisenson', 'Yahalomit Simionovici']","Traceability in software involves discovering links between different artifacts, and is useful for a myriad of tasks in the software life cycle. We compare several different Information Retrieval techniques for this task, across two datasets involving real-world software with the accompanying specifications and documentation. The techniques compared include dimensionality reduction methods, probabilistic and information theoretic approaches, and the standard vector space model.",A Traceability Technique for Specifications
"['Weikang Lim', 'Guilin Yang', 'Song Huat Yeo', 'Shabbir Kurbanhusen Mustafa', 'I.M. Chen']","Cable-driven parallel manipulators (CDPMs) are a special class of parallel manipulators that are driven by cables instead of rigid links. Due to the unilateral property of the cables, all the driving cables in a fully-constrained CDPM must always maintain positive tension. As a result, tension analysis is the most essential issue for these CDPMs. By drawing upon the mathematical theory from convex analysis, a sufficient and necessary tension-closure condition is proposed in this paper. The key point of this tension-closure condition is to construct a critical vector that must be positively expressed by the tension vectors associated with the driving cables. It has been verified that such a tension-closure condition is general enough to cater for CDPMs with different numbers of cables and DOFs. Using the tension-closure condition, a computationally efficient algorithm is developed for the tension-closure pose analysis of CDPMs, in which only a limited set of deterministic linear equation systems need to be resolved. This algorithm has been employed for the tension-closure workspace analysis of CDPMs and verified by a number of computational examples. The computational time required by the proposed algorithm is always shorter as compared to other existing algorithms.",A generic tension-closure analysis method for fully-constrained cable-driven parallel manipulators
"['Hans Peter Graf', 'Eric Cosatto', 'L??on Bottou', 'Igor Dourdanovic', 'Vladimir Vapnik']","We describe an algorithm for support vector machines (SVM) that can be parallelized efficiently and scales to very large problems with hundreds of thousands of training vectors. Instead of analyzing the whole training set in one optimization step, the data are split into subsets and optimized separately with multiple SVMs. The partial results are combined and filtered again in a 'Cascade' of SVMs, until the global optimum is reached. The Cascade SVM can be spread over multiple processors with minimal communication overhead and requires far less memory, since the kernel matrices are much smaller than for a regular SVM. Convergence to the global optimum is guaranteed with multiple passes through the Cascade, but already a single pass provides good generalization. A single pass is 5x - 10x faster than a regular SVM for problems of 100,000 vectors when implemented on a single processor. Parallel implementations on a cluster of 16 processors were tested with over 1 million vectors (2-class problems), converging in a day or two, while a regular SVM never converged in over a week.",Parallel Support Vector Machines: The Cascade SVM
"['Daryl G. Beetner', 'R.M. Arthur']","Direct inference of heart-surface potentials from body-surface potentials has been the goal of most recent work on electrocardiographic inverse solutions. We developed and tested indirect methods for inferring heart-surface potentials based on estimation of regularized multipole sources. Regularization was done using Tikhonov, constrained-least-squares, and multipole-truncation techniques. These multipole-equivalent methods (MEMs) were compared to the conventional mixed boundary-value method (BVM) in a realistic torso model with up to 20% noise added to body-surface potentials and /spl plusmn/1 cm error in heart position and size. Optimal regularization was used for all inverse solutions. The relative error of inferred heart-surface potentials of the MEM was significantly less (p<0.05) than that of the BVM using zeroth-order Tikhonov regularization in 10 of the 12 cases tested. These improvements occurred with a fourth-degree (24 coefficients) or smaller multipole moment. From these multipole coefficients, heart-surface potentials can be found at an unlimited number of heart-surface locations. Our indirect methods for estimating heart-surface potentials based on multipole inference appear to offer significant improvement over the conventional direct approach.",Estimation of heart-surface potentials using regularized multipole sources
"['Christian Bradatsch', 'Theo Ungerer', 'Rafael Zalman', 'Andre Lajtkep']","Runtime testing is a common way to detect faults during normal system operation. To achieve a specific diagnostic coverage runtime testing is also used in safety critical, automotive embedded systems. In this paper we propose a test architecture to consolidate the hardware resource consumption and timing needs of runtime tests and of application and system tasks in a hard real-time embedded system as applied to the automotive domain. Special emphasis is put to timing requirements of embedded systems with respect to hard real-time and concurrent hardware resource accesses of runtime tests and tasks running on the target system.",Towards runtime testing in automotive embedded systems
"['Pierre-Henri Horrein', 'Christine Hennebert', 'Fr??d??ric P??trot']","This paper presents the Flexible Radio Kernel (FRK), a configuration and execution management environment for hybrid hardware/software flexible radio platform. The aim of FRK is to manage platform reconfiguration for multi-mode, multi-standard operation, with different levels of abstraction. A high level framework is described, to manage multiple MAC layers, and to enable MAC cooperation algorithms for cognitive radio. A low-level environment is also available to manage platform reconfiguration for radio operations. Radio can be implemented using hardware or software elements. Configuration state is hidden to the high-level layers, offering pseudo concurrency (time sharing) properties. This study presents a global view of FRK, with details on some specific parts of the environment. A practical study with algorithmic description is presented.",An Environment for (re)configuration and Execution Managenment of Flexible Radio Platforms
"['Muhammad Moinuddin', 'Imran Naseem']","In this article, we propose a novel method to derive exact closed-form ergodic capacity and outage probability expressions for correlated Rayleigh fading channels with receive diversity. Unlike the existing works, the proposed method employ a simple approach for the capacity and outage analysis for receiver diversity channels operating at different signal-to-noise ratios depicted in the diagonal elements of matrix ??. With x being the channel gain vector, random variable of the form Y(a)=a + xÉ???? x is considered. Novelty of the work resides in the fact that the distribution of Y(a) is accurately determined by employing Fourier representation of unit step function followed by complex integration in a straight forward way. The ergodic channel capacity is thus calculated by using the first-order moment, #N#                  #N#                    #N#                  #N#                  #N#                    #N#                      E#N#                      [#N#                      #N#                        #N#                          log#N#                        #N#                        #N#                          2#N#                        #N#                      #N#                      (#N#                      Y#N#                      (#N#                      1#N#                      )#N#                      )#N#                      ]#N#                    #N#                  #N#                , while the outage probability for a certain threshold ??0is evaluated using #N#                  #N#                    #N#                  #N#                  #N#                    #N#                      #N#                        #N#                          É?Æ#N#                        #N#                        #N#                          0#N#                        #N#                        #N#                          #N#                            #N#                              ??#N#                            #N#                            #N#                              0#N#                            #N#                          #N#                        #N#                      #N#                      #N#                        #N#                          f#N#                        #N#                        #N#                          Y#N#                          (#N#                          0#N#                          )#N#                        #N#                      #N#                      (#N#                      y#N#                      )#N#                      dy#N#                    #N#                  #N#                . Extensive experiments have been conducted demonstrating the accuracy of the proposed approach.",A simple approach to evaluate the ergodic capacity and outage probability of correlated Rayleigh diversity channels with unequal signal-to-noise ratios
"['Dorothy E. Setliff', 'Rob A. Rutenbar']","The application of program synthesis techniques to the generation of technology-sensitive VLSI physical design tools is described. The architecture and implementation of a particular software generator (called ELF) targeted at the generation of maze routing software is described. ELF strives to meet the demands of the target technology by automatically generating maze router implementations to match the application requirements. ELF has three key features. First, a very high level language, lacking data structure implementation specifications, is used to describe algorithm design styles. Second, application-specific expertise about routing and application independent code synthesis techniques are used to guide search among alternative design styles for algorithms and data structures. Third, code generation is used to transform the resulting abstract descriptions of selected algorithms and data structures into final, executable code. Code generation is an incremental, stepwise refinement process. Experimental results are presented covering several correct. fully functional routers synthesized by ELF from varying high-level specifications. Results from synthetic and industrial benchmarks are examined to illustrate ELF's capabilities. >",On the feasibility of synthesizing CAD software from specifications: generating maze router tools in ELF
"['Chung-Chih Li', 'Hikyoo Koh']","We present an approach used in CSDTA (canonical sequence directed tactics analyzer) that uses canonical sequences (Joseki) in hoping to improve computer Go programs. We collect 1278 canonical sequences and their deviations in our system. Instead of trivially matching the current game to the collected sequences, we define a notion of similarity to extract the most suitable move from the candidate sequences for the next move. The simplicity of our method and its positive outcome make our approach a promising tool to be integrated into a complete computer Go program for a foreseeable improvement",Canonical Sequence Directed Tactics Analyzer for Computer Go Games
"['David W. Salt', 'Subhash Ajmani', 'Ray Crichton', 'David J. Livingstone']","Variable selection methods are routinely applied in regression modeling to identify a small number of descriptors which ""best"" explain the variation in the response variable. Most statistical packages that perform regression have some form of stepping algorithm that can be used in this identification process. Unfortunately, when a subset of p variables measured on a sample of n objects are selected from a set of k(>p) to maximize the squared sample multiple regression coefficient, the significance of the resulting regression is upwardly biased. The extent of this bias is investigated by using Monte Carlo simulation and is presented as an inflation factor which when multiplied by the usual tabulated F ratio gives an estimate of the true 5% critical value. The results show that selection bias can be very high even for moderate-size data sets. Selecting three variables from 50 generated at random with 20 observations will almost certainly provide a significant result if the usual tabulated F values are used. An interpolation formula is provided for the calculation of the inflation factor for different combinations of (n, p, k). Four real data sets are examined to illustrate the effect of correlated descriptor variables on the degree of inflation.",An improved approximation to the estimation of the critical F values in best subset regression.
"['Zhe Wang', 'Songcan Chen', 'Tingkai Sun']","In this paper, we develop a new effective multiple kernel learning algorithm. First, we map the input data into m different feature spaces by m empirical kernels, where each generated feature space is taken as one view of the input space. Then, through borrowing the motivating argument from Canonical Correlation Analysis (CCA) that can maximally correlate the m views in the transformed coordinates, we introduce a special term called Inter-Function Similarity Loss R IFSI . into the existing regularization framework so as to guarantee the agreement of multiview outputs. In implementation, we select the Modification of Ho-Kashyap algorithm with Squared approximation of the misclassification errors (MHKS) as the incorporated paradigm and the experimental results on benchmark data sets demonstrate the feasibility and effectiveness of the proposed algorithm named MultiK-MHKS.",MultiK-MHKS: A Novel Multiple Kernel Learning Algorithm
"['Kevin P. Costello', 'Van H. Vu']","We investigate the rank of random (symmetric) sparse matrices. Our main finding is that with high probability, any dependency that occurs in such a matrix is formed by a set of few rows that contains an overwhelming number of zeros. This allows us to obtain an exact estimate for the co-rank.",On the rank of random sparse matrices
"['Eren G?¨rses', 'Anna N. Kim']","In the recent years, the peer-to-peer (P2P) overlay network has been a promising architecture for multimedia streaming services besides its common use for efficient file sharing. By simply increasing the number of peers, the P2P overlay network can meet the high bit rate requirements of multimedia applications. Optimal peer selection for newly joining peers is one of the important problems, especially in wireless networks which have limited resources and capacity, since the peer selection process has a direct impact on the throughput of the underlay network and the co-existing unicast traffic. In this paper we tackle the problem of peer selection for streaming applications over wireless ad hoc networks. We devise a novel peer selection algorithm which maximizes the throughput of the underlay network, and at the same time makes P2P streaming friendly towards the co-existing data traffic. The proposed receiver based rate allocation and peer selection (RPS) algorithm is derived using the network utility maximization (NUM) framework. The algorithm solves the peer selection and rate allocation problem distributedly while optimally adapting the medium access control (MAC) layer parameters and is easily extensible to large P2P networks. Simulation results show that by using the proper price exchange mechanism, the peer receivers can effectively maximize the throughput of the underlay network by intelligently selecting its source peers.",Maximum Utility Peer Selection for P2P Streaming in Wireless Ad Hoc Networks
"['Halil Karahan', 'M. Tamer Ayvaz']","Abstract#R##N##R##N#Time-dependent groundwater modeling using spreadsheet simulation (TGMSS) model is developed as solution technique. It is a practical method that uses spreadsheets instead of the conventional solution methods. All of the aquifer parameters can easily be described in TGMSS model. The results of TGMSS are validated with MODFLOW. Results showed that TGMSS and MODFLOW results were in good agreement in terms of resulting values of hydraulic heads. ?? 2005 Wiley Periodicals, Inc. Comput Appl Eng Educ 13: 192É??199, 2005; Published online in Wiley InterScience (www.interscience.wiley.com); DOI 10.1002/cae.20048",TimeÉ?êdependent groundwater modeling using spreadsheet
"[""George D. O'Clock"", 'Yong Wan Lee', 'Jongwong Lee', 'Warren J. Warwick']","High-frequency chest compression (HFCC) can be used as a therapeutic intervention to assist in the transport and clearance of mucus and enhance water secretion for cystic fibrosis patients. An HFCC pump-vest and half chest-lung simulation, with 23 lung generations, has been developed using inertance, compliance, viscous friction relationships, and Newton's second law. The simulation has proven to be useful in studying the effects of parameter variations and nonlinear effects on HFCC system performance and pulmonary system response. The simulation also reveals HFCC waveform structure and intensity changes in various segments of the pulmonary system. The HFCC system simulation results agree with measurements, indicating that the HFCC energy transport mechanism involves a mechanically induced pulsation or vibration waveform with average velocities in the lung that are dependent upon small air displacements over large areas associated with the vest-chest interface. In combination with information from lung physiology, autopsies and a variety of other lung modeling efforts, the results of the simulation can reveal a number of therapeutic implications.",A Simulation Tool to Study High-Frequency Chest Compression Energy Transfer Mechanisms and Waveforms for Pulmonary Disease Applications
"['Marco Baglietto', 'Thomas Parisini', 'R. Zoppoli']","Large-scale traffic networks can be modeled as graphs in which a set of nodes are connected through a set of links that cannot be loaded above their traffic capacities. Traffic flows may vary over time. Then the nodes may be requested to modify the traffic flows to be sent to their neighboring nodes. In this case, a dynamic routing problem arises. The decision makers are realistically assumed 1) to generate their routing decisions on the basis of local information and possibly of some data received from other nodes, typically, the neighboring ones and 2) to cooperate on the accomplishment of a common goal, that is, the minimization of the total traffic cost. Therefore, they can be regarded as the cooperating members of informationally distributed organizations, which, in control engineering and economics, are called team organizations. Team optimal control problems cannot be solved analytically unless special assumptions on the team model are verified. In general, this is not the case with traffic networks. An approximate resolutive method is then proposed, in which each decision maker is assigned a fixed-structure routing function where some parameters have to be optimized. Among the various possible fixed-structure functions, feedforward neural networks have been chosen for their powerful approximation capabilities. The routing functions can also be computed (or adapted) locally at each node. Concerning traffic networks, we focus attention on store-and-forward packet switching networks, which exhibit the essential peculiarities and difficulties of other traffic networks. Simulations performed on complex communication networks point out the effectiveness of the proposed method.",Distributed-information neural control: the case of dynamic routing in traffic networks
"['Guido Dieterich', 'Dirk W. Heinz', 'Joachim Reichelt']","The 3D structures of biomacromolecules stored in the Protein Data Bank [1] were correlated with different external, biological information from public databases. We have matched the feature table of SWISS-PROT [2] entries as well InterPro [3] domains and function sites with the corresponding 3D-structures. OMIM [4] (Online Mendelian Inheritance in Man) records, containing information of genetic disorders, were extracted and linked to the structures. The exhaustive all-against-all 3D structure comparison of protein structures stored in DALI [5] was condensed into single files for each PDB entry. Results are stored in XML format facilitating its incorporation into related software. The resulting annotation of the protein structures allows functional sites to be identified upon visualization. Availability: http://leger.gbf.de/PDBXML/",Matching of PDB chain sequences to information in public databases as a prerequisite for 3D functional site visualization
"['Yaron Levinson', 'Leonid Mirkin']","This paper studies the $L^2$ (mean-square) optimal design of discrete-time FIR estimators. A solution procedure, which reduces the problem to a static matrix optimization problem admitting a closed-form solution, is proposed. In the latter solution, a special state-space structure of the associated matrices is exploited to obtain efficient formulae with the computational complexity proportional to the length of the impulse response of the estimator. Unlike previously available least-square FIR results, our treatment does not impose unnecessarily restrictive assumptions on the process dynamics and can handle interpolation constraints on the unit circle, which facilitates the inclusion of steady-state performance requirements.",$L^2$ Optimization in Discrete FIR Estimation: Exploiting State-Space Structure
"['Sivaprasad Gogineni', 'Kenneth C. Jezek', 'L. Peters', 'J. Young', 'Scott G. Beaven', 'Elias M. Nassar']",The authors utilized the concept of a compact antenna range to obtain plane-wave illumination to accurately measure scattering properties of simulated sea ice. They also made simultaneous measurements using conventional antennas. Measured scattering coefficients obtained with the plane-wave system at 10 GHz decreased by about 35 dB when the incidence angle increased from 0/spl deg/ to 10/spl deg/. Scattering coefficients derived from data collected with the radar system at 13.5 GHz using conventional far-field antennas decreased by about 20 dB over the same angular region. This demonstrates that the far-field properties of a widebeam antenna are inadequate for measuring the angular scattering response of smooth surfaces. They believe that application of the compact antenna range concept for scattering measurements has a wide range of applications and is the solution to the long-standing problem of how to directly measure scattering consisting of coherent and incoherent components. >,Application of plane waves for accurate measurement of microwave scattering from geophysical surfaces
"['Y. Ma', 'Kurt Saetzler']","In this paper, we describe a novel 3D subdivision strategy to extract the surface of binary image data. This iterative approach generates a series of surface meshes that capture different levels of detail of the underlying structure. At the highest level of detail, the resulting surface mesh generated by our approach uses only about 10 percent of the triangles in comparison to the Marching Cube (MC) algorithm, even in settings where almost no image noise is present. Our approach also eliminates the so-called ""staircase effect,"" which voxel-based algorithms like the MC are likely to show, particularly if nonuniformly sampled images are processed. Finally, we show how the presented algorithm can be parallelized by subdividing 3D image space into rectilinear blocks of subimages. As the algorithm scales very well with an increasing number of processors in a multithreaded setting, this approach is suited to process large image data sets of several gigabytes. Although the presented work is still computationally more expensive than simple voxel-based algorithms, it produces fewer surface triangles while capturing the same level of detail, is more robust toward image noise, and eliminates the above-mentioned ""staircase"" effect in anisotropic settings. These properties make it particularly useful for biomedical applications, where these conditions are often encountered.",A Parallelized Surface Extraction Algorithm for Large Binary Image Data Sets Based on an Adaptive 3-D Delaunay Subdivision Strategy
"['Andreas Herkersdorf', 'Walter Stechele']","Summary form only given. Future automotive security systems will benefit from visual scene analysis based on a fusion of video, infrared, and radar images. Today we have already functions like lane departure warning and automatic cruise control (ACC) for pretty well defined driving environments, such as highways and primary roads. Recent research activities concentrate on more complex environments, such as city traffic with a wide variety of traffic participants moving in an unpredictable manner, e.g. bikes, pedestrians, children, and even animals, and under changing weather and lighting conditions. The ITRS semiconductor roadmap for microelectronics forecasts a continued doubling of transistor capacity per chip every 2 to 2.5 years enabling billion transistor ASIC designs in the near future. Multi processor system on chip (MPSoC) solutions with 8, 16 or even more standard RISC CPU cores, mega-bytes of fast (ns access latencies) on-chip SRAM memories, giga-byte per second interconnect buses or NoC (network on chip) meshes, high-speed serial I/Os and, last but not least, million gate equivalent dedicated hardware accelerator functions in eFPGA (embedded field programmable gate array) logic are becoming reality on a single silicon substrate. Examples of current research projects shall illustrate our perception on how this tremendous increase in functionality and computational performance per chip area may impact automotive control unit (ACU) architectures for driver assistance applications. The AutoVision processor is a dynamically reconfigurable MPSoC prototype where video-specific pixel processing engines are on-the-fly loaded or exchanged without interrupting regular system operations. For the time being, pixel processing engines cover functions such as object edge detection or luminance segmentation, and are implemented as dedicated hardware accelerators to ensure real-time frame processing capabilities of the AutoVision processor. Dynamic replacement of processing engines ensures an automatic and area efficient adaptation to various driving conditions. Segmented objects are, in a subsequent step, characterized by means of standard MPEG-7 descriptors and entered as search criteria into traffic scene analysis databases. Goal is to obtain a clean distinction between passenger cars, trucks, and big rectangular traffic signs, and to identify pedestrians or bikers in complex traffic situations. The AutoVision processor project is supported by the German Research Foundation (DFG) in the special emphasis research programme ""reconfigurable computing"".",AutoVision - flexible processor architecture for video-assisted driving
"['Daniel Ferr??s', 'Horacio Rodr??guez']","This paper describes our experiments in Geographical Information Retrieval with the Wikipedia collection in the context of our participation in the GikiCLEF 2009 Multilingual task in English and Spanish. Our system, called gikiTALP, follows a very simple approach that uses standard Information Retrieval with the Sphinx full-text search engine and some Natural Language Processing techniques without Geographical Knowdledge.",TALP at GikiCLEF 2009
"['Yang-wen Liang', 'Robert Schober', 'Wolfgang H. Gerstacker']","In this paper, we propose beamforming schemes for frequency-selective channels with decision-feedback equalization (DFE) at the receiver. We consider both finite impulse response (FIR) and infinite impulse response (IIR) beamforming filters (BFFs). In case of IIR beamforming, we are able to derive closed-form expressions for the optimum BFFs. In addition, we provide an efficient numerical method for recursive calculation of the optimum FIR BFFs. Simulation and numerical results for typical GSM/EDGE channels confirm the significant performance gains achievable with beamforming compared to single-antenna transmission and optimized delay diversity.",Transmit Beamforming for Frequency-Selective Channels
"['Ahmed S. Elwakil', 'Muhammad Ali Al-Radhawi']",The authors report all the possible four-impedance settings that yield a valid second-order two-stage Colpitts oscillator. These settings are obtained following an exhaustive search conducted on two possible structures of the oscillator modelled through two-port network transmission parameters. Only valid second-order cases with a maximum of three reactive elements are reported. Experimental and Spice verification of a selected example using both MOS and BJT transistors is given.,All possible second-order four-impedance two-stage Colpitts oscillators
"['Matthieu Capelle', 'Cyrille Masson', 'Jean-Fran??ois Boulicaut']","Many practical applications are related to frequent sequential pattern mining, ranging from Web Usage Mining to Bioinformatics. To ensure an appropriate extraction cost for useful mining tasks, a key issue is to push the user-defined constraints deep inside the mining algorithms. In this paper, we study the search for frequent sequential patterns that are also similar to an user-defined reference pattern. While the effective processing of the frequency constraints is well-understood, our contribution concerns the identification of a relaxation of the similarity constraint into a convertible anti-monotone constraint. Both constraints are then used to prune the search space during a levelwise search. Preliminary experimental validations have confirmed the algorithm efficiency.",Mining Frequent Sequential Patterns under a Similarity Constraint
['David Ronen'],"Petroleum products are distributed worldwide from refineries and lube plants to retail outlets and industrial customers. Proper dispatching of shipments of such products, packaged and in bulk, may result in significant transportation and inventory cost savings. This work examines the variety of operational environments which exist in dispatching petroleum products, and the operations research tools used by oil companies to dispatch such products. In addition, it identifies gaps where additional research is needed.",Dispatching Petroleum Products
"['Alberto Bartesaghi', 'Pablo Sprechmann', 'Gregory Randall', 'Guillermo Sapiro', 'Sriram Subramaniam']","Electron tomography provides opportunities to determine three-dimensional cellular architecture at resolutions high enough to identify individual macromolecules such as proteins. Image analysis of such data poses a challenging problem due to the extremely low signal-to-noise ratios that makes individual volumes simply too noisy to allow reliable structural interpretation. This requires using averaging techniques to boost the signal-to-noise ratios, a common practice in electron microscopy single particle analysis where they have proven to be very powerful in elucidating high resolution structure. Although there are significant similarities in the way data is processed, several new problems arise in the tomography case that have to be properly dealt with. Such problems involve dealing with the missing wedge characteristic of limited angle tomography, the need for robust and efficient 3D alignment routines, and design of methods that account for diverse conformations through the use of classification. We present a framework for reconstruction via alignment, classification and averaging of volumes obtained from limited angle electron tomography, providing a powerful tool for high resolution structure determination and description of conformational variability in a biological context","CLASSIFICATION, AVERAGING AND RECONSTRUCTION OF MACROMOLECULES IN ELECTRON TOMOGRAPHY"
"['Scott C. Douglas', 'Danilo P. Mandic']","Recently, the augmented complex LMS (ACLMS) algorithm has been proposed for modeling complex-valued signal relationships in which a widely-linear model can be more appropriate [1]. It is not clear, however, how the behavior of ACLMS differs from that of the conventional complex LMS (CCLMS) algorithm. In this paper, we leverage a recently-developed analysis for the complex LMS algorithm [2] to illuminate the performance relationships between the ACLMS and CCLMS algorithms. Our analysis shows that the ACLMS algorithm can potentially achieve a lower steady-state mean-squared error as compared to that of CCLMS, but the convergence speed of ACLMS is slowed in the presence of highly non-circular complex-valued input signals. An adaptive beamforming example indicates the utility of the results.",Performance analysis of the conventional complex LMS and augmented complex LMS algorithms
"['Chien-Chih Liao', 'Chuan-Kang Ting']","Extending the lifetime is a key issue in wireless sensor networks. An effective way to extend the lifetime is to partition the sensors into several covers and activate the covers one by one. Thus, the more the covers, the longer the lifetime. To find the maximum number of covers has been modeled as the Set K-Cover problem. In this paper we propose using order-based genetic algorithm to solve the Set K-Cover problem for extending the lifetime of wireless sensor networks. The proposed algorithm needs neither an upper bound nor any assumption about the maximum number of covers. Experimental results show that the order-based genetic algorithm can achieve near-optimal solutions efficiently.",Extending wireless sensor network lifetime through order-based genetic algorithm
"['Min Han', 'Xinying Wang']","A robust neural predictor is designed for noisy chaotic time series prediction in this paper. The main idea is based on the consideration of the bounded uncertainty in predictor input, and it is a typical Errors-in-Variables problem. The robust design is based on the linear-in-parameters ESN (Echo State Network) model. By minimizing the worst-case residual induced by the bounded perturbations in the echo state variables, the robust predictor is obtained in coping with the uncertainty in the noisy time series. In the experiment, the classical Mackey-Glass 84-step benchmark prediction task is investigated. The prediction performance is studied for the nominal and robust design of ESN predictors.",Robust neural predictor for noisy chaotic time series prediction
"['Artiom Alhazov', 'Sergey Verlan']","Maximally parallel multiset rewriting systems (MPMRS) give a convenient way to express relations between unstructured objects. The functioning of various computational devices may be expressed in terms of MPMRS (e.g., register machines and many variants of P systems). In particular, this means that MPMRS are Turing universal; however, a direct translation leads to quite a large number of rules. Like for other classes of computationally complete devices, there is a challenge to find a universal system having the smallest number of rules. In this article we present different rule minimization strategies for MPMRS based on encodings and structural transformations. We apply these strategies to the translation of a small universal register machine (Korec (1996) [9]) and we show that there exists a universal MPMRS with 23 rules. Since MPMRS are identical to a restricted variant of P systems with antiport rules, the results we obtained improve previously known results on the number of rules for those systems.",Minimization strategies for maximally parallel multiset rewriting systems
"['DongInn Kim', 'Jeffrey M. Squyres', 'Andrew Lumsdaine']","The OSCAR [14] cluster installation toolkit was created by the Open Cluster Group (OCG) for one particular type of High Performance Computing (HPC) cluster. OSCAR is currently one of the widely used cluster installation toolkits; it boasts hundreds of thousands of downloads and active mailing lists. OSCAR has expanded its area with several sub-projects targeting other types of HPC clusters. Each of these projects share a core set of OSCAR code, including the OSCAR Database and its access API, ""ODA"" (OSCAR Database API). The ODA abstraction layer, consisting of a database schema and corresponding API, hides a commodity back-end database (e.g., MySQL [15]). Because OSCAR and its sub-projects are targeted at new, innovative environments (including non-HPC environments), there are significant issues with managing various configurations of each project. For example, as we previously showed [8], ??previous versions of ODA were unable to represent the complex, ever-growing set of data required to accurately describe the clusters that it manages. Further, its API was extremely complex, requiring a steep learning curve for OSCAR developers??. Therefore, we have designed and implemented a new database schema to deal with these issues. This new version of ODA has not only resolved the above problems but also, as proposed in our previous paper, enabled storage and retrieval of various configuration information, and encouraged data re-use between the main OSCAR project and its derivative projects. In addition, the new version of ODA has sped up the OSCAR installation process. This document presents a simpler, highly flexible design and implementation of ODA slated to be included in OSCAR v5.0. It also suggests a blueprint for maintaining the database modules of ODA in a systematic, organized way.",The Introduction of the OSCAR Database API (ODA)
"['Halina Szejnwald Brown', 'Philip J. Vergragt', 'Ken Green', 'Luca Berchicci']","Abstract A bounded socio-technical experiment (BSTE) attempts to introduce a new technology, service, or a social arrangement on a small scale. Many such experiments in personal mobility are ongoing worldwide. They are carried out by coalitions of diverse actors, and are driven by long term and large scale visions of advancing societyÉ??s sustainability agenda. This paper focuses on the processes of higher-order learning that occur through BSTEs. Based on the conceptual frameworks from theories of organizational learning, policy-oriented learning, and diffusion of innovation, we identify two types of learning: the first type occurs among the participants in the experiment and their immediate professional networks; the second type occurs in the society at large. Both types play a key role in the societal transition towards sustainable mobility systems. Two case studies, in which the Design for Sustainability Group at Technical University of Delft has participated, provide empirical data for the analysis. One...",Learning for Sustainability Transition through Bounded Socio-technical Experiments in Personal Mobility
"['Luther R. Palmer', 'David E. Orin']","During a complete running stride, which involves significant periods of flight during which no legs are contacting the ground, a quadruped cannot employ static stability techniques. Instead, the corrective forces necessary to maintain dynamic stability must be applied during the short stance intervals inherent to high-speed running. Because of this complexity and the large coupled forces required to run, much of the research on the control of quadruped running has focused on planar systems which are not required to simultaneously control attitude in all three dimensions. The 3D trot controller presented here overcomes these and other complexities to control a trot up to 3.75 m/s, approximately 3 body lengths per second, and turning rates up to 20 deg/s. The biomimetic method of banking into a high-speed turn is also investigated here. Along with the details of the attitude control algorithm, a set of control principles for high-speed legged motion is presented. These principles, such as the need to counteract the disturbance of swing leg return and the usefulness of force redistribution during stance, are not dependent on a particular scale or actuation scheme and can be applied to a wider range of legged systems.",Attitude Control of a Quadruped Trot While Turning
"['Jean Bosco Mbede', 'Xinhan Huang', 'Min Wang']","An integration of a fuzzy controller and modified Elman neural networks (NN) approximation-based computed-torque controller is proposed for motion control of autonomous manipulators in dynamic and partially known environments containing moving obstacles. The navigation technique of robot control using artificial potential fields is based on the fuzzy controller. The NN controller can deal with unmodeled bounded disturbances and or unstructured unmodeled dynamics of the robot arm. The NN weights are tuned online, with no off-line learning phase required. The stability of the closed-loop system is guaranteed by the Lyapunov theory. The purpose of the controller, which is designed as a neuro-fuzzy controller, is to generate the commands for the servo-systems of the robot so it may choose its way to its goal autonomously, while reacting in real-time to unexpected events. The proposed scheme has been successfully tested. The controller also demonstrates remarkable performance in adaptation to changes in manipulator dynamics. Sensor-based motion control is an essential feature for dealing with model uncertainties and unexpected obstacles in real-time world systems.",Robust fuzzy and recurrent neural network motion control among dynamic obstacles for robot manipulators
"['L. Costantini', 'Balazs Matuz', 'Gianluigi Liva', 'Enrico Paolini', 'Marco Chiani']","SUMMARY#R##N##R##N#Protograph-based non-binary low-density parity-check (LDPC) codes with ultra-sparse parity-check matrices are compared with binary LDPC and turbo codes (TCs) from space communication standards. It is shown that larger coding gains are achieved, outperforming the binary competitors by more than 0.3É??dB on the additive white Gaussian noise channel (AWGN). In the short block length regime, the designed codes gain more than 1É??dB with respect to the binary protograph LDPC codes recently proposed for the next generation up-link standard of the Consultative Committee for Space Data Systems. Copyright ?? 2012 John Wiley & Sons, Ltd.",NonÉ?êbinary protograph lowÉ?êdensity parityÉ?êcheck codes for space communications
"['Unoma Ndili', 'Robert D. Nowak', 'M?≠rio A. T. Figueiredo']","This paper introduces multi-scale tree-based approaches to image segmentation, using Rissanen's coding theoretic minimum description length (MDL) principle to penalize overly complex segmentations. Images are modelled as Gaussian random fields of independent pixels, with piecewise constant mean and variance. This model captures variations in both intensity (mean value) and texture (variance). Segmentation thus amounts to detecting changes in the mean and/or variance. One algorithm is based on an adaptive (greedy) rectangular recursive partitioning scheme. The second algorithm is an optimally pruned ""wedgelet"" decorated dyadic partitioning. We compare the two schemes with an alternative constant variance dyadic CART (classification and regression tree) scheme which accounts only for variations in mean, and demonstrate their performance on SAR images.",Coding theoretic approach to image segmentation
"['Juliane Sch??fer', 'Korbinian Strimmer']","Motivation: Genetic networks are often described statistically using graphical models (e.g. Bayesian networks). However, inferring the network structure offers a serious challenge in microarray analysis where the sample size is small compared to the number of considered genes. This renders many standard algorithms for graphical models inapplicable, and inferring genetic networks an 'ill-posed' inverse problem.#R##N##R##N#Methods: We introduce a novel framework for small-sample inference of graphical models from gene expression data. Specifically, we focus on the so-called graphical Gaussian models (GGMs) that are now frequently used to describe gene association networks and to detect conditionally dependent genes. Our new approach is based on (1) improved (regularized) small-sample point estimates of partial correlation, (2) an exact test of edge inclusion with adaptive estimation of the degree of freedom and (3) a heuristic network search based on false discovery rate multiple testing. Steps (2) and (3) correspond to an empirical Bayes estimate of the network topology.#R##N##R##N#Results: Using computer simulations, we investigate the sensitivity (power) and specificity (true negative rate) of the proposed framework to estimate GGMs from microarray data. This shows that it is possible to recover the true network topology with high accuracy even for small-sample datasets. Subsequently, we analyze gene expression data from a breast cancer tumor study and illustrate our approach by inferring a corresponding large-scale gene association network for 3883 genes.#R##N##R##N#Availability: The authors have implemented the approach in the R package 'GeneTS' that is freely available from http://www.stat.uni-muenchen.de/~strimmer/genets/, from the R archive (CRAN) and from the Bioconductor website.#R##N##R##N#Contact: korbinian.strimmer@lmu.de",An empirical Bayes approach to inferring large-scale gene association networks
"['Wei-Yu Chen', 'Sandeep K. Gupta', 'Melvin A. Breuer']","The authors develop a general methodology to analyze crosstalk effects that are likely to cause errors in deep submicron high-speed circuits. They focus on crosstalk due to capacitive coupling between a pair of lines. Closed form equations are derived that quantify the severity of these effects and describe qualitatively the dependence of these effects on the values of circuit parameters, the rise/fall times of the input transitions, and the skew between the transitions. For noise propagation, they present a new way for predicting the output waveform produced by an inverter due to a nonsquare wave pulse at its input. To expedite the computation of the response of a logic gate to an input pulse, the authors have developed a novel way of modeling such gates by an equivalent inverter. The results of their analysis provide conditions that must be satisfied by a sequence of vectors used for validation of designs as well as post-manufacturing testing of devices in the presence of significant crosstalk. They present data to demonstrate accuracy of their results, including example runs of a test generator that uses these results.",Analytical models for crosstalk excitation and propagation in VLSI circuits
"['Xianlong Zhu', 'Feng Mao', 'Jianxi Huang']","Remote sensing dynamic monitoring of land use can detect the change information of land use and update the current land use map, which is important for rational utilization and scientific management to land resources. This paper discussed the technological procedure of land use dynamic monitoring, including the process of remote sensed images, the information classification and extraction of remote sensed imagery, and analysis of land use changes. Based on SPOT imagery data in three periods, the paper took Beijing city as an example, extracted the land use information during 1986-2004, and the land use changes were required in the period. The object-oriented method was used to extract information, and contrastive method after classification was used to confirm change zones.",Land use Dynamic Monitoring using Multi-Temporal SPOT Data in Beijing City from 1986 to 2004
['James A. Larson'],"VoiceXML is a markup language for creating voice-user interfaces. It uses speech and telephone touchtone recognition for input and prerecorded audio and text-to-speech synthesis (TTS) for output. It's based on the World Wide Web Consortium's (W3C's) Extensible Markup Language (XML) and leverages the Web paradigm for application development and deployment. By having a common language, application developers, platform vendors, and tool providers all can benefit from code portability and reuse. The paper discusses VoiceXML and the W3C speech interface framework.",VoiceXML and the W3C speech interface framework
"['Jennifer L. Wong', 'Farinaz Kourshanfar', 'Miodrag Potkonjak']","ASIC provides more than an order of magnitude advantage in terms of density, speed, and power requirement per gate. However, economic (cost of masks) and technological (deep micron manufacturability) trends favor FPGA as an implementation platform. In order to combine the advantages of both platforms and alleviate their disadvantages, recently a number of approaches, such as structured ASIC/regular fabrics, have been proposed. Our goal is to introduce an approach that has the same objective, but is orthogonal to those already proposed. The idea is to implement several ASIC designs in such a way that they share the datapath, memory structure, and several bottom layers of interconnect, while each design has only a few unique metal layers. We identified and addressed two main problems in our quest to develop a CAD flow for realization of such designs. They are: (i) the creation of the datapath, and (ii) the identification of common and unique interconnects for each design. Both problems are solved optimally using ILP formulations. We assembled a design flow platform using two new programs and the Trimaran and Shade tools. We quantitatively analyzed the advantages and disadvantages of the approach using the Mediabench benchmark suite.",Flexible ASIC: shared masking for multiple media processors
"['Spyridon Antonakopoulos', 'Chandra Chekuri', 'F. Bruce Shepherd', 'Lisa Zhang']","We consider approximation algorithms for buy-at-bulk network design, with the additional constraint that demand pairs be protected against edge or node failures in the network. In practice, the most popular model used in high speed telecommunication networks for protection against failures, is the so-called 1+1 model. In this model, two edge or node-disjoint paths are provisioned for each demand pair. We obtain the first non-trivial approximation algorithms for buy-at-bulk network design in the 1+1 model for both edge and node-disjoint protection requirements. Our results are for the single-cable cost model, which is prevalent in optical networks. More specifically, we present a constant-factor approximation for the single-sink case, and an O(log 3  n) approximation for the multi-commodity case. These results are of interest for practical applications and also suggest several new challenging theoretical problems.",Buy-at-Bulk Network Design with Protection
"['Joon-Myung Kang', 'Hadi Bannazadeh', 'Hesam Rahimi', 'Thomas Lin', 'Mohammad Faraji', 'Alberto Leon-Garcia']","This paper discusses the role of virtualization and software-defined infrastructure (SDI) in the design of future application platforms, and in particular the Future Central Office (CO). A multi-tier computing cloud is presented in which resources in the Smart Edge of the network play a crucial role in the delivery of low-latency and data-intensive applications. Resources in the Smart Edge are virtualized and managed using cloud computing principles, but these resources are more diverse than in conventional data centers, including programmable hardware, GPUs, etc. We propose an architecture for future application platforms, and we describe the SAVI Testbed (TB) design for the Smart Edge. The design features a novel Software-Defined Infrastructure manager that operates on top of OpenStack and OpenFlow. We conclude with a discussion of the implications of the Smart Edge design on the Future CO.",Software-defined infrastructure and the Future Central Office
"['Florent Perronnin', 'Jose A. Rodriguez-Serrano']","The Fisher kernel is a generic framework which combines the benefits of generative and discriminative approaches to pattern classification. In this contribution, we propose to apply this framework to handwritten word-spotting. Given a word image and a keyword generative model, the idea is to generate a vector which describes how the parameters of the keyword model should be modified to best fit the word image.This vector can then be used as the input of a discriminative classifier. We compare the performance of the proposed approach with that of a generative baseline on a challenging real-world dataset of customer letters. When the kernel used by the classifier is linear, the performance improvement is marginal but the proposed system is approximately 15 times faster than the baseline. If we use a non-linear kernel devised for this task, we obtain a 15\% relative reduction of the error but the detector is approximately 15 times slower.",Fisher Kernels for Handwritten Word-spotting
"['Steven R. Shaw', 'Steven B. Leeb', 'Leslie K. Norford', 'Robert W. Cox']","This paper describes a transient event classification scheme, system identification techniques, and implementation for use in nonintrusive load monitoring. Together, these techniques form a system that can determine the operating schedule and find parameters of physical models of loads that are connected to an AC or DC power distribution system. The monitoring system requires only off-the-shelf hardware and recognizes individual transients by disaggregating the signal from a minimal number of sensors that are installed at a central location in the distribution system. Implementation details and field tests for AC and DC systems are presented.",Nonintrusive Load Monitoring and Diagnostics in Power Systems
"['Amirhossein Tavanaei', 'Hossein Sameti', 'Seyyed Hamidreza Mohammadi']",This paper proposes four methods for improving the performance of keyword spotting (KWS) systems. Keyword models are usually created by concatenating the phoneme HMMs and garbage models consist of all phonemes HMMs. We present the results of investigations involving the use of skips in states of keyword HMMs and we focus on improving the hit ratio; then for false alarm reduction in KWS we model the words that are similar to keywords and we create HMMs for highly frequent words. These models help to improve the performance of the filler model. Two post-processing steps based on phoneme and word probabilities are used on the results of KWS to reduce the false alarms. We evaluate the performance of the improved keyword spotting in FarsDat corpus and compare the approaches. The presented techniques depict better performances than the popular KWS systems.,False alarm reduction by improved filler model and post-processing in speech keyword spotting
"['Dragomir Milojevic', 'Trevor E. Carlson', 'K. Croes', 'Riko Radojcic', 'Diana F. Ragett', 'Dirk Seynhaeve', 'Federico Angiolini', 'Geert Van der Plas', 'Pol Marchal']","New technologies for manufacturing 3D Stacked ICs offer numerous opportunities for the design of complex and effcient embedded systems. But these technologies also introduce many design options at system/chip design level, hard to grasp during the complete design cycle. Because of the sequential nature of current design practices, designers are often forced to introduce design margins to meet required specications, resulting in sub-optimal designs. In this paper we introduce new design methodology and practical tool chain, called PathFinding Flow, that can help designers to easily trade-off between different system level design choices, physical design and/or technology options and understand their impact on typical design parameters such as cost, performance and power. Proposed methodology and the tool chain will be demonstrated on a practical case study, involving fairly complex Multi-Processor System-on-Chip using Network-on-Chip for communication medium. With this example we will show how High-Level Synthesis can be used to quickly move from high-level to RTL models, necessary for accurate physical prototyping for both computation and communication. We will also show how the possibility of design iteration, through the mechanism of feedback based on physical information from physical prototyping, can improve design performance. Finally, we will show how we can move in no time from traditional 2D to 3D design and how we can measure benets of such design choice.",Automated Pathfinding tool chain for 3D-stacked integrated circuits: Practical case study
"['Mh Monique Jansen-Vullers', 'van Ca Kees-Jan Dorp', 'Ajm Beulens']","In this paper, an approach to design information systems for traceability is proposed. The paper applies gozinto graph modelling for traceability of the goods flow. A gozinto graph represents a graphical listing of raw materials, parts, intermediates and subassemblies, which a process transforms into an end product, through a sequence of operations. Next, the graphical listing has been translated into a reference data model that is the basis for designing an information system for tracking and tracing. Materials that are modelled this way represent production and/or purchase lots or batches. The composition of a certain end product is then represented through modelling all its constituent materials along with their intermediate relations. By registering all relations between sub-ordinate and super-ordinate material lots, a method of tracking the composition of the end product is obtained. When the entire sequence of operations required for manufacturing an end product adheres to this registering of relations, a multilevel bill of lots can be compiled. That bill of lots then, provides the necessary information to determine the composition of a material item out of component items. These composition data can be used to recall any items having consumed a certain component of specific interest (e.g., deficient), but also to certify product quality or to pro-actively adjust production processes to optimise the product quality in relation to its production characteristics (e.g., scarcity, costs or time).",Managing traceability information in manufacture
"['Nathan Jacobs', 'Robert Pless']","The vast imaging resources available via the Internet are underutilized. We propose to lay the foundation for the use of cameras attached to the Internet, also known as webcams, as free and flexible sensors. Developing an understanding of the relationship between signals in the world and the image variations they cause is critical to this effort. We use this understanding to develop methods to calibrate webcams, to estimate scene properties, and to report the weather.",PhD forum: Calibrating and using the global network of outdoor webcams
['Iouri Belski'],"Engineering knowledge is complex. Therefore, discipline concepts cannot usually be introduced to students all at once. Normally, individual models and behaviours are taught in several courses, by different teaching staff and in separate years of study. This often results in inadequate comprehension of the disciplinepsilas ldquobig picturerdquo - that is, the interrelationship of various concepts, models and behaviours. Students studying electronic circuit design, for example, often overlook important aspects of linear circuit performance. Their knowledge of the effects of steady-state and transient responses requires improved unification. Simple computer-based simulators can significantly help students to establish well-developed holistic views of systems in many engineering disciplines. The 4Screens Web-based simulator has helped hundreds of students in becoming more proficient with electronics systems. It has been developed by a group of undergraduates to make the Four screens model easy to utilise. A student using the simulator can simultaneously display four important characteristics of an electronic system on a computer screen: its algebraic transfer function H(s), systempsilas pole-zero plot, its Bode plots of magnitude and phase, as well as a graph of its time response to a unit step. Questionnaire responses and test performances over the last seven years confirm the effectiveness of the Four screens and the 4Screens Web-based simulator in improving student learning.",Acquiring a Holistic Picture: The 4Screens Web-Based Simulator Helping Students to Unify Behaviours of Electronic Systems
"['Leonardo A. Clarke', 'Joseph Skobla']","In an effort to address students' complaints regarding the tedious and frustrating nature of the practical component of the department's introductory communication course we embarked on a restructuring exercise. The restructuring of this component of the course shows promise based on preliminary results. The students are obtaining higher grades and gave the practical component a more favorable rating. Additionally the restructuring exercise allows for the addition of several more experiments enabling us to cover a broader portion of the course in the practical component. This paper looks at the changes that were made in the restructuring exercise and present preliminary results on the improvement in student's grade which can be attributed to this exercise. Over 16% of students are now obtaining grades over 80% compare to 0% before the restructuring exercise. Results from students' evaluations of the practical component of the course before and after the restructuring exercise is also presented, which shows an effective 24% improvement in the student rating.",Benefits derived from restructuring the practical component of an introductory course in electronic communication systems
"['Mahmut T. Kandemir', 'Alok N. Choudhary', 'Prithviraj Banerjee', 'J. Ramanujam', 'U. Nagaraj Shenoy']","Minimizing communication and synchronization costs is crucial to the realization of the performance potential of parallel computers. This paper presents a general technique which uses a global data-flow framework to optimize communication and synchronization in the context of the one-way communication model. In contrast to the conventional send/receive message-passing communication model, one-way communication is a new paradigm that decouples message transmission and synchronization. In parallel machines with appropriate low-level support, this may open up new opportunities not only to further optimize communication, but also to reduce the synchronization overhead. We present optimization techniques using our framework for eliminating redundant data communication and synchronization operations. Our approach works with the most general data alignments and distributions in languages like High Performance Fortran (HPF) and uses a combination of the traditional data-flow analysis and polyhedral algebra. Empirical results for several scientific benchmarks on a Cray T3E multiprocessor machine demonstrate that our approach is successful in reducing the number of data (communication) and synchronization messages, thereby reducing the overall execution times.",Minimizing data and synchronization costs in one-way communication
"['Lexing Xie', 'Shih-Fu Chang']","Pattern mining algorithms are often much easier applied than quantitatively assessed. In this paper we address the pattern evaluation problem by looking at both the capability of models and the difficulty of target concepts. We use four different data mining models: frequent itemset mining, k-means clustering, hidden Markov model, and hierarchical hidden Markov model to mine 39 concept streams from the a 137-video broadcast news collection from TRECVID-2005. We hypothesize that the discovered patterns can reveal semantics beyond the input space, and thus evaluate the patterns against a much larger concept space containing 192 concepts defined by LSCOM. Results show that HHMM has the best average prediction among all models, however different models seem to excel in different concepts depending on the concept prior and the ontological relationship. Results also show that the majority of the target concepts are better predicted with temporal or combination hypotheses, and there are novel concepts found that are not part of the original lexicon. This paper presents the first effort on temporal pattern mining in the large concept space. There are many promising directions to use concept mining to help construct better concept detectors or to guide the design of multimedia ontology.",Pattern Mining in Visual Concept Streams
"['Luciano Baresi', 'Liliana Pasquale']","Service compositions need to continuously self- adapt to cope with unexpected failures. In this context adaptation becomes a fundamental requirement that must be elicited along with the other functional and non functional requirements. Beside modelling, effective adaptation also demands means to trigger it at runtime as soon as the actual behavior of the composition deviates from stated requirements. This paper extends traditional goal models with adaptive goals to support continuous adaptation. Goals become live, runtime entities whose satisfaction level is dynamically updated. Furthermore, boundary infringement triggers adaptation capabilities. The paper also provides a methodology to trace goals onto the underlying composition, assess goals satisfaction at runtime, and activate adaptation consequently. All the key elements are demonstrated on the definition of the process to control an advanced washing machine.",Adaptive Goals for Self-Adaptive Service Compositions
"['Guillermo Gallego', '??zalp ??zer']","There is a growing consensus that a portfolio of customers with different demand lead times can lead to higher, more regular revenues and better capacity utilization. Customers with positive demand lead times place orders in advance of their needs, resulting inadvance demand information. This gives rise to the problem of finding effective inventory control policies under advance demand information. We show that state-dependent ( s, S) and base-stock policies are optimal for stochastic inventory systems with and without fixed costs. The state of the system reflects our knowledge of advance demand information. We also determine conditions under which advance demand information has no operational value. A numerical study allows us to obtain additional insights and to evaluate strategies to induce advance demand information.",Integrating Replenishment Decisions with Advance Demand Information
"['Ligang Ke', 'Michael W. Marcellin']","Digital magnetic and optical storage systems employing NRZI recording use (d, k) codes. The d-parameter specifies the minimum number of 0's occurring between 1's while the k-parameter specifies the maximum number of 0's between l's. The n-track (d,k) codes (denoted as (d,k;n) codes) are extensions of (d, k) codes for use in multiple-track systems. Instead of imposing each track to individually satisfy both constraints, (d,k;n) codes satisfy the d-constraint in each track individually while relaxing the k-constraint by allowing it to be satisfied jointly by the multiple tracks. Although (d,k;n) codes can provide significant capacity increases over (d, k) codes, they suffer from the fact that a single faulty track can cause loss of synchronization and hence, loss of the data on all tracks. Orcutt and Marcellin (see IEEE Trans. on Inform. Theory, Sept., 1993) introduced n-track (d,k) codes with a redundancy of r (denoted as (d,k;n,r) codes) which allow for r faulty tracks by mandating that all subsets of n-r tracks satisfy the joint k-constraint. We propose a new method to construct (d,k; n, r) codes. These codes have simple encoding and decoding schemes, gain a large part of the capacity increase possible when using (d,k; n,r) codes, and are considerably more robust to faulty tracks. >","A new construction for n-track (d, k) codes with redundancy"
"['Krisztina Boda', 'Thomas Seidel', 'Johann Gasteiger']","De novo design systems provide powerful methods to suggest a set of novel structures with high estimated binding affinity. One deficiency of these methods is that some of the suggested structures could be synthesized only with great difficulty. We devised a scoring method that rapidly evaluates synthetic accessibility of structures based on structural complexity, similarity to available starting materials and assessment of strategic bonds where a structure can be decomposed to obtain simpler fragments. These individual components were combined to an overall score of synthetic accessibility by an additive scheme. The weights of the scoring function components were calculated by linear regression analysis based on accessibility scores derived from medicinal chemists. The calculated values for synthetic accessibility agree with the values proposed by chemists to an extent that compares well with how chemists agree with each other.",Structure and reaction based evaluation of synthetic accessibility
['Sheng Chen'],"The market of metro optical networking has increased rapidly over the last few years. Traditional telecommunication infrastructure has an emphasis on long-haul optical transmission with ultra broadband capacity, relying mostly on large pure Dense Wavelength Division Multiplexing (DWDM) systems. Today, however, metro core optical networks take the major role in provisioning local access services and interconnecting service points of presences (POPs) with long-haul transmission. This represents a pivotal point in business operations of data communication services for service providers and large enterprises. In addition, the upper layer data services completely leans upon the substrate wavelength communication, and hence the survivability and reliability issues in the optical domain are now becoming crucial topics. This paper provides a detailed discussion around the development process of protection technologies in metro core optical transport infrastructure.",Evolution of Protection Technologies in Metro Core Optical Networks
"['Edward J. Coyle', 'James S. Lehnert']",Progress is reported in the design and analysis of spread-spectrum packet radio networks for the factory of the future. This progress includes the accurate analysis of the probability of packet success in a direct-sequence spread-spectrum communication channel and analytical results on the transient behavior of packet radio networks. The first use of these networks will be to provide communications between a transport system controller and a fleet of autonomous guided vehicles. >,Packet radio and the factory of the future
['William R. Lilegdon'],"The FACTOR system is designed to support the effective management of the capacity of manufacturing organization. This philosophy is best described as total capacity management (TCM). The TCM fundamental principle suggests that through a thorough understanding of a system's capacity and the ability to control that capacity, a manufacturing system can profitably and predictably deliver quality products to its customers. This tutorial covers the basic concepts of FACTOR. FACTOR has been applied to engineering, design, scheduling and planning problems within many manufacturing organizations. Topics covered include: the FACTOR modeling constructs, integration with existing production data, the use of FACTOR for schedule creation and adjustment, FACTOR/AIM, and new enhancements to the products.",Manufacturing decision making with FACTOR
['Zuhrieh Shana'],"There is considerable evidence that using technology as an instructional tool improves student learning and educational outcomes (Hanna & de Nooy, 2003). In developing countries, pre-university education focuses on memorization, although meting the mission of AUST requires students to manage technology and to think more independently. This study examines the impact of incorporating a discussion forum on the achievement of university students enrolled in a Distance Education course, Educational Technology Department at Ajman University of Science and Technology (AUST), United Arab Emirates. The study was conducted with 34 students divided into two sections, one a treatment group and one a control group. Both sections were exposed to the same teaching techniques covering the same course material on Distance Education. Four weeks after the course had commenced they were given the same teacher constructed test. However, after the first test, the treated group was exposed to the use of a World Wide Web (WWW) interactive discussion forum. At the end of the semester-long treatment period, a final test was given to both groups, and student scores were analyzed for any statistically significant difference. Questionnaires and interviews were also conducted to see if students had enjoyed the experience. The results of the study indicated that students in both groups showed learning improvement over the course of one semester, but discussion forums had an obvious impact on student achievement and attitude in distance learning/ educational technology course.",Learning with Technology: Using Discussion Forums to Augment a Traditional-Style Class
"['Lisa Bartoli', 'Piero Fariselli', 'Anders Krogh', 'Rita Casadio']","Motivation: The widespread coiled-coil structural motif in proteins is known to mediate a variety of biological interactions. Recognizing a coiled-coil containing sequence and locating its coiled-coil domains are key steps towards the determination of the protein structure and function. Different tools are available for predicting coiled-coil domains in protein sequences, including those based on positionspecific score matrices and machine learning methods. Results: In this article, we introduce a hidden Markov model (CCHMM_PROF) that exploits the information contained in multiple sequence alignments (profiles) to predict coiled-coil regions. The new method discriminates coiled-coil sequences with an accuracy of 97% and achieves a true positive rate of 79% with only 1% of false positives. Furthermore, when predicting the location of coiled-coil segments in protein sequences, the method reaches an accuracy of 80% at the residue level and a best per-segment and perprotein efficiency of 81% and 80%, respectively. The results indicate that CCHMM_PROF outperforms all the existing tools and can be adopted for large-scale genome annotation. Availability: The dataset is available at http://www.biocomp.unibo .it/É?¨lisa/coiled-coils. The predictor is freely available at http://gpcr .biocomp.unibo.it/cgi/predictors/cchmmprof/pred_cchmmprof.cgi. Contact: piero@biocomp.unibo.it",CCHMM_PROF: a HMM-based Coiled-Coil Predictor with Evolutionary Information
"['Joseph Kee-Yin Ng', 'Jane W. S. Liu']","Simulation experiments show that the token ring protocol gave a lower average message delay at low transfer rates, but the token bus protocol gave a better overall performance for applications where only average delay is of interest. On the other hand, in hard real-time systems, the criterion of importance is not the average message delay, but the maximum message delay and the ability to meet deadlines. Slotted ring in this case is a much better protocol than the others because of its low maximum message delay and more predictable message delay. Because of this, and because the average performance of the slotted ring remains good as the size or the transfer rate of the network increases, the slotted ring protocol is preferred over the token ring and token bus protocols for hard real-time systems. >",Performance of local area network protocols for hard real-time applications
"['Ronaldo Fumio Hashimoto', 'Junior Barrera']","A finite subset of Z/sup 2/ is called a structuring element. The paper presents a new and simple algorithm for decomposing a convex structuring element as a sequence of Minkowski additions of a minimum number of subsets of the elementary square (i.e., the 3/spl times/3 square centered at the origin). Besides its simplicity, the advantage of this algorithm over some known algorithms is that it generates a sequence of non necessarily convex subsets, which means subsets with smaller cardinality and consequently faster implementation of the corresponding dilations and erosions. The algorithm is based on algebraic and geometrical properties of Minkowski additions. Theoretical analysis of correctness and computational time complexity are also presented.",A simple algorithm for decomposing convex structuring elements
"['Mitrajit Chatterjee', 'Dhiraj K. Pradhan']","A new design methodology for a pattern generator is proposed, formulated in the context of on-chip BIST. The design methodology is circuit-specific and uses synthesis techniques to design BIST generators. The pattern generator consists of two components: a pseudorandom pattern generator (like an LFSR or, preferably, a GLFSR) and a combinational logic to map the outputs of the pseudorandom pattern generator. This combinational logic is synthesized to produce a given set of target patterns by mapping the outputs of the pseudorandom pattern generator. It is shown that, for a particular CUT, an area-efficient combinational logic block can be designed/synthesized to achieve 100 (or almost 100) percent single stuck-at fault coverage using a small number of test the This method is significantly different from weighted pattern generation and can guarantee testing of all hard-to-detect faults without expensive test point insertion. Experimental results on common benchmark netlists demonstrate that the fault coverage of the proposed pattern generator is significantly higher compared to conventional pattern generation techniques. The design technique for the logic mapper is unique and can be used effectively to improve existing pattern generators for combinational logic and scan-based BIST structures.",A BIST pattern generator design for near-perfect fault coverage
"['Alexander Loskutov', 'Sergei Rybalko', 'Ekaterina Zhuchkova']","The model of the cardiac tissue as a conductive system with two interacting pacemakers and a refractory time is proposed. In the parametric space of the model the phase locking areas are investigated in detail. The obtained results make possible to predict the behavior of excitable systems with two pacemakers, depending on the type and intensity of their interaction and the initial phase. Comparison of the described phenomena with intrinsic pathologies of cardiac rhythms is given.",MODEL OF CARDIAC TISSUE AS A CONDUCTIVE SYSTEM WITH INTERACTING PACEMAKERS AND REFRACTORY TIME
['Pam Mills'],Abstract#R##N##R##N#The Systolic Pixel or Spixel is a novel architecture for an intelligent pixel-based graphics database for geometric-solid models. An algorithm is described which performs visible surface calculations for any complexity of coloured 3-dimensional (3-D) surface and which structures geometric-solid model data in a natural way. The algorithm/architecture of the spixel features a simple set of priority rules acting upon data in nearest neighbour locations and a simple set of movement rules of data to nearest neighbour locations. The spixel is constructed out of identical functional units. These features are attractive for an implementation of the algorithm in Very Large Scale Integration (VLSI).,The Systolic Pixel: A Visible Surface Algorithm for VLSI.
"['Kristina Toutanova', 'Hisami Suzuki']",We study the use of rich syntax-based statistical models for generating grammatical case for the purpose of machine translation from a language which does not indicate case explicitly (English) to a language with a rich system of surface case markers (Japanese). We propose an extension of n-best re-ranking as a method of integrating such models into a statistical MT system and show that this method substantially outperforms standard n-best re-ranking. Our best performing model achieves a statistically significant improvement over the baseline MT system according to the BLEU metric. Human evaluation also confirms the results.,Generating Case Markers in Machine Translation
['Gordon L. St?¨ber'],"The performance of a soft-limiter metric and a quantized soft-limiter metric is evaluated for coded DS/DPSK (direct sequence/differential phase shift keying) in the presence of worst case pulse jamming and background noise. The metrics are easy to implement and do not require jammer state information. Instead they rely on the use of receiver thresholds, which must be adjusted according to the code rate and the received bit-energy-to-background-noise ratio. The performance of the metrics is evaluated by using the cutoff rate criterion and a number of specific convolutional and block codes. It is shown that the metrics can offer a significant soft-decision decoding gain and can perform to within 0.5-1.5 dB of the maximum-likelihood soft-decision metric with perfect jammer state information. >",Soft-limiter receivers for coded DS/DPSK systems
"['June M. Verner', 'Graham Tate', 'Barry Jackson', 'Richard G. Hayward']","Because Function Point Analysis (FPA) has now been in use for a decade, and in spite of its increasing popularity has met with some recent criticisms, it is time to review how appropriate it still is for today's technologies. A critical review of the FPA approach examines in particular the pioneering and continuing work of Albrecht and more recent work by Symons. Technological dependencies in FPA-type metrics are identified and a general model for deriving a new FPA-type metric for a new software technology is given. A model for the calibration of FPA-type metrics for new technologies in terms of a reference technology is also presented. Such calibration is essential for comparative productivity studies. The role of module estimation in exposing parts of the 'anatomy' of the FPA approach is investigated. The derivation and calibration models are applied to a significant case study in which a new FPA-type metric suited to a particular software development technology is derived, calibrated and compared with other published versions of FPA metrics.",Technology Dependence In Function Point Analysis: A Case Study And Critical Review
"['Christian W. Omlin', 'C.L. Giles']","The experimental results in this paper demonstrate that a simple pruning/retraining method effectively improves the generalization performance of recurrent neural networks trained to recognize regular languages. The technique also permits the extraction of symbolic knowledge in the form of deterministic finite-state automata (DFA) which are more consistent with the rules to be learned. Weight decay has also been shown to improve a network's generalization performance. Simulations with two small DFA (/spl les/10 states) and a large finite-memory machine (64 states) demonstrate that the performance improvement due to pruning/retraining is generally superior to the improvement due to training with weight decay. In addition, there is no need to guess a 'good' decay rate. >",Pruning recurrent neural networks for improved generalization performance
"['Bernard Widrow', 'Paul F. Titchener', 'Richard P. Gooch']","In this paper, we present a novel technique for the design of FIR and IIR digital filters. The design approach begins with the specification of a discrete set of arbitrary magnitude and phase characteristics which describe a desired filter response. These frequency domain characteristics are used to create an ideal ""pseudo-filter"" whose impulse response is unknown and possibly non-causal, but whose input/output characteristics can be determined for a finite sum of sinusoids. Time-domain techniques common to adaptive system identification are then used to identify a realizable FIR or IIR digital filter which best matches the pseudo-filter. The advantages of this method include the ability to specify response at arbitrarily-spaced frequencies, to use arbitrary cost weighting, and to apply (possibly non-linear) constraints to the range of the filter coefficients.",Adaptive design of digital filters
['Takafumi Matsumaru'],"This paper presents the result of the experimental examination by ""passing each other"" and ""positional prediction"" in simulated interactive situation between people and mobile robot. We have developed four prototype robots based on four proposed methods for preliminarily announcing and indicating to people the speed and direction of upcoming movement of mobile robot moving on two-dimensional plane. We observed significant difference between when there was a preliminary-announcement and indication (PAI) function and when there was not even in each experiment. Therefore the effect of preliminary-announcement and indication of upcoming operation was declared. In addition the feature and effective usage of each type of preliminary-announcement and indication method were clarified. That is, the method of announcing state of operation just after the present is effective when a person has to judge to which direction he should get on immediately due to the feature that simple information can be quickly transmitted. The method of indicating operations from the present to some future time continuously is effective when a person wants to avoid contact or collision surely and correctly owing to the feature that complicated information can be accurately transmitted. We would like to verify the result in various conditions such as the case that traffic lines are obliquely crossed.",Experimental Examination in simulated interactive situation between people and mobile robot with preliminary-announcement and indication function of upcoming operation
"['Ozgur Ozdemir', 'Murat Torlak']","In recent years, diversity techniques have evolved into highly attractive technology for wireless communications in different forms. For instance, the channel fluctuations of the users in a network are exploited as multiuser diversity by scheduling the user with the best signal-to-noise ratio (SNR). When fading is slow, beamforming at a multiple antenna transmitter is used to induce artificial channel fluctuations to ensure multiuser diversity in the network. Such a beamforming scheme is called opportunistic beamforming since the transmitter uses random beamforming to artificially induce opportunism in the network [1]. Opportunism requires a large number of users in the system in order to reach the performance of the true beamforming that uses perfect channel state information (CSI). In this paper we investigate the benefit of having partial CSI at an opportunistic transmitter. In the investigation, we focus on the maximum normalized SNR scheduling where user's feedback consists of SNR relative to its channel gain. We show that opportunism can be beneficially used to increase the average throughput of the system. Simulations support the analytical average throughput results obtained as the amount of CSI and the number of users vary.",Opportunistic Beamforming over Rayleigh Channels with Partial Side Information
"['Krishnan Srinivasan', 'Karam S. Chatha']","System-level low power scheduling techniques are required for optimizing the performance and power of embedded applications that are mapped to multiprocessor System-on-Chip (SoC) architectures. In this paper, we present an integer linear programming (ILP) formulation that combines loop transformations (pipelining and unrolling) and system-level low power optimization techniques (dynamic voltage scaling (DVS) and power management (DPM)) to minimize the power consumption, while satisfying the period and deadline constraints of the application. We also present three modifications that relax one or more constraints in the optimal formulation in order to obtain smaller run times. We present experimental analysis by applying the formulations on an MPEG decoder algorithm. All results are compared against two existing techniques. Our formulations result in large system-level power reductions (max: 48.2%, min: 15.92%, avg: 31.9%). The modified ILP formulations result in exponential decrease in runtimes, and a corresponding linear degradation in the result quality.",An ILP formulation for system level throughput and power optimization in multiprocessor SoC architectures
"['Ziqiang Feng', 'David Tien']","Although much research has been done in the area of content based image retrieval (CBIR), little progress has been made to fully implement an engine solely based on the search of image content. This paper examines one of the basic problems in pattern recognition which highlights the difficulty in the area of content understanding in CBIR, i.e. the inability of current systems to fully incorporate low level features of image, such as intensity, colour, texture, shape and spatial constraints characteristics, with the high level features such as semantic content. To further the development of content based image processing, semantic algorithms should be combined with low level features and be used to process the image objects.",Enhancement of semantics in CBIR
"['Miquel Miquel', 'Ignacio L??pez-Ribera', 'Miquel R?ˇmia', 'S?˝nia Casillas', 'Antonio Barbadilla', 'C. M. Vicient']","Grass seeds are complex organs composed by multiple tissues and cell types which develop coordinately to produce a viable embryo. The identification of genes involved in seed development is of great interest, but systematic spatial analyses of gene expression on maize seeds at the cell level have not yet been performed. MASISH is an online database holding information for gene expression spatial patterns in maize seeds based on in situ hybridization experiments. The web-based query interface allows the execution of gene queries and provides hybridization images, published references and information of the analyzed genes. Availability: http://masish.uab.cat/ Contact: cvsgmp@cid.csic.es The maize kernel is classified botanically as a caryopsis. In consequence it is a fruit composed by one seed and the remnants of the seed coats and nucellus and is permanently enclosed in the pericarp. The endosperm occupies most of the seed and is basically a storage organ that accumulates starch and proteins. The aleurone layer is part of the endosperm and consists in a continuous layer of large cubical cells which accumulate protein and lipid granules and surrounds most of the endosperm. In the area of the pedicel, which connects the seed to the mother plant, the cells adopt a special morphology, typical of transfer cells, and form the basal transfer cell layer. The embryo consists of an embryonic axis and a single cotyledon, which is called the scutellum. The embryo axis is formed by the plumule, covered by the coleoptile and the radicle, covered by coleorhiza. All these organs are almost completely surrounded by the scutellum, an organ whose major function is to accumulate nutrient reserves, mainly lipids and proteins. A single layer of cells directly in contact with the endosperm, which is called the scutellar epithelium, is important in the digestion and transport of the nutrients from the endosperm to the embryo axis during germination. Both endosperm and embryo derive from the fusion of gametes, but while the embryo is derived from the fertilized egg, triploid endosperm is derived from fertilized polar nuclei. Surrounding the endosperm and embryo lays the pericarp, a protective organ derived from maternal tissues (more information at http://masish.uab.cat/masish/images/maizeseedanatomy.pdf). Full genome sequencing allows the identification of the complete catalog of genes in a species. However, the roles of a high proportion of these genes remain unknown. The description of",MASISH: a database for gene expression in maize seeds
"['Esmaeil Faramarzi', 'Dinesh Rajan', 'Marc P. Christensen']","This paper presents, for the first time, a unified blind method for multi-image super-resolution (MISR or SR), single-image blur deconvolution (SIBD), and multi-image blur deconvolution (MIBD) of low-resolution (LR) images degraded by linear space-invariant (LSI) blur, aliasing, and additive white Gaussian noise (AWGN). The proposed approach is based on alternating minimization (AM) of a new cost function with respect to the unknown high-resolution (HR) image and blurs. The regularization term for the HR image is based upon the Huber-Markov random field (HMRF) model, which is a type of variational integral that exploits the piecewise smooth nature of the HR image. The blur estimation process is supported by an edge-emphasizing smoothing operation, which improves the quality of blur estimates by enhancing strong soft edges toward step edges, while filtering out weak structures. The parameters are updated gradually so that the number of salient edges used for blur estimation increases at each iteration. For better performance, the blur estimation is done in the filter domain rather than the pixel domain, i.e., using the gradients of the LR and HR images. The regularization term for the blur is Gaussian (L2 norm), which allows for fast noniterative optimization in the frequency domain. We accelerate the processing time of SR reconstruction by separating the upsampling and registration processes from the optimization procedure. Simulation results on both synthetic and real-life images (from a novel computational imager) confirm the robustness and effectiveness of the proposed method.",Unified Blind Method for Multi-Image Super-Resolution and Single/Multi-Image Blur Deconvolution
"['Faisal Shah Khan', 'Marek A. Perkowski']","Recent research in generalizing quantum computation from 2-valued qudits to d-valued qudits has shown practical advantages for scaling up a quantum computer. A further generalization leads to quantum computing with hybrid qudits where two or more qudits have different finite dimensions. Advantages of hybrid and d-valued gates (circuits) and their physical realizations have been studied in detail by Muthukrishnan and Stroud [Multi-valued logic gates for quantum computation, Phys. Rev. A 62 (2000) 052309. [10]], Daboul et al. [Quantum gates on hybrid qudits, J. Phys. A Math. Gen. 36 (2003) 2525-2536. [5]], and Bartlett et al. [Quantum encodings in spin systems and harmonic oscillators, Phys. Rev. A 65 (2002) 052316. [17]]. In both cases, a quantum computation is performed when a unitary evolution operator, acting as a quantum logic gate, transforms the state of qudits in a quantum system. Unitary operators can be represented by square unitary matrices. If the system consists of a single qudit, then Tilma et al. [Generalized Euler angle parameterization for SU(N), J. Phys. A Math. Gen. 35 (2002) 10467-10501. [15]] have shown that the unitary evolution matrix (gate) can be synthesized in terms of its Euler angle parametrization. However, if the quantum system consists of multiple qudits, then a gate may be synthesized by matrix decomposition techniques such as QR factorization and the cosine-sine decomposition (CSD). In this article, we present a CSD based synthesis method for n qudit hybrid quantum gates, and as a consequence, derive a CSD based synthesis method for n qudit gates where all the qudits have the same dimension.",Synthesis of multi-qudit hybrid and d-valued quantum logic circuits by decomposition
"['Mark de Berg', 'Marc J. van Kreveld', 'Otfried Schwarzkopf', 'Jack Snoeyink']",Abstract   Let  A ( H ) be the arrangement of a set H of n hyperplanes in  d -space. A  k -flat is a  k -dimensional affine subspace of  d -space. The  zone  of a  k -flat f with respect to H is the set of all faces in  A ( H ) that intersect f. this paper we study some problems on zones of  k -flats. Our most important result is a data structure for point location in the zone of a  k -flat. This structure uses   O  (n      É??  d  2  É??+e    +n     k+e   )   preprocessing time and space and has a query time of O(log 2   n ). We also show how to test efficiently whether two flats are visible from each other with respect to a set of hyperplanes. Then point location in m faces in arrangements is studied. Our data structure for this problem has size   O  (n      É??  d  2  É??+e    m      É??  d  2  É??  d    )   and the query time is O(log 2   n ).,Point location in zones of k -flats in arrangements
"['Yun She', 'Xu She']","The goal of this paper is to address the problem of dynamic optimal battery array management to extend the life of battery array used in the hybrid power source. Unlike previous efforts which mainly solve the problem using the off-line optimization, our design addresses the problem using the idea of É??feedbackÉ?ù. The proposed approach can handle the possible dynamic uncertainty of the hybrid power source introduced by battery failure or insert of a new battery. The detailed battery model and the converter model are derived to facilitate development of dynamic optimal management algorithm. The other goal of this paper is to call possible attention of the control community in potential contribution for newest smart grid technology.",Dynamic optimal battery array management in high energy density fuel cell/battery hybrid power source
"['A. A. Frolov', 'Duè≠an H?ßsek', 'Pavel Polyakov']","Methods for hidden structure of high-dimensional binary data discovery are one of the most important challenges facing machine learning community researchers. There are many approaches in literature that try to solve this hitherto rather ill-defined task. In the present study, we propose a most general generative model of binary data for Boolean factor analysis and introduce new Expectation-Maximization Boolean Factor Analysis algorithm which maximizes likelihood of Boolean Factor Analysis solution. Using the so-called bars problem benchmark, we compare efficiencies of Expectation-Maximization Boolean Factor Analysis algorithm with Dendritic Inhibition neural network. Then we discuss advantages and disadvantages of both approaches as regards results quality and methods efficiency.",Expectation-maximization approach to Boolean factor analysis
"['Mohammad Mahdavi', 'Hosein Farzanehfard']","In this paper, a new bridgeless single-ended primary inductance converter power-factor-correction rectifier is introduced. The proposed circuit provides lower conduction losses with reduced components simultaneously. In conventional PFC converters (continuous-conduction-mode boost converter), a voltage loop and a current loop are required for PFC. In the proposed converter, the control circuit is simplified, and no current loop is required while the converter operates in discontinuous conduction mode. Theoretical analysis and simulation results are provided to explain circuit operation. A prototype of the proposed converter is realized, and the results are presented. The measured efficiency shows 1% improvement in comparison to conventional SEPIC rectifier.",Bridgeless SEPIC PFC Rectifier With Reduced Components and Conduction Losses
"['M.S. Oude Alink', 'Andre B. J. Kokkeler', 'E.A.M. Klumperink', 'Kenneth C. Rovers', 'G.J.M. Smit', 'Bram Nauta']","Quantization plays an important role in many systems where analog-to-digital conversion and/or digital-to-analog conversion take place. If the quantization error is correlated with the input signal, then the spectrum of the quantization error will contain spurious peaks. Although analytical formulas describing this effect exist, numerical evaluation can take much effort. This brief provides approximations for the spurious-free dynamic range (SFDR) of a uniform quantizer with a single sinusoidal input, with and without additive Gaussian noise. It is shown that the SFDR increases by approximately 8 dB/bit, in case there is no noise. Generalizing this result to multitone inputs results in an additional 2 dB/bit per additional tone. Additive Gaussian noise decorrelates the sinusoid(s) and the quantization error, which results in a dramatic increase in SFDR.",Spurious-Free Dynamic Range of a Uniform Quantizer
"['Jinseok Kong', 'Gyungho Lee']","The paper revisits three distributed shared memory (DSM) architectures to clarify them with their binding times for new addresses at the local memory: page fault time, node miss time, and cache miss time. The DSM architectures which have different binding times arrange data in different ways with different overheads at an event of reference. Since a large number of cache misses can occur in a large (relative to the cache size) working set, binding at the page fault time alone cannot efficiently utilize locality of reference at the local memory. In a small working set, most of the addresses bound to the local memory at a node miss time are not effective due to the low cache miss rate. The paper shows that binding at the cache miss time can improve system performance.",Binding time in distributed shared memory architectures
"['Hua Ming', 'Carl K. Chang', 'Katsunori Oyama', 'Hen-I Yang']","While software evolution has been studied extensively in software engineering, few of these efforts have involved a systematic exploration of human epistemological attitudes, such as human desire and intention, as the driving force of software service evolution. Our work proposes a theoretical framework to monitor and reason about human intention and its changes, which in turn can be used to determine how software and services should evolve to be individualized and better serve each user. Extending the Situ framework, we explore the service satisfiability problem through sub-world coverage following Kripke semantics, which enjoys wide application in AI and other fields related to human epistemic reasoning.",Reasoning about Human Intention Change for Individualized Runtime Software Service Evolution
"['Carmen Marincu', 'Barry McMullin']","The Internet is playing a progressively more important part in our dayÉ??toÉ??day life, through its power of making information universally available. People with disabilities have particular opportunities to benefit. Using the Internet in conjunction with dedicated assistive technologies, tasks that were very difficult if not impossible to achieve for people with various types of disability can now be made fully accessible É?? at least, in principle. However, in practice, many online resources and services are still poorly accessible to those with disability due to unsatisfactory Web content design.#N##N#Design of accessible Web content is codified in standards and guidelines of the World Wide Web Consortium (W3C). Conformance with W3CÉ??s Web Content Accessibility Guidelines 1.0 (WCAG) (and/or similar, derivative guidelines) is now the subject of considerable activity, both legal and technical, in many different jurisdictions.#N##N#This paper presents results of a comparative survey of Web accessibility guidelines and HTML standards conformance for samples of Web sites drawn from Ireland, the United Kingdom, France and Germany. It also gives some recommendations on how to improve the accessibility level of Web content.#N##N#A particular conclusion of the study is that the general level of Web accessibility guidelines and HTML standards conformance in all of the samples studied is very poor; and that the pattern of failure is strikingly consistent in the four samples. Although considerable efforts are being made to promote Web accessibility for users with disabilities, this is certainly not yet manifesting itself in improving Web accessibility and HTML validity.",A comparative assessment of Web accessibility and technical standards conformance in four EU states
"['Arun Kumar Singh', 'Vijay Eathakota', 'K. Madhava Krishna', 'Arun. H. Patil']",In this paper we deduce the evolution of a four wheeled active suspension rover from a five wheeled passive suspension rover. The aim of this paper is to design a suspension mechanism which utilizes the advantages of both passive suspension and active suspension rover. Both the design considered here are simpler than the existing suspension mechanisms in the sense that the number of links as wells as the number of joints have been significantly reduced without compromising the climbing capability of the rover. We first analyze the kinematics of the five wheeled rover and its motion pattern while climbing an obstacle and try to deduce the same motion pattern and capability in the four wheeled rover. Both the suspension mechanism consists of two planar closed kinematic chains on each side of the rover. We also deduce the control strategy for the active suspension rover wherein only two actuators are used to control the internal configuration of the rover. To the best of author's knowledge this is the minimum number of actuators required to control the internal configuration of a active suspension while operating on a fully 3D rough terrain. Extensive uneven terrain simulations are performed for both 5-wheeled and 4-wheeled rover and a comparative analysis has been done on maximum coefficient of friction and torque requirements.,Evolution of a four wheeled active suspension rover with minimal actuation for rough terrain mobility
"['Vasilis Chatzigiannakis', 'Georgios Androulidakis', 'K. Pelechrinis', 'Symeon Papavassiliou', 'Vasilis Maglaris']","In this paper, the problem of discovering anomalies in a large-scale network based on the data fusion of heterogeneous monitors is considered. We present a classification of anomaly detection algorithms based on data fusion, and motivated by this classification, the operational principles and characteristics of two different representative approaches, one based on the Demster-Shafer theory of evidence and one based on principal component analysis, are described. The detection effectiveness of these strategies are evaluated and compared under different attack scenarios, based on both real data and simulations. Our study and corresponding numerical results revealed that in principle the conditions under which they operate efficiently are complementary, and therefore could be used effectively in an integrated way to detect a wider range of attacks..",Data fusion algorithms for network anomaly detection: classification and evaluation
"['Masafumi Watari', 'Peng Jiang', 'Atsuo Tachibana', 'Shigehiro Ano']","The BGP routing system is one of the key component of today's Internet infrastructure responsible for carrying data traffic across different Autonomous Systems (ASes). Recently, malformed BGP messages have become a threat to the operational community as they repeatedly cause BGP session resets until identified. However, the identification of the message itself is often difficult in large ISP networks. In this paper, we propose a novel method for real-time identification of these messages by using passively collects BGP messages. Our method focuses on the frequency of observed attributes and values of prefixes advertised by each AS. Based on our heuristics that common attributes are observed at similar time scale, we periodically measure the usage frequency of attributes from BGP messages observed in real-time and mark attributes and values used by minority of the AS as suspicious. We verify the efficiency of our method using BGP data obtained from operational networks.",A Method for Real-Time Identification of Malformed BGP Messages
"['Guodong Sun', 'Guofu Qiao', 'Bin Xu']","With the increasing demands on the remote healthcare and the rich human-machine interacting, body area sensor network (BASN) has been attracting more and more attention. In practice, understanding the link performance and its dynamics in the emerging BASN applications is very important to design reliable, real-time, and energy-efficient protocols. In this paper we study the link characteristics of body area sensor network (BASN) through extensive experiments with very realistic configurations. We evaluate the packet reception ratio, RSSI, LQI, and movement intensity of body under indoor and outdoor environments, all of which can provide direct insights to practical account.",Link Characteristics Measuring in 2.4 GHz Body Area Sensor Networks
"[""Deirdre O'Regan"", 'Anil C. Kokaram']","This paper introduces a novel way to leverage the implicit geometry of sparse local features (e.g. SIFT operator) for the purposes of object detection and segmentation. A two-class Bayesian scheme is used as a framework, and the likelihood is derived from the real-valued classification of machine learning algorithm Gentle AdaBoost, whose output is transformed to a probabilistic distribution using either of two models investigated; Log-Sigmoid or Bi-Gaussian. The main contribution is a novel scheme for the injection of prior contextual spatial information. This occurs on a uniquely designed Markov Random Field defined by Delaunay Tri- angulation of the feature points. Our experiments show that this framework is useful for object detection and segmentation, and we achieve good, mostly invariant results in these tasks.",Implicit spatial inference with sparse local features
"['Anne Flieller', 'Pascal Larzabal', 'H. Clergeot']","In signal subspace parameter estimation techniques, like MUSIC, degradations may occur due to parasite peaks in the spectrum, which may be connected to high sidelobes in the beam pattern or to ambiguities themselves. This paper studies the presence of ambiguities in an array of given planar geometry. We propose a general framework for the analysis and thus we obtain a generalisation of results published by Lo and Marple (1992) and by Proukakis and Manikas (see Proc. ICASSP'94, vol.4, p.549-52, 1994) for rank one and two ambiguities. For rank k/spl ges/3 ambiguities the study is restricted to linear arrays, for which we derive original and synthetic results. We present a geometrical construction that is able to determine all the ambiguous directions which can appear for a given linear array. The method allows determination of any rank ambiguities and for each ambiguous direction set, the rank of ambiguity is obtained. The search is exhaustive. Application of the method requires no assumption for the linear array and is easy to implement. An example is detailed for a non-uniform linear array.",A geometrical framework for the determination of ambiguous directions in subspace methods
"['Christine DeLorenzo', 'Xenophon Papademetris', 'Lawrence H. Staib', 'Kenneth P. Vives', 'Dennis D. Spencer', 'James S. Duncan']","During neurosurgery, nonrigid brain deformation prevents preoperatively acquired images from accurately depicting the intraoperative brain. Stereo vision systems can be used to track cortical surface deformation and update preoperative brain images in conjunction with a biomechanical model. However, these stereo systems are often plagued with calibration error, which can corrupt the deformation estimation. In order to decouple the effects of camera calibration and surface deformation, a framework is needed which can solve for disparate and often competing variables. Game theory, which was developed specifically to handle decision making in this type of competitive environment, has been applied to various fields from economics to biology. In this paper, we apply game theory to cortical surface tracking and use it to infer information about the physical processes of brain deformation and image acquisition.",Nonrigid Intraoperative Cortical Surface Tracking Using Game Theory
"['Kazutoshi Fujikawa', 'Seiwoong Oh', 'Shinji Shimojo', 'Tomohiro Taira', 'Daisuke Kado', 'Hideo Miyahara']","Introduction of motion video including live video into networked virtual reality systems makes virtual spaces more attractive. To handle live video in networked virtual reality systems based on VRML, the scalability of networked virtual reality systems becomes very important on the Internet where the performance of the network and the end systems varies dynamically. We propose a new quality control mechanism suitable for networked virtual reality systems with live video capability. Our approach is to introduce the notion of the importance of presence (IoP) which represents the importance of objects in virtual spaces. According to IoP, the degree of the deterioration of object presentation will be determined in case of the starvation of system resources.",A quality control mechanism for networked virtual reality system with video capability
"['Rajiv D. Banker', 'Robert J. Kauffman']","The development of the information systems (IS) literature inManagement Science during the past 50 years reflects the inception, growth, and maturation of several different research streams. The five research streams we identify incorporate different definitions of the managerial problems that relate to IS, the alternate theoretical perspectives and different methodological paradigms to study them, and the levels of the organization at which their primary results impact managerial practice. Thedecision support and design science research stream studies the application of computers in decision support, control, and managerial decision making. Thevalue of information research stream reflects relationships established based on economic analysis of information as a commodity in the management of the firm. Thehuman-computer systems design research stream emphasizes the cognitive basis for effective systems design. TheIS organization and strategy research stream focuses the level of analysis on the locus of value of the IS investment instead of on the perceptions of a system or its user. Theeconomics of information systems and technology research stream emphasizes the application of theoretical perspectives and methods from analytical and empirical economics to managerial problems involving IS and information technologies (IT). Based on a discussion of these streams, we evaluate the IS literature's core contributions to theoretical and managerial knowledge, and make some predictions about the road that lies ahead for IS researchers.",50th Anniversary Article: The Evolution of Research on Information Systems: A Fiftieth-Year Survey of the Literature in Management Science
"['Jose C. Principe', 'Abir Zahalka']","Matched filtering has been one of the most powerful techniques employed for transient detection. Here we will show that a dynamic neural network outperforms the conventional approach. When the artificial neural network (ANN) is trained with supervised learning schemes there is a need to supply the desired signal for all time, although we are only interested in detecting the transient. In this paper we also show the effects on the detection agreement of different strategies to construct the desired signal. The extension of the Bayes decision rule (0/1 desired signal), optimal in static classification, performs worse than desired signals constructed by random noise or prediction during the background.",Transient Signal Detection with Neural Networks: The Search for the Desired Signal
"['Nikolay Mehandjiev', 'Iain Duncan Stalker', 'Martin Carpenter']","Virtual Enterprises (VEs) bring together expertise and processes of different companies to react to a market opportunity.#N#Here we propose a novel approach to support the collaborative construction and evolution of such VEs and their business processes,#N#comprising a model of the VE, and a set of model construction rules and operators. Our approach is based on the principles#N#of iterative elaboration, devolved decision-making and situatedness, and achieves flexibility by treating the processes of#N#work, coordination and selection in a uniform manner. We argue that certain assumptions behind existing approaches make them#N#unsuitable to the business practices we observed in the target business ecosystem. We then show how the proposed approach#N#can underpin software support for informal business practices of collaborative process construction by manufacturing SMEs.",Recursive construction and evolution of collaborative business processes
"['William W. Cimino', 'G. R. Pennock']",In this paper we study the working space of a six-revolute decoupled robot manipulator. A simple and direct method is presented to obtain the boundaries of the total and primary workspace. The technique is based on finding the limit configurations of the general geometry positioning mechanism of the decoupled manipulator. In order to do this we derive a fourth-order displacement equation in the first joint variable. It is shown that the method only requires the simultaneous solution of two second-order nonlinear equations.,Workspace of a six-revolute decoupled robot manipulator
['Neil McBride'],"The progress of an executive information system project within a manufacturing organization over a period of 9 years is described. The case study illustrates the importance of the interaction between the business environment, the organizational environment and the perceptions and interpretations of events and facts by stakeholders on the success or failure of an information system. It shows the importance of context in the development and implementation of an executive information system and the dynamic nature of the influence of social, economic and technical factors. The reasons for the initial success and the subsequent failure of the EIS within the company are explored from a contingency perspective.",The rise and fall of an executive information system: a case study
"['Anuradha Phalke', 'Susan Lysecky']","The benefits of project-based learning environments are well documented; however, setting up and maintaining these environments can be challenging due to the high cost and expertise associated with these platforms. To alleviate some of these roadblocks, the existing eBlock platform which is composed of fixed function building blocks targeted to enable nonexperts users to easily build a variety of interactive electronic systems is expanded to incorporate newly defined integer-based building blocks to enable a wider range of project possibilities for middle school STEM projects. We discuss various interface possibilities, including initial usability experiments, and summarize our overall experiences and observations in working with local middles school students utilizing the eBlock platform.",Adapting the eBlock Platform for Middle School STEM Projects: Initial Platform Usability Testing
"['Qianqian Zhang', 'Wenbo Wang', 'Wei Lan Huang', 'Jie Zhang']","In this paper, we investigate the performance of heterogeneous networks with multi-antenna cooperative relays. Specifically, threshold-based maximum ratio combining (MRC) and selection combining (SC) schemes are adopted for decoding at the relays and the end-to-end (E2E) error rate performance is analyzed by assuming a Nakagami channel model. Numerical results show that the deployment of multi-antennas can reduce the number of required relay nodes, and thus significantly reduce the system cost. On the other hand, the selection of optimal decoding threshold depends on the number of relay nodes, number of antennas, as well as the average SNR value at the receiver. It is also demonstrated that when the BER requirement is not high, the SC relaying scheme is sufficient to provide satisfactory performance, such that the complexity of relays can be effectively reduced.",Cooperative Multi-Antenna Relaying in Heterogeneous Networks
['Michael Travers'],"Knowledge-based systems often represent their knowledge as a network of interrelated units. Such networks are commonly presented to the user as a diagram of nodes connected by lines. These diagrams have provided a powerful visual metaphor for knowledge representation. However, their complexity can easily become unmanageable as the knowledge base (KB) grows.  This paper describes an alternate visual representation for navigating knowledge structures, based on a virtual museum metaphor. This representation uses nested boxes rather than linked nodes to represent relations. The intricate structure of the knowledge base is conveyed by a combination of position, size, color, and font cues, MUE (Museum Unit Editor) was implemented using this representation to provide a graphic front end for the Cyc knowledge base.",A visual representation for knowledge structures
"[""Carol O'Sullivan""]","Before an environment can be populated with characters, a set of models must first be acquired and prepared. Sometimes it may be possible for artists to create each virtual character individually - for example, if only a small number of individuals are needed, or there are many artists available to create a larger population of characters. However, for most applications that need large and heterogeneous groups or crowds, more automatic methods of generating large numbers of humans, animals or other characters are needed. Fortunately, depending on the context, it is not the case that all types of variety are equally important. Sometimes quite simple methods for creating variations, which do not over-burden the computing resources available, can be as effective as, and perceptually equivalent to, far more resource-intensive approaches. In this paper, we present some recent research and development efforts that aim to create and evaluate variety for characters, in their bodies, faces, movements, behaviours and sounds.",Variety Is the Spice of (Virtual) Life
"['Jing Zhou', 'Qian Wang']","This paper studies the linear quadratic regulation problem for linear discrete time-varying systems with one or multiple delays in control input. This type of input-delay system can be used to model delayed actuation, where the system depends on the input after various (could be more than one) time delays. We provide explicit forms of the finite horizon closed-loop optimal control laws. Numerical examples are also provided to show the performance of our derived control laws.",Finite horizon linear quadratic regulation for linear discrete time-varying systems with single/multiple input delay(s)
"['Lei Liu', 'Guanhua Yan', 'Xinwen Zhang', 'Songqing Chen']","Due to the rapid advancement of mobile communication technology, mobile devices nowadays can support a variety of data services that are not traditionally available. With the growing popularity of mobile devices in the last few years, attacks targeting them are also surging. Existing mobile malware detection techniques, which are often borrowed from solutions to Internet malware detection, do not perform as effectively due to the limited computing resources on mobile devices.#R##N##R##N#In this paper, we propose VirusMeter, a novel and general malware detection method, to detect anomalous behaviors on mobile devices. The rationale underlying VirusMeter is the fact that mobile devices are usually battery powered and any malicious activity would inevitably consume some battery power. By monitoring power consumption on a mobile device, VirusMeter catches misbehaviors that lead to abnormal power consumption. For this purpose, VirusMeter relies on a concise user-centric power model that characterizes power consumption of common user behaviors. In a real-time mode, VirusMeter can perform fast malware detection with trivial runtime overhead. When the battery is charging (referred to as a battery-charging mode), VirusMeter applies more sophisticated machine learning techniques to further improve the detection accuracy. To demonstrate its feasibility and effectiveness, we have implemented a VirusMeter prototype on Nokia 5500 Sport and used it to evaluate some real cellphone malware, including FlexiSPY and Cabir. Our experimental results show that VirusMeter can effectively detect these malware activities with less than 1.5% additional power consumption in real time.",VirusMeter: Preventing Your Cellphone from Spies
"['Dang Duc Pham', 'Giang Binh Tran', 'Son Bao Pham']","This paper presents the first work in the task of author profiling for Vietnamese blogs. This task is important in threat identification and marketing intelligence. We have developed a Vietnamese Blog Profiling framework to automatically predict age, gender, geographic origin and occupation of weblogsÉ?? authors purely based on language use. The experiments on the blogs corpus we collected show very promising results with accuracy of around 80% across all traits.",Author Profiling for Vietnamese Blogs
"['Marc Jansen', 'Philipp Rossmanith', 'Ismail Uzun', 'Ulrich Hoppe']","Based on a requirements analysis for public location information displays in on-campus settings, we describe the implementation of a system called ""SynchroBoard"". Especially, we elaborate on mechanisms to integrate different personal devices in this framework.",Integrating heterogeneous personal devices with public display-based information services
"['Liang Zhang', 'Tak-Wai Lee']","Scheduling packet transmission over wireless links requires quantification of the QoS performance such as delay and packet loss in terms of known system parameters. One of the key issues is how to account for effects of the compensation mechanism on the system's QoS performance. In this paper, we develop a model, namely the two-stage tandem queuing (TSTQ), to characterize the behaviors of packet flows in the system, which applies the wireless fair scheduling with the compensation mechanism. Using queueing analysis, we derive performance parameters: average delay and packet loss rate, in closed-form expressions. These expressions are functions of the source, wireless channel and compensation mechanism parameters. Moreover, the trade-off relationship between delay and packet loss rate is revealed, which is controlled by the parameter of lagging bound. Numerical and simulation results are used to verify the validity of the modeling and analysis work.",Performance analysis of wireless fair queuing algorithms with compensation mechanism
"['Aniket Dubhashi', 'Shashanka Mvs', 'Amrita Pati', 'R Shashank', 'Anil M. Shende']","In this paper, we study the problem of channel assignment for wireless networks modelled as d-dimensional grids. In particular, for d-dimensional square grids, we present optimal assignments that achieve a channel separation of 2 for adjacent stations where the reuse distance is 3 or 4. We also introduce the notion of a colouring schema for d- dimensional square grids, and present an algorithm that assigns colours to the vertices of the grid satisfying the schema constraints.",Channel Assignment for Wireless Networks Modelled as d-Dimensional Square Grids
"['Andrei Alexandru', 'Michael Lujan', 'Craig Pelissier', 'Ben Gamari', 'Frank X. Lee']",Lattice QCD calculations were one of the first applications to show the potential of GPUs in the area of high performance computing. Our interest is to find ways to effectively use GPUs for lattice calculations using the overlap operator. The large memory footprint of these codes requires the use of multiple GPUs in parallel. In this paper we show the methods we used to implement this operator efficiently. We run our codes both on a GPU cluster and a CPU cluster with similar interconnects. We find that to match performance the CPU cluster requires 20-30 times more CPU cores than GPUs.,Efficient Implementation of the Overlap Operator on Multi-GPUs
"['Ali Vahdat', 'Malcolm I. Heywood']","Subspace clustering coevolves the attribute space supporting clusters at the same time as parameterizing the cluster location and combination. Typically, a 'flat' representation is pursued in which individuals describe both the property of individual clusters as well as the combination of clusters used to define the overall solution; hereafter F-ESC. Conversely, a symbiotic approach was recently proposed in which candidate clusters and the combination of clusters are coevolved from independent populations; hereafter S-ESC. In this work a common framework is pursued in order for flat and symbiotic evolutionary subspace clustering to be compared directly. We show that F-ESC might match S-ESC results for data sets with high proportions of cluster support, however, the gap between the two algorithm increases as cluster support decreases.",Flat vs. symbiotic evolutionary subspace clusterings
"['Luca Minin', 'Stefano Marzani', 'Francesco Tesauri', 'Roberto Montanari', 'Caterina Calefato']","In this paper the topic of the augmented cognition applied to the driving task, and specifically to the steering maneuver, is discussed. We analyze how the presence of haptic feedback on the steering wheel could help drivers to perform a visually-guided task by providing relevant information like vehicle speed and trajectory. Starting from these considerations, a Context-Dependant Steering Wheel force feedback (CDSW) had been developed, able to provide to the driver the most suitable feeling of the vehicle dynamics according to the driven context. With a driving simulator the CSWD software had been tested twice and then compared with a traditional steering wheel.",Context-Dependent Force-Feedback Steering Wheel to Enhance Drivers' On-Road Performances
"['P?≠l Benk??', 'Tam?≠s V?≠rady']","The purpose of reverse engineering is to convert a large point cloud into an accurate, fair and consistent CAD model. For a class of conventional engineering objects we have the 'a priori' assumption that the object is bounded exclusively by simple, analytic surfaces. In this case it is possible to generate the model with a minimal amount of user interaction. The key issue is segmentation, i.e., to separate the point cloud into smaller regions, where each can be approximated by a single surface. While this is relatively simple, where the regions are bounded by sharp edges, problems arise when smoothly connected regions need to be separated. The direct segmentation method described in this paper is based on a special sequence of tests, by means of which a large point cloud can be robustly splitted into smaller subregions until no further subdivision is possible. Surfaces of linear extrusion and revolution are also detected. The structure of the smooth, multiple regions is the basis of constrained surface fitting in the final model building phase.","Direct segmentation of smooth, multiple point regions"
"['Jue Wang', 'Jian Peng', 'Daping Zhang']","For the purpose of developing a usable trust relationship between the resource providers (hosts) and the resource consumers (users) in an open computing environment and providing a unified management of the reputation degree of the resource provides and users, a dynamic reputation management model based on Google PageRank (DRMPR) is proposed. The DRMPR system can achieve self-study from a large amount of data and feedback, and with the system obtaining a plenty of resources, the judgment is more accurate. At the end of the paper, an experimental project has been built to demonstrate that the DRMPR can provide a unified management of the reputation degree of the resource provides and users accurately.",Research on Dynamic Reputation Management Model Based on PageRank
"['Johannes Lindblom', 'Erik G. Larsson', 'Eduard A. Jorswieck']","We study the achievable rate region of the multiple-input single-output (MISO) interference channel (IFC), under the assumption that all receivers treat the interference as additive Gaussian noise. We assume the case of two users, and that the channel state information (CSI) is only partially known at the transmitters. Our main result is a characterization of Pareto-optimal transmit strategies, for channel matrices that satisfy a certain technical condition. Numerical examples are provided to illustrate the theoretical results.",Parameterization of the MISO IFC rate region: the case of partial channel state information
"['Peter A. M. Ruijten', 'Diane H.L. Bouten', 'Dana C.J. Rouschop', 'Jaap Ham', 'Cees J. H. Midden']","In human-robot interaction research, much attention is given to the extent to which people perceive humanlike attributes in robots. Generally, the concept anthropomorphism is used to describe this process. Anthropomorphism is defined in different ways, with much focus on either typical human attributes or uniquely human attributes. This difference has caused different measurement tools to be developed. We argue that anthropomorphism can best be described as a continuum ranging from low to high human likeness, and should be measured accordingly. We found that anthropomorphic characteristics can be invariantly ordered according to the ease with which these can be ascribed to robots.",Introducing a rasch-type anthropomorphism scale
"['Kathy Panton', 'Pierluigi Miraglia', 'Nancy Salay', 'Robert C. Kahlert', 'David Baxter', 'Roland Reagan']","The KRAKEN toolset is a comprehensive interface for knowledge acquisition that operates in conjunction with the Cyc knowledge base. The KRAKEN system is designed to allow subject-matter experts to make meaningful additions to an existing knowledge base, without the benefit of training in the areas of artificial intelligence, ontology development, or logical representation. Users interact with KRAKEN via a natural-language interface, which translates back and forth between English and the KB's logical representation language. A variety of specialized tools are available to guide users through the process of creating new concepts, stating facts about those concepts, and querying the knowledge base. KRAKEN has undergone two independent performance evaluations. In this paper we describe the general structure and several of the features of KRAKEN, focussing on key aspects of its functionality in light of the specific knowledge-formation and acquisition challenges they are intended to address.",Knowledge formation and dialogue using the KRAKEN toolset
"['Arati Manjeshwar', 'Dharma P. Agrawal']","Wireless sensor networks with thousands of tiny sensor nodes, are expected to find wide applicability and increasing deployment in coming years, as they enable reliable monitoring and analysis of the environment. In this paper, we propose a hybrid routing protocol (APTEEN) which allows for comprehensive information retrieval. The nodes in such a network not only react to time-critical situations, but also give an overall picture of the network at periodic intervals in a very energy efficient manner. Such a network enables the user to request past, present and future data from the network in the form of historical, one-time and persistent queries respectively. We evaluated the performance of these protocols and observe that these protocols are observed to outperform existing protocols in terms of energy consumption and longevity of the network.",APTEEN: A Hybrid Protocol for Efficient Routing and Comprehensive Information Retrieval in Wireless Sensor Networks
"['Saurav Subedi', 'Yimin Zhang', 'Moeness G. Amin', 'Braham Himed']","Multi-static passive radar (MPR) systems typically use narrowband signals and operate under weak signal conditions, making them difficult to reliably estimate motion parameters of ground moving targets. On the other hand, the availability of multiple spatially separated illuminators of opportunity provides a means to achieve multi-static diversity and overall signal enhancement. In this paper, we consider the problem of estimating motion parameters, including velocity and acceleration, of multiple closely located ground moving targets in a typical MPR platform with focus on weak signal conditions, where traditional time-frequency analysis-based methods become unreliable or infeasible. The underlying problem is reformulated as a sparse signal reconstruction problem in a discretized parameter search space. While the different bistatic links have distinct Doppler signatures, they share the same set of motion parameters of the ground moving targets. Therefore, such motion parameters act as a common sparse support to enable the exploitation of group sparsity-based methods for robust motion parameter estimation. This provides a means of combining signal energy from all available illuminators of opportunity and, thereby, obtaining a reliable estimation even when each individual signal is weak. Because the maximum likelihood (ML) estimation of motion parameters involves a multi-dimensional search and its performance is sensitive to target position errors, we also propose a technique that decouples the target motion parameters, yielding a two-step process that sequentially estimates the acceleration and velocity vectors with a reduced dimensionality of the parameter search space. We compare the performance of the sequential method against the ML estimation with the consideration of imperfect knowledge of the initial target positions. The Cramer-Rao bound (CRB) of the underlying parameter estimation problem is derived for a general multiple-target scenario in an MPR system. Simulation results are provided to compare the performance of the sparse signal reconstruction-based methods against the traditional time-frequency-based methods as well as the CRB.",Motion parameter estimation of multiple ground moving targets in multi-static passive radar systems
"['Robin Strand', 'Peer Stelldinger']","The well-known marching cubes algorithm is modified to apply to the face-centered cubic (fee) grid. Thus, the local configurations that are considered when extracting the local surface patches are not cubic anymore. This paper presents three different partitionings of the fee grid to be used for the local configurations. The three candidates are evaluated theoretically and experimentally and compared with the original marching cubes algorithm. It is proved that the reconstructed surface is topologically equivalent to the surface of the original object when the surface of the original object that is digitized is smooth and a sufficiently dense fee grid is used.",Topology Preserving Marching Cubes-like Algorithms on the Face-Centered Cubic Grid
"['Julian Fierrez-Aguilar', 'Loris Nanni', 'Jaime Lopez-Pe?Òalba', 'Javier Ortega-Garcia', 'Davide Maltoni']","An on-line signature verification system exploiting both local and global information through decision-level fusion is presented. Global information is extracted with a feature-based representation and recognized by using Parzen Windows Classifiers. Local information is extracted as time functions of various dynamic properties and recognized by using Hidden Markov Models. Experimental results are given on the large MCYT signature database (330 signers, 16500 signatures) for random and skilled forgeries. Feature selection experiments based on feature ranking are carried out. It is shown experimentally that the machine expert based on local information outperforms the system based on global analysis when enough training data is available. Conversely, it is found that global analysis is more appropriate in the case of small training set size. The two proposed systems are also shown to give complementary recognition information which is successfully exploited using decision-level score fusion.",An on-line signature verification system based on fusion of local and global information
"['Steffen Eikenberry', 'Craig J. Thalhauser', 'Yang Kuang']","Malignant melanoma is a cancer of the skin arising in the melanocytes. We present a mathematical model of melanoma invasion into healthy tissue with an immune response. We use this model as a framework with which to investigate primary tumor invasion and treatment by surgical excision. We observe that the presence of immune cells can destroy tumors, hold them to minimal expansion, or, through the production of angiogenic factors, induce tumorigenic expansion. We also find that the tumorÉ??immune system dynamic is critically important in determining the likelihood and extent of tumor regrowth following resection. We find that small metastatic lesions distal to the primary tumor mass can be held to a minimal size via the immune interaction with the larger primary tumor. Numerical experiments further suggest that metastatic disease is optimally suppressed by immune activation when the primary tumor is moderately, rather than minimally, metastatic. Furthermore, satellite lesions can become aggressively tumorigenic upon removal of the primary tumor and its associated immune tissue. This can lead to recurrence where total cancer mass increases more quickly than in primary tumor invasion, representing a clinically more dangerous disease state. These results are in line with clinical case studies involving resection of a primary melanoma followed by recurrence in local metastases.","Tumor-Immune Interaction, Surgical Treatment, and Cancer Recurrence in a Mathematical Model of Melanoma"
"['David Liebowitz', 'Andrew Zisserman']","We describe the geometry constraints and algorithmic implementation for metric rectification of planes. The rectification allows metric properties, such as angles and length ratios, to be measured on the world plane from a perspective image. The novel contributions are: first, that in a stratified context the various forms of providing metric information, which include a known angle, two equal though unknown angles, and a known length ratio; can all be represented as circular constraints on the parameters of an affine transformation of the plane-this provides a simple and uniform framework for integrating constraints; second, direct rectification from right angles in the plane; third, it is shown that metric rectification enables calibration of the internal camera parameters; fourth, vanishing points are estimated using a Maximum Likelihood estimator; fifth, an algorithm for automatic rectification. Examples are given for a number of images, and applications demonstrated for texture map acquisition and metric measurements.",Metric rectification for perspective images of planes
"['Yi Cheng', 'Dharma P. Agrawal']","Due to the resource constraints, pre-distributing secret keys into sensor nodes before they are deployed is an applicable approach to achieve information security in wireless sensor networks. Several key pre-distribution schemes have been proposed in literature to establish pairwise keys between sensor nodes; they are either too complicated, or insecure for some common attacks. To address these weaknesses, we propose an improved pairwise key establishment mechanism for wireless sensor networks in this paper. Compared with existing approaches, our scheme has better network resilience against node capture attack. Analysis and simulation results show that our scheme performs better than earlier proposed schemes in terms of network connectivity, key storage overhead, maximum supported network size, computational and communication overheads",Improved Pairwise Key Establishment for Wireless Sensor Networks
"['Richard Statman', 'Henk Barendregt']","This theoretical pearl is about the closed term model of pure untyped lambda-terms modulo ?˝-convertibility. A consequence of one of the results is that for arbitrary distinct combinators (closed lambda terms) M, MÉ?˝, N, NÉ?˝ there is a combinator H such thatdisplay formula hereThe general result, which comes from Statman (1998), is that uniformly r.e. partitions of the combinators, such that each É??blockÉ?? is closed under ?˝-conversion, are of the form {HÉ??1{M}}MÉ??????. This is proved by making use of the idea behind the so-called Plotkin-terms, originally devised to exhibit some global but non-uniform applicative behaviour. For expository reasons we present the proof below. The following consequences are derived: a characterization of morphisms and a counter-example to the perpendicular lines lemma for ?˝-conversion.",Applications of Plotkin-terms: partitions and morphisms for closed terms
"['Fernando J. Velez', 'Nuno Anastacio', 'Francisco Merca', 'O. Cabrai']","An overview of all-IP enhanced Universal Mobile Telecommunication System, E-UMTS, service needs in the business city centre, BCC, scenario is first presented. Then, E-UMTS traffic generation and activity models are described and characterised. System level simulations are carried out and the enhanced performance is demonstrated based in a single quality parameter, which simultaneously accounts for call blocking and handover failure probabilities. End-to-end delays do not present a limitation. By considering a grade of service of 1% for the quality parameter, and different hypothesis for costs and prices, an optimum coverage distance is obtained around ~200-425 m, which maximises the supported throughput per km 2 . However, results for the profit in percentage indicates that coverage distances in the range 395-425 m should be used in BCC.",Cost/Revenue Optimisation of Multi-Service Cellular Planning for City Centre E-UMTS
"['Tolga Ayav', 'Pascal Fradet', 'Alain Girault']","We present a formal approach to implement and certify fault-tolerance in real-time embedded systems. The fault-intolerant initial system consists of a set of independent periodic tasks scheduled onto a set of fail-silent processors. We transform the tasks such that, assuming the availability of an additional spare processor, the system tolerates one failure at a time (transient or permanent). Failure detection is implemented using heartbeating, and failure masking using checkpointing and roll-back. These techniques are described and implemented by automatic program transformations on the tasks' programs. The proposed formal approach to fault-tolerance by program transformation highlights the benefits of separation of concerns and allows us to establish correctness properties.",Implementing fault-tolerance in real-time systems by automatic program transformations
"['Dimitar Hristovski', 'Borut Peterlin', 'Joyce A. Mitchell', 'Susanne M. Humphrey']","Summary  We present BITOLA, an interactive literature-based biomedical discovery support system. The goal of this system is to discover new, potentially meaningful relations between a given starting concept of interest and other concepts, by mining the bibliographic database MEDLINE ?? . To make the system more suitable for disease candidate gene discovery and to decrease the number of candidate relations, we integrate background knowledge about the chromosomal location of the starting disease as well as the chromosomal location of the candidate genes from resources such as LocusLink and Human Genome Organization (HUGO). BITOLA can also be used as an alternative way of searching the MEDLINE database. The system is available at http://www.mf.uni-lj.si/bitola/.",Using literature-based discovery to identify disease candidate genes
['James G. Williams'],"A formal definition is given of nondisclosure for a computing system and the author describes a functional decomposition of the system into two kinds of activities, namely, the selection and execution of subject instructions. Security requirements for each of the two resulting subsystems are given, and it is proved that, if each subsystem satisfies its security requirements, then the entire system satisfies the given nondisclosure property. Finally, in order to show how security can be enforced by the system, an access-control model is given for subject-instruction processing that guarantees satisfaction of the given security requirements for subject-instruction processing. >",Modeling nondisclosure in terms of the subject-instruction stream
"['Ronald Brown', 'George Janelidze']","The authors have used generalised Galois Theory to construct a homotopy double groupoid of a surjective ??bration of Kan simplicial sets. Here we apply this to construct a new homotopy double groupoid of a map of spaces, which includes constructions by others of a 2-groupoid, cat 1 -group or crossed module. An advantage of our construction is that the double groupoid can give an algebraic model of a foliated bundle. 1",Galois theory and a new homotopy double groupoid of a map of spaces
"['J. Diaz Alonso', 'E. Ros Vidal', 'A. Rotter', 'Michael M?¨hlenberg']","Overtaking and lane changing are very dangerous driving maneuvers due to possible driver distraction and blind spots. We propose an aid system based on image processing to help the driver in these situations. The main purpose of an overtaking monitoring system is to segment the rear view and track the overtaking vehicle. We address this task with an optic-flow-driven scheme, focusing on the visual field in the side mirror by placing a camera on top of it. When driving a car, the ego-motion optic-flow pattern is very regular, i.e., all the static objects (such as trees, buildings on the roadside, or landmarks) move backwards. An overtaking vehicle, on the other hand, generates an optic-flow pattern in the opposite direction, i.e., moving forward toward the vehicle. This well-structured motion scenario facilitates the segmentation of regular motion patterns that correspond to the overtaking vehicle. Our approach is based on two main processing stages: First, the computation of optical flow in real time uses a customized digital signal processor (DSP) particularly designed for this task and, second, the tracking stage itself, based on motion pattern analysis, which we address using a standard processor. We present a validation benchmark scheme to evaluate the viability and robustness of the system using a set of overtaking vehicle sequences to determine a reliable vehicle-detection distance.",Lane-Change Decision Aid System Based on Motion-Driven Vehicle Tracking
"['Dewayne E. Perry', 'Steven S. Popovich']","There are four fundamental aspects of use and reuse in building systems from components: conceptualization, retrieval, selection and correct use. The most important barrier to use and reuse is that of conceptualization. The Inscape environment is a specification-based software development environment integrated by the constructive use of formal interface specifications. The purpose of the formal interface specifications and the semantic interconnections is to make explicit the invisible semantic dependencies that result in conventionally-built systems. The important ingredient provided by Inquire in conceptualization, retrieval, selection and use is the set of predicates that describe the semantics of the elements in the interface. These predicates define the abstractions that are germane to the module interface and describe the properties of data objects and the assumptions and results of operations in a module. Use and reuse of components is based on a component's ability to provide needed semantics at a particular point in a system. It is the purpose of Inquire, the browser and predicate-based search mechanism, to aid both the environment and the user in the search for the components that will provide the desired predicates that are required to build and evolve an implementation correctly. >",Inquire: predicate-based use and reuse
"['Martial Coulon', 'Ananthram Swami']","We address the problem of estimating changes in fractional integrated ARMA (FARIMA) processes. These changes may be in the long range dependence (LRD) parameter or the ARMA parameters. The signal is divided into ""elementary"" segments: the objective is then to estimate the segments in which the changes occur. This estimation is achieved by minimizing a penalized least-squares criterion based on the parameter estimates computed in each segment. The optimization problem is then solved using a dynamic programming algorithm. Simulation results on synthetic data (computer network traffic) are reported.",Least squares detection of multiple changes in fractional ARIMA processes
"['Steve Gu', 'Ying Zheng', 'Carlo Tomasi']","We present a nonparametric and efficient method for shape localization that improves on the traditional sub-window search in capturing the fine geometry of an object from a small number of feature points. Our method implies that the discrete set of features capture more appearance and shape information than is commonly exploited. We use the a-complex by Edelsbrunner et al. to build a filtration of simplicial complexes from a user-provided set of features. The optimal value of a is determined automatically by a search for the densest complex connected component, resulting in a parameter-free algorithm. Given K features, localization occurs in O(K logK) time. For VGA-resolution images, computation takes typically less than 10 milliseconds. We use our method for interactive object cut, with promising results.",Shape from point features
"['James R. Riehl', 'Ming Cao']","We present a decentralized controller to keep a group of agents at equal spacing while moving around the perimeter of a loop defined by a constant distance from a convex polygon, motivated by a cooperative containment problem. Traveling at constant speed, the agents achieve and maintain their formation by using small steering adjustments to equalize the distance between themselves and their respective leading and following neighbors. Since the formation moves around a common loop, an agent can move forward or back in formation by respectively steering slighting inside or outside the reference loop. These adjustments are controlled with the use of variable radius parameters for each agent that are shown to converge to the desired reference loop as equal spacing is achieved. We show that the proposed controller renders the desired formation locally asymptotically stable and provide simulations to demonstrate the performance of the controller for an example scenario in which the formation must recover from the loss of an agent.",Formation control for cooperative containment of a diffusing substance
"['Enrique Herrera-Viedma', 'L. Mart??nez', 'Francisca Hornos Mata', 'Francisco Chiclana']","The group decision-making framework with linguistic preference relations is studied. In this context, we assume that there exist several experts who may have different background and knowledge to solve a particular problem and, therefore, different linguistic term sets (multigranular linguistic information) could be used to express their opinions. The aim of this paper is to present a model of consensus support system to assist the experts in all phases of the consensus reaching process of group decision-making problems with multigranular linguistic preference relations. This consensus support system model is based on i) a multigranular linguistic methodology, ii) two consensus criteria, consensus degrees and proximity measures, and iii) a guidance advice system. The multigranular linguistic methodology permits the unification of the different linguistic domains to facilitate the calculus of consensus degrees and proximity measures on the basis of experts' opinions. The consensus degrees assess the agreement amongst all the experts' opinions, while the proximity measures are used to find out how far the individual opinions are from the group opinion. The guidance advice system integrated in the consensus support system model acts as a feedback mechanism, and it is based on a set of advice rules to help the experts change their opinions and to find out which direction that change should follow in order to obtain the highest degree of consensus possible. There are two main advantages provided by this model of consensus support system. Firstly, its ability to cope with group decision-making problems with multigranular linguistic preference relations, and, secondly, the figure of the moderator, traditionally presents in the consensus reaching process, is replaced by the guidance advice system, and in such a way, the whole group decision-making process is automated",A Consensus Support System Model for Group Decision-Making Problems With Multigranular Linguistic Preference Relations
"['Tae-Hyung Kim', 'Carl K. Chang']","Petri net has been widely used for modeling software systems due to its mathematical soundness and support of various tools. In many cases, performance-related analyses of Petri net for a software system need to consider the resource limitations caused by a specific platform. In terms of aspect-oriented approach, the interference of various resources scattered across a software system can be regarded as a specific concern that is only necessary during its performance-related analysis process. To capture the interactions of resources within the Petri net-based software architectural model, we propose an aspect-oriented resource composition model and the XML-based representation language called the resource extension markup language (ReML) for its description. In our approach, one or more resource composition models can be developed and described in ReML separately from the development of base software architectural models. Through the weaving process, a resource composition model can be applied to extend several Petri net-based software architectural models. The resource weaver generates an augmented Petri net used for analyzing performance characteristics of the extended software architectural model with resource interactions. Our aspect-oriented approach facilitates selecting an optimal or superior resource composition model for a software architectural model, and vice versa, which is illustrated using two exemplary server models.",An Aspect-Oriented Approach to Resource Composition in Petri net-based Software Architectural Models
['Janos J. Sarbo'],A theory of concept (Galois) lattices was first introduced by Wille. An extension of his work to simple structures called concept sublattices has also been published. This paper shows that concept sublattices can be applied to (i) determining subsumption of specifications and (ii) decomposing specifications in terms of others. I show that the latter application of the theory may provide us with new conceptualizations of a specification.,Building sub-knowledge bases using concept lattices
"['Alessandro Casavola', 'Giuseppe Franze']","In this paper we present a distributed supervisory strategy for load/frequency control problems in networked multi-area power systems. Coordination between the control center and the areas is accomplished via data networks subject to communication latency which is modelled by time-varying time-delay. The aim here is at finding strategies able of reconfiguring, whenever necessary in response to unexpected load changes and/or faults, the nominal set-points on frequency and generated power of each area so that viable evolutions arise for the overall networked system and a new suitable equilibrium is reached.",Coordination strategies for networked control systems: A power system application
"['Tao Wu', 'Arun K. Somani']","The effects of an attack connection can propagate quickly to different parts of an all-optical transparent network. Such attacks affect the normal traffic and can either cause service degradation or outright service denial. Quick detection and localization of an attack source can avoid losing large amounts of data in an all-optical network. Attack monitors can collect the information from connections and nodes for diagnostic purpose. However, to detect attack sources, it is not necessary to put monitors at all nodes. Since those connections affected by the attack connection would provide valuable information for diagnosis, we show that by placing a relatively small number of monitors on a selected set of nodes in a network is sufficient to achieve the required level of performance. However, the actual monitor placement, routing, and attack diagnosis are challenging problems that need research attention. In this paper, we first develop our models of crosstalk attack and monitor node. With these models, we prove the necessary and sufficient condition for one-crosstalk-attack diagnosable networks. Next, we develop a scalable diagnosis method which can localize the attack connection efficiently with sparse monitor nodes in the network.",Cross-talk attack monitoring and localization in all-optical networks
"['Clement T. Yu', 'Tsang Ming Jiang']","The G-K-D tree (generalized K-D tree) method aims at reducing the average number of data page accesses per query, but it ignores the cost of index search. The authors propose two adaptive algorithms that take into consideration both data page access cost and index page access cost. It attempts to find a minimum total cost. Experimental results indicate that the proposed algorithms are superior to the G-K-D tree method. >",Adaptive algorithms for balanced multidimensional clustering
"['Simone Madeo', 'Riccardo Pelliccia', 'Claudio Salvadori', 'Jes?ßs Mart??nez del Rinc??n', 'Jean-Christophe Nebel']","The aim of this paper is to demonstrate the applicability and the effectiveness of a computationally demanding stereo-matching algorithm in different low-cost and low-complexity embedded devices, by focusing on the analysis of timing and image quality performances. Various optimizations have been implemented to allow its deployment on specific hardware architectures while decreasing memory and processing time requirements: (1) reduction of color channel information and resolution for input images; (2) low-level software optimizations such as parallel computation, replacement of function calls or loop unrolling; (3) reduction of redundant data structures and internal data representation. The feasibility of a stereo vision system on a low-cost platform is evaluated by using standard datasets and images taken from infra-red cameras. Analysis of the resulting disparity map accuracy with respect to a full-size dataset is performed as well as the testing of suboptimal solutions.",An optimized stereo vision implementation for embedded systems: application to RGB and Infra-Red images
"['Giuseppe Agnello', 'Richard M. Dansereau']","VoIP conferencing with a centralized speech mixing bridge introduces additional end-to-end latency into packetized voice communication. This paper investigates how full tandem speech decoding, time-domain mixing, speech encoding cycle can be circumvented by instead extracting the coded speech parameters and performing the speech packet mixing without time-domain reconstruction. By mixing through coded speech parameters, we show that nearly an 85% decrease in computational complexity can be achieved over full tandem mixing of two speakers for G.722.2, thus significantly reducing the packet latency at the centralized speech mixing bridge. For the G.722.2 parametric mixer presented, linear prediction coefficients (LPCs), pitch lags, fixed codebooks, and gains, are extracted (without full speech reconstruction) from the encoded bit stream, mixed, and then re-encoded instead of the full tandem approach where each speech frame must be fully reconstructed. We investigate the mixing in two scenarios: i) mix two 12.65 kbps G.722.2 speech streams at a mixed rate of 12.65 kbps, and ii) mix two 12.65 kbps G.722.2 speech streams at a mixed rate of 18.25 kbps. PAMS is used to evaluate the speech quality of the parametric mixer, resulting in an average distortion of 0.37 MOS (compared to tandem mixing) as shown by simulations using typical conversation models.",Parametric Mixing for Centralized VOIP Conferencing using ITU-T Recommendation G.722.2
"['William J. McIver', 'Roger King']","A likely trend in the development of future CAD, CASE and office information systems will be the use of object-oriented database systems to manage their internal data stores. The entities that these applications will retrieve, such as electronic parts and their connections or customer service records, are typically large complex objects composed of many interconnected heterogeneous objects, not thousands of tuples. These applications may exhibit widely shifting usage patterns due to their interactive mode of operation. Such a class of applications would demand clustering methods that are appropriate for clustering large complex objects and that can adapt on-line to the shifting usage patterns. While most object-oriented clustering methods allow grouping of heterogeneous objects, they  are usually static and can only be changed off-line. We present one possible architecture for performing complex object reclustering in an on-line manner that is adaptive to changing usage patterns. Our architecture involves the decomposition of a clustering method into concurrently operating components that each handle one of the fundamental tasks involved in reclustering, namely statistics collection, cluster analysis, and reorganization. We present the results of an experiment performed to evaluate its behavior. These results show that the average miss rate for object accesses can be effectively reduced using a combination of rules that we have developed for deciding when cluster analyses and reorganizations should be performed.","Self-adaptive, on-line reclustering of complex object data"
"['W. Huang', 'Ana Milanova', 'Werner Dietl', 'Michael D. Ernst']","Reference immutability ensures that a reference is not used to modify the referenced object, and enables the safe sharing of object structures. A pure method does not cause side-effects on the objects that existed in the pre-state of the method execution. Checking and inference of reference immutability and method purity enables a variety of program analyses and optimizations. We present ReIm, a type system for reference immutability, and ReImInfer, a corresponding type inference analysis. The type system is concise and context-sensitive. The type inference analysis is precise and scalable, and requires no manual annotations. In addition, we present a novel application of the reference immutability type system: method purity inference.   To support our theoretical results, we implemented the type system and the type inference analysis for Java. We include a type checker to verify the correctness of the inference result. Empirical results on Java applications and libraries of up to 348kLOC show that our approach achieves both scalability and precision.",Reim & ReImInfer: checking and inference of reference immutability and method purity
"['Michel Fliess', 'C??dric Join']","We are introducing a model-free control and a control with a restricted model for finite-dimensional complex systems. This control design may be viewed as a contribution to ``intelligent'' PID controllers, the tuning of which becomes quite straightforward, even with highly nonlinear and/or time-varying systems. Our main tool is a newly developed numerical differentiation. Differential algebra provides the theoretical framework. Our approach is validated by several numerical experiments.",MODEL-FREE CONTROL AND INTELLIGENT PID CONTROLLERS: TOWARDS A POSSIBLE TRIVIALIZATION OF NONLINEAR CONTROL?
"['Efr??n Mezura-Montes', 'Jes?ßs Vel?≠zquez-Reyes', 'Carlos A. Coello Coello']","In this paper, we present a Differential-Evolution based approach to solve constrained optimization problems. The aim of the approach is to increase the probability of each parent to generate a better offspring. This is done by allowing each solution to generate more than one offspring but using a different mutation operator which combines information of the best solution in the population and also information of the current parent to find new search directions. Three selection criteria based on feasibility are used to deal with the constraints of the problem and also a diversity mechanism is added to maintain infeasible solutions located in promising areas of the search space. The approach is tested in a set of test problems proposed for the special session on Constrained Real Parameter Optimization. The results obtained are discussed and some conclusions are established.",Modified Differential Evolution for Constrained Optimization
"['Rastin Pries', 'Dirk Staehle', 'Daniel Marsico']","WiMAX (Worldwide Interoperability for Microwave Access) is a wireless access technology that aims to provide last mile wireless broadband access for fixed and mobile users as an alternative to the wired DSL and cable access. It is specified in the IEEE 802.16 standard. The standard defines several possible bandwidth request methods that can be implemented in an actual deployment of a WiMAX network. In this paper, we will study the performance of two different bandwidth request mechanisms, namely piggyback and broadcast requests and will show in which situations piggybacking performs better than the contention based broadcast bandwidth requests.",Performance Evaluation of Piggyback Requests in IEEE 802.16
"['Soo-Yong Lee', 'Jae-Bok Song']","This paper presents mobile robot localization using coded infrared light as artificial landmarks. Different from RFID, identification using infrared light has highly deterministic characteristics. IRID(infrared identification) is implemented with IR LEDs and photo transistors. By putting several infrared LEDs on the ceiling, the floor is divided into several sectors and each sector is set to have a unique identification. The coded infrared light tells which sector the robot is in, but the size of the uncertainty is still too large if the sector size is large, which usually occur. Dead-reckoning provides the estimated robot configuration but the error is getting accumulated as the robot travels. This paper presents an algorithm which fuses both the encoder and the IRID information so that the size of the uncertainty becomes smaller. It also introduces a framework which can be used with other types of the artificial landmarks. The characteristics of the developed IRID and the proposed algorithm are verified from the experiments.",Use of coded infrared light as artificial landmarks for mobile robot localization
"['Daniel Teodorescu', 'Tudorel Andrei']","In the last two decades international collaboration in the Eastern European academic communities has strongly intensified. Scientists from developed countries within the European Union play a key role in stimulating the international collaboration of academics in this region. In addition, many of the research projects that engage East-European scholars are only possible in the framework of the large European programmes. The present study focuses on the role of EU and other developed nations as a partner of these countries and the analysis of the performance of collaborative research as reflected by the citation impact of internationally co-authored publications.",The growth of international collaboration in East European scholarly communities: a bibliometric analysis of journal articles published between 1989 and 2009
"['F. Feitzinger', 'Timo Hylla', 'Ekkehard W. Sachs']",In this paper we consider the numerical solution of the algebraic Riccati equation using Newton's method. We propose an inexact variant which allows one control the number of the inner iterates used in an iterative solver for each Newton step. Conditions are given under which the monotonicity and global convergence result of Kleinman also hold for the inexact Newton iterates. Numerical results illustrate the efficiency of this method.,INEXACT KLEINMAN-NEWTON METHOD FOR RICCATI EQUATIONS
"['Yuan Yun', 'Wnag L', 'Liwen Guan']","Kinematics analysis and dimensional synthesis are two important problems of a parallel manipulator. Dimensional synthesis is optimization of the kinematic parameters according to desired workspace or other design requirements. In this paper, the dimensional synthesis of a 3-DOF parallel manipulator, which mimics DELTA robots, is studied considering maximum inscribed workspace and reciprocal of the condition number on the workspace section based on the concept. The kinematic model of the 3-DOF parallel manipulator is given at first. The golden section search is used to search the workspace of the manipulator and mesh the boundary. Then the algorithm for calculating the inscribed workspace and dimensional synthesis considering the inscribed workspace are presented. Thirdly, the distribution of dexterity index on the workspace section and dimensional synthesis considering the reciprocal of condition number is studied. The two dimensional synthesis results are compared at the end of the paper. While the synthesis methodology of the manipulator is studied. It is helpful to improve design efficiency of the 3-DOF parallel manipulator. The quickly design for the manipulator can be performed through the proposed methods of dimensional synthesis.",Dimensional synthesis of a 3-DOF parallel manipulator
"['Weidong Wang', 'Zhijiang Du', 'Lining Sun']","A tracked robot is designed for destroyed mine search and rescue. The mechanical system is introduced from reconfigurable structure, suspension system and anti- explosive and waterproof. The sensors include CCD camera, CO, CH4, temperature and air speed are equipped on the robot. Two pairs of swing arms are equipped on the robot. Their motions help robot climb up obstacle. Because the center of gravity (CG) plays an important role in the process of climbing up an obstacle, the CG kinematics model is built. Using this model, the CG change situation is obtained, and the maximum height of the obstacle which can be climbed up is obtained, and the stability angle margin is obtained too. The relationship between the robot pitch angle and the height of the obstacle is obtained. Using this relationship, the geometry parameter of the uncertain environment can be known. These analysis help to design and control the robot.",Kinematics analysis for obstacle-climbing performance of a rescue robot
"['Suriya Subramanian', 'Michael Hicks', 'Kathryn S. McKinley']","Software evolves to fix bugs and add features. Stopping and restarting programs to apply changes is inconvenient and often costly. Dynamic software updating (DSU) addresses this problem by updating programs while they execute, but existing DSU systems for managed languages do not support many updates that occur in practice and are inefficient. This paper presents the design and implementation of J volve , a DSU-enhanced Java VM. Updated programs may add, delete, and replace fields and methods anywhere within the class hierarchy. Jvolve implements these updates by adding to and coordinating VM classloading, just-in-time compilation, scheduling, return barriers, on-stack replacement, and garbage collection. J volve , is  safe : its use of bytecode verification and VM thread synchronization ensures that an update will always produce type-correct executions. Jvolve is  flexible : it can support 20 of 22 updates to three open-source programs--Jetty web server, JavaEmailServer, and CrossFTP server--based on actual releases occurring over 1 to 2 years. Jvolve is  efficient : performance experiments show that incurs  no overhead  during steady-state execution. These results demonstrate that this work is a significant step towards practical support for dynamic updates in virtual machines for managed languages.",Dynamic software updates: a VM-centric approach
"['Gisele L. Pappa', 'Alex Alves Freitas']","Ensembles are a set of classification models that, when combined, produce better predictions than when used by themselves. This chapter proposes a new evolutionary algorithm-based method for creating an ensemble of rule sets consisting of two stages. First, an evolutionary algorithm (more precisely, a genetic programming algorithm) is used to automatically create complete rule induction algorithms. Secondly, the automatically-evolved rule induction algo- rithms are used to produce rule sets that are then combined into an ensemble. Concerning this second stage, we investigate the effectiveness of two different approaches for combining the votes of all rule sets in the ensemble and two dif- ferent approaches for selecting which subset of evolved rule induction algorithms (out of all evolved algorithms) should be used to produce the rule sets that will be combined into an ensemble.",Creating Rule Ensembles from Automatically-Evolved Rule Induction Algorithms
['Wolfram W????'],"Today in many cases electronic data interchange (EDI) is limited to large scale industry connected to their own value added networks. Small-scale enterprises are not yet integrated in the communication flow, because actual EDI solutions are to complex, to inflexible or to expensive.#R##N##R##N#The approach presented in this paper separates knowledge about data structures and data formats from the process of generation of destination files. This knowledge is transformed into a meta data structure represented by XML document type definitions (DTD) which itself are stored within a database system. If any changes of the data interchange specification are necessary, it is sufficient to update the corresponding meta data information within the XML DTDs. The implementation of the data interchange processor remains unchanged. This type of adaptation does not require a software specialist and therefore it meets an important requirement of small-scale enterprises. Data transmission is done using the advantages of XML and Internet technology.",XML and Meta Data Based EDI for Small Enterprises
"['Giovanni Danese', 'Mauro Giachero', 'Francesco Leporati', 'Giulia Matrone', 'Nelson Nazzicari']","Biometric identification systems are defined as systems exploiting automated methods of personal recognition based on physiological or behavioural characteristics. Among these, fingerprints are very reliable biometric identifiers. Trying to fasten the image processing step makes the recognition process more efficient, especially concerning embedded systems for real-time authentication. In this paper we propose an FPGA-based architecture that efficiently implements the high computationally demanding core of a matching algorithm based on phase-only spatial correlation. Moreover, we show how it is possible to use COTS components to embed an entire AFIS on chip and so reducing cost, space and energy used.",An FPGA-Based Embedded System for Fingerprint Matching Using Phase-Only Correlation Algorithm
"['T?ßlio C??sar Soares dos Santos Andr??', 'Rangaraj M. Rangayyan']","We propose an approach using artificial neural networks to classify masses in mammograms as malignant or benign. Single- layer and multilayer perceptron networks are used in a study on perceptron topologies and training procedures for pattern classifica- tion of breast masses. The contours of a set of 111 regions on mam- mograms related to breast masses and tumors are manually delin- eated and represented by polygonal models for shape analysis. Ribbons of pixels are extracted around the boundaries of a subset of 57 masses by dilating and eroding the contours. Three shape fac- tors, three measures of edge sharpness, and 14 texture features based on gray-level co-occurrence matrices of the pixels in the rib- bons are computed. Several combinations of the features are used with perceptrons of varying topology and training procedures for the classification of benign masses and malignant tumors. The results are compared in terms of the area Az under the receiver operating characteristics curve. Values of Az up to 0.99 are obtained with the shape factors and texture features. However, only feature sets that included at least one shape factor provide consistently high perfor- mance with respect to variations in network topology and training.","Classification of breast masses in mammograms using neural networks with shape, edge sharpness, and texture features"
"['Huiyong Li', 'Xun Li', 'Chen Wei']","With the increasingly diverse and complex requirements of radar systems and communication systems, the application of multifunction-phased array radar has become a trend, and the digital multi-beamforming technology plays a crucial role in it. In practice, power amplifier (PA) is an essential component in radar systems and communication systems. Unfortunately, it is always nonlinear to provide a high output power. With the purpose of a high output power and efficiency, it is necessary to study the influence of PA nonlinear characteristics on the digital multi-beamforming. In this paper, a form of the multi-beamforming signal and a nonlinear model with memory for PA are given. The output signal via the PA model has been analyzed subsequently. As the result of analysis, it can be found that the output signal is divided into the original signal and the interferential signal. The power ratio of original signal to interference signal can reflect the influence of PA nonlinear characteristics on the digital multi-beamforming. Finally, according to the ratio, the results of computer simulation show that the memory effect plays a key role for the small power signal, while the nonlinearity plays an important role for the large power signal.",The analysis of the performance of multi-beamforming in memory nonlinear power amplifier
"['Tarik C. Cicic', 'Audun Fosselie Hansen', 'Amund Kvalbein', 'Matthias Hartman', 'R?¨diger Martin', 'Michael Menth']","Multi-topology routing is an increasingly popular IP network management concept that allows transport of different traffic types over disjoint network paths. The concept is of particular interest for implementation of IP fast reroute (IP FRR). First, it can support guaranteed, instantaneous recovery from any link or node failure. Second, different failures result in routing over different network topologies, which augments the parameter space for load distribution optimizations. Multiple routing configurations (MRC) is the state-of-the-art IP FRR scheme based on multi-topology routing today. In this paper we present a new, enhanced IP FRR scheme which we call ldquorelaxed MRCrdquo (rMRC). rMRC simplifies the topology construction and increases the routing flexibility in each topology. According to our experimental evaluation, rMRC has several benefits compared to MRC. The number of backup topologies required to provide protection against the same set of failures is reduced, hence reducing state in routers. In addition, the backup paths are shorter, and the link utilization is significantly better.",Relaxed multiple routing configurations for IP fast reroute
"['Yanhong Li', 'David L. Olson', 'Zheng Qin']","Existing similarity measures between intuitionistic fuzzy sets/vague sets are analyzed, compared and summarized by their counter-intuitive examples in pattern recognition. The positive aspects of each similarity measure are demonstrated, along with counter cases and discussion of the conditions under which each may not work as desired. The research presented here could benefit selection and applications of similarity measures for intuitionistic fuzzy sets and vague sets in practice.",Similarity measures between intuitionistic fuzzy (vague) sets: A comparative analysis
"['Fabian Kuhn', 'Thomas Moscibroda']","We study local, distributed algorithms for the capacitated minimum dominating set (CapMDS) problem, which arises in various distributed network applications. Given a network graph  G  = ( V,E ), and a capacity  cap(v)  É??  N  for each node  v  É??  V  , the CapMDS problem asks for a subset  S  É??  V  of minimal cardinality, such that every network node not in  S  is covered by at least one neighbor in  S , and every node  v  É??  S  covers at most  cap(v)  of its neighbors. We prove that in general graphs and even with uniform capacities, the problem is inherently  non-local , i.e., every distributed algorithm achieving a non-trivial approximation ratio must have a time complexity that essentially grows linearly with the network diameter. On the other hand, if for some parameter e > 0, capacities can be violated by a factor of 1 + e, CapMDS becomes much more local. Particularly, based on a novel distributed randomized rounding technique, we present a distributed bi-criteria algorithm that achieves an O(log ??)-approximation in time O(log 3  n  + log( n )/e), where  n  and ?? denote the number of nodes and the maximal degree in  G , respectively. Finally, we prove that in geometric network graphs typically arising in wireless settings, the uniform problem can be approximated within a constant factor in logarithmic time, whereas the non-uniform problem remains entirely non-local.",Distributed approximation of capacitated dominating sets
"['Yoonjae Jeong', 'Sung-Hyon Myaeng']","The goal of this research is to devise a method for recognizing and classifying TimeML events in a more effective way. TimeML is the most recent annotation scheme for processing the event and temporal expressions in natural language processing fields. In this paper, we argue and demonstrate that unit feature dependency information and deep-level WordNet hypernyms are useful for event recognition and type classification. The proposed method utilizes various features including lexical semantic and dependency-based combined features. The experimental results show that our proposed method outperforms a state-of-the-art approach, mainly due to the new strategies. Especially, the performance of noun and adjective events, which have been largely ignored and yet significant, is significantly improved.",Using wordnet hypernyms and dependency features for phrasal-level event recognition and type classification
"['Gianfranco Fornaro', 'Francesco Serafino', 'Francesco Soldovieri']","Deals with the use of multipass synthetic aperture radar (SAR) data in order to achieve three-dimensional tomography reconstruction in presence of volumetric scattering. Starting from azimuth- and range-focused SAR data relative to the same area, neglecting any mutual interaction between the targets, and assuming the propagation in homogeneous media, we investigate the possibility to focus the data also in the elevation direction. The problem is formulated in the framework of linear inverse problem and the solution makes use of the singular value decomposition of the relevant operator. This allows us to properly take into account nonuniform orbit separation and to exploit a priori knowledge regarding the size of the volume interested by the scattering mechanism, thus leading to superresolution in the elevation direction. Results obtained on simulated data demonstrate the feasibility of the proposed processing technique.",Three-dimensional focusing with multipass SAR data
"['Denise M. Woit', 'David V. Mason']","Independence is a fundamental requirement for calculating system reliability from component reliabilities, whether in hardware or software systems. Markov analysis is often used in such calculation; however, procedures as conventionally used do not qualify as nodes in a Markov system. We outline the requirements for several classes of component independence and use the CPS (continuation passing style) transformation to convert conventional procedures into fragments that are appropriate to Markov analysis.",Software component independence
"['Huseyin Hacihabiboglu', 'Banu Gunel', 'Ahmet M. Kondoz']","Fractional-delay filter is the general name given to filters modelling non-integer delays. Such filters have a flat phase delay for a wide frequency band, with the value of the phase delay approximating the fractional delay. A maximally-flat delay IIR fractional-delay filter can be obtained by the Thiran approximation. A simple and efficient method for obtaining filters modelling intermediate fractional delays from two Thiran fractional-delay filters is proposed. The proposed method allows continuously modifying the fractional delay. Computational complexity of the proposed method is discussed. A practical application of the method in model-based sound synthesis is given as an example.",Interpolated Allpass Fractional-Delay Filters Using Root Displacement
"['Joel Masciocchi', 'Gianfranco Frau', 'Marco Fanton', 'Mattia Sturlese', 'Matteo Floris', 'Luca Pireddu', 'Piergiorgio Palla', 'Fabian Chatwin Cedrati', 'Patricia Rodriguez-Tom??', 'Stefano Moro']","MMsINC (http://mms.dsfarm.unipd.it/MMsINC/search) is a database of non-redundant, richly annotated and biomedically relevant chemical structures. A primary goal of MMsINC is to guarantee the highest quality and the uniqueness of each entry. MMsINC then adds value to these entries by including the analysis of crucial chemical properties, such as ionization and tautomerization processes, and the in silico prediction of 24 important molecular properties in the biochemical profile of each structure. MMsINC is consequently a natural input for different chemoinformatics and virtual screening applications. In addition, MMsINC supports various types of queries, including substructure queries and the novel É??molecular scissoringÉ?? query. MMsINC is interfaced with other primary data collectors, such as PubChem, Protein Data Bank (PDB), the Food and Drug Administration database of approved drugs and ZINC.",MMsINC: a large-scale chemoinformatics database
"['Brian T. Denton', 'John J. Forrest', 'R. John Milne']","IBM Systems and Technology Group uses operations research models and methods extensively for solving large-scale supply chain optimization (SCO) problems for planning its extended enterprise semiconductor supply chain. The large-scale nature of these problems necessitates the use of computationally efficient solution methods. However, the complexity of the models makes developing robust solution methods a challenge. We developed a mixed-integer programming (MIP) model and supporting heuristics for optimizing IBM's semiconductor supply chain. We designed three heuristics, driven by practical applications, for capturing the discrete aspects of the MIP. We leverage the model structure to overcome computational hurdles resulting from the large-scale problem. IBM uses the model and method daily for operational and strategic planning decisions and has saved substantial costs.",IBM Solves a Mixed-Integer Program to Optimize Its Semiconductor Supply Chain
"['Satoshi Satoh', 'Kenji Fujimoto']","This paper introduces Stochastic Port-Hamiltonian Systems (SPHS's), whose dynamics are described by Ito stochastic differential equations. SPHS's are extension of the deterministic port-Hamiltonian systems which are used to express various passive systems. First, we show a necessary and sufficient condition to preserve the stochastic port-Hamiltonian structure of the system under a class of coordinate transformations. Second, we derive a condition for the system to be stochastic passive. Third, we equip Stochastic Generalized Canonical Transformations (SGCT's), which are pairs of coordinate and feedback transformations preserving the stochastic port-Hamiltonian structure. Finally, we propose a stochastic stabilization framework based on stochastic passivity and SGCT's.",On passivity based control of stochastic port-Hamiltonian systems
"['Jian Song', 'Kwan-Wu Chin']","Wireless communication at 60GHz, aka mmWave, provides extremely high data rates, i.e., several Gb/s. Moreover, devices have a much shorter transmission range as compared to those operating in the 2.4 and 5GHz bands. Indeed, links can be treated as pseudo-wires with minimal interference leakage. As a result, future 60GHz systems will have very high spatial reuse. This, however, is at the expense of high propagation loss, which can be overcome using directional or smart antennas. Another promising solution is to employ relays to boost the signal of weak links. In particular, if relays are properly selected, they are able to offer higher data rates than direct links, and also help circumvent obstacles. To this end, we review state-of-the-art schedulers that take advantage of the high spatial re-use afforded by 60GHz wireless systems to activate multiple links within a channel time allocation. Moreover, we survey works that use passive and active relays to overcome obstacles and to facilitate novel applications. We also survey those that maximize both spatial reuse and throughput of both direct and indirect (relay) links simultaneously.",A survey of single and multi-hop link schedulers for mmWave wireless systems
"['Xiaoguang Yang', 'Jianhua Cheng']","More and more economic and financial data have been collected by the governmental departments in China since China started its É??socialist market economyÉ?ù in late 1980s. These government departments, in particular the departments in charge of economic development, pay a great attention to the economic information in their decision-making. There is an urgent demand for efficient decision support systems in macroeconomic decision-making. In this paper we present a data-driven approach for building macroeconomic decision support system. We first give a comprehensive discussion about the basic elements and their data-processing methods for Macroeconomic Decision Support Systems according to China's situation. These elements include: leading indicator system about business cycle, state identification of economic movement, forecasting of economic trend, promotion of successful cases, choice of regulation instruments, evaluation method of macroeconomic policies. Based on the discussion, we put forward to a general structure of macroeconomic decision support system. Premier implementation shows that the structure can not only satisfy the governmental departments' demand fairly, but also reflect the future trend.",A data-driven approach for building macroeconomic decision support system
"['Xia Li', 'Peng Yong Kong', 'Kee Chaing Chua']","We propose a packet-level model to investigate the impact of channel error on the transmission control protocol (TCP) performance over IEEE-802.11-based multihop wireless networks. A Markov renewal approach is used to analyze the behavior of TCP Reno and TCP Impatient NewReno. Compared to previous work, our main contributions are listed as follows: 1) modeling multiple lossy links, 2) investigating the interactions among TCP, Internet Protocol (IP), and media access control (MAC) protocol layers, specifically the impact of 802.11 MAC protocol and dynamic source routing (DSR) protocol on TCP throughput performance, 3) considering the spatial reuse property of the wireless channel, the model takes into account the different proportions between the interference range and transmission range, and 4) adopting more accurate and realistic analysis to the fast recovery process and showing the dependency of throughput and the risk of experiencing successive fast retransmits and timeouts on the packet error probability. The analytical results are validated against simulation results by using GloMoSim. The results show that the impact of the channel error is reduced significantly due to the packet retransmissions on a per-hop basis and a small bandwidth delay product of ad hoc networks. The TCP throughput always deteriorates less than ~ 10 percent, with a packet error rate ranging from 0 to 0.1. Our model also provides a theoretical basis for designing an optimum long retry limit for IEEE 802.11 in ad hoc networks.",TCP Performance in IEEE 802.11-Based Ad Hoc Networks with Multiple Wireless Lossy Links
"['Jiadi Yu', 'Peng Lu', 'Yanmin Zhu', 'Guangtao Xue', 'Minglu Li']","Cloud computing has emerging as a promising pattern for data outsourcing and high-quality data services. However, concerns of sensitive information on cloud potentially causes privacy problems. Data encryption protects data security to some extent, but at the cost of compromised efficiency. Searchable symmetric encryption (SSE) allows retrieval of encrypted data over cloud. In this paper, we focus on addressing data privacy issues using SSE. For the first time, we formulate the privacy issue from the aspect of similarity relevance and scheme robustness. We observe that server-side ranking based on order-preserving encryption (OPE) inevitably leaks data privacy. To eliminate the leakage, we propose a two-round searchable encryption (TRSE) scheme that supports top-k multikeyword retrieval. In TRSE, we employ a vector space model and homomorphic encryption. The vector space model helps to provide sufficient search accuracy, and the homomorphic encryption enables users to involve in the ranking while the majority of computing work is done on the server side by operations only on ciphertext. As a result, information leakage can be eliminated and data security is ensured. Thorough security and performance analysis show that the proposed scheme guarantees high security and practical efficiency.",Toward Secure Multikeyword Top-k Retrieval over Encrypted Cloud Data
"['Yuheng Huang', 'James A. Ritcey']","We design constellation labeling maps for bit-interleaved space-time coded modulation with iterative decoding (BI-STCM-ID) over Rayleigh block-fading channels using the Alamouti scheme and N/sub r/ receive antennas. To achieve the largest asymptotic coding gain from the constellation labeling, we propose a new design criterion that maximizes the (-2N/sub r/)-th power mean of the squared Euclidean distances associated with all ""error-free feedback"" events in the constellation. Based on this power mean criterion, we show that the labeling optimization problem falls into the category of quadratic assignment problems. We propose two novel 16-QAM labeling maps that are particularly designed for N/sub r/=1 and N/sub r/=2, respectively. Numerical results show that both labeling maps achieve about 1 dB coding gain over the conventional 16-QAM modified set partitioning labeling.",Improved 16-QAM constellation labeling for BI-STCM-ID with the Alamouti scheme
"['Samir Medina Perlaza', 'Ravi Tandon', 'H. Vincent Poor', 'Zhu Han']","In this paper, the    $\eta $   -Nash equilibrium (   $\eta $   -NE) region of the two-user Gaussian interference channel (IC) with perfect output feedback is approximated to within 1 bit/s/Hz and    $\eta $    arbitrarily close to 1 bit/s/Hz. The relevance of the    $\eta $   -NE region is that it provides the set of rate pairs that are achievable and stable in the IC when both transmitterÉ??receiver pairs autonomously tune their own transmitÉ??receive configurations seeking an    $\eta $   -optimal individual transmission rate. Therefore, any rate tuple outside the    $\eta $   -NE region is not stable as there always exists one link able to increase by at least    $\eta $    bits/s/Hz its own transmission rate by updating its own transmitÉ??receive configuration. The main insights that arise from this paper are as follows. First, the    $\eta $   -NE region achieved with feedback is larger than or equal to the    $\eta $   -NE region without feedback. More importantly, for each rate pair achievable at an    $\eta $   -NE without feedback, there exists at least one rate pair achievable at an    $\eta $   -NE with feedback that is weakly Pareto superior. Second, there always exists an    $\eta $   -NE transmitÉ??receive configuration that achieves a rate pair that is at most 1 bit/s/Hz per user away from the outer bound of the capacity region.",Perfect Output Feedback in the Two-User Decentralized Interference Channel
"['Roberto Oboe', 'Paolo Fiorini']","This paper describes an environment for the design, simulation, and control of Internet-based force-reflecting telerobotic systems. We define these systems as using a segment of the computer network to connect the master to the slave. Computer networks introduce a time delay that is best described by a time-varying random process. Thus, known techniques for controlling time-delay telerobots are not directly applicable, and an environment for iterative designing and testing is necessary. The underlying software architecture sup ports tools for modeling the delay of the computer network, design ing a stable controller, simulating the performance of a telerobotic system, and testing the control algorithms using a force-reflecting input device. Furthermore, this setup provides data about including the Internet into more general telerobotic control architectures. To demonstrate the features of this environment, the complete proce dure for the design of a telerobotic controller is discussed. First, the delay pa...",A Design and Control Environment for Internet-Based Telerobotics
"['Hua Lu', 'K.C. Hung', 'Stoyan Stoyanov', 'C. Bailey', 'Y. C. Chan']","In the flip-chip assembly process, no-flow underfill materials have a particular advantage over traditional underfill: the application and curing of the former can be undertaken before and during the reflow process. This advantage can be exploited to increase the flip-chip manufacturing throughput. However, adopting a no-flow underfill process may introduce reliability issues such as underfill entrapment, delamination at interfaces between underfill and other materials, and lower solder joint fatigue life. This paper presents an analysis on the assembly and the reliability of flip-chips with no-flow underfill. The methodology adopted in the work is a combination of experimental and computer-modeling methods. Two types of no-flow underfill materials have been used for the flip chips. The samples have been inspected with X-ray and scanning acoustic microscope inspection systems to find voids and other defects. Eleven samples for each type of underfill material have been subjected to thermal shock test and the number of cycles to failure for these flip chips have been found. In the computer modeling part of the work, a comprehensive parametric study has provided details on the relationship between the material properties and reliability, and on how underfill entrapment may affect the thermalÉ??mechanical fatigue life of flip chips with no-flow underfill.",No-flow underfill flip chip assembly--an experimental and modeling analysis
"[""Florian 'Floyd' Mueller"", 'Martin R. Gibbs', 'Frank Vetere']","Research in human-computer interaction has begun to acknowledge the benefits of physicality in the way people interact with computers. However, the role of physicality is often understood in terms of the characteristics of physical smart objects and their digital augmentation. We are stressing that the physicality lies within the interaction, not the object, and use a subset of bodily actions, exertion interactions, as an example to demonstrate our point. Emerging game designs have shown that supporting such exertion interactions can enable beneficial experiences between geographically distant participants. Based on several designs from our own work as well as others in this area we articulate reflections for the design of systems that support and facilitate bodily aspects of physicality in networked environments. We believe our work can serve as guidance for designers who are interested in creating future systems that support networked exertion interactions.",Reflections on designing networked exertion games
"['Chris J. M. Booth', 'Donald I. Bruce']","The process interaction world view is widely used in the general simulation community for its expressive power, and is supported by most modern simulation languages. In parallel discrete event simulation, however, its use remains comparatively rare due to the perceived inefficiency (and difficulty) of parallel implementations.We present a new implementation strategy for parallel process-oriented simulation languages. This innovative, semantics-based approach directly addresses two common concerns of such languages. By concentrating on the intrinsic threads of control, we avoid the proliferation of simulation objects (and their associated costs) that might result from a naive translation. More fundamentally, the primary costs associated with process-oriented languages -- those of context switching between stacks and, in an optimistic setting, of saving the state of these stacks -- are entirely eliminated since our explicit use of continuations avoids the need for stacks in the first place. We similarly obtain cheap and natural thread preemption.",Stack-free process-oriented simulation
"['Roberto Cominetti', 'Cristobal Guzman']","In this paper we consider an integrated model for TCP/IP protocols with multipath routing. The model combines a Network Utility Maximization for rate control based on end-to-end queuing delays, with a Markovian Traffic Equilibrium for routing based on total expected delays. We prove the existence of a unique equilibrium state which is characterized as the solution of an unconstrained strictly convex program. A distributed algorithm for solving this optimization problem is proposed, with a brief discussion of how it can be implemented by adapting the current Internet protocols.",Network congestion control with Markovian multipath routing
"['Ashraf A. Kassim', 'Pingkun Yan', 'Wei Siong Lee', 'Kuntal Sengupta']","This paper proposes a method for progressive lossy-to-lossless compression of four-dimensional (4-D) medical images (sequences of volumetric images over time) by using a combination of three-dimensional (3-D) integer wavelet transform (IWT) and 3-D motion compensation. A 3-D extension of the set-partitioning in hierarchical trees (SPIHT) algorithm is employed for coding the wavelet coefficients. To effectively exploit the redundancy between consecutive 3-D images, the concepts of key and residual frames from video coding is used. A fast 3-D cube matching algorithm is employed to do motion estimation. The key and the residual volumes are then coded using 3-D IWT and the modified 3-D SPIHT. The experimental results presented in this paper show that our proposed compression scheme achieves better lossy and lossless compression performance on 4-D medical images when compared with JPEG-2000 and volumetric compression based on 3-D SPIHT.",Motion compensated lossy-to-lossless compression of 4-D medical images using integer wavelet transforms
"['Urs Fawer', 'Behnaam Aazhang']","A multiuser communication system is considered where K users share a channel with multipath propagation by using code division for multiple access. Data modulation is carried out by binary phase shift keying and direct sequence spread spectrum signaling. The micro-cellular communication media is modeled as a frequency selective fading channel with multipath propagation. The multipath diversity of the received signals from the K users is exploited by a bank of K RAKE correlators. Algorithms based on the maximum likelihood rule have been developed for estimating the complex channel coefficients as well as for detection of the desired data packets from the sufficient statistics provided by the RAKE correlators. The performance of the resulting multiuser detector is evaluated analytically and via Monte Carlo simulations. The results indicate that the estimator of the channel coefficients has a variance close to the Cramer-Rao lower bound, and that the proposed multiuser detector is capable of eliminating the near-far effect as well as processing the signals propagated through multiple paths. >",A multiuser receiver for code division multiple access communications over multipath channels
"['Eugene T. Lin', 'Edward J. Delp']","Recently, we proposed a method for constructing a template for efficient temporal synchronization in video watermarking. 1 Our temporal synchronization method uses a state machine key generator for producing the watermark embedded in successive frames of video. A feature extractor allows the watermark key schedule to be content dependent, increasing the difficulty of copy and ownership attacks. It was shown that efficient synchronization can be achieved by adding temporal redundancy into the key schedule. In this paper, we explore and extend the concepts of our temporal synchronization method to spatial synchronization. The key generator is used to construct the embedded watermark of non-overlapping blocks of the video, creating a tiled structure. 2É??4 The autocorrelation of the tiled watermark contains local maxima or peaks with a grid-like structure, where the distance between the peaks indicates the scale of the watermark and the orientation of the peaks indicate the watermark rotation. Experimental results are obtained using digital image watermarks. Scaling and rotation attacks are investigated.",Spatial Synchronization Using Watermark Key Structure
"['Fan Yang', 'Sirish L. Shah', 'Deyun Xiao']","In the area of fault analysis, SDG (Signed Directed Graph) models can be used to describe the system states and the fault propagation paths which are the composition of qualitative deviation from the normal state. In control systems, besides the natural relations caused by the physical properties, the forced control actions determine the dynamic properties of the systems, which cause the particularity of SDG model-based analysis. In this paper, the SDG description and the analysis methods of fault propagation in control systems are presented, and the typical cases like PID control, feedforward control, split-range control, cascade control etc are illustrated. A graphical analysis method is proposed to substitute the algebraic methods based on equations. These results can be expanded to various control systems and even be applied to large-scale industrial systems by the combination and connection of several basic elements.",SDG model-based analysis of fault propagation in control systems
"['Clare E. Burke', 'Derek J. de Solla Price']","Following the methodology established byPrice, this paper analyzes the empirical evidence of citation matrices. Using the data cleaned and tabulated by Computer Horizons, Inc. from the Science Citation Index data banks, it is shown that the non-diagonal elements of the square citation matrices can be accounted for very satisfactorily by assigning each nation a characteristic output and input coefficient in each field measured; the ratio of these coefficients provides a measure of quality. Deviations from this simple model give measures of particular linkage strengths between nations showing some evidence of preferences and avoidances that exist for reason of language, social structure, etc. It is also shown that the diagonal data can be accounted for by the measurable phenomenon that each nation seems to publish partly for the international knowledge system and party for its own domestic purposes. Thus, three parameters and a cluster map can parsimoniously describe the citation data within the limits of random error.",The distribution of citations from nation to nation on a field by field basis É?? A computer calculation of the parameters
"['John P. Shewchuk', 'Tien-Chien Chang']","It is shown how the object-oriented approach can be applied to discrete-event simulation and, in particular, to discrete-event simulation of manufacturing systems. A hierarchical structure of object classes is proposed, consisting of three class libraries: base classes, simulation support object classes, and manufacturing systems simulation object classes. The definition of each class and how the class objects interact with one another are discussed. An example of a discrete-event simulation model developed using the object classes is presented. The example illustrates the basic nature, merits, and drawbacks of this approach. >",An approach to object-oriented discrete-event simulation of manufacturing systems
"['Qijia Liu', 'Robert J. Baxley', 'G.T. Zhou']","Peak-to-average power ratio (PAR) reduction techniques are often employed to increase the power efficiency of orthogonal frequency division multiplexing (OFDM) systems. A recently proposed PAR optimization method demonstrates how the PAR can be minimized when free subcarriers and a certain distortion allowance on the error vector magnitude (EVM) are available. In this paper, we derive the lower bound on the capacity for such a system and investigate the capacity- maximizing number of free subcarriers that should be used.",Free subcarrier optimization for peak-to-average power ratio minimization in OFDM systems
"['Minwook Ahn', 'Jonghee W. Yoon', 'Yunheung Paek', 'Yoonjin Kim', 'Mary Kiemb', 'Kiyoung Choi']","In this work, we investigate the problem of automatically mapping applications onto a coarse-grained reconfigurable architecture and propose an efficient algorithm to solve the problem. We formalize the mapping problem and show that it is NP-complete. To solve the problem within a reasonable amount of time, we divide it into three subproblems: covering, partitioning and layout. Our empirical results demonstrate that our technique produces nearly as good performance as hand-optimized outputs for many kernels.",A spatial mapping algorithm for heterogeneous coarse-grained reconfigurable architectures
"['Nikhil R. Devanur', 'Christos H. Papadimitriou', 'Amin Saberi', 'Vijay V. Vazirani']",We give the first polynomial time algorithm for exactly computing an equilibrium for the linear utilities case of the market model defined by Fisher. Our algorithm uses the primal--dual paradigm in the enhanced setting of KKT conditions and convex programs. We pinpoint the added difficulty raised by this setting and the manner in which our algorithm circumvents it.,Market equilibrium via a primal--dual algorithm for a convex program
"['Edouard Buchoud', 'Valeriu Vrabie', 'J??r??me I. Mars', ""Guy D'Urso"", 'Alexandre Girard', 'Sylvain Blairon', 'Jean-Marie Henault']","Distributed optical fiber sensors have gained an increasingly prominent role in structural-health monitoring. These are composed of an optical fiber cable in which a light impulse is launched by an opto-electronic device. The scattered light is of interest in the spectral domain: the spontaneous Brillouin spectrum is centered on the Brillouin frequency, which is related to the local strain and temperature changes in the optical fiber. When coupled with an industrial Brillouin optical time-domain analyzer (B-OTDA), an optical fiber cable can provide distributed measurements of strain and/or temperature, with a spatial resolution over kilometers of 40 cm. This paper focuses on the functioning of a B-OTDA device, where we address the problem of the improvement of spatial resolution. We model a Brillouin spectrum measured within an integration base of 1 m as the superposition of the elementary spectra contained in the base. Then, the spectral distortion phenomenon can be mathematically explained: if the strain is not constant within the integration base, the Brillouin spectrum is composed of several elementary spectra that are centered on different local Brillouin frequencies. We propose a source separation methodology approach to decompose a measured Brillouin spectrum into its spectral components. The local Brillouin frequencies and amplitudes are related to a portion of the integration base where the strain is constant. A layout algorithm allows the estimation of a strain profile with new spatial resolution chosen by the user. Numerical tests enable the finding of the optimal parameters, which provides a reduction to 1 cm of the 40-cm spatial resolution of the B-OTDA device. These parameters are highlighted during a comparison with a reference strain profile acquired by a 5-cm-resolution Rayleigh scatter analyzer under controlled conditions. In comparison with the B-OTDA strain profile, our estimated strain profile has better accuracy, with centimeter spatial resolution.",Enhancement of an Optical Fiber Sensor: Source Separation Based on Brillouin Spectrum
"['Yawei Liang', 'Yongkui Wang', 'Jerzy Jarmasz']","In this paper we present the composition of a general dynamic scenario, the relations of its components and a dynamic making algorithm called ECA (Evaluation and Correction Algorithm). The paper proposes three relations, namely, stable relation, controllable relation and uncontrollable relation. Numerical results are given by some simple dynamic scenarios.",Intra dynamic scenario relations and a dynamic decision making algorithm
"['Javier Arnedo', 'Coral del Val', 'Gabriel Alejandro de Erausquin', 'Roc??o Romero-Zaliz', 'Dragan Svrakic', 'Claude Robert Cloninger', 'Igor Zwir']","It has been proposed that single nucleotide polymorphisms (SNPs) discovered by genome-wide association studies (GWAS) account for only a small fraction of the genetic variation of complex traits in human population. The remaining unexplained variance or missing heritability is thought to be due to marginal effects of many loci with small effects and has eluded attempts to identify its sources. Combination of different studies appears to resolve in part this problem. However, neither individual GWAS nor meta-analytic combinations thereof are helpful for disclosing which genetic variants contribute to explain a particular phenotype. Here, we propose that most of the missing heritability is latent in the GWAS data, which conceals intermediate phenotypes. To uncover such latent information, we propose the PGMRA server that introduces phenomicsÉ??the full set of phenotype features of an individualÉ??to identify SNP-set structures in a broader sense, i.e. causally cohesive genotypeÉ?ê phenotype relations. These relations are agnostically identified (without considering disease status of the subjects) and organized in an interpretable fashion. Then, by incorporating a posteriori the subject status within each relation, we can establish the risk surface of a disease in an unbiased mode. This approach complementsÉ??instead of replacesÉ?? current analysis methods. The server is publically available at http://phop.ugr.es/fenogeno.",PGMRA: a web server for (phenotype ?? genotype) many-to-many relation analysis in GWAS
['Laurence T. Yang'],"In the robot navigation problem, noisy sensor data must be filtered to obtain the best estimate of the robot position. The discrete Kalman filter, which usually is used for prediction and detection of signals in communication and control problems has become a commonly used method to reduce the effect of uncertainty from the sensor data. However, due to the special domain of robot navigation, the Kalman approach is very limited. The use of total least squares filter has been proposed (Boley and Sutherland, 1993) which is capable of converging with many fewer readings and achieving greater accuracy than the classical Kalman filter. The main disadvantage of those approaches is that they can not deal with the case where the noise subspace is of dimension higher than one. Here a parallel Krylov subspace method on parallel distributed memory computers which uses the Lanczos bidiagonalization process with updating techniques is proposed which is more computationally attractive to solve the total least squares problems. The parallel algorithm is derived such that all inner products of a single iteration step are independent. Therefore, the cost of global communication which represents the bottleneck of the parallel performance on parallel distributed memory computers can be significantly reduced. This filter is very promising for very large data information and from our very preliminary experiments we can obtain more precise accuracy and better speedup.",Parallel Lanczos bidiagonalization for total least squares filter in robot navigation
"['Lu Li', 'Rainer Moorfeld', 'Adolf Finger']","In this paper we present two different bit and power loading algorithms for the non-coherent multiband impulse UWB architecture. The first one is a very simple threshold based bit loading algorithm and an extension of the detect and avoid (DAA) algorithm presented in [1]. The second one is a more powerful algorithm, enabling also power loading. These algorithms allow a more efficient and flexible spectrum use, higher data rates and use less transmission power. The bit error rate performance is significantly improved. Both algorithms support inherent powerful DAA.",Bit and power loading for the multiband impulse radio UWB architecture
"['Dong-Ling Xu', 'Grace McCarthy', 'Jian-Bo Yang']","In this paper, it is described how a multiple criteria decision analysis software tool, the Intelligent Decision System (IDS), can be used to help business self-assessment. Following a brief outline of a model for assessing business innovation capability and the IDS software, the process of using IDS to implement different types of assessment questions is discussed. It is demonstrated that IDS is a flexible tool capable of handling different types of data in self-assessment, including uncertain and incomplete data, and providing a wide range of information including scores, performance diversity, strength and weakness profile and graphics.",Intelligent decision system and its application in business innovation self assessment
"['Chao-Wen Tseng', 'Edward J. McCluskey', 'Xiaoping Shao', 'David M. Wu']","Delay defects can escape detection during the normal production test flow; particularly if they do not affect any of the long paths included in the test flow. Some delay defects can have their delay increased, making them easier to detect, by carrying out the test with a very low supply voltage (VLV testing). However, VLV testing is not effective for delay defects caused by high resistance interconnects. This paper presents a screening technique for such defects. This technique, cold testing, relies on carrying out the test at low temperature. One particular type of defect, silicide open, is analyzed and experimental data are presented to demonstrate the effectiveness of cold testing.",Cold delay defect screening
"['Ming Zhou', 'Young Jun Son', 'Zhimin Chen']","Simulation is a powerful tool that helps decision makers in business and industry to solve difficult and complex problems, reduce cost, improve quality and productivity, and shorten time-to-market. However the technology is still underutilized in many applications due to several reasons. In this study we address these issues using a knowledge engineering approach, i.e. develop efficient and robust models and formats to capture, represent and organize the knowledge for developing conceptual simulation models that can be generalized and interfaced with different applications and implementation tools. The research fits into a larger project effort that aims to create a sustained research program on knowledge-based simulation.",Knowledge representation for conceptual simulation modeling
['Andrew Berry'],"Maintenance of causality information in distributed systems has previously been implemented in the communications infrastructure with the focus on providing reliability and availability for distributed services. While this approach has a number of advantages, moving causality information up into the view and control of the application programmer is useful, and in some cases, preferable. In an experiment at the University of Queensland, libraries to support application-level maintenance of causality information have been implemented. The libraries allow the collection and use of causality information under programmer control, supplying a basis for making causal dependency information available for application management and troubleshooting. The libraries are also unique in supporting existing distributed systems based on the remote procedure call paradigm. This paper describes the underlying theory of causality, and the design and implementation of the libraries. An event reporting service example is used to motivate the approach, and a number of previously unresolved practical problems are addressed in the design process.",An application-level implementation of causal timestamps and causal ordering
"['Peter Kroon', 'Kumar Swaminathan']","The design and implementation of a real-time CELP coder for mobile communication applications are discussed. To realize a single-chip implementation, several tradeoffs were made without compromising speech quality. In addition, techniques that make the coder more robust under a variety of channel conditions are discussed. The real-time coder can be operated at different bit rates (8, 6.8, 4.6 kb/s) by simply changing the frame update rates. The speech quality was evaluated through a formal listening test, and it was found that this coder compares favorably with other (standardized) coders operating at similar or higher rates. >",A high-quality multirate real-time CELP coder
"['?Ångel Navia-V?≠zquez', 'Fernando Perez-Cruz', 'Antonio Art??s-Rodr??guez', 'An??bal R. Figueiras-Vidal']","An iterative block training method for support vector classifiers (SVCs) based on weighted least squares (WLS) optimization is presented. The algorithm, which minimizes structural risk in the primal space, is applicable to both linear and nonlinear machines. In some nonlinear cases, it is necessary to previously find a projection of data onto an intermediate-dimensional space by means of either principal component analysis or clustering techniques. The proposed approach yields very compact machines, the complexity reduction with respect to the SVC solution is especially notable in problems with highly overlapped classes. Furthermore, the formulation in terms of WLS minimization makes the development of adaptive SVCs straightforward, opening up new fields of application for this type of model, mainly online processing of large amounts of (static/stationary) data, as well as online update in nonstationary scenarios (adaptive solutions). The performance of this new type of algorithm is analyzed by means of several simulations.",Weighted least squares training of support vector classifiers leading to compact and adaptive schemes
"['Wissam Hamzeh', 'Abdelhakim Hafid Hafid']","The rapid growth of routing tables represents a major challenge facing the scalability of BGP and indeed the whole Internet infrastructure. In this paper, we introduce a novel distributed algorithmic scheme for partitioning the BGP routing table on multiple controller cards, where we exploit parallelism to enhance both the lookup speed and the scalability of the RIB (Routing Information Base). The proposed scheme increases the lookup performance by letting unrelated tasks, such as the Best Match Prefix (BMP) lookup and the BGP decision process to be executed in parallel at different controller cards. Simulations show that our proposal outperforms classical central lookup mechanisms with a reasonably acceptable cost, while it increases considerably the space scalability of the BGP routing table.",A distributed parallel approach for BGP routing table partitioning in next generation routers
['Harris Drucker'],"In many data mining applications we are given a set of training examples and asked to construct a regression machine or a classifier that has low prediction error or low error rate on new examples, respectively. An important issue is speed especially when there are large amounts of data. We show how both classification and prediction error can be reduced by using boosting techniques to implement committee machines. In our implementation of committees using either classification trees or regression trees, we show how we can trade off speed against either error rate or prediction error.",Fast committee machines for regression and classification
"['Jose Tellado', 'Louise M C Hoo', 'John M. Cioffi']","This paper proposes a new method for decoding multicarrier symbols with severe nonlinear distortion. The first part evaluates mutual information expressions for practical nonlinear models and shows the performance bounds for commonly used receiver structures. Then, we derive the maximum-likelihood (ML) sequence estimator, which unfortunately has an exponential complexity due to the nonlinear distortion. This extremely large complexity can be reduced with a simple algorithm that iteratively estimates the nonlinear distortion, thereby reducing the exponential ML to the standard ML without nonlinear distortion. The proposed method can be used to reduce the peak-to-average power ratio of multicarrier signals by clipping the transmit sequence. It can also be used to correct any nonlinear distortion present in transmitter/receiver amplifiers that are operating close to saturation.",Maximum-likelihood detection of nonlinearly distorted multicarrier symbols by iterative decoding
"['Chengli Zheng', 'Ting He']","In order to hedge the longevity risk, longevity bonds are designed, whose payoff structure depends on the changes in mortality. To forecast the mortality more precisely, we use a time-dynamic stochastic model by utilizing a panel data approach to forecast the mortality rates and get a survival index. Empirical study is conducted with the data in China. Then we apply these forecasting mortality rates to evaluate one kind of longevity bond. It turns out that it is reliable for the social security systems and the life insurance industry.",Pricing Longevity Bonds Based on Stochastic Mortality Forecasting by Panel Data Procedures
"['Alexandr V. Kostochka', 'Michael Stiebitz']","Erdè?s and Lovasz conjectured in 1968 that for every graph $G$ with $\chi(G)>\omega(G)$ and any two integers $s,t\geq 2$ with $s+t=\chi(G)+1$, there is a partition $(S,T)$ of the vertex set $V(G)$ such that $\chi(G[S])\geq s$ and $\chi(G[T])\geq t$. Except for a few cases, this conjecture is still unsolved. In this note we prove the conjecture for line graphs of multigraphs.",Partitions and Edge Colourings of Multigraphs
"['Chiang-Cheng Chiang', 'Chih-Cheng Yang']","In this paper, a robust adaptive fuzzy sliding mode control scheme is presented for a class of uncertain nonlinear systems preceded by an unknown dead-zone. Dead-zone characteristics are quite commonly encountered in actuators, such as hydraulic and pneumatic valves, electric servomotors, and electronic circuits, etc. Therefore, by using a description of a dead-zone and exploring the properties of this dead-zone model intuitively and mathematically, a robust adaptive fuzzy sliding control method is presented without constructing the dead-zone inverse. The unknown nonlinear functions of the plant are approximated by the fuzzy logic system according to some adaptive laws. Based on Lyapunov stability theorem and the theory of variable structure control, the proposed robust adaptive fuzzy sliding mode control scheme can guarantee the robust stability of the whole closed-loop system with an unknown dead-zone in the actuator and obtain good tracking performance as well. Finally, an example and simulation results are provided to illustrate the effectiveness of the proposed method.",Robust Adaptive Fuzzy Sliding Mode Control for a Class of Uncertain Nonlinear Systems with Unknown Dead-Zone
"['Sang Ho Lee', 'Wenli Cai', 'Hiroyuki Yoshida']","Most of the existing tracer kinetic models for dynamic contrast-enhanced CT or MRI do not fully describe the principles of intra- and transcapillary transport of tracers. One point is to disregard the concentration profiles between the inlets and outlets of capillaries, which may cause a biased estimation of tissue parameters by a systematic error. The Morales-Smith hypothesis enables one to resolve this ambiguity by assuming that the difference between arterial and venous concentrations is proportional to the difference between the arterial and capillary concentrations. If the backflow of administered tracer into the plasma compartment is negligible compared to its outflow into the interstitial compartment during the initial enhancement phase after tracer administration, the capillary concentration can be considered to fall exponentially along the capillary from the arterial concentration to the venous concentration by the Renkin-Crone model, i.e., unidirectional extraction fraction, which can be incorporated in the concept of the Morales-Smith hypothesis. In this study, we reformed the mass-balance equations and mathematical solutions of several representative and well-known tracer kinetic models so that the Morales-Smith hypothesis could be incorporated into their compartment tracer kinetics, considering a tissue-specific factor independent of time as proposed by Brix et al. [5]. The tissue-specific factor was applied to a liver tumor case study in perfusion CT to illustrate the potential effectiveness of the Morales-Smith hypothesis. The proposed scheme was shown to be potentially useful for more consistent and reliable estimation of physiologic tissue parameters.",Tracer kinetic modeling by morales-smith hypothesis in hepatic perfusion CT
"['Ramdas Kumaresan', 'Ashwin Rao']","An analytic signal permits unambiguous characterization of the phase and envelope of a real signal. But the analytic signal's phase-derivative, i.e. the instantaneous frequency (IF) is typically a wild function and can take on values ranging from negative infinity to positive infinity. Fortunately, any analytic signal can be decomposed into a minimum phase (MinP) signal component and an all-phase (AllP) signal component. While the MinP signal's log-envelope and its phase form a Hilbert transform pair, the AllP signal has a positive definite instantaneous frequency (PIF) unlike that of the original analytic signal. We propose an elegant computational algorithm that separates the MinP and AllP components of the analytic signal. The envelope of the MinP component corresponds to the AM and the PIF of the AllP component corresponds to the positive FM.",Algorithm for decomposing an analytic signal into AM and positive FM components
"['Mo Liu', 'Medhabi Ray', 'Elke A. Rundensteiner', 'Daniel J. Dougherty', 'Chetan Gupta', 'Song Wang', 'Ismail Ari', 'Abhay Mehta']","Complex event processing (CEP) has become increasingly important for tracking and monitoring applications ranging from health care, supply chain management to surveillance. These monitoring applications submit complex event queries to track sequences of events that match a given pattern. As these systems mature the need for increasingly complex nested sequence queries arises, while the state-of-the-art CEP systems mostly focus on the execution of flat sequence queries only. In this paper, we now introduce an iterative execution strategy for nested CEP queries composed of sequence, negation, AND and OR operators. Lastly we have introduced the promising direction of applying selective caching of intermediate results to optimize the execution. Our experimental study using real-world stock trades evaluates the performance of our proposed iterative execution strategy for different query types.",Processing nested complex sequence pattern queries over event streams
"['Bj??rn Ommer', 'Michael Sauter', 'Joachim M. Buhmann']","The complexity of real world image categorization and scene analysis requires compositional strategies for object representation. This contribution establishes a compositional hierarchy by first performing a perceptual bottom-up grouping of edge pixels to generate salient contour curves. A subsequent recursive top-down grouping yields a hierarchy of compositions. All entities in the compositional hierarchy are incorporated in a Bayesian network that couples them together by means of a shape model. The probabilistic model underlying top-down grouping as well as the shape model is learned automatically from a set of training images for the given categories. As a consequence, compositionality simplifies the learning of complex category models by building them from simple, frequently used compositions. The architecture is evaluated on the highly challenging Caltech 101 database1 which exhibits large intra-category variations. The proposed compositional approach shows competitive retrieval rates in the range of 53 .0 ?Ò 0 .49%.",Learning Top-Down Grouping of Compositional Hierarchies for Recognition
"['Nana B. Sam', 'Martin Burtscher']","Microprocessor trends are moving towards wider architectures and more aggressive speculation. With the increasing transistor budgets, energy consumption has become a critical design constraint. To address this problem, several researchers have proposed and evaluated energy-efficient variants of speculation mechanisms. However, such hardware is typically evaluated in isolation and its impact on the energy consumption of the rest of the processor, for example, due to wrong-path executions, is ignored. Moreover, the available metrics that would provide a thorough evaluation of an architectural optimization employ somewhat complicated formulas with hard-to-measure parametersIn this paper, we introduce a simple method to accurately compare the energy-efficiency of speculative architectures. Our metric is based on runtime analysis of the entire processor chip and thus captures the energy consumption due to the positive as well as the negative activities that arise from the speculation activities. We demonstrate the usefulness of our metric on the example of value speculation, where we found some proposed value predictors, including low-power designs, not to be energy-efficient",On the energy-efficiency of speculative hardware
"['Guy Cloutier', 'Danmin Chen', 'Louis-Gilles Durand']","Several strategies, known as clutter or wall Doppler filtering, were proposed to remove the strong echoes produced by stationary or slow moving tissue structures from the Doppler blood flow signal. In this study, the matching pursuit (MP) method is proposed to remove clutter components. The MP method decomposes the Doppler signal into wavelet atoms that are selected in a decreasing energy order. Thus, the high-energy clutter components are extracted first. In the present study, the pulsatile Doppler signal s(n) was simulated by a sum of random-phase sinusoids. Two types of high-amplitude clutter signals were then superimposed on s(n): time-varying low-frequency components, covering systole and early diastole, and short transient clutter signals, distributed within the whole cardiac cycle. The Doppler signals were modeled with the MP method and the most dominant atoms were subtracted from the time-domain signal s(n) until the signal-to-clutter (S/C) ratio reached a maximum. For the low-frequency clutter signal, the improvement in S/C ratio was 19.0 /spl plusmn/ 0.6 dB, and 72.0 /spl plusmn/ 4.5 atoms were required to reach this performance. For the transient clutter signal, ten atoms were required and the maximum improvement in S/C ratio was 5.5 /spl plusmn/ 0.5 dB. The performance of the MP method was also tested on real data recorded over the common carotid artery of a normal subject. Removing 15 atoms significantly improved the appearance of the Doppler sonogram contaminated with low-frequency clutter. Many more atoms (over 200) were required to remove transient clutter components. These results suggest the possibility of using this signal processing approach to implement clutter rejection filters on ultrasound commercial instruments.",A new clutter rejection algorithm for Doppler ultrasound
"['Alard Roebroeck', 'Anil K. Seth', 'Pedro Valdes-Sosa']","This review focuses on dynamic causal analysis of functional magnetic resonance (fMRI) data to infer brain connectivity from a time series analysis and dynamical systems perspective. Causal influence is expressed in the Wiener-Akaike-Granger-Schweder (WAGS) tradition and dynamical systems are treated in a state space modeling framework. The nature of the fMRI signal is reviewed with emphasis on the involved neuronal, physiological and physical processes and their modeling as dynamical systems. In this context, two streams of development in modeling causal brain connectivity using fMRI are discussed: time series approaches to causality in a discrete time tradition and dynamic systems and control theory approaches in a continuous time tradition. This review closes with discussion of ongoing work and future perspectives on the integration of the two approaches.",Causal time series analysis of functional magnetic resonance imaging data
"['Karthikeyan Sundaresan', 'Raghupathy Sivakumar', 'Mary Ann Ingram', 'Tae-Young Chang']","we present a medium access control (MAC) protocol for ad hoc networks with multiple input multiple output (MIMO) links. MIMO links provide extremely high spectral efficiencies in multipath channels by simultaneously transmitting multiple independent data streams in the same channel. MAC protocols have been proposed in related work for ad hoc networks with other classes of smart antennas such as switched beam antennas. However, as we substantiate in the paper, the unique characteristics of MIMO links coupled with several key optimization considerations, necessitate an entirely new MAC protocol. We identify several advantages of MIMO links, and discuss key optimization considerations that can help in realizing an effective MAC protocol for such an environment. We present a centralized algorithm called stream-controlled medium access (SCMA) that has the key optimization considerations incorporated in its design. Finally, we present a distributed SCMA protocol that approximates the centralized algorithm and compare its performance against that of baseline protocols that are CSMA/CA variants.",Medium access control in ad hoc networks with MIMO links: optimization considerations and algorithms
"['Lorenzo Mucchi', 'Luca Simone Ronga', 'E. Del Re']","Due to the enormous spreading of applied wireless networks, security is actually one of the most important issues for telecommunications. One of the main issue in the field of securing wireless information exchanging is the initial common knowledge between source and destination. A shared secret is normally mandatory in order to decide the encryption (algorithm or code or key) of the information stream. It is usual to exchange this common a priori knowledge by using a ""secure"" channel. Now a days a secure wireless channel is not possible. In fact normally the common a priori knowledge is already established (but this is not secure) or by using a non-radio channel (that implies a waste of time and resource). This contribution deals with the proposal of a new modulation technique ensuring secure communication in a full wireless environment. The information is modulated, at physical layer, by the thermal noise experienced by the link between two terminals. A loop scheme is designed for unique recovering of mutual information. The probability of error/detection is analytically derived for the legal users and for the third unwanted listener. The proposed scheme has also been implemented in a Xilinx Virtex II FPGA.#R##N##R##N#All the results show that the performance of the proposed scheme yields the advantage of intrinsic security, i.e., the mutual information cannot be physically demodulated (passive attack) or denied (active attack) by a third terminal, leading us to conclude that the proposed technique is really useful for private key distribution in every wireless network.",Design and Implementation of Physical Layer Private Key Setting for Wireless Networks
"['Nan Zou', 'Arye Nehorai']","Undersea warfare relies heavily on acoustic means to detect a submerged vessel. The frequency of the acoustic signal radiated by the vessel is typically very low, thus requires a large array aperture to achieve acceptable angular resolution. In this paper, we present a novel approach for low-frequency direction-of-arrival (DOA) estimation using miniature circular vector-sensor array mounted on the perimeter of a cylinder. Under this approach, we conduct beamforming using decomposition in the acoustic mode domain rather than frequency domain, to avoid the long wavelength constraints. We first introduce a multi-layer acoustic gradient scattering model to provide a guideline and performance predication tool for the mode beamformer design and algorithm. We optimize the array gain and frequency response with this model. We further develop the adaptive DOA estimation algorithm based on this model. We formulate the Capon spectra of the mode beamformer which is independent of the frequency band after the mode decomposition. Numerical simulations are conducted to quantify the performance and evaluate the theoretical results developed in this study.",Circular Acoustic Vector-Sensor Array for Mode Beamforming
"['Rocco De Nicola', 'Daniele Gorla', 'Rosario Pugliese']","We study the expressive power of variants of KLAIM, an experimental language with programming primitives for network-aware programming that combines the process algebra approach with the coordination-oriented one. KLAIM has proved to be suitable for programming a wide range of distributed applications with agents and code mobility, and has been implemented on the top of a runtime system written in Java. In this paper, the expressivity of its constructs is tested by distilling from it a few, more and more foundational, languages and by studying the encoding of each of them into a simpler one. The expressive power of the considered calculi is finally tested by comparing one of them with asynchronous ??-calculus.",On the expressive power of KLAIM-based calculi
"['Liang Liu', 'Yunhong Wang', 'Qian Wang', 'Tieniu Tan']","In this paper, we propose a fast algorithm for principal component analysis (PCA) dealing with large high-dimensional data sets. A large data set is firstly divided into several small data sets. Then, the traditional PCA method is applied on each small data set and several eigenspace models are obtained, where each eigenspace model is computed from a small data set. At last, these eigenspace models are merged into one eigenspace model which contains the PCA result of the original data set. Experiments on the FERET data set show that this algorithm is much faster than the traditional PCA method, while the principal components and the reconstruction errors are almost the same as that given by the traditional method.",Fast Principal Component Analysis using Eigenspace Merging
"['Steven A. Wright', 'Simon J. Pennycook', 'Stephen A. Jarvis']","As the High Performance Computing industry moves towards the exascale era of computing, parallel scientific and engineering applications are becoming increasingly complex. The use of simulation allows us to predict how an application's performance will change with the adoption of new hardware or software, helping to inform procurement decisions. In this paper, we present a disk simulator designed to predict the performance of read and write operations to a single hard disk drive (HDD). Our simulator uses a geometry discovery benchmark (Diskovery) in order to estimate the data layout of the HDD, as well as the time spent moving the read/write head. We validate our simulator against two different HDDs, using a benchmark designed to simulate common disk read and write patterns, demonstrating accuracy to within 5% of the observed I/O time for sequential operations, and to within 10% of the observed time for seek-heavy workloads.",Towards the Automated Generation of Hard Disk Models through Physical Geometry Discovery
"['Zhike Zhang', 'Weiqiang Wang']","In this paper, we presents a new binarization approach to extract text pixels from complex background in video frames. The binarization computation is a crucial step for, video text recognition, which can greatly increase the recognition, accuracy of an OCR software. The proposed approach consists, of four phases. First, the text polarity is determined, i.e. light text with dark background or dark text with light background., Then the pixels in the given image are clustered into K clusters, using the K-means algorithm in the RGB color space and the, text cluster is selected based on the text polarity. Further, the, MRF Model is exploited to get the binarization result. Finally, the, result is further refined by the Log-Gabor filter. The Experimental, results on a large dataset show that the significant gains have been, obtained according to the segmentation performance on the pixel, level as well as the OCR accuracy.",A Novel Approach for Binarization of Overlay Text
"['Bing-Fei Wu', 'Chung-Fu Lin']","JPEG2000 is a new international standard for still image compression. It provides various functions in one single coding stream and the better compression quality than the traditional JPEG, especially in the high compression ratio. However, the heavy computation and large internal memory requirement still restrict the consumer electronics applications. In this paper, we propose a QCB (quad code block)-based DWT method to achieve the higher parallelism than the traditional DWT approach of JPEG2000 coding process. Based on the QCB-based DWT engine, three code blocks can be completely generated after every fixed time slice recursively. Thus, the DWT and EBCOT processors can process simultaneously and the high computational EBCOT then has the higher parallelism of the JPEG2000 encoding system. By changing the output timing of the DWT process and parallelizing with EBCOT, the internal tile memory size can be reduced by a factor of 4. The memory access cycles between the internal tile memory and the code block memory also decrease with the smooth encoding flow.",An efficient architecture for JPEG2000 coprocessor
"['Noam Koenigstein', 'Yuval Shavitt', 'Noa Zilberman']","Peer to Peer networks are the leading cause for music piracy but also used for music sampling prior to purchase. In this paper we investigate the relations between music file sharing and sales (both physical and digital)using large Peer-to-Peer query database information. We compare file sharing information on songs to their popularity on the Billboard Hot 100 and the Billboard Digital Songs charts, and show that popularity trends of songs on the Billboard have very strong correlation (0.88-0.89) to their popularity on a Peer-to-Peer network. We then show how this correlation can be utilized by common data mining algorithms to predict a song's success in the Billboard in advance, using Peer-to-Peer information.",Predicting Billboard Success Using Data-Mining in P2P Networks
"['Jonas Lefevere', 'Knut De Swert', 'Stefaan Walgrave']","Common people that are apparently randomly selected by journalists to illustrate a news story (popular exemplars) have a substantial effect on what the audience think about the issue. This effect may be partly due to the mere fact that popular exemplars attract attention and act as attention commanders just like many other speaking sources in the news. Yet, popular exemplarsÉ?? effects extend well beyond that of other talking sources. Due to their similarity, trustworthiness, and the vividness of their account, popular exemplars have significantly more impact than experts that are being interviewed or, in particular, than politicians that are quoted in the news. We show this drawing on an internet-based experiment that uses fake television news items as stimuli and that systematically compares the effect of these talking sources in the news. We also find that taking into account preexisting attitudes changes the findings substantially. The effects are more robust and yield a more nuanced picture of what typ...",Effects of Popular Exemplars in Television News
"['Yariv Ephraim', 'William J. Roberts']",We derive an explicit expression for the covariance of the log-periodogram power spectral density estimator for a zero mean Gaussian process. We do not make the assumption that the spectral components of the process are uncorrelated. Applications to spectral estimation and to cepstral modeling in automatic speech recognition are discussed.,On second-order statistics of log-periodogram with correlated components
"['Yl Tsai', 'Christopher Rose', 'Ruochen Song', 'I. Saira Mian']","We derive the maximum mutual information for an additive exponential noise (AEN) channel with a peak input constraint. We find that the optimizing input density is mixed (with singularities) similar to previous results for AEN channels with a mean input constraint. Likewise, the maximum mutual information takes a similar form, though obviously the maximum for the peak constraint is smaller than for the corresponding mean-constrained channel. This model is inspired by multiple biological phenomena and processes which can be abstracted as follows: inscribed matter is sent by an emitter, moves through a medium, and arrives eventually at its destination receptor. The inscribed matter can convey information in a variety of ways such as the number of signaling quanta - molecules, macromolecular complexes, organelles, cells and tissues - that are emitted as well as the detailed pattern of their release. However, rather than focus on a general class of emitter-receptor systems or a particular exemplar of biomedical importance, our ultimate goal is to provide bounds on the potential efficacy of timed-release signaling for any system which emits identical signaling quanta. That is, we seek to apply one of the most potent aspects of information theory to biological signaling - mechanism blindness - in the hopes of gaining insights applicable to diverse systems that span a wide range of spatiotemporal scales.",An additive exponential noise channel with a transmission deadline
"['Gyu Bum Kyung', 'Chih-Chun Wang']","This work provides an exhaustive search algorithm for finding small fully absorbing sets (FASs) of arbitrary low-density parity-check (LDPC) codes. In particular, given any LDPC code, the problem of finding all FASs of size less than t is formulated as an integer programming problem, for which a new branch-&-bound algorithm is devised. New node selection and the tree-trimming mechanisms are designed to further enhance the efficiency of the algorithm. The proposed algorithm is capable of finding all FASs of size É?? 11 with no larger than 2 induced odd-degree check nodes for LDPC codes of length É?? 1000. The resulting exhaustive list of small FASs is then used to devise a new post-processing decoder. Numerical results show that by taking advantage of the exhaustive list of small FASs, the proposed decoder significantly lowers the error floor for codes of practical lengths and outperforms the state-of-the-art low-error-floor decoders.",Exhaustive search for small fully absorbing sets and the corresponding low error-floor decoder
"['Ilhan Aydin', 'Mehmet Karakose', 'Erhan Akin']","Support vector machine (SVM) is a classification method based on the structured risk minimization principle. Penalize, C; and kernel, @s parameters of SVM must be carefully selected in establishing an efficient SVM model. These parameters are selected by trial and error or man's experience. Artificial immune system (AIS) can be defined as a soft computing method inspired by theoretical immune system in order to solve science and engineering problems. A multi-objective artificial immune algorithm has been used to optimize the kernel and penalize parameters of SVM in this paper. In training stage of SVM, multiple solutions are found by using multi-objective artificial immune algorithm and then these parameters are evaluated in test stage. The proposed algorithm is applied to fault diagnosis of induction motors and anomaly detection problems and successful results are obtained.",A multi-objective artificial immune algorithm for parameter optimization in support vector machine
"['Youngsoo Kim', 'Jeonggyun Yu', 'Sunghyun Choi', 'Kyunghun Jang']","The popular IEEE 802.11 wireless local area network (WLAN) is based on a carrier sense multiple access with collision avoidance (CSMA/CA), where a station listens to the medium before transmission in order to avoid collision. If there exist stations which can not hear each other, i.e., hidden stations, the potential collision probability increases, thus dramatically degrading the network throughput. The RTS/CTS (request-to-send/clear-to-send) frame exchange is a solution for the hidden station problem, but the RTS/CTS exchange itself consumes the network resources by transmitting the control frames. In order to maximize the network throughput, we need to use the RTS/CTS exchange adaptively only when hidden stations exist in the network. In this letter, a simple but very effective hidden station detection mechanism is proposed. Once a station detects the hidden stations via the proposed detection mechanism, it can trigger the usage of the RTS/CTS exchange. The simulation results demonstrate that the proposed mechanism can provide the maximum system throughput performance",A novel hidden station detection mechanism in IEEE 802.11 WLAN
"['Tanaphol Thaipanich', 'Ping-Hao Wu', 'C.-c. Kuo']","Two challenging situations for video frame rate up-conversion (FRUC) are first identified and analyzed; namely, when the input video has abrupt illumination change and/or a low frame rate. Then, a low-complexity processing technique and robust FRUC algorithm are proposed to address these two issues. The proposed algorithm utilizes a translational motion vector model of the first- and the second-order and detects the continuity of these motion vectors. Additionally, in order to improve perceptual quality of interpolated frame, spatial smoothness criterion is employed. The superior performance of the proposed algorithm has been tested extensively and representative examples are given in this work.",Low complexity algorithm for robust video frame rate up-conversion (FRUC) technique
"['Christos Tryfonas', 'Anujan Varma']","We propose and analyze several strategies for performing timestamping of an MPEG-2 Transport Stream transmitted over a packet-switched network using the PCR-unaware encapsulation scheme, and analyze their effect on the quality of the recovered clock at the MPEG-2 Systems decoder. When the timestamping scheme is based on a timer with a fixed period, the PCR values in the packet stream may switch polarity deterministically, at a frequency determined by the timer period and the transport rate of the MPEG signal. This, in turn, can degrade the duality of the recovered clock at the receiver beyond acceptable limits. We consider three timestamping schemes for solving this problem: (1) selecting a deterministic timer period to avoid the phase difference in PCR values altogether, (2) fine-tuning the deterministic timer period to maximize the frequency of PCR polarity changes, and (3) selecting the timer period randomly to eliminate the deterministic PCR polarity changes. For the case of deterministic timer period, we derive the frequency of the PCR polarity changes as a function of the timer period and the transport rate, and use it to find ranges of the timer period for acceptable quality of the recovered clock. We also analyze a random timestamping procedure based on a random telegraph process and obtain lower bounds on the rate of PCR polarity changes such that the recovered clock does not violate the PAL/NTSC clock specifications. The analytical results are verified by simulations with both synthetic and actual MPEG-2 Transport Streams sent to a simulation model of an MPEG-2 Systems decoder.",Timestamping schemes for MPEG-2 systems layer and their effect on receiver clock recovery
"['Yoshihiko Mochizuki', 'Atsushi Imiya']","The pyramid transform compresses images while preserving global features such as edges and segments. The pyramid transform is efficiently used in optical flow computation starting from planar images captured by pinhole camera systems, since the propagation of features from coarse sampling to fine sampling allows the computation of both large displacements in low-resolution images sampled by a coarse grid and small displacements in high-resolution images sampled by a fine grid.#R##N##R##N#The image pyramid transform involves the resizing of an image by downsampling after convolution with the Gaussian kernel. Since the convolution with the Gaussian kernel for smoothing is derived as the solution of a linear diffusion equation, the pyramid transform is performed by applying a downsampling operation to the solution of the linear diffusion equation.",Pyramid transform and scale-space analysis in image analysis
"['Vijay Parsa', 'Donald G. Jamieson']","Adaptive modeling of digital hearing aids is useful in characterizing the hearing aid behavior in response to É??real worldÉ?ù stimuli such as speech and music. Most modern hearing aids employ amplitude compression in different frequency bands for effective mapping of the wide dynamic range audio signals into the reduced dynamic range of the hearing impaired listeners. Due to the presence of independent compression channels, the conventional fullband adaptive model might not adequately characterize the performance of a multichannel compression hearing aid (MCHA). In this paper, we propose a subband adaptive modeling approach to characterize the electroacoustic performance of a MCHA. The proposed structure employs uniform, oversampled DFT filterbanks for analysis and synthesis, and the affine projection algorithm for adaptive modeling in each subband. Experiments with simulated MCHAs showed that the subband structure outperforms the fullband structure under a variety of operating conditions.",Adaptive modelling of digital hearing aids using a subband affine projection algorithm
"['Ran Manevich', 'Israel Cidon', 'Avinoam Kolodny', ""Isask'har Walter"", 'Shmuel Wimer']","As the number of applications and programmable units in CMPs and MPSoCs increases, the Network-on-Chip (NoC) encounters unpredictable, heterogeneous and time dependent traffic loads. This motivates the introduction of adaptive routing mechanisms that balance the NoC's loads and achieve higher throughput compared with traditional oblivious routing schemes. An effective adaptive routing scheme should be based on a global view of the network state. However, most current adaptive routing schemes, following off-chip networks, are based on distributed reactions to local congestion. In this paper we leverage the unique on-chip capabilities and introduce a novel paradigm of NoC centralized adaptive routing. Our scheme continuously monitors the global traffic load in the network and modifies the routing of packets to improve load balancing accordingly. We present a specific design for the case of mesh topology, where XY or YX routes are adaptively selected for each source-destination pair. We show that while our implementation is lightweight and scalable in hardware costs, it outperforms oblivious and distributed adaptive routing schemes in terms of load balancing and average packet delay.",A Cost Effective Centralized Adaptive Routing for Networks-on-Chip
"['Petra Bilane', 'St??phane Bres', 'Hubert Emptoz']","This paper presents a contribution to Word Spotting applied for digitized Syriac manuscripts. The Syriac language was wrongfully accused of being a dead language and has been set aside by the domain of handwriting recognition. Yet it is a very fascinating handwriting that combines the word structure and calligraphy of the Arabic handwriting with the particularity of being intentionally written tilted by an angle of approximately 45deg. For the spotting process, we developed a method that should find all occurrences of a certain query word image, based on a selective sliding window technique, from which we extract directional features and afterwards perform a matching using Euclidean distance correspondence between features. The proposed method does not require any prior information, and does not depend of a word to character segmentation algorithm which would be extremely complex to realize due to the tilted nature of the handwriting.",Robust directional features for wordspotting in degraded Syriac manuscripts
"['Ahmed K. Farahat', 'Mohamed S. Kamel']",Document clustering algorithms usually use vector space model (VSM) as their underlying model for document representation. VSM assumes that terms are independent and accordingly ignores any semantic relations between them. This results in mapping documents to a space where the proximity between document vectors does not reflect their true semantic similarity. This paper proposes new models for document representation that capture semantic similarity between documents based on measures of correlations between their terms. The paper uses the proposed models to enhance the effectiveness of different algorithms for document clustering. The proposed representation models define a corpus-specific semantic similarity by estimating measures of termÉ??term correlations from the documents to be clustered. The corpus of documents accordingly defines a context in which semantic similarity is calculated. Experiments have been conducted on thirteen benchmark data sets to empirically evaluate the effectiveness of the proposed models and compare them to VSM and other well-known models for capturing semantic similarity.,Statistical semantics for enhancing document clustering
"['Catalin I. Tomai', 'Bin Zhang', 'Venu Govindaraju']","There is a large number of scanned historical documents that need to be indexed for archival and retrieval purposes. A visual word spotting scheme that would serve these purposes is a challenging task even when the transcription of the document image is available. We propose a framework for mapping each word in the transcript to the associated word image in the document. Coarse word mapping based on document constraints is used for lexicon reduction. Then, word mappings are refined using word recognition results by a dynamic programming algorithm that finds the best match while satisfying the constraints.",Transcript mapping for historic handwritten document images
"['Rahul Chipalkatty', 'Greg Droge', 'Magnus Egerstedt']","This paper presents a new method for injecting human inputs into mixed-initiative interactions between humans and robots. The method is based on a model-predictive control (MPC) formulation, which inevitably involves predicting the system (robot dynamics as well as human input) into the future. These predictions are complicated by the fact that the human is interacting with the robot, causing the prediction method itself to have an effect on future human inputs. We investigate and develop different prediction schemes, including fixed and variable horizon MPCs and human input estimators of different orders. Through a search-and-rescue-inspired human operator study, we arrive at the conclusion that the simplest prediction methods outperform the more complex ones, i.e., in this particular case, less is indeed more.",Less Is More: Mixed-Initiative Model-Predictive Control With Human Inputs
"['Tzu-Chien Chang', 'Kuochen Wang', 'Yi-Ling Hsieh']","There are few localization schemes targeted at mobile wireless sensor networks. This paper proposed an enhanced color-theory-based dynamic localization (E-CDL) which is based on the CDL algorithm (Shee, 2005). However, the location accuracy of this algorithm depends on the accuracy of the average hop distance derivation. Therefore, the authors present two novel schemes to estimate the average hop distance. The authors analyzed the behavior of sensor nodes communication, and computed the expected value of the average hop distance, which is 7r/9 where r is the radio range. In addition, since CDL is based on the DV-hop scheme, the derived shortest path length is usually larger than the corresponding Euclidean distance. With this observation, the derived shortest path length can be adjusted by the ratio of the Euclidean distance and the shortest path distance to further enhance the location accuracy. Finally, in mobile wireless sensor networks, sensor nodes may become isolated. By employing mobile anchor nodes, the isolation problem can be relieved and hence the location accuracy can be improved. Simulation results have shown that the location accuracy of E-CDL is 50%-55% better than that of CDL, and 75%-80% better than that of MCL (Monte Carlo localization) (Lingxuan and David, 2004), In addition, the authors have implemented and verified our algorithm on the MICAz Mote developer's kit.",Enhanced Color-Theory-Based Dynamic Localization in Mobile Wireless Sensor Networks
"['Chun Chen', 'Raymond N. J. Veldhuis', 'Tom A. M. Kevenaar', 'Anton H. M. Akkermans']","Extracting binary strings from real-valued templates has been a fundamental issue in many biometric template protection systems. In this paper, we present an optimal bit allocation method (OBA). By means of it, a binary string at a pre-defined length with maximized overall detection rate is generated. Experiments with the binary strings and a Hamming distance classifier on FRGC and FERET databases show promising performance in terms of FAR and FRR.",Biometric binary string generation with detection rate optimized bit allocation
"['Maxime Gautier', 'Pierre-Olivier Vandanjon', 'Alexandre Janot']","Off-line robot dynamic identification methods are mostly based on the use of the inverse dynamic model, which is linear with respect to the dynamic parameters. This model is calculated with torque and position sampled data while the robot is tracking reference trajectories that excite the system dynamics. This allows using linear least-squares techniques to estimate the parameters. This method requires the joint force/torque and position measurements and the estimate of the joint velocity and acceleration, through the bandpass filtering of the joint position at high sampling rates. A new method called DIDIM (Direct and Inverse Dynamic Identification Models) has been proposed and validated on a 2 degree-of-freedom robot [1]. DIDIM method requires only the joint force/torque measurement. It is based on a closed-loop simulation of the robot using the direct dynamic model, the same structure of the control law, and the same reference trajectory for both the actual and the simulated robot. The optimal parameters minimize the 2-norm of the error between the actual force/torque and the simulated force/torque. A validation experiment on a 6 dof Staubli TX40 robot shows that DIDIM method is very efficient on industrial robots.",Dynamic identification of a 6 dof robot without joint position data
"['Wen Sun', 'Yan Lu', 'Feng Wu']","It has been commonly recognized that multiple bit-rate (MBR) encoding provides a concise method for video streaming over bandwidth-fluctuant networks. The key problem of the MBR technique lies in how to seamlessly switch one bit-stream to another one. To tackle this problem, we propose a bit-stream switching framework based on the Wyner-Ziv coding. Within the propose framework, the multiple bit-streams can be individually encoded without data exchange, which also supports the random switching at any desired frame without affecting the original coding efficiency of the regular bit-stream. In particular, two different implementation schemes under the same framework are presented. Different from the traditional switching schemes, the proposed method can use the same switching frame for the switching from any other bit-stream to the current one, which means less storage and less encoding efforts. Simulation results and comparison between the proposed method and the traditional switching method in H.264 are also presented.",Bit-Stream Switching in Multiple Bit-Rate Video Streaming using Wyner-Ziv Coding
"['Efrat German', 'Akiva Leibowitz', 'Yuval Shahar']","We describe and evaluate a framework, the Medical Database Adaptor (MEIDA), for linking knowledge-based medical decision-support systems (MDSSs) to multiple clinical databases, using standard medical schemata and vocabularies. Our solution involves a set of tools for embedding standard terms and units within knowledge bases (KBs) of MDSSs; a set of methods and tools for mapping the local database (DB) schema and the terms and units relevant to the KB of the MDSS into standardized schema, terms and units, using three heuristics (choice of a vocabulary, choice of a key term, and choice of a measurement unit); and a set of tools which, at runtime, automatically map standard term queries originating from the KB, to queries formulated using the local DB's schema, terms and units. The methodology was successfully evaluated by mapping three KBs to three DBs. Using a unit-domain matching heuristic reduced the number of term-mapping candidates by a mean of 71% even after other heuristics were used. Runtime access of 10,000 records required one second. We conclude that mapping MDSSs to different local clinical DBs, using the three-phase methodology and several term-mapping heuristics, is both feasible and efficient.",An architecture for linking medical decision-support applications to clinical databases and its evaluation
"['Jorma Sajaniemi', 'Pauli Byckling', 'Petri Gerdt']","Program visualization and animation has traditionally been done at the level of the programming language and its implementation in a computer. However, novices do not know these concepts and visualizations that build upon programming language implementation may easily fail in helping novices to learn programming concepts. Metaphor, on the contrary, involves the presentation of a new idea in terms of a more familiar one and can facilitate active learning. This paper applies a metaphor approach to object-oriented programming by presenting new metaphors for such concepts as class, object, object instantiation, method invocation, parameter passing, object reference, and garbage collection. The use of these metaphors in introductory programming education is also discussed.",Animation Metaphors for Object-Oriented Concepts
"['Jeff Riegel', 'Jai Menon']","A software RAID is a RAID implemented purely in software running on a host computer. One problem with software RAIDs is that they do not have access to special hardware such as NVRAM. Thus, software RAIDs may need to check every parity group of an array for consistency following a host crash or power failure. This process of checking parity groups is called recovery, and results in long delays when the software RAID is restarted. The authors review two algorithms to reduce this recovery time for software RAIDs: the PGS bitmap algorithm and the list algorithm. They compare the performance of these two algorithms using trace-driven simulations. Their results show that the PGS bitmap algorithm can reduce recovery time by a factor of 12 with a response time penalty of less than 1%, or by a factor of 50 with a response time penalty of less than 2%, and a memory requirement of around 9 Kbytes. The list algorithm can reduce recovery time by a factor of 50 but cannot achieve a response time penalty of less than 16%.",Performance of recovery time improvement algorithms for software RAIDs
"['Naveen Kaushik', 'Brajesh Kumar Kaushik', 'Davinder Kaur', 'Manoj Kumar Majumder']","The read-write ability of SRAM cells is one of the major concern in nanometer regime. This paper analyzes the stability and performance of asymmetric FinFET based different schematic of 6T SRAM cells. The proposed structure exploits asymmetrical behavior of current to improve read-write stability of SRAM. By exploiting the asymmetricity in proposed structure, contradiction between read and write noise margin (RNM and WNM) is relaxed. The overall improvements in static, read and write noise margins for proposed asymmetric FinFET based independent gate SRAM (IGSRAM) are 28%, 71%, and 31% respectively.",Independent gate SRAM based on asymmetric gate to source/drain overlap-underlap device FinFET
"['James M. Conrad', 'Dharma P. Agrawal']","Several arc consistency algorithms for sequential and parallel processing computers are reviewed. Three distributed parallel arc consistency algorithms-DSPAC-1, DSPAC-2, and DSPAC-3-are introduced and compared with existing algorithms. Through actual machine experimentation the time required for the DSPAC algorithms was measured and compared with that for existing sequential algorithms. Results indicate that the parallel arc consistency algorithms are very effective and that scalability can be efficiently maintained. >","Distributed, scalable, and static parallel arc consistency algorithms on private memory machines"
"['Mosin Mondal', 'Tamer Ragheb', 'Xiang Wu', 'Adnan Aziz', 'Yehia Massoud']","A network-on-chip (NoC) replaces on-chip communication implemented by point-to-point interconnects in a multi-core environment by a set of shared interconnects connected through programmable crosspoints. Since an NoC may provide a number of paths between a given source and destination, manufacturing or runtime faults on one interconnect does not necessarily render the chip useless. It is partly because of this fault tolerance that NoCs have emerged as a viable alternative for implementing communication between functional units of a chip in the nanometer regime, where high defect rates are prevalent. In this paper, the authors quantify the fault tolerance offered by an NoC against process variations. Specifically, the authors develop an analytical model for the probability of failure in buffered global NoC links due to interconnect dishing, and effective channel length variation. Using the developed probability model, the authors study the impact of link failure on the number of cycles required to establish communications in NoC applications",Provisioning On-Chip Networks under Buffered RC Interconnect Delay Variations
"['Frankie K. W. Chan', 'Hing-Cheung So']","Localization of sensor nodes is a fundamental and important problem in wireless sensor networks. In this correspondence, a recursive distributed positioning algorithm is devised with the use of range measurements. Computer simulations are included to contrast the performance of the proposed approach with the conventional semi-definite relaxation positioning method as well as Crameacuter-Rao lower bound.",Accurate Distributed Range-Based Positioning Algorithm for Wireless Sensor Networks
"['T. William J. Moorhead', 'Viktoria-Eleni Gountouna', 'Dominic Job', 'Andrew M. McIntosh', 'Liana Romaniuk', 'G. Katherine S. Lymer', 'Heather C. Whalley', 'Gordon D. Waiter', 'David Brennan', 'Trevor S. Ahearn', 'Jonathan Cavanagh', 'Barrie Condon', 'J. Douglas Steele', 'Joanna M. Wardlaw', 'Stephen M. Lawrie']","Background#R##N#Structural Magnetic Resonance Imaging (sMRI) of the brain is employed in the assessment of a wide range of neuropsychiatric disorders. In order to improve statistical power in such studies it is desirable to pool scanning resources from multiple centres. The CaliBrain project was designed to provide for an assessment of scanner differences at three centres in Scotland, and to assess the practicality of pooling scans from multiple-centres.",Prospective multi-centre Voxel Based Morphometry study employing scanner specific segmentations : procedure development using CaliBrain structural MRI data
"['Lu ?? is Soares', 'Jos?? Pereira']","Many rely now on public cloud infrastructure-as-a-service for database servers, mainly, by pushing the limits of existing pooling and replication software to operate large shared-nothing virtual server clusters. Yet, it is unclear whether this is still the best architectural choice, namely, when cloud infrastructure provides seamless virtual shared storage and bills clients on actual disk usage.   This paper addresses this challenge with Resilient Asynchronous Commit (RAsC), an improvement to awell-known shared-nothing design based on the assumption that a much larger number of servers is required for scale than for resilience. Then we compare this proposal to other database server architectures using an analytical model focused on peak throughput and conclude that it provides the best performance/cost trade-off while at the same time addressing a wide range of fault scenarios.",Improving the scalability of cloud-based resilient database servers
"['Claudio Sacchi', 'Carlo S. Regazzoni', 'Gianni Vernazza']","Lately, the interest in advanced video-based surveillance applications has been increasing. This is especially true in the field of urban railway transport where video-based surveillance can be exploited to face many relevant security aspects (e.g. vandalism, overcrowding, abandoned object detection etc.). This paper aims at investigating an open problem in the implementation of video-based surveillance systems for transport applications, i.e., the implementation of reliable image understanding modules in order to recognize dangerous situations with reduced false alarm and misdetection rates. We considered the use of a neural network-based classifier for detecting vandal behavior in metro stations. The achieved results show that the classifier achieves very good performance even in the presence of high scene complexity.",A neural network-based image processing system for detection of vandal acts in unmanned railway environments
"['Desikachari Nadadur', 'Robert M. Haralick']","Performing morphological operations such as dilation and erosion of binary images, using very long line structuring elements is computationally expensive when performed brute-force following definitions. We present two-pass algorithms that run at constant time for obtaining binary dilations and erosions with all possible length line structuring elements, simultaneously. The algorithms run at constant time for any orientation of the line structuring element. Another contribution of this paper is the use of the concept of orientation error between a continuous line and its discrete counterpart. The orientation error is used in determining the minimum length of the basic digital line structuring element used in obtaining what we call dilation and erosion transforms. The transforms are then thresholded by the length of the desired structuring element to obtain the dilation and erosion results. The algorithms require only one maximum operation for erosion transform and only one minimum operation for dilation transform, and one thresholding step and one translation step per result pixel. We tested the algorithms on Sun Sparc Station 10, on a set of 240/spl times/250 salt and pepper noise images with probability of a pixel being a 1-pixel set to 0.25, for orientations of the normals of the structuring elements in the range [/spl pi//2,3/spl pi//2] and lengths, in pixels, in the range [5,145]. We achieved a speed up of about 50 (and for special orientations /spl theta/ /spl isin/ {(/spl pi//2), (3/spl pi//4), /spl pi/, (5/spl pi//4), (3/spl pi//2)} a speed up of about 100) when the structuring elements had lengths of 145 pixels, over the brute-force methods in these experiments. We compared the results of our dilation algorithm with those of the algorithm discussed by Soille et al. (see IEEE Trans. Pattern Anal. Machine Intell., vol.18, p.562-67, 1996) and showed that for binary dilation (and erosion since it is just the dilation of the background with the reflected structuring element) our algorithm performed better and achieved a speed up of about four when dilation or erosion transform alone is obtained.",Recursive binary dilation and erosion using digital line structuring elements in arbitrary orientations
"['Dongsheng Yu', 'Herbert Ho-Ching Iu', 'Andrew Lewis Fitch', 'Yan Liang']",,A Floating Memristor Emulator Based Relaxation Oscillator
"['Vincent Mousseau', 'Jos?? Rui Figueira', 'J.-Ph. Naux']","Given a finite set of alternatives A, the sorting (or assignment) problem consists in the assignment of each alternative to one of the pre-defined categories. In this paper, we are interested in multiple criteria sorting problems and, more precisely, in the existing method ELECTRE TRI. This method requires the elicitation of preferential parameters (weights, thresholds, category limits,É??) in order to construct a preference model which the decision maker (DM) accepts as a working hypothesis in the decision aid study. A direct elicitation of these parameters requiring a high cognitive effort from the DM (V. Mosseau, R. Slowinski, Journal of Global Optimization 12 (2) (1998) 174), proposed an interactive aggregationÉ??disaggregation approach that infers ELECTRE TRI parameters indirectly from holistic information, i.e., assignment examples. In this approach, the determination of ELECTRE TRI parameters that best restore the assignment examples is formulated through a nonlinear optimization program.#R##N##R##N#In this paper, we consider the subproblem of the determination of the weights only (the thresholds and category limits being fixed). This subproblem leads to solve a linear program (rather than nonlinear in the global inference model). Numerical experiments were conducted so as to check the behaviour of this disaggregation tool. Results showed that this tool is able to infer weights that restores in a stable way the assignment examples and that it is able to identify É??inconsistenciesÉ?ù in the assignment examples.",Using assignment examples to infer weights for ELECTRE TRI method: Some experimental results
"['Adam Teman', 'Lidor Pergament', 'Omer Cohen', 'Alexander Fish']","Low voltage operation of digital circuits continues to be an attractive option for aggressive power reduction. As standard SRAM bitcells are limited to operation in the strong-inversion regimes due to process variations and local mismatch, the development of specially designed SRAMs for low voltage operation has become popular in recent years. In this paper, we present a novel 9T bitcell, implementing a Supply Feedback concept to internally weaken the pull-up current during write cycles and thus enable low-voltage write operations. As opposed to the majority of existing solutions, this is achieved without the need for additional peripheral circuits and techniques. The proposed bitcell is fully functional under global and local variations at voltages from 250 mV to 1.1 V. In addition, the proposed cell presents a low-leakage state reducing power up to 60%, as compared to an identically supplied 8T bitcell. An 8 kbit SF-SRAM array was implemented and fabricated in a low-power 40 nm process, showing full functionality and ultra-low power.",A 250 mV 8 kb 40 nm Ultra-Low Power 9T Supply Feedback SRAM (SF-SRAM)
"['Marek Kurdej', 'Julien Moras', 'Veronique Cherfaoui', 'Philippe Bonnifait']","Evidential grids have recently been shown to have interesting properties for mobile object perception. Possessing only partial information is a frequent situation when driving in complex urban areas, and by making use of the Dempster-Shafer framework, evidential grids are able to handle partial information efficiently. This article deals with a lidar perception scheme that is enhanced by geo-referenced maps used as an additional source of information in a multi-grid fusion framework. The paper looks at the key stages of such a data fusion process and presents an adaptation of the conjunctive combination rule for refining the analysis of conflicting information. This method relies on temporal accumulation to distinguish between stationary and moving objects, and applies contextual discounting for modeling information obsolescence. As a result, the method is able to better characterize the state of the occupied cells by differentiating moving objects, parked cars, urban infrastructure and buildings. Another advantage of this approach is its ability to separate the drivable from the non-drivable free space. Experiments carried out in real traffic conditions with a specially equipped car illustrate the performance of this approach.",Map-Aided Evidential Grids for Driving Scene Understanding
"['Tomoya Tandai', 'Kiyoshi Toshimitsu', 'Takafumi Sakamoto']","In this manuscript, an interferential packet detection scheme in IEEE 802.11 WLANs is proposed. If another Basic Service Set (BSS) is overlapping domestic BSS, some Stations (STAs) may suffer from interference from a hidden terminal in overlapping BSS (OBSS). One of the best ways to avoid this undesirable situation is channel switching after detecting OBSS. However there are some difficulties to recognize existence of OBSS. One difficulty is that STAs in domestic BSS can't receive frames from OBSS correctly and can't check BSSID of frames if traffic in domestic BSS is heavy and if transmission rates of frames from OBSS are higher. Another difficulty is that if the cell radius of domestic BSS is smaller than that of OBSS, some STAs in OBSS may transmit frames asynchronously and interfere with transmissions in domestic BSS. If interference causes frame error, domestic STA can?t distinguish interference from degradation of channel condition. This paper proposes a method whereby STAs can detect interferential packet even while STAs receive frames from domestic BSS. If STAs detect interference, channel switching is performed dynamically. The probability of detecting interferential packets is evaluated by computer simulation, and the results confirm the effectiveness of the proposed method.",Interferential Packet Detection Scheme for a Solution to Overlapping BSS Issues in IEEE 802.11 WLANs
"['Dmitriy Leykekhman', 'Dominik Meidner', 'Boris Vexler']","In this paper we consider a model elliptic optimal control problem with finitely many state constraints in two and three dimensions. Such problems are challenging due to low regularity of the adjoint variable. For the discretization of the problem we consider continuous linear elements on quasi-uniform and graded meshes separately. Our main result establishes optimal a priori error estimates for the state, adjoint, and the Lagrange multiplier on the two types of meshes. In particular, in three dimensions the optimal second order convergence rate for all three variables is possible only on properly refined meshes. Numerical examples at the end of the paper support our theoretical results.",Optimal error estimates for finite element discretization of elliptic optimal control problems with finitely many pointwise state constraints
"['Sergio Marti', 'Hector Garcia-Molina']","The field of peer-to-peer reputation systems has exploded in the last few years. Our goal is to organize existing ideas and work to facilitate system design. We present a taxonomy of reputation system components, their properties, and discuss how user behavior and technical constraints can conflict. In our discussion, we describe research that exemplifies compromises made to deliver a useable, implementable system.",Taxonomy of trust: Categorizing P2P reputation systems
"['Marta Beltran', 'Jesper Bevensee Jensen', 'Xianbin Yu', 'Roberto Llorente', 'Roberto Rodes', 'M. Ortsiefer', 'Christian Neumeyr', 'Idelfonso Tafur Monroy']","The performance of radio-over-fiber optical transmission employing vertical-cavity surface-emitting lasers (VCSELs), and further wireless transmission, of the two major ultra-wideband (UWB) implementations is reported when operating in the 60-GHz radio band. Performance is evaluated at 1.44 Gbit/s bitrate. The two UWB implementations considered employ dual-carrier modulation orthogonal frequency-division multiplexing (DCM-OFDM) and binary phase-shift keying impulse radio (BPSK-IR) modulation respectively. Optical transmission distances up to 40 km in standard single-mode fiber and up to 500 m in bend-insensitive single-mode fiber with wireless transmission up to 5 m in both cases is demonstrated with no penalty. A simulation analysis has also been performed in order to investigate the operational limits. The analysis results are in excellent agreement with the experimental work and indicate good tolerance to chromatic dispersion due to the chirp characteristics of electro-optical conversion when a directly-modulated VCSEL is employed. The performance comparison indicates that BPSK-IR UWB exhibits better tolerance to optical transmission impairments requiring lower received optical power than its DCM-OFDM UWB counterpart when operating in the 60-GHz band.",Performance of a 60-GHz DCM-OFDM and BPSK-Impulse Ultra-Wideband System with Radio-Over-Fiber and Wireless Transmission Employing a Directly-Modulated VCSEL
['Peter F. Driessen'],A second-order DPLL with time-varying loop gains is applied to the symbol synchronization of burst mode data signals. An algorithm to control the DPLL loop gains is derived from adaptive Kalman filtering theory. Simulation results for the variable gain DPLL compared to a fixed gain DPLL demonstrate the improved acquisition performance. >,DPLL bit synchronizer with rapid acquisition using adaptive Kalman filtering techniques
"['Ahmad Mirzaei', 'Hooman Darabi', 'Ahmad Yazdi', 'Zhimin Zhou', 'Ethan Chang', 'Puneet Suri']","A quad-band 2.5G receiver is designed to replace the front-end SAW filters with on-chip bandpass filters and to integrate the LNA matching components, as well as the RF baluns. The receiver achieves a typical sensitivity of -110 dBm or better, while saving a considerable amount of BOM. Utilizing an arrangement of four baseband capacitors and MOS switches driven by 4-phase 25% duty-cycle clocks, high-Q BPF's are realized to attenuate the 0 dBm out-of-band blocker. The 65 nm CMOS SAW-less receiver integrated as a part of a 2.5G SoC, draws 55 mA from the battery, and measures an out-of-band 1 dB-compression of greater than +2 dBm. Measured as a stand-alone, as well as the baseband running in call mode in the platform level, the receiver passes the 3GPP specifications with margin.",A 65 nm CMOS Quad-Band SAW-Less Receiver SoC for GSM/GPRS/EDGE
"['Marios A. Gavrielides', 'Hela Masmoudi', 'Nicholas Petrick', 'Kyle J. Myers', 'Stephen M. Hewitt']","HER-2/neu (HER2) has been shown to be a valuable biomarker for breast cancer. However, inter-observer variability has been reported in the evaluation of HER2 with immunohistochemistry. It has been suggested that automated computer-based evaluation can provide a consistent and objective measure of HER2 expression. In this manuscript, we present an automated method for the quantitative assessment of HER2 using digital microscopy. The method employs imaging algorithms on whole slide images of tissue specimens for the extraction of two features describing HER2 membrane staining, namely membrane staining completeness and membrane staining intensity. A classifier was trained to merge the extracted features into an overall slide assessment score. Preliminary results showed good agreement with the provided truth. The developed automated method has the potential to be used as a computer aid for the immunohistochemical evaluation of HER2 expression with the objective of increasing observer reproducibility.",Automated evaluation of HER-2/neu immunohistochemical expression in breast cancer using digital microscopy
"['Sergio Canazza', 'G. De Poli', 'Antonio Rod?ˇ', 'Alvise Vidolin']","A system to add expressiveness to musical messages has been developed, starting from the results of acoustic and perceptual analyses. The system allows to obtain different performances, by modifying the acoustic parameters of a given neutral performance. The modification of the input performance is performed by a model that uses the hierarchical segmentation of the musical organization. For every hierarchical level, opportune curves are applied to the principal acoustic parameters. Level's self-similarity is the main criteria to construct the curves. The modular structure of the system defines an open architecture, where the rendering steps can be realized both with synthesis and post-processing techniques. Different synthesis techniques, like FM, physical models or wavetable have been explored.",Adding expressiveness to musical messages
['David F. Redmiles'],"There exist a wide variety of techniques for performing empirical studies which researchers in human-computer interaction have adapted from fields of cognitive psychology, sociology and anthropology. An analysis of several of these techniques is presented through an approach that balances empirical study with tool development. The analysis is based on, and illustrated with, a several-year experience of consulting in a scientific software environment and in building an evaluating a prototype knowledge-based tool to capture aspects of that experience. Guidelines for applying specific techniques and cautions about potential pitfalls are discussed. Many additional examples of using the techniques are cited from the literature. >",Observations on using empirical studies on developing a knowledge-based software engineering tool
"['G. Tasselli', 'F. Alimenti', 'Stefania Bonafoni', 'P. Basili', 'L. Roselli']","This paper deals with the problem of fire detection in the presence of obstacles that are nontransparent to visible or infrared wavelengths. Exploiting the obstacle penetration capability of microwaves, a solution based on passive microwave radiometry has been proposed. To investigate such a solution, a theoretical model of the scene sensed by a microwave radiometer is developed, accounting for the presence of both fire spot and wall-like obstacles. By reversing the model's equations, it is possible to directly relate the obstacle emissivity, reflectivity, and transmissivity to the antenna noise temperatures measured in several conditions. These temperatures have been sensed with a portable low-cost instrument. The selected 12.65-GHz operation frequency features good wall penetration capability to be balanced with a reasonable antenna size. In order to verify the aforementioned model, several fire experiments have been carried out, resulting in an overall good agreement between measurements and developed theory. In particular, a 2-cm-thick plasterboard wall, typically used for indoor building construction, shows a transmissivity equal to 0.86 and can easily be penetrated by a microwave radiometer in the X-band.",Fire Detection by Microwave Radiometric Sensors: Modeling a Scenario in the Presence of Obstacles
"['Surajit Chaudhuri', 'Vivek R. Narasayya', 'Sunita Sarawagi']","Modern relational database systems are beginning to support ad-hoc queries on data mining models. In this paper, we explore novel techniques for optimizing queries that apply mining models to relational data. For such queries, we use the internal structure of the mining model to automatically derive traditional database predicates. We present algorithms for deriving such predicates for some popular discrete mining models: decision trees, naive Bayes, and clustering. Our experiments on a Microsoft SQL Server 2000 demonstrate that these derived predicates can significantly reduce the cost of evaluating such queries.",Efficient evaluation of queries with mining predicates
"['A. M. Mathai', 'Hans J. Haubold']","After collecting data from observations or experiments, the next step is to analyze the data to build an appropriate mathematical or stochastic model to describe the data so that further studies can be done with the help of the model. In this article, the input-output type mechanism is considered first, where reaction, diffusion, reaction-diffusion, and production-destruction type physical situations can fit in. Then techniques are described to produce thicker or thinner tails (power law behavior) in stochastic models. Then the pathway idea is described where one can switch to different functional forms of the probability density function through a parameter called the pathway parameter. The paper is a continuation of related solar neutrino research published previously in this journal.",Stochastic processes via the pathway model
"['He Kong', 'Graham C. Goodwin', 'Mar??a M. Seron']","Model Predictive Control (MPC) has become widely accepted in industry. The reason for its success are manifold including easy implementation, ability to handle constraints, capacity to deal with nonlinearities, etc. However, the method does have drawbacks including tuning difficulties. In this paper, we propose an embellishment to the basic MPC strategy by incorporating a tuning parameter such that one can move continuously from an existing controller to a new MPC strategy. The continuous change of this tuning parameter leads to a continuously varying stabilizing control law. Since the proposed strategy allows one to slowly move from an existing control law to a new and better one, we term the strategy Predictive Metamorphic Control. For the case of an infinite horizon problem without constraints and for the general case with state and input constraints, stability results are established. The merits of the proposed method are illustrated by examples.",Predictive Metamorphic Control
"['Honghua Hannah Yang', 'D. F. Wong']","We consider the problem of bipartitioning a circuit into two balanced components that minimizes the number of crossing nets. Previously, the Kernighan and Lin type (K&L) heuristics, the simulated annealing approach, and the spectral method were given to solve the problem. However, network flow techniques were overlooked as a viable approach to min-cut balanced bipartition to due its high complexity. In this paper we propose a balanced bipartition heuristic based on repeated max-flow min-cut techniques, and give an efficient implementation that has the same asymptotic time complexity as that of one max-flow computation. We implemented our heuristic algorithm in a package called FBB. The experimental results demonstrate that FBB outperforms the K&L heuristics and the spectral method in terms of the number of crossing nets, and the efficient implementation makes it possible to partition large, circuit instances with reasonable runtime. For example, the average elapsed time for bipartitioning a circuit S35932 of almost 20K gates is less than 20 minutes.",Efficient network flow based min-cut balanced partitioning
"['Vinay Varadan', 'Henry Leung']","The problem of functional reconstruction of a polynomial system from its noisy time-series measurement is addressed in this paper. The reconstruction requires the determination of the embedding dimension and the unknown polynomial structure. The authors propose the use of genetic programming (GP) to find the exact functional form and embedding dimension of an unknown polynomial system from its time-series measurement. Using functional operators of addition, multiplication and time delay, they use GP to reconstruct the exact polynomial system and its embedding dimension. The proposed GP approach uses an improved least-squares (ILS) method to determine the parameters of a polynomial system. The ILS method is based on the orthogonal Euclidean distance to obtain an accurate parameter estimate when the series is corrupted by measurement noise. Simulations show that the proposed ILS-GP method can successfully reconstruct a polynomial system from its noisy time-series measurements.",Reconstruction of polynomial systems from noisy time-series measurements using genetic programming
"['Mihai Pantelimon', 'Florin Pop', 'Valentin Cristea']","Considering that one of today's biggest global concerns is related to the climate change and its imminent undesired effects, we present the approach of creating and offering a public Web service to provide real-time access to environmental data and information. One of service's direct usages is natural disasters detection, but it could be further used for developing complex statistics and prediction generators, or for other environment related applications. The data is extracted from a satellite imagery repository implemented on a Grid infrastructure. For testing the capabilities of the service for different type of users, a visualization and interaction Web application has been developed. The service is integrated in the MedioGRID system.",Grid Service for Environmental Data Retrieval and Disasters Detection Based on Satellite Image Analysis
"['Alexandre David', 'Dehui Du', 'Kim Guldstrand Larsen', 'Marius Mikucionis', 'Arne Skou']","Cyber-physical systems are to be found in numerous applications throughout society. The principal barrier to develop trustworthy cyber-physical systems is the lack of expressive modelling and specification formalisms supported by efficient tools and methodologies. To overcome this barrier, we extend in this paper the modelling formalism of the tool UPPAAL-SMC to stochastic hybrid automata, thus providing the expressive power required for modelling complex cyber-physical systems. The application of Statistical Model Checking provides a highly scalable technique for analyzing performance properties of this formalisms.",An Evaluation Framework for Energy Aware Buildings using Statistical Model Checking
"['Stephen R. Piccolo', 'Lewis Frey']","Motivated by a need to classify high-dimensional, heterogeneous data from the bioinformatics domain, we developed ML-Flex, a machine-learning toolbox that enables users to perform two-class and multi-class classification analyses in a systematic yet flexible manner. ML-Flex was written in Java but is capable of interfacing with third-party packages written in other programming languages. It can handle multiple input-data formats and supports a variety of customizations. MLFlex provides implementations of various validation strategies, which can be executed in parallel across multiple computing cores, processors, and nodes. Additionally, ML-Flex supports aggregating evidence across multiple algorithms and data sets via ensemble learning. This open-source software package is freely available from http://mlflex.sourceforge.net.",ML-Flex: a flexible toolbox for performing classification analyses in parallel
"['Trevor W. Dawson', 'Maria A. Stuchly', 'Krzysztof Caputa', 'Antonio Sastre', 'Richard Shepard', 'Robert Kavet']","The possibility of interference by low-frequency external electric fields with cardiac pacemakers is a matter of practical concern. For pragmatic reasons, experimental investigations into such interference have used contact electrode current sources. However, the applicability to the external electric field problem remains unclear. The recent development of anatomically based electromagnetic models of the human body, together with progress in computational electromagnetics, enable the use of numerical modeling to quantify the relationship between external field and contact electrode excitation. This paper presents a comparison between the computed fields induced in a 3.6-mm-resolution conductivity model of the human body by an external electric field and by several electrode source configurations involving the feet and either the head or shoulders. The application to cardiac pacemaker interference is also indicated.",Pacemaker interference and low-frequency electric induction in humans by external fields and electrodes
"['Guillermo Obregon-Pulido', 'B. Castillo-Toledo', 'Alexander G. Loukianov']","The problem of compensating an uncertain disturbance and/or tracking some reference signals for a general linear MIMO system is studied in this work using the robust regulation theory frame. The disturbances are assumed to be composed by a known number of distinct sinusoidal signals with unknown phases, amplitude and frequencies. Under suitable assumptions, an exponentially convergent estimator of the unknown disturbance parameters is proposed and introduced into the classical robust regulator design to obtain an adaptive controller. This controller guarantees that the closed-loop robust regulation is attained in some neighborhood of the nominal values of the parameters of system. A simulated example shows the validity of the proposed approach.",A Structurally Stable Globally Adaptive Internal Model Regulator for MIMO Linear Systems
"['Chang Liu', 'Lili Lu', 'Quan Kong', 'Yan Li', 'Haihua Wu', 'William Yang', 'Shandan Xu', 'Xinyu Yang', 'Xiaolei Song', 'Yang J', 'Mary Qu Yang', 'Youping Deng']","Background#R##N#Diabetes mellitus of type 2 (T2D), also known as noninsulin-dependent diabetes mellitus (NIDDM) or adult-onset diabetes, is a common disease. It is estimated that more than 300 million people worldwide suffer from T2D. In this study, we investigated the T2D, pre-diabetic and healthy human (no diabetes) bloodstream samples using genomic, genealogical, and phonemic information. We identified differentially expressed genes and pathways. The study has provided deeper insights into the development of T2D, and provided useful information for further effective prevention and treatment of the disease.",Developing discriminate model and comparative analysis of differentially expressed genes and pathways for bloodstream samples of diabetes mellitus type 2
"['Alireza Ahrary', 'Li Tian', 'Sei-ichiro Kamata', 'Masumi Ishikawa']","In this paper, we propose a method for autonomous sewer robots to navigate through a sewer pipe system based on stereo camera information. In this method, local features such as manholes and pipe joints are extracting as a feature pixels in the region of interest (ROI) of left image. Then, an accurate and fast stereo matching measure named linear computation is implemented in this ROI image to compute the distance between the robots and local features. Finally, the distance data can be used for navigation map in sewer pipe system. The experimental results show that our method can provide sufficient information for autonomous sewer robots navigation",An autonomous sewer robots navigation based on stereo camera information
"['A.B. Ariza-Villaverde', 'E. Guti??rrez de Rav??', 'F. J. Jim??nez-Hornero', 'P. Pav??nÉ?êDom??nguez', 'F. Mu?ÒozÉ?êBermejo']","Abstract#R##N##R##N#A geographic information system (GIS) is presented in this work with the aim of helping the application of problem-based learning process to show the students how to adopt the appropriate decisions for the adaptation of architectural barriers to ensure the universal accessibility in public buildings. The GIS developed here consists of three layers based on vector maps corresponding to buildings, potential routes and architectural barriers. Hyperlinks in the last layer allow access to some relevant information about each barrier, such as type, description and adaptation cost. Several tests have been carried out to show the capability of the implemented GIS to locate indoor barriers, determine suitable indoor routes by considering criteria such as paths lengths or the total cost of barrier elimination, and update the information corresponding to each architectural barrier. In addition, the application of the proposed GIS has also been explored for indoor route guidance with promising results. This investigation has been carried out with the aim of being combined with the problem-based learning process. ?? 2010 Wiley Periodicals, Inc. Comput Appl Eng Educ 21: 573É??580, 2013",Introducing a geographic information system as computer tool to apply the problem-based learning process in public buildings indoor routing
"['Xizhi An', 'Kyungsup Kwak']","In this paper, a practical location-aided routing method for sensor networks based on ultra-wideband (UWB) technique is proposed and evaluated. This method makes use of the positioning function of UWB and takes into account the energy consumption in the network. By modeling the property of energy consumption, we find that energy and quality-of-service (QOS) issues are greatly influenced by the route selected. Accordingly, a new routing algorithm is derived to search for energy-efficient routes that can support adequate QOS requirements. The simulation results have proved the advantages of this routing scheme.",A Practical Location-aided Energy-aware Routing Method for UWB-based Sensor Networks
"['Xiaojun Cao', 'Vishal Anand', 'Jikai Li', 'Chunsheng Xin']",We study reconfigurable multi-granular optical cross-connects (MG-OXCs) in waveband switching networks with limited wavelength conversion and propose a heuristic algorithm to minimize the number of used wavelength converters while reducing the blocking probability.,Waveband switching networks with limited wavelength conversion
['Marc Girault'],"We first describe a modification of Schnorr's identification scheme, in which the modulus is composite (instead of prime). This modification has some similarity with Brickell-McCurky's one, presented at the same conference. Then, by establishing a new set-up, we derive the first identity-based identification scheme based on discrete logarithms. More precisely, it is based on discrete logarithm modulo a composite number, a problem known to be harder than factorization problem. This scheme has interesting and somewhat paradoxical features. In particular, any user can choose his own secret, and, provided the parameters have convenient sizes, even the trusted center is unable to retrieve it from the public key (contrary to any identity-based scheme known until now).",An identity-based identification scheme based on discrete logarithms modulo a composite number
"['Tae-Yong Choi', 'Joon-Yong Lee', 'Ju-Jang Lee']","Pneumatic muscle has many advantages such as elasticity, high power and structural similarity to a living thing's muscle. There has been many researches to control robot actuated by pneumatic muscles, but conventional theories are hard to apply on real robot plants because of their assumptions and disregards of pneumatic muscle's physical aspects like size of pneumatic muscle and its controller. Here, the new method for saving space which is occupied by many controllers to operate robot actuated by pneumatic muscles is proposed. Actually there is easy way to control pneumatic muscle using the commercial proportional pressure regulator, but its size is not suitable to be embedded on stand alone robot. So, new method using the pressure switches of compact size and encoders is suggested. This new method is tested on a robot link with ball joints, actuated by four pneumatic muscles.",Control of Artificial Pneumatic Muscle for Robot Application
"['Erik Borglund', 'Anneli Sundqvist']","Electronic document management (EDM) is a new form of information management. EDM is described to have certain business values in organizations, but no research has been found about EDM and Small and Medium sized Enterprises (SME). In this paper we present an ongoing investigation in two SMEs guided by the following research questions: ""How are electronic documents used in the SMEs? ""and ""What are the business needs of the SMEs, and how do they correspond with stated EDM business values?"". The study was carried out as two qualitative case studies in two SMEs in the north of Sweden. The results show that the business need for an SME corresponds with the business values of EDM. Yet is management of electronic document too dependent on individuals' competence, very complex when many systems are involved, and the context where the document is created is not preserved. There is also an emergent need for an organization-specific classification scheme to enable information sharing between systems.",The role of EDM in information management within SMEs
['Bruce J. Nikkel'],"Recovering evidential data from magnetic tapes in a forensically sound manner is a difficult task. There are many different tape technologies in existence today and an even greater number of archive formats used. This paper discusses the issues and challenges involved in the forensic acquisition and analysis of magnetic tapes. It identifies areas of slack space on tapes and discusses the challenges of low level acquisition of an entire length of tape. It suggests a basic methodology for determining the contents of a tape, acquiring tape files, and preparing them for forensic analysis.",Forensic acquisition and analysis of magnetic tapes
"['Tanmoy Banerjee', 'Bishwajit Paul', 'B. C. Sarkar']","This paper reports the detailed parameter space study of the nonlinear dynamical behaviors and their control in a time-delay digital tanlock loop (TDTL). At first, we explore the nonlinear dynamics of the TDTL in parameter space and show that beyond a certain value of loop gain parameter the system manifests bifurcation and chaos. Next, we consider two variants of the delayed feedback control (DFC) technique, namely, the time-delayed feedback control (TDFC) technique, and its modified version, the extended time-delayed feedback control (ETDFC) technique. Stability analyses are carried out to find out the stable phase-locked zone of the system for both the controlled cases. We employ two-parameter bifurcation diagrams and the Lyapunov exponent spectrum to explore the dynamics of the system in the global parameter space. We establish that the control techniques can extend the stable phase-locked region of operation by controlling the occurrence of bifurcation and chaos. We also derive an estimate of the optimum parameter values for which the controlled system has the fastest convergence time even for a larger acquisition range. The present study provides a necessary detailed parameter space study that will enable one to design an improved TDTL system.","BIFURCATION, CHAOS AND THEIR CONTROL IN A TIME-DELAY DIGITAL TANLOCK LOOP"
"['Marie-Ang??le Abellan', 'H. Zahouani', 'Jean-Michel Bergheau']","This paper proposes a triphasic model of intact skin in vivo based on a general phenomenological thermohydromechanical and physicochemical (THMPC) approach of heterogeneous media. The skin is seen here as a deforming stratified medium composed of four layers and made out of different fluid-saturated materials which contain also an ionic component. All the layers are treated as linear, isotropic materials described by their own behaviour law. The numerical simulations of in vivo indentation test performed on human skin are given. The numerical results correlate reasonably well with the typical observations of indented human skin. The discussion shows the versatility of this approach to obtain a better understanding on the mechanical behaviour of human skin layers separately.",Contribution to the Determination of In Vivo Mechanical Characteristics of Human Skin by Indentation Test
"['Jiho Jang', 'Kwang Bok Lee']","In this paper, we develop a transmit power adaptation method that maximizes the total data rate of multiuser orthogonal frequency division multiplexing (OFDM) systems in a downlink transmission. We generally formulate the data rate maximization problem by allowing that a subcarrier could be shared by multiple users. The transmit power adaptation scheme is derived by solving the maximization problem via two steps: subcarrier assignment for users and power allocation for subcarriers. We have found that the data rate of a multiuser OFDM system is maximized when each subcarrier is assigned to only one user with the best channel gain for that subcarrier and the transmit power is distributed over the subcarriers by the water-filling policy. In order to reduce the computational complexity in calculating water-filling level in the proposed transmit power adaptation method, we also propose a simple method where users with the best channel gain for each subcarrier are selected and then the transmit power is equally distributed among the subcarriers. Results show that the total data rate for the proposed transmit power adaptation methods significantly increases with the number of users owing to the multiuser diversity effects and is greater than that for the conventional frequency-division multiple access (FDMA)-like transmit power adaptation schemes. Furthermore, we have found that the total data rate of the multiuser OFDM system with the proposed transmit power adaptation methods becomes even higher than the capacity of the AWGN channel when the number of users is large enough.",Transmit power adaptation for multiuser OFDM systems
"['Yong Sheng Ma', 'C.H. Bong']",This paper explores the vast domain of systematic collaborative engineering with reference to product lifecycle management approach from the angle of feature-level collaboration among partners. A new method of fine grain feature association modelling and reasoning is proposed. The original contribution is on the explicit modelling and reasoning of collaborative feature relations within a dynamic context. A case study has been carried out to illustrate the interweaving feature relations in collaborative oil rig space management and the effective application of such relations modelled in design solution optimisation.,Fine grain associative feature reasoning in collaborative engineering
"['Mary Fletcher', 'Anna Dornhaus', 'Min C. Shin']","Motion and behavior analysis of social insects such as ants requires tracking many ants over time. This process is highly labor-intensive and tedious. Automatic tracking is challenging as ants often interact with one another, resulting in frequent occlusions that cause drifts in tracking. In addition, tracking many objects is computationally expensive. In this paper, we present a robust and efficient method for tracking multiple ants. We first prevent drifts by maximizing the coverage of foreground pixels at at global scale. Secondly, we improve speed by reducing markov chain length through dynamically changing the target proposal distribution for perturbed ant selection. Using a real dataset with ground truth, we demonstrate that our algorithm was able to improve the accuracy by 15% (resulting in 98% tracking accuracy) and the speed by 76%.",Multiple ant tracking with global foreground maximization and variable target proposal distribution
"['Luiz C. S. Rozante', 'Marco Dimas Gubitoso', 'Sergio Russo Matioli']","Juxtacrine signaling is intercellular communication, in which the receptor of the signal (typically a protein) as well as the ligand (also typically a protein, responsible for the activation of the receptor) are anchored in the plasma membranes, so that in this type of signaling the activation of the receptor depends on direct contact between the membranes of the cells involved. Juxtacrine signaling is present in many important cellular events of several organisms, especially in the development process. We propose a generic formal model (a modeling framework) for juxtacrine signaling systems that is a class of dynamic discrete systems. It possesses desirable characteristics in a good modeling framework, such as: a) structural similarity with biological models, b) capacity of operating in different scales of time and c) capacity of explicitly treating both the events and molecular elements that occur in the membrane, and those that occur in the intracellular environment and are involved in the juxtacrine signaling process. We implemented this framework and used to develop a new discrete model for the neurogenic network and its participation in neuroblast segregation",A Framework for Discrete Modeling of Juxtacrine Signaling Systems
"['Muhammad Sohail Khan', 'Do-Hyeun Kim']","DIY vision for the design of a smart and customizable world in the form of IoT demands the involvement of general public in its development process. General public lacks the technical depths for programming state-of-the-art prototyping and development kits. Latest IoT kits, for example, Intel Edison, are revolutionizing the DIY paradigm for IoT and more than ever a DIY intuitive programming interface is required to enable masses to interact with and customize the behavior of remote IoT devices on the Internet. This paper presents the novel implementation of such a system enabling general public to customize the behavior of remote IoT devices through a visual interface. The interface enables the visualization of the resources exposed by a remote CoAP device in the form of graphical virtual objects. The VOs are used to create service design through simple operations like drag-and-drop and properties settings. The design is maintained as an XML document, thus being easily distributable and recognizable. CoAP proxy acts as an operation client for the remote device and also provides communication link between the designer and the device. The paper presents the architecture, detailed design, and prototype implementation of the system using state-of-the-art technologies.",DIY interface for enhanced service customization of remote IoT devices: a CoAP based prototype
"['Wenwen Yi', 'Yong Sun', 'Shukui Zhang', 'Yingfeng Wu', 'Zhenhua Chu']","In the field of search, the application of ontology is an important research topic. Introduction of ontology technology in the retrieval system with massive data can make the searching results more comprehensive. However, now days the ontology is constructed by domain experts, and there are a lot of shortcomings, such as complex process, long time for the project, and difficulty to update. Thereby, in this paper, a method of semiautomaticly building ontology is proposed, after synthetically analyzing a variety of methods and techniques about it. The building process which is based on user interests, mines not only the concepts but also the potential relationships between concepts from the texts by the method of concepts clustering. On the basis of such research, an unique patent information retrieval system based on ontology has been completed.",The Application and Research of Ontology Construction Technology
"[""Daniel E. O'Leary""]","This paper analyses the citations from Intelligent Systems in Accounting, Finance and Management that have occurred in ISI's Web of Knowledge in February 2010. I found roughly 1000 citations to the journal under 10 different journal name abbreviations, with roughly 25p of the citations occurring during 2008É??2009, associated with 27 of the more frequently cited papers. Using that citation data, the H-index and the 40 (42 with ties) most-cited papers are presented. I found that ISI's new proceedings data appear to have a different citation pattern than ISI's journal citation data, resulting in citations to more sources, but fewer citations per source. I also examine the research methodologies and applications of the most-cited papers in an attempt to determine what areas have been cited most and where there are potential gaps in the research. Copyright ?? 2010 John Wiley & Sons, Ltd.","Intelligent systems in accounting, finance and management: ISI journal and proceeding citations, and research issues from most-cited papers"
"['Zachary Miller', 'William Deitrick', 'Wei Hu']","In recent years, significant research has been devoted to the development of Intrusion Detection Systems (IDS) able to detect anomalous computer network traffic indicative of malicious activity. While signature-based IDS have proven effective in discovering known attacks, anomaly-based IDS hold the even greater promise of being able to automatically detect previously undocumented threats. Traditional IDS are generally trained in batch mode, and therefore cannot adapt to evolving network data streams in real time. To resolve this limitation, data stream mining techniques can be utilized to create a new type of IDS able to dynamically model a stream of network traffic. In this paper, we present two methods for anomalous network packet detection based on the data stream mining paradigm. The first of these is an adapted version of the DenStream algorithm for stream clustering specifically tailored to evaluate network traffic. In this algorithm, individual packets are treated as points and are flagged as normal or abnormal based on their belonging to either normal or outlier clusters. The second algorithm utilizes a histogram to create a model of the evolving network traffic to which incoming traffic can be compared using Pearson correlation. Both of these algorithms were tested using the first week of data from the DARPA É??99 dataset with Generic HTTP, Shell-code and Polymorphic attacks inserted. We were able to achieve reasonably high detection rates with moderately low false positive percentages for different types of attacks, though detection rates varied between the two algorithms. Overall, the histogram-based detection algorithm achieved slightly superior results, but required more parameters than the clustering-based algorithm. As a result of its fewer parameter requirements, the clustering approach can be more easily generalized to different types of network traffic streams.",Anomalous Network Packet Detection Using Data Stream Mining
['Fabrizio Sebastiani'],"The automated categorization (or classification) of texts into predefined categories has witnessed a booming interest in the last 10 years, due to the increased availability of documents in digital form and the ensuing need to organize them. In the research community the dominant approach to this problem is based on machine learning techniques: a general inductive process automatically builds a classifier by learning, from a set of preclassified documents, the characteristics of the categories. The advantages of this approach over the knowledge engineering approach (consisting in the manual definition of a classifier by domain experts) are a very good effectiveness, considerable savings in terms of expert labor power, and straightforward portability to different domains. This survey discusses the main approaches to text categorization that fall within the machine learning paradigm. We will discuss in detail issues pertaining to three different problems, namely, document representation, classifier construction, and classifier evaluation.",Machine learning in automated text categorization
"['Jong-Seob Won', 'Reza Langari']","This paper represents the second part of a two-part paper on development of an intelligent energy management agent (IEMA) for parallel hybrid vehicles. In this part, energy management strategies for the torque distribution and charge sustenance tasks are established and implemented. Driving situation awareness-based fuzzy rule bases are developed to make intelligent decisions on the power split function. A charge sustenance strategy is developed in parallel to maintain adequate reserves of energy in the storage device for supporting an extended range of driving. Simulation study is conducted for the proposed IEMA and performance results are analyzed to evaluate its viability as a possible solution to and an extendable framework for energy management for parallel hybrid electric vehicles.","Intelligent energy management agent for a parallel hybrid vehicle-part II: torque distribution, charge sustenance strategies, and performance results"
"['J. Urzelai', 'Dario Floreano']","This paper is concerned with adaptation capabilities of evolved neural controllers. We propose to evolve mechanisms for parameter self-organization instead of evolving the parameters themselves. The method consists of encoding a set of local adaptation rules that synapses follow while the robot freely moves in the environment. In the experiments presented here, the performance of the robot is measured in environments that are different in significant ways from those used during evolution. The results show that evolutionary adaptive controllers solve the task much faster and better than evolutionary standard fixed-weight controllers, that the method scales up well to large architectures, and that evolutionary adaptive controllers can adapt to environmental changes that involve new sensory characteristics (including transfer from simulation to reality and across different robotic platforms) and new spatial relationships.",Evolution of Adaptive Synapses: Robots with Fast Adaptive Behavior in New Environments
"['Luisa Monroy', 'Miguel A. Hinojosa', 'Amparo M. M?≠rmol', 'Francisco R. Fern?≠ndez']","In this paper we study cooperative games with fuzzy payoffs. The main advantage of the approach presented is the incorporation into the analysis of the problem of ambiguity inherent in many real-world collective decision situations. We propose extensions of core concepts which maintain the fuzzy nature of allocations, and lead to a more satisfactory study of the problem within the fuzzy context. Finally, we illustrate the extended core concepts and the approach to obtain the corresponding allocations through the analysis of assignment games with uncertain profits.",Set-valued cooperative games with fuzzy payoffs. The fuzzy assignment game
"['Changho Suh', 'David Tse']","We characterize the symmetric capacity of the two-user Gaussian interference channel with feedback to within 1 bit/s/Hz. The result makes use of a deterministic model to provide insights into the Gaussian channel. We derive a new outer bound to show that a proposed scheme can achieve the symmetric capacity to within one bit for all channel parameters. One consequence of the result is that feedback provides unbounded gain, i.e., the gain becomes arbitrarily large for certain channel parameters. It is a surprising result because feedback has been so far known to provide no gain in memoryless point-to-point channels and only power gain (bounded gain) in the multiple access channels. The gain comes from using feedback to fully exploit the side information provided by the broadcast nature of the wireless medium.",Symmetric feedback capacity of the Gaussian interference channel to within one bit
"['Thomas Brinkhoff', 'Hans-Peter Kriegel', 'Bernhard Seeger']","We show that spatial joins are very suitable to be processed on a parallel hardware platform. The parallel system is equipped with a so called shared virtual memory which is well suited for the design and implementation of parallel spatial join algorithms. We start with an algorithm that consists of three phases: task creation, task assignment and parallel task execution. In order to reduce CPU and I/O cost, the three phases are processed in a fashion that preserves spatial locality. Dynamic load balancing is achieved by splitting tasks into smaller ones and reassigning some of the smaller tasks to idle processors. In an experimental performance comparison, we identify the advantages and disadvantages of several variants of our algorithm. The most efficient one shows an almost optimal speed up under the assumption that the number of disks is sufficiently large.",Parallel processing of spatial joins using R-trees
"['Huiqun Yu', 'Guisheng Fan', 'Liqiong Chen', 'Dongmei Liu']","Service composition is an important means for integrating the individual Web services for creating new value added systems that satisfy complex demands. Since, Web services exist in the heterogeneous environments on the Internet, study on how to guarantee the reliability of service composition in a distributed, dynamic and complex environment becomes more and more important. This paper proposes a service composition net(SCN) and fault-tolerant strategy to improve the reliability of service composition. The strategy consists of static strategy, dynamic strategy and exception handling mechanism, which can be used to dynamically adjust component service for achieving good reliability as well as good overall performance. SCN is adopted to model different components of service composition. The fault detection and fault recovery mechanisms are also considered. Based on the constructed model, theories of Petri nets help prove the consistency of processing states and the effectiveness of the strategy. A case study of Export Service illustrates the feasibility of proposed method.",A Fault-Tolerant Strategy for Improving the Reliability of Service Composition
"['Nandakishore Santhi', 'Guanhua Yan', 'Stephan Eidenbenz']","Cyber-infractions into a nation's strategic security envelope pose a constant and daunting challenge. We present the modular CyberSim tool which has been developed in response to the need to realistically simulate at a national level, software vulnerabilities and resulting malware propagation in online social networks. CyberSim suite (a) can generate realistic scale-free networks from a database of geo-coordinated computers to closely model social networks arising from personal and business email contacts and online communities; (b) maintains for each host a list of installed software, along with the latest published vulnerabilities; (c) allows to designate initial nodes where malware gets introduced; (d) simulates using distributed discrete event-driven technology, the spread of malware exploiting a specific vulnerability, with packet delay and user online behavior models; (e) provides a graphical visualization of spread of infection, its severity, businesses affected etc to the analyst. We present sample simulations on a national level network with millions of computers.","Cybersim: geographic, temporal, and organizational dynamics of malware propagation"
"['Ralph Matthes', 'Sergei Soloviev']","The problem of the commutativity of algebraic (categorical) diagrams has attracted the attention of researchers for a long time. For example, the related notion of coherence was discussed in Mac Lane's homology book Mac Lane (1963), see also his AMS presidential address Mac Lane (1976). Researchers in category theory view this problem from a specific angle, and for them it is not just a question of convenient notation, though it is worth mentioning the important role that notation plays in the development of science (take, for example, the progress made after the introduction of symbolic notation in logics or matrix notation in algebra). In 1976, Peter Freyd published the paper 'Properties Invariant within Equivalence Types of Categories' (Freyd 1976), where the central role is played by the notion of a 'diagrammatic property'. We may also recall the process of 'diagram chasing', and its applications in topology and algebra. But before we can use diagrams (and the principal property of a diagram is its commutativity), it is vital for us to be able to check whether a diagram is commutative.",Preface to the special issue: Commutativity of algebraic diagrams
"['Ross Deming', 'Shawn Higbee', 'Derek Dwyer', 'Michael Welser', 'Leonid I. Perlovsky', 'Paola Pellegrini']","We describe a new approach for performing pseudo-imaging of point energy sources from spectral-temporal sensor data. Pseudo-imaging, which involves the automatic localization, spectrum estimation, and identification of energetic sources, can be difficult for dim sources and/or noisy images, or in data containing multiple sources which are closely spaced such that their signatures overlap. The new approach is specifically designed for these difficult cases. It is developed within the framework of modeling field theory (MFT), a biologically-inspired neural network system that has demonstrated practical value in many diverse areas. MFT performs an efficient optimization over the space of all model parameters and mappings between image pixels and sources, or clutter. The optimized set of parameters is then used for detection, localization and identification of the multiple sources in the data. The paper includes results computed from experimental spectrometer data.",Adaptive Estimation for Spectral-Temporal Characterization of Energetic Transient Events
"['Bo Pang', 'Kevin Knight', 'Daniel Marcu']","We describe a syntax-based algorithm that automatically builds Finite State Automata (word lattices) from semantically equivalent translation sets. These FSAs are good representations of paraphrases. They can be used to extract lexical and syntactic paraphrase pairs and to generate new, unseen sentences that express the same meaning as the sentences in the input sets. Our FSAs can also predict the correctness of alternative semantic renderings, which may be used to evaluate the quality of translations.",Syntax-based alignment of multiple translations: extracting paraphrases and generating new sentences
"['Nazaraf Shah', 'Kuo Ming Chao', 'Nick Godwin', 'Muhammad Younas', 'Christopher Laing']","Diagnosing exceptions in multi-agent systems (MAS) is a complex task due to the distributed nature of the data and control in such systems. This complexity is exacerbated in open environments where independently developed autonomous agents interact with each other in order to achieve their goals. Inevitably, exceptions would occur in such MAS and these exceptions can arise at one of three levels, namely environmental, knowledge and social levels. In this paper we propose a novel exception diagnosis system that is able to analyse and detect exceptions effectively. The proposed architecture consists of specialised exception diagnosis agents called sentinel agents. The sentinel agents are equipped with knowledge of observable abnormal situations, their underlying causes, and resolution strategies associated with these causes. The sentinel agent applies a heuristic classification approach to collect related data from affected agents in order to uncover the underlying causes of the observed symptoms. We illustrate and evaluate our proposed architecture using an agent-based grid computing case study.",Exception diagnosis in agent-based grid computing
"['HongÉ??xuan Huang', 'Yu Zhao']",In this paper we will extend the definition of a filled function and propose a new definition of a locally filled function. The difference between the locally filled function and the classical filled function is illustrated by an example. The existence of a locally filled function is also studied in theory. Based on the locally filled function and cluster analysis technique we will present a hybrid global optimisation algorithm. The algorithm integrates the deterministic and stochastic searching techniques and has a very powerful globally searching ability. Numerical performance of the new hybrid algorithm is demonstrated by two examples about the Shubert I and Sine-Square I functions.,A hybrid global optimisation algorithm based on locally filled functions and cluster analysis
"['Peter Dimopoulos', 'Panlop Zeephongsekul', 'Zahir Tari']","On the Internet many different paths exist between each source and destination. When single path routing is used these paths can be under utilized, not used fairly or not used at all. One way to overcome this is to allow multipath routing. But when multiple paths are used TCP congestion control can be negatively affected and cause poor goodput performance due to the reordering of packets. We proposeMATCP (Multipath Aware TCP) which makes modifications to TCP that allows it to monitor and select which path it takes through the network for each flow. MATCP is compared to single path routing and is validated using extensive simulation. MATCP is found to greatly improve fairness between flows while providing equal or better utilization of links than single best path networks.",Multipath Aware TCP (MATCP)
"['Nadir Weibel', 'Moira C. Norrie', 'Beat Signer']","The first steps towards bridging the paper-digital divide have been achieved with the development of a range of technologies that allow printed documents to be linked to digital content and services. However, the static nature of paper and limited structural information encoded in classical paginated formats make it difficult to map between parts of a printed instance of a document and logical elements of a digital instance of the same document, especially taking document revisions into account. We present a solution to this problem based on a model that combines metadata of the digital and printed instances to enable a seamless mapping between digital documents and their physical counterparts on paper. We also describe how the model was used to develop iDoc, a framework that supports the authoring and publishing of interactive paper documents.",A model for mapping between printed and digital document instances
"['Liyang Rui', 'K. C. Ho']","The nonlinear nature of the source localization problem creates bias to a location estimate. The bias could play a significant role in limiting the performance of localization and tracking when multiple measurements at different instants are available. This paper performs bias analysis of the source location estimate obtained by the maximum likelihood estimator, where the positioning measurements can be TOA, TDOA, or AOA. The effect of bias to the mean-square localization error is examined and the amounts of bias introduced by the three types of measurements are contrasted.",Bias analysis of source localization using the maximum likelihood estimator
"['Manas Saksena', 'A Ptak', 'Paul D. Freedman', 'Pawel Rodziewicz']","The increasing complexity of real time software has led to a recent trend in the use of high level modeling languages for development of real time software. One representative example is the modeling language ROOM (real time object oriented modeling), which provides features such as object orientation, state machine description of behaviors, formal semantics for executability of models, and possibility of automated code generation. However these modeling languages largely ignore the timeliness aspect of real time systems, and fail to provide any guidance for a designer to a priori predict and analyze temporal behavior. We consider schedulability analysis for automated implementations of ROOM models, based on the ObjecTime toolset. This work builds on results presented by M. Saksena (1997), where we developed some guidelines for the design and implementation of real time object oriented models. Using the guidelines, we have modified the run time system library provided by the ObjecTime toolset to make it amenable to schedulability analysis. Based on the modified toolset, we show how a ROOM model can be analyzed for schedulability, taking into account the implementation overheads and structure. The analysis is validated experimentally, first using simple periodic models, and then using a large case study of a train tilting system.",Schedulability analysis for automated implementations of real-time object-oriented models
"['Luciano Spinello', 'Rudolph Triebel', 'Roland Siegwart']","This paper presents a novel people detection and tracking method based on a multi-modal sensor fusion approach that utilizes 2D laser range and camera data. The data points in the laser scans are clustered using a novel graph-based method and an SVM based version of the cascaded AdaBoost classifier is trained with a set of geometrical features of these clusters. In the detection phase, the classified laser data is projected into the camera image to define a region of interest for the vision-based people detector. This detector is a fast version of the Implicit Shape Model (ISM) that learns an appearance codebook of local SIFT descriptors from a set of hand-labeled images of pedestrians and uses them in a voting scheme to vote for centers of detected people. The extension consists in a fast and detailed analysis of the spatial distribution of voters per detected person. Each detected person is tracked using a greedy data association method and multiple Extended Kalman Filters that use different motion models. This way, the filter can cope with a variety of different motion patterns. The tracker is asynchronously updated by the detections from the laser and the camera data. Experiments conducted in real-world outdoor scenarios with crowds of pedestrians demonstrate the usefulness of our approach.",Multimodal people detection and tracking in crowded scenes
"['Benjamin Letham', 'Cynthia Rudin', 'Tyler H. McCormick', 'David Madigan']","We aim to produce predictive models that are not only accurate, but are also interpretable to human experts. Our models are decision lists, which consist of a series of if ...then...statements (e.g., if high blood pressure, then stroke) that discretize a high-dimensional, multivariate feature space into a series of simple, readily interpretable decision statements. We introduce a generative model called Bayesian Rule Lists that yields a posterior distribution over possible decision lists. It employs a novel prior structure to encourage sparsity. Our experiments show that Bayesian Rule Lists has predictive accuracy on par with the current top algorithms for prediction in machine learning. Our method is motivated by recent developments in personalized medicine, and can be used to produce highly accurate and interpretable medical scoring systems. We demonstrate this by producing an alternative to the CHADS2 score, actively used in clinical practice for estimating the risk of stroke in patients that have atrial fibrillation. Our model is as interpretable as CHADS2, but more accurate.",Interpretable classifiers using rules and Bayesian analysis: Building a better stroke prediction model
"['Bo Wang', 'Tiejun Zhao', 'Muyun Yang', 'Hongfei Jiang', 'Sheng Li']","We describe an improved strategy to combine the outputs of machine translation on sentence-level balancing the stability and the effectiveness of the combination. The new method alternates the classical MBR-based sentence-level combination with weighted Minimum Bayes Risk (wMBR). During the calculation of the risk, we weight the hypotheses with the performance of the MT system, which is measured by the automatic evaluation metrics on the development data. In experiments, the wMBR-based method stably achieve better results than other sentence-level methods and get the best position in CWMT08 evaluation track outperforming the other word-level and sentence-level combination systems.",Stability vs. Effectiveness: Improved Sentence-Level Combination of Machine Translation Based on Weighted MBR
"['Zihuai Lin', 'Tor Aulin']","Joint source and channel (JSC) coding using combined trellis coded quantization (TCQ) and continuous phase modulation (CPM) is studied. The channel is assumed to be the additive white Gaussian noise (AWGN) channel. Optimal soft decoding for JSC coding using jointly designed TCQ/CPM is studied in this paper. The soft decoder is based on the a posteriori probability (APP) algorithm for trellis coded CPM. It is shown that the systems with soft decoding outperform the systems with hard decoding especially when the systems operate at low to medium signal-to-noise ratio (SNR). Furthermore, a TCQ design algorithm for the noisy channel is developed. It has been demonstrated that the combined TCQ/CPM systems are both power and bandwidth efficient compared with the combined TCQ/TCM/8PSK systems. The novelty of this work is the use of a soft decoder and the APP algorithm for combined TCQ/CPM systems.",Joint source and channel coding using trellis coded CPM: soft decoding
"['Scott F. Midkiff', 'Luiz A. DaSilva', 'Allen B. MacKenzie']",Sixteen PhD students presented their research at the PhD Forum at PerCom 2009. The PhD forum offered the students an opportunity for interaction with and feedback from senior researchers in pervasive computing.,Google PhD Forum at PerCom 2009
['Kyoung-Mi Lee'],"To detect a human body and recognize its posture, a component-based approach is less susceptible to changes in posture and lighting conditions. This paper proposes a component-based human-body model that comprises ten components and their flexible links. Each component contains geometrical information, appearance information, and information on the links with other components. The proposed method in this paper uses hierarchical links between components of human body, so that it allows to make coarse-to-fine searches and makes human-body matching more time-efficient. To adaptively estimate the posture in change of posture and illumination, we update the component online every time a new human body is incoming.",Adaptive estimation of human posture using a component-based model
"['Mohammad Hayajneh', 'Chaouki T. Abdallah', 'Walid Ibrahim']","Our goal in this paper is to study the performance of the game-theoretic power control algorithms for wireless data introduced by Saraydar et al [1] in two realistic channels: (a1) fast flat fading channel and (a2) Slow flat fading channel. The fading coefficients under both (a1) and (a2) are studied under an appropriate small scale channel model that is used in the CDMA cellular systems, namely Nakagami channel model. To do so, we derive a closed- form expression of the average utility function which represents the number of bits received correctly at the receiver per one Joule expended. Then, using this expression we study the existence, uniqueness of Nash equilibrium (NE), and the social desirability of NE in the Pareto sense.",Impact of fading wireless channel on the performance of game theoretic power control algorithms for CDMA wireless data
"['Rayne Reid', 'Johan Van Niekerk', 'Rossouw von Solms']","Most current approaches towards information security education do not have a sound theoretical basis. This could lead to the failure of these educational programs. Furthermore, the need for information security knowledge is no longer only of concern to organizations, but has also become a concern for individuals using online services for personal entertainment, social networking, banking, and other activities. Thus, there is a need for É??cyber securityÉ?ù education for both individuals and organizations. Such cyber security educational programs should be based on sound pedagogical theories. One such a pedagogically sound approach that could potentially play a role in cyber security educational programs is É??brain compatible learningÉ?ù. This paper will perform a critical evaluation of an existing information security education course, and evaluate the subject matter in terms of brain compatible learning approaches. The aim of the paper is to propose a set of brain compatible learning guidelines for the creation of cyber security educational material. The paper will also argue in favour of the use e-learning as a delivery mechanism for such content. As such, the guidelines will be proposed in the context of a Moodle 2.0 e-learning environment.",Guidelines for the creation of brain-compatible cyber security educational material in Moodle 2.0
['Stefan Kaiser'],An OFDM-CDM (orthogonal frequency division multiplexing code division multiplexing) system with adaptive symbol mapping is presented. This combination enables a robust transmission with flexible error protection and data rate adaptation for parallel data streams by exploiting additional diversity due to CDM. Performance results are presented for fading channels where OFDM-CDM with adaptive symbol mapping and soft interference cancellation is compared to conventional OFDM systems also taking into account channel coding with variable code rates.,OFDM code division multiplexing with unequal error protection and flexible data rate adaptation
"['Takuro Kitayama', 'Akihiko Miyoshi', 'Tetsuya Saito', 'Hideyuki Tokuda']","Recent modern operating system technology enables protocol processing in user space using in-kernel packet filter and user-level protocol processing library for flexibility without sacrificing the performance of traditional kernelized protocol processing. This technology can be adapted to build a highly preemptable protocol processing mechanism for distributed real-time environment. In this paper, we discuss the structural difference of various operating systems from the protocol processing point of view, and propose an extended mechanism of packet filter and user-level protocol processing library for real-time communication. Using this mechanism, priority of the client can be handed off to the server without priority inversion problem during protocol processing.",Real-time communication in distributed environment-real-time packet filter approach
"['Hans L. Cycon', 'Thomas C. Schmidt', 'Gabriel Hege', 'Matthias W??hlisch', 'Mark Palkow']","Mobile phones and related gadgets in networks are omnipresent at our students, advertising itself as the platform for mobile, pervasive learning. Currently, these devices rapidly open and enhance, being soon able to serve as a major platform for rich, open multimedia applications and communication. In this report we introduce a video conferencing software, which seamlessly integrates mobile with stationary users into fully distributed multi-party conversations. Following the paradigm of flexible, user-initiated group communication, we present an integrated solution, which scales well for medium-size conferences and accounts for the heterogeneous nature of mobile and stationary participants. This approach allows for a spontaneous, location independent establishment of video dialogs, which is of particular importance in interactive learning scenarios. The work is based on a highly optimized realization of a H.264 codec.",Let's Meet at the Mobile - Learning Dialogs with a Video Conferencing Software for Mobile Devices
"['Akira Kawaguchi', 'Stewart Russell', 'Guoliang Qian']","This paper is a progress report on an ongoing research effort that makes use of wireless biometric data management, specifically a wireless blood-glucose monitoring system (WBgM). The goal of this research is to realize appropriate timely intervention by health-care providers in response to records of unregulated blood-glucose level in order to maintain near-normal levels. The focus of this paper is security-their is the possibility that medical information may be used consciously by malicious eavesdropper to the detriment of a patient in search of new employment or a new insurer. There is the possibility of malicious or accidental data corruption from database intrusion. Mobile data transmission is especially problematic in preventing these. We present requirements and our approach taken to realize a secure system architecture. Key techniques and their implementation details to maintain privacy, authentication, and data integrity are discussed here.",Security issues in the development of a wireless blood-glucose monitoring system
"['Nicholas K. Jong', 'Peter Stone']","Abstraction is a powerful form of domain knowledge that allows reinforcement-learning agents to cope with complex environments, but in most cases a human must supply this knowledge. In the absence of such prior knowledge or a given model, we propose an algorithm for the automatic discovery of state abstraction from policies learned in one domain for use in other domains that have similar structure. To this end, we introduce a novel condition for state abstraction in terms of the relevance of state features to optimal behavior, and we exhibit statistical methods that detect this condition robustly. Finally, we show how to apply temporal abstraction to benefit safely from even partial state abstraction in the presence of generalization error.",State abstraction discovery from irrelevant state variables
"['Francesco Chiani', 'Camilla Iannone', 'Rodolfo Negri', 'Daniele Paoletti', 'Mattia DÉ??Antonio', ""Paolo D'Onorio De Meo"", 'Tiziana Castrignan?˝']","The analysis of the great extent of data generated by using DNA microarrays technologies has shown that the transcriptional response to radiation can be considerably different depending on the quality, the dose range and dose rate of radiation, as well as the timing selected for the analysis. At present, it is very difficult to integrate data obtained under several experimental conditions in different biological systems to reach overall conclusions or build regulatory models which may be tested and validated. In fact, most available data is buried in different websites, public or private, in general or local repositories or in files included in published papers; it is often in various formats, which makes a wide comparison even more difficult. The Radiation Genes Database (http://www.caspur.it/RadiationGenes) collects microarrays data from various local and public repositories or from published papers and supplementary materials. The database classifies it in terms of significant variables, such as radiation quality, dose, dose rate and sampling timing, as to provide user-friendly tools to facilitate data integration and comparison.",Radiation Genes: a database devoted to microarrays screenings revealing transcriptome alterations induced by ionizing radiation in mammalian cells
"['Kijeung Choi', 'Thomas G. Robertazzi']","Optimal data scheduling strategies in a hierarchical wireless sensor network (WSN) are considered. Data aggregation in clusterheads is considered to find a closed form solution for the optimal amount of data that is to be reported by each sensor node using a newly introduced parameter, information utility. The optimal conditions for the feasible measurement instruction assignment time and for the minimum round time are derived and examined. Based on the optimal conditions, a performance evaluation is demonstrated via simulation study.",Divisible Load Scheduling inWireless Sensor Networks with Information Utility
"['Yusung Lee', 'Hyuncheol Park']","In this paper, we investigate and propose the detection techniques for spatially layered multiple-input multiple-output (MIMO) multi-carrier code division multiple access (MC-CDMA) systems. First, we propose a noise-predictive linear detector. It has the same bit error rate (BER) performance as symbol-level detector and reduces the complexity significantly when the system load is almost full. We also propose a partial minimum mean square error (MMSE)-ordered successive interference cancellation (OSIC) based on multi-user detection, which first detects the most powerful interfering data symbols transmitted through the determined transmit antenna and then cancels their contribution from the received signal by the multiplexed data symbol vector. The nulling and cancelling processes between the user data symbols from the same transmit antenna are not performed. The proposed algorithms are verified by computer simulation.",Low-Complexity Detections for Downlink MIMO MC-CDMA Systems
['Reinaldo A. Bergamaschi'],"SKOL, a system for the synthesis of combinational logic using a library of cells that emphasizes technology-mapping algorithms, is described. It combines current multilevel optimization techniques with a novel approach to technology mapping. Each factor (or the factorized Boolean equation) can be implemented by itself or collapsed into the higher level expression containing it, which is then implemented. An expression can be implemented in several ways, which differ in the degree of factorization. A number of selected implementations is evaluated and the one with minimal cost (area or delay) is chosen. The mapping algorithms are independent of the library of cells, which can be easily modified. Results from benchmark examples were better than or comparable to those for existing systems. >",Automatic synthesis and technology mapping of combinational logic
"['Mauro Barni', 'Franco Bartolini', 'Alessandro Piva']","In the field of image watermarking, research has been mainly focused on grayscale image watermarking, whereas the extension to the color case is usually accomplished by marking the image luminance, or by processing each color channel separately. A DCT domain watermarking technique expressly designed to exploit the peculiarities of color images is presented. The watermark is hidden within the data by modifying a subset of full-frame DCT coefficients of each color channel. Detection is based on a global correlation measure which is computed by taking into account the information conveyed by the three color channels as well as their interdependency. To ultimately decide whether or not the image contains the watermark, the correlation value is compared to a threshold. With respect to existing grayscale algorithms, a new approach to threshold selection is proposed, which permits reducing the probability of missed detection to a minimum, while ensuring a given false detection probability. Experimental results, as well as theoretical analysis, are presented to demonstrate the validity of the new approach with respect to algorithms operating on image luminance only.",Multichannel watermarking of color images
"['Lars Kai Hansen', 'Seliz G. Karadogan', 'Letizia Marchegiani']","Top-down attention is modeled as decision making based on incomplete information. We consider decisions made in a sequential measurement situation where initially only an incomplete input feature vector is available, however, where we are given the possibility to acquire additional input values among the missing features. The procecure thus poses the question what to do next? We take an information theoretical approach implemented for generality in a generative mixture model. The framework allows us reduce the decision about what to measure next in a classification problem to the estimation of a few one-dimensional integrals per missing feature. We demonstrate the viability of the framework on four well-known classification problems.",What to measure next to improve decision making? On top-down task driven feature saliency
"['Rahul Kala', 'Harsh Vazirani', 'Anupam Shukla', 'Ritu Tiwari']","Handwriting Recognition enables a person to scribble something on a piece of paper and then convert it into text. If we look into the practical reality there are enumerable styles in which a character may be written. These styles can be self combined to generate more styles. Even if a small child knows the basic styles a character can be written, he would be able to recognize characters written in styles intermediate between them or formed by their mixture. This motivates the use of Genetic Algorithms for the problem. In order to prove this, we made a pool of images of characters. We converted them to graphs. The graph of every character was intermixed to generate styles intermediate between the styles of parent character. Character recognition involved the matching of the graph generated from the unknown character image with the graphs generated by mixing. Using this method we received an accuracy of 98.44%.",Offline Handwriting Recognition using Genetic Algorithm
"['Mohammad Shahin Mahanta', 'Amirhossein S. Aghaei', 'Konstantinos N. Plataniotis', 'Subbarayan Pasupathy']","Classification of mental tasks from electroencephalogram (EEG) signals has important applications in brain-computer interfacing (BCI). However, classification of the highly redundant and high-dimensional EEG signal, with high spatial and spectral correlations, is quite challenging. Therefore, the discriminant information, especially that of the first and second data moments, need to be extracted in the form of uncorrelated features. This work addresses this need by approximating a linear minimal-dimension sufficient statistic of the EEG matrix data in both spatial and spectral domains. As a result of the two-dimensional spatio-temporal approach and the generalized sufficiency approximation, a significant improvement on the classification accuracy is achieved.",Spatio-spectral sufficient statistic for mental imagery EEG signals
"['Rakesh Agrawal', 'Ramakrishnan Srikant', 'Dilys Thomas']",We present techniques for privacy-preserving computation of multidimensional aggregates on data partitioned across multiple clients. Data from different clients is perturbed (randomized) in order to preserve privacy before it is integrated at the server. We develop formal notions of privacy obtained from data perturbation and show that our perturbation provides guarantees against privacy breaches. We develop and analyze algorithms for reconstructing counts of subcubes over perturbed data. We also evaluate the tradeoff between privacy guarantees and reconstruction accuracy and show the practicality of our approach.,Privacy preserving OLAP
"['Koichi Yamada', 'Vilany Kimala', 'Muneyuki Unehara']","In Evidence theory, several conditioning rules for updating belief have been proposed, including Dempster's rule of conditioning. The paper views the conditioning rules proposed so far and proposes a new rule of conditioning based on three requirements. Then, it generalizes the rule to be applied to the case where condition is given by an uncertain belief. The paper also discusses a few interpretations of an equation used for evidential reasoning, one of which is interpreted as conditioning with an uncertain condition.","A New Conditioning Rule, Its Generalization and Evidential Reasoning"
"['Alan A. Bertossi', 'Maria Cristina Pinotti', 'Romeo Rizzi']","Given a vector (/spl delta//sub 1/, /spl delta/2,..., /spl delta//sub t/) of non increasing positive integers, and an undirected graph G = (V, E), an L(/spl delta//sub 1/, /spl delta/2,..., /spl delta//sub t/)-coloring of G is a function f from the vertex set V to a set of nonnegative integers such that |f(u) - f (v)| /spl ges/ /spl delta//sub i/, if d(u, v) = i, 1 /spl les/ i /spl les/ t, where d(u,v) is the distance (i.e. the minimum number of edges) between the vertices u and v. This paper presents efficient algorithms for finding optimal L(1,..., 1)-colorings of trees and interval graphs. Moreover, efficient algorithms are also provided for finding approximate L(/spl delta//sub 1/, 1,..., 1)-colorings of trees and interval graphs, as well as approximate L(/spl delta//sub 1/, /spl delta//sub 2/) colorings of unit interval graphs.",Channel assignment on strongly-simplicial graphs
"['Andr?? Girard', 'Ezzedine Abassi']","The optimal synthesis of private telecommunication networks subject to multiple nonsimultaneous demands is considered. A model is given for the multiservice network synthesis model with an accurate representation of the conversion and access costs, while maintaining a classical multicommodity flow representation of the network. This is done by increasing the original network to produce an augmented network. Two decomposition methods for solving the network synthesis problem are presented. The first one is a classical Lagrangian relaxation method and the second is a resource-decomposition-type technique. Some preliminary computational results in both cases are presented. These show that the two methods converge within reasonable computation times. The results also demonstrate that the multihour aspect of the synthesis of multiservice networks should be taken into account in order to realize fully the economies that are possible due to the noncoincidence of different types of demands. >",Decomposition methods for multihour synthesis of private telecommunication networks
"['Veronica Barassi', 'Emiliano Trer??']","Current internet research has been influenced by application developers and computer engineers who see the development of the Web as being divided into three different stages: Web 1.0, Web 2.0 and Web 3.0. This article will argue that this understanding É?? although important when analysing the political economy of the Web É?? can have serious limitations when applied to everyday contexts and the lived experience of technologies. Drawing from the context of the Italian student movement, we show that the division between Web 1.0, Web 2.0 and Web 3.0 is often deconstructed by activistsÉ?? media practices. Therefore, we highlight the importance of developing an approach that É?? by focusing on practice É?? draws attention to the interplay between Web platforms rather than their transition. This approach, we believe, is essential to the understanding of the complex relationship between Web developments, human negotiations and everyday social contexts.",Does Web 3.0 come after Web 2.0? Deconstructing theoretical assumptions through practice
"['Nurit Gronau', 'Maital Neta', 'Moshe Bar']","Visual context plays a prominent role in everyday perception. Contextual information can facilitate recognition of objects within scenes by providing predictions about objects that are most likely to appear in a specific setting, along with the locations that are most likely to contain objects in the scene. Is such identity-related (semantic) and location-related (spatial) contextual knowledge represented separately or jointly as a bound representation? We conducted a functional magnetic resonance imaging (fMRI) priming experiment whereby semantic and spatial contextual relations between prime and target object pictures were independently manipulated. This method allowed us to determine whether the two contextual factors affect object recognition with or without interacting, supporting a unified versus independent representations, respectively. Results revealed a Semantic Spatial interaction in reaction times for target object recognition. Namely, significant semantic priming was obtained when targets were positioned in expected (congruent), but not in unexpected (incongruent), locations. fMRI results showed corresponding interactive effects in brain regions associated with semantic processing (inferior prefrontal cortex), visual contextual processing (parahippocampal cortex), and object-related processing (lateral occipital complex). In addition, activation in fronto-parietal areas suggests that attention and memory-related processes might also contribute to the contextual effects observed. These findings indicate that object recognition benefits from associative representations that integrate information about objects' identities and their locations, and directly modulate activation in object-processing cortical regions. Such context frames are useful in maintaining a coherent and meaningful representation of the visual world, and in providing a platform from which predictions can be generated to facilitate perception and action.",Integrated contextual representation for objects' identities and their locations
"['Yves Chiaramella', 'Jian-Yun Nie']","This paper focuses on the query processing module of RIME, an experimental prototype of an intelligent information retrieval system designed to manage high-precision queries on a corpus of medical reports. Though highly specific this particular corpus is representative of an important class of applications: information retrieval among full-text specialized documents which constitute critical sources of information in several organizations (medicine, law, space industryÉ??). This experience allowed us to design and implement an elaborate model for the semantic content of the documents which is an extension of the Conceptual Dependency approach. The underlying retrieval model is inspired from the Logic model proposed by C.J. Van Rijsbergen, which has been considerably refined using an Extended Modal Logic. After presenting the context of the RIME project, we briefly describe the models designed for the internal representation of medical reports and queries. The main part of the paper is then devoted to the retrieval model and its application to the query processing module of RIME which has a natural language interface. Processing a query involves two main phases: the interpretation which transforms the natural language query into a search expression, and the evaluation phases which retrieves the corresponding medical reports. We focus here on the evaluation phases and show its relationship with the underlying retrieval model. Evaluations from practical experiments are also given, along with indications about current developments of the project.",A retrieval model based on an extended modal logic and its application to the RIME experimental approach
"['Liyun Dai', 'Bican Xia']","A simple linear loop is a simple while loop with linear assignments and linear loop guards. If a simple linear loop has only two program variables, we give a complete algorithm for computing the set of all the inputs on which the loop does not terminate. For the case of more program variables, we show that the non-termination set cannot be described by Tarski formulae in general.",Non-termination sets of simple linear loops
"['Christoph Bregler', 'Jitendra Malik']","This paper demonstrates a new visual motion estimation technique that is able to recover high degree-of-freedom articulated human body configurations in complex video sequences. We introduce the use of a novel mathematical technique, the product of exponential maps and twist motions, and its integration into a differential motion estimation. This results in solving simple linear systems, and enables us to recover robustly the kinematic degrees-of-freedom in noise and complex self occluded configurations. We demonstrate this on several image sequences of people doing articulated full body movements, and visualize the results in re-animating an artificial 3D human model. We are also able to recover and re-animate the famous movements of Eadweard Muybridge's motion studies from the last century. To the best of our knowledge, this is the first computer vision based system that is able to process such challenging footage and recover complex motions with such high accuracy.",Tracking people with twists and exponential maps
"['Bram Klievink', 'Marijn Janssen']","Information infrastructures of businesses and government are increasingly interwoven. The development of these information infrastructures often has a technological focus and the concurrent social innovation is ill understood. To address this gap, we study publicÉ??private information infrastructure developments at three layers over a prolonged period of time. Stakeholders have to alter existing social practices to realize the potential of information infrastructures. New social practices need to be developed and sustaining innovations requires new governance mechanisms.",Developing Multi-Layer Information Infrastructures: Advancing Social Innovation through PublicÉ??Private Governance
"['Hyun-chong Cho', 'Kittipat Kampa', 'K.C. Slatton']","Our paper proposes an approach for the extraction of stream channels from Airborne Laser Swath Mapping (ALSM) data. Recent advances in technology have led to high-resolution topographic data acquisition by means of airborne lidar (i.e. ALSM), which can yield Digital Elevation Model (DEM) datasets with horizontal resolutions of 1 m and vertical rms errors in the range of 10 - 15 cm. The extraction of a stream network from a DEM plays a fundamental role in modeling spatially distributed hydrological processes and flow routing. We apply morphological filtering to an ALSM DEM to detect and characterize stream channels in forested terrain. Since the size and shape of morphological Structuring Elements (SEs) is known to strongly affect filtered results, we test for accuracy by developing a set of error measures over simulated terrain. We subsequently apply the filter to actual ALSM data. For linking disconnected stream segments, a measure of pixel connectedness known as the Connectivity Number is used. The method presented is shown to enable systematic characterization and comparisons of streams, even in heavily forested terrain.",Morphological segmentation of Lidar Digital Elevation Models to extract stream channels in forested terrain
['Giorgio De Santi'],Abstract#R##N##R##N#VLSI technologies led to the possibility of integration of more than a million of active devices on a single silicon chip. A significant part of the effort in the geometrical circuitry shrinkage was set in development of suitable interconnection technologies. The goals persued were the improvement of the electrical properties (conductivity and contact resistance) with the simultaneous improvement of reliability performances.,Interconnections technologies for VLSI circuits
"['Ravinder Thirumala', 'N. Usha Rani', 'Timothy A. Gonsalves']","With the size and increasing complexity of telecom networks, there is a need to interconnect management systems at different levels. This requires information flow across many applications in a domain independent way. XML is a widely deployed standard which is being used for integration of network management system(NMS) with other applications. We examine the performance of different transport mechanisms, JMS, CORBA, HTTP and RMI for an XML message based event Notification Service. To improve the performance of XML message based event notifications, event grouping is examined and is found to perform well.",An Event Notification Service based on XML Messaging on Different Transport Technologies
"['Tao-Sheng Ou', 'Yi-Hsin Huang', 'Homer H. Chen']","The quality of video is ultimately judged by human eye; however, mean squared error and the like that have been used as quality metrics are poorly correlated with human perception. Although the characteristics of human visual system have been incorporated into perceptual-based rate control, most existing schemes do not take rate-distortion optimization into consideration. In this paper, we use the structural similarity index as the quality metric for rate-distortion modeling and develop an optimum bit allocation and rate control scheme for video coding. This scheme achieves up to 25% bit-rate reduction over the JM reference software of H.264. Under the rate-distortion optimization framework, the proposed scheme can be easily integrated with the perceptual-based mode decision scheme. The overall bit-rate reduction may reach as high as 32% over the JM reference software.",SSIM-Based Perceptual Rate Control for Video Coding
"['John E. Smee', 'Stuart C. Schwartz']","To improve compensation to channel or interference changes, we propose adapting an auxiliary feedback filter (FBF) in the receiver of systems which use Tomlinson-Harashima (1971, 1972) precoding. We show how the auxiliary FBF can be adapted in conjunction with the receiver feedforward filter (FFF). Simulations demonstrate the performance advantage of our auxiliary FBF technique relative to FFF updating alone, and how the FFF combines interference suppression with despreading in wideband applications. Error propagation can be effectively avoided by using the auxiliary FBF values to decide when to update the precoder, while transient increases in mean-squared error are avoided by using the FBF values in the update equation.",Adaptive compensation techniques for communications systems with Tomlinson-Harashima precoding
"['MyungJoo Ham', 'Gul Agha']","Market-based mechanisms can be used to coordinate self-interested multi-robot systems in fully distributed environments, where by self-interested we mean that each robot agent attempts to maximize a payoff function that accounts for both the resources consumed and the contribution made by the robot. In previous work, we have studied the effect of various market rules and bidding strategies on the global performance of the multi-robot system. However, rather than use a central monitoring and enforcement mechanisms, we rely on agents to self-report their actions. This assumes that the agents act honestly. In this paper, we drop the honesty assumption, raising the possibility that agents may exaggerate their contribution in order to increase their payoff. To address the problem of such malicious behavior, we propose an audit mechanism to maintain the integrity of reported payoffs. Our algorithm extends previous work on preventing free-riding in peer-to-peer networks. Specifically, we consider locality and mobility in multi-robot systems. We show that our approach efficiently detects malicious behaviors with a high probability.",A Robust Audit Mechanism to Prevent Malicious Behaviors in Multi-robot Systems
"['Dong Keun Kim', 'Jonghwa Kim', 'Eui Chul Lee', 'Mincheol Whang', 'Yongjoo Cho']","In this paper, we implemented an interactive emotional content communication system using a portable wireless biofeedback device to support convenient emotion recognition and immersive emotional content representation for users. The newly designed system consists of the portable wireless biofeedback device and a novel emotional content rendering system. The former performs the acquisition and transmission of three different physiological signals (photoplethysmography, skin temperature, and galvanic skin response) to the remote emotional content rendering system via Bluetooth links in real time. The latter displays video content concurrently manipulated using the feedback of the user?s emotional state. The results of effectiveness of the system indicated that the response time of the emotional content communication system was nearly instant, the changes of between emotional contents and emotional states base on physiological signals was corresponded. The user?s concentration was increased by watching the measuredemotion- based rendered visual stimuli. In the near future, the users of this proposed system will be able to create further substantial user-oriented content based on emotional changes.",Interactive emotional content communications system using portable wireless biofeedback device
"['Erfan Najmi', 'Khayyam Hashmi', 'Fayez Khazalah', 'Zaki Malik']",The volume of information available on the World Wide Web and the rate of its growth requires new techniques to handle and organize this data. Ontologies are becoming the pivotal methodology to represent domain-specific conceptual knowledge and hence help in providing solutions for Question Answering (QA) systems. This paper introduces an approach for enhancing the capabilities of QA systems using semantic technologies. We implemented an approach to convert the natural language user queries to Resource Description Framework (RDF) triples and find relevant answers. The experiment results show that the proposed technique works very well for single word answers. We believe that with some modifications this approach can be expanded to a wider scale.,Intelligent semantic question answering system
"['James H. Duncan', 'Tsai-Chia Chou']",A new method for the detection of motion and the computation of optical flow is presented. In the first step of the calculation the intensity history at each pixel is convolved with the second derivative in time of a temporal Gaussian smoothing function. The zero crossings in a single frame of the resulting function indicate the positions of moving edges. Spatial and temporal derivatives of the function at the zero-crossing locations are then used to compute the component of the flow that is normal to the zero-crossing contours. Both the detection of motion and the computation of the normal velocity are insensitive to slow temporal and spatial changes in the image intensity that are caused by illumination effects rather than motion. A framework in which to relate the present work to a number of gradient based flow measurement techniques is also presented.,Temporal Edges: The Detection Of Motion And The Computation Of Optical Flow
"['Venu Madhav Govindu', 'A. Pooja']","In this paper, we present an extension of the iterative closest point (ICP) algorithm that simultaneously registers multiple 3D scans. While ICP fails to utilize the multiview constraints available, our method exploits the information redundancy in a set of 3D scans by using the averaging of relative motions. This averaging method utilizes the Lie group structure of motions, resulting in a 3D registration method that is both efficient and accurate. In addition, we present two variants of our approach, i.e., a method that solves for multiview 3D registration while obeying causality and a transitive correspondence variant that efficiently solves the correspondence problem across multiple scans. We present experimental results to characterize our method and explain its behavior as well as those of some other multiview registration methods in the literature. We establish the superior accuracy of our method in comparison to these multiview methods with registration results on a set of well-known real datasets of 3D scans.",On Averaging Multiview Relations for 3D Scan Registration
"['James Potter', 'William Singhose']","OBJECTIVE: The objective was to study the performance of a manual tracking task with system flexibility and time delays in the input channel and to examine the effects of input shaping the human operator's commands. BACKGROUND: It has long been known that low-frequency, lightly damped vibration hinders performance of a manually controlled system. Recently, input shaping has been shown to improve the performance of such systems in a compensatory-display tracking task. It is unknown if similar improvements are seen with pursuit-display tasks, or how the improvement changes when time delays are added to the system. METHOD: A total of 18 novice participants performed a pursuit-view tracking experiment with a spring-centered joystick. Controlled elements included an integrator, an integrator with a lightly damped flexible mode, and an input-shaped integrator with a flexible mode. The input to these controlled elements was delayed between 0 and 1 s. Tracking performance was quantified by root mean square tracking error, and subjective difficulty was quantified by ratings on a Cooper-Harper scale. RESULTS: Performance was best with the undelayed integrator. Both time delay and flexibility degraded performance. Input shaping improved control of the flexible element, with a diminishing benefit as the time delay increased. Tracking error and subjective rating were significantly related. Some operators used a pulsive control strategy. CONCLUSION: Input shaping can improve the performance of a manually controlled system with flexibility, even when time delays are present. APPLICATION: This study is useful to designers of human-controlled systems, especially those with problematic flexibility and/or time delays. Language: en",Effects of Input Shaping on Manual Control of Flexible and Time-Delayed Systems
"['Michael F. Hoffmann', 'Sarah C. Preissner', 'Janette Nickel', 'Mathias Dunkel', 'Robert Preissner', 'Saskia Preissner']","As the number of prescribed drugs is constantly rising, drugÉ??drug interactions are an important issue. The simultaneous administration of several drugs can cause severe adverse effects based on interactions with the same metabolizing enzyme(s). The Transformer database (http://bioinformatics.charite.de/transformer) contains integrated information on the three phases of biotransformation (modification, conjugation and excretion) of 3000 drugs and >350 relevant food ingredients (e.g. grapefruit juice) and herbs, which are catalyzed by 400 proteins. A total of 100 000 interactions were found through text mining and manual validation. The 3D structures of 200 relevant proteins are included. The database enables users to search for drugs with a visual display of known interactions with phase I (Cytochrome P450) and phase II enzymes, transporters, food and herbs. For each interaction, PubMed references are given. To detect mutual impairments of drugs, the drug-cocktail tool displays interactions between selected drugs. By choosing the indication for a drug, the tool offers suggestions for alternative medications to avoid metabolic conflicts. Drug interactions can also be visualized in an interactive network view. Additionally, prodrugs, including their mechanisms of activation, and further information on enzymes of biotransformation, including 3D models, can be viewed.",The Transformer database: biotransformation of xenobiotics
"['Ljubomir Jovanov', 'Aleksandra Pizurica', 'Vladimir Zlokolica', 'Stefan Schulte', 'Etienne E. Kerre', 'Wilfried Philips']","In this paper, we introduce the idea of using motion estimation resources from a video codec for video denoising. This is not straightforward because the motion estimators aimed for video compression and coding, tolerate errors in the estimated motion field and hence are not directly applicable to video denoising. To solve this problem, we propose a novel motion field filtering step that refines the accuracy of the motion estimates to a degree that is required for denoising. We illustrate the use of the proposed motion estimation method within a wavelet-based video denoising scheme. The resulting video denoising method is of low-complexity and receives comparable results with respect to the latest video denoising methods.",Combinedwavelet Domain and Motion Compensated Filtering Compliant with Video Codecs
"['Stephan Arlt', 'Ishan Banerjee', 'Cristiano Bertolini', 'Atif M. Memon', 'Martin Sch??f']","Graphical user interfaces (GUIs) encode, as event sequences, potentially unbounded ways to interact with software. During testing it becomes necessary to effectively sample the GUIÉ??s event space. Ideally, for increasing the efficiency and effectiveness of GUI testing, one would like to sample the GUIÉ??s event space by only generating sequences that (1) are allowed by the GUIÉ??s structure, and (2) chain together only those events that have data dependencies between their event handlers. We propose a new model, called an eventdependency graph (EDG) of the GUI that captures data dependencies between the code of event handlers. We develop a mapping between an EDG and an existing black-box model of the GUIÉ??s structure, called an event-flow graph (EFG). We automate the EDG construction in a tool that analyzes the bytecode of each event handler. We evaluate ourÉ??grey-boxÉ?ùapproach using four open-source applications and compare it with the EFG approach. Our results show that using the EDG reduces the number of event sequences with respect to the EFG, while still achieving at least the same coverage. Furthermore, we are able to detect 2 new bugs in the subject applications.",Grey-box GUI Testing: Efficient Generation of Event Sequences
"['W.E.L. Grimson', 'Gil J. Ettinger', 'S.J. White', 'Tomas Lozano-Perez', 'William M. Wells', 'Ron Kikinis']","There is a need for frameless guidance systems to help surgeons plan the exact location for incisions, to define the margins of tumors, and to precisely identify locations of neighboring critical structures. The authors have developed an automatic technique for registering clinical data, such as segmented magnetic resonance imaging (MRI) or computed tomography (CT) reconstructions, with any view of the patient on the operating table. The authors demonstrate on the specific example of neurosurgery. The method enables a visual mix of live video of the patient and the segmented three-dimensional (3-D) MRI or CT model. This supports enhanced reality techniques for planning and guiding neurosurgical procedures and allows us to interactively view extracranial or intracranial structures nonintrusively. Extensions of the method include image guided biopsies, focused therapeutic procedures, and clinical studies involving change detection over time sequences of images.","An automatic registration method for frameless stereotaxy, image guided surgery, and enhanced reality visualization"
"['Sara Vaziri', 'Michael Lustig']","A method for finding the fastest possible gradient waveforms for any given k-space trajectory is presented. It is an extension of our previously introduced solution. The original scheme provides an efficient and non-iterative method for designing the fastest freely rotatable gradient waveforms. Here, the hardware constraints are relaxed so that each axis is constrained independently. This produces the fastest possible non-rotatable waveforms that can be up to 10% faster than their previous counterparts. In addition, for circular trajectories we relax the path constraints. This results in new diamond-shaped trajectories, which are more optimized than circles for separable gradient sets, reducing the total travel time by up to an additional 11%. Analysis of performance for a variety of parameters including the sensitivity to field inhomogeneity compared to freely rotatable circle trajectories is presented.",The fastest gradient waveforms for arbitrary and optimized k-space trajectories
"['Hilde Tobi', 'Paul B. van den Berg', 'Lolkje T. W. de Jong-van den Berg']","In social pharmacy and pharmacoepidemiology the distribution, use and performance of medication after registration is studied. In both fields, the pharmacists are the main source of data on drug use. To increase the value of research, we think it important to exchange ideas and suggestions between scientific researchers and pharmacists who work in community pharmacies. Hence, the department of Social Pharmacy and Pharmacoepidemiology of the University of Groningen sought close collaboration with some community pharmacies in the region, resulting in the InterAction project. The pharmacists deliver data to the InterAction database and are explicitly invited to raise questions and issues from their practice, and to participate in research. Consequently, science and practice benefit from each other's input and expertise. This paper describes the architecture and contents of the InterAction project. Additionally, the first experiences with the database as a laboratory for social pharmacy and pharmacoepidemiology are discussed.",The InterAction Database: Synergy of Science and Practive in Pharmacy
"['Luca Maria Aiello', 'Alain Barrat', 'Rossano Schifanella', 'Ciro Cattuto', 'Benjamin Markines', 'Filippo Menczer']","Social media have attracted considerable attention because their open-ended nature allows users to create lightweight semantic scaffolding to organize and share content. To date, the interplay of the social and topical components of social media has been only partially explored. Here, we study the presence of homophily in three systems that combine tagging social media with online social networks. We find a substantial level of topical similarity among users who are close to each other in the social network. We introduce a null model that preserves user activity while removing local correlations, allowing us to disentangle the actual local similarity between users from statistical effects due to the assortative mixing of user activity and centrality in the social network. This analysis suggests that users with similar interests are more likely to be friends, and therefore topical similarity measures among users based solely on their annotation metadata should be predictive of social links. We test this hypothesis on several datasets, confirming that social networks constructed from topical similarity capture actual friendship accurately. When combined with topological features, topical similarity achieves a link prediction accuracy of about 92p.",Friendship prediction and homophily in social media
"['Vincent Ng', 'Sajib Dasgupta', 'S. M. Niaz Arifin']","This paper examines two problems in document-level sentiment analysis: (1) determining whether a given document is a review or not, and (2) classifying the polarity of a review as positive or negative. We first demonstrate that review identification can be performed with high accuracy using only unigrams as features. We then examine the role of four types of simple linguistic knowledge sources in a polarity classification system.",Examining the Role of Linguistic Knowledge Sources in the Automatic Identification and Classification of Reviews
"['Whai-En Chen', 'Quincy Wu']","This paper presents an IPv6-based SIP VoIP network deployed in Taiwan. This deployment project is supported by NICI IPv6 R&D Division. The major contributions of this paper are exercising the ENUM deployment on IPv6 SIP network, developing the IPv6 SIP User Agent and the IPv6 SIP Analyzer.",Development and Deployment of IPv6-Based SIP VoIP Networks
"['Tomonori Izumi', ""Shin'ichi Kouyama"", 'Hiroyuki Ochi', 'Yukihiro Nakamura']","This paper presents an approach of logic mapping into LUT-Array-Based PLD where Boolean functions in the form of the sum of generalized complex terms (SGCTs) can be mapped directly. While previous mapping approach requires predetermined variable ordering, our approach performs mapping and variable reordering simultaneously. For the purpose, we propose a directed acyclic graph based on the multiple valued decision diagram (MDD) and an algorithm to construct the graph. Our algorithm generates candidates of SGCT expressions for each node in a bottom-up manner and selects the variables in the current level by evaluating the sizes of SGCT expressions directly. Experimental results show that our approach reduces the number of terms maximum to 71 percent for the MCNC benchmark circuits.",An Integrated Approach of Variable Ordering and Logic Mapping into LUT-Array-Based PLD
"['Karthikeyan Lingasubramanian', 'Sanjukta Bhanja']","In sequential logic circuits the transient errors that occur in a particular time frame will propagate to consecutive time frames thereby making the device more vulnerable. In this work we propose a probabilistic error model for sequential logic that can measure the expected output error probability, given a probabilistic input space, that account for both spatial dependencies and temporal correlations across the logic, using a time evolving causal network. We demonstrate our error model using MCNC and ISCAS benchmark circuits and validate it with HSpice simulations. Our observations show that, significantly low individual gate error probabilities produce at least 5 fold higher output error probabilities. The average error percentage of our results with reference to HSpice simulation results is only 4.43%. Our observations show that the order of temporal dependency of error varies for different sequential circuits.",An Error Model to Study the Behavior of Transient Errors in Sequential Circuits
"['Imran Shafi', 'Jamil Ahmad', 'Syed Ismail Shah', 'Ataul Aziz Ikram', 'Adnan Ahmad Khan', 'Sajid Bashir']","This paper describes the validity-guided fuzzy clustering evaluation for optimal training of localized neural networks (LNNs) used for reassigning time-frequency representations (TFRs). Our experiments show that the validity-guided fuzzy approach ameliorates the difficulty of choosing correct number of clusters and in conjunction with neural network-based processing technique utilizing a hybrid approach can effectively reduce the blur in the spectrograms. In the course of every partitioning problem the number of subsets must be given before the calculation, but it is rarely known apriori, in this case it must be searched also with using validity measures. Experimental results demonstrate the effectiveness of the approach.",Validity-guided fuzzy clustering evaluation for neural network-based time-frequency reassignment
"['Olivier Contant', 'St??phane Lafortune', 'Demosthenis Teneketzis']",The diagnosis of unobservable faults in large and complex discrete event systems modeled by parallel composition of automata is considered. A modular approach is developed for diagnosing such systems. The notion of modular diagnosability is introduced and the corresponding necessary and sufficient conditions to ensure it are presented. The verification of modular diagnosability is performed by a new algorithm that incrementally exploits the modular structure of the system to save on computational effort. The correctness of the algorithm is proved. Online diagnosis of modularly diagnosable systems is achieved using only local diagnosers.,Diagnosability of Discrete Event Systems with Modular Structure
"['M. Fujita', 'S. Komatsu', 'S. Saito', 'Kenshu Seto', 'Thanyapat Sakunkonchak', 'Yasufumi Kojima']","In the age of highly integrated system LSIs, design methodologies for shorter time-to-market and higher re-programmability after the chip fabrications are now key research issues because of the difficulty of complete verification before tape-out of LSI designs. In this paper, we first introduce an IP-based VLSI architecture that consists of a main processor and an additional hardware (both custom hard macros and FPGA on a single chip) specialized to be in charge of the specific instructions. We further replace the controller circuits of the specialized hardware with compact micro-controllers and memories by using IP libraries (hard macros), which results in the increase of the debuggability and the flexibility of design even for computations realized by hard macros. We call the proposed architecture as field modifiable architecture (FMA). Experimental results confirm that our architecture can achieve significant performance improvement in terms of execution cycles and that EC (engineering change) can be successfully accommodated ""after"" chip fabrications.",Field modifiable architecture with FPGAs and its design/verification/debugging methodologies
"['Ankur Agarwal', 'Bill Triggs']","We present a novel approach to modelling the non-linear and time- varying dynamics of human motion, using statistical methods to capture the char- acteristic motion patterns that exist in typical human activities. Our method is based on automatically clustering the body pose space into connected regions ex- hibiting similar dynamical characteristics, modelling the dynamics in each region as a Gaussian autoregressive process. Activities that would require large numbers of exemplars in example based methods are covered by comparatively few motion models. Different regions correspond roughly to different action-fragments and our class inference scheme allows for smooth transitions between these, thus mak- ing it useful for activity recognition tasks. The method is used to track activities including walking, running, etc., using a planar 2D body model. Its effectiveness is demonstrated by its success in tracking complicated motions like turns, without any key frames or 3D information.",Tracking Articulated Motion using a Mixture of Autoregressive Models
"['Fedja Hadzic', 'Tharam S. Dillon', 'Elizabeth Chang']",Tree-structured knowledge representations are increasingly being used since the relationships between data objects can be represented in a more meaningful way. A number of tree mining algorithms were developed for mining different subtree types using different parameters. At this point in research it would be useful to discuss what kind of sub-problems can be solved within the current tree mining framework. In this paper we provide a general overview of the development in the area of tree mining and discuss motivations and useful application areas for each development. Implications of using different tree mining parameters and constraints are discussed. Such an overview will be particularly useful for those not so familiar with the area of tree mining as it can reveal useful applications within their domain of interest. It gives guidance as to which type of tree mining will be most useful for their particular application.,Knowledge Analysis with Tree Patterns
"['Olga Goussevskaia', 'Thomas Moscibroda', 'Roger Wattenhofer']","In this work we analyze the complexity of local broadcasting in the physical interference model. We present two distributed randomized algorithms: one that assumes that each node knows how many nodes there are in its geographical proximity, and another, which makes no assumptions about topology knowledge. We show that, if the transmission probability of each node meets certain characteristics, the analysis can be decoupled from the global nature of the physical interference model, and each node performs a successful local broadcast in time proportional to the number of neighbors in its physical proximity. We also provide worst-case optimality guarantees for both algorithms and demonstrate their behavior in average scenarios through simulations.",Local broadcasting in the physical interference model
"['Justin Nduhura Munga', 'St??phane Dauz??re-P??r??s', 'Philippe Vialletelle', 'Claude Yugma']","In a worldwide environment, sustaining high yield with a minimum number of quality controls is key for manufacturing plants to remain competitive. In high-mix semiconductor plants, where more than 200 products are concurrently run, the complexity of designing efficient control plans comes from the larger amount of data and number of production parameters to handle. Several sampling algorithms were proposed in the literature, but most of them are seen impracticable when coming to an industrial implementation. In this paper, we present and discuss the industrial implementation of a dynamic sampling algorithm in a high-mix semiconductor plant. We describe how the sampling algorithm has been modified, and point out the set of questions that have been raised by the industrial program. Results indicate that more than 30% of control operations on lots could be avoided without increasing the material at risk in production.",Industrial implementation of a dynamic sampling algorithm in semiconductor manufacturing: approach and challenges
"['Eckart Zitzler', 'Joshua D. Knowles', 'Lothar Thiele']","This chapter reviews methods for the assessment and comparison of Pareto set approximations. Existing set quality measures from the literature are critically evaluated based on a number of orthogonal criteria, including invariance to scaling, monotonicity and computational effort. Statistical aspects of quality assessment are also considered in the chapter. Three main methods for the statistical treatment of Pareto set approximations deriving from stochastic generating methods are reviewed. The  dominance ranking method  is a generalization to partially-ordered sets of a standard non-parametric statistical test, allowing collections of Pareto set approximations from two or more stochastic optimizers to be directly compared statistically. The  quality indicator method  -- the dominant method in the literature -- maps each Pareto set approximation to a number, and performs statistics on the resulting distribution(s) of numbers. The  attainment function method  estimates the probability of attaining each goal in the objective space, and looks for significant differences between these probability density functions for different optimizers. All three methods are valid approaches to quality assessment, but give different information. We explain the scope and drawbacks of each approach and also consider some more advanced topics, including multiple testing issues, and using combinations of indicators. The chapter should be of interest to anyone concerned with generating and analysing Pareto set approximations.",Quality Assessment of Pareto Set Approximations
"['Ha T. Nguyen', 'Minh N. Do']","This paper is motivated by multichannel sampling applications. We consider a hybrid filter banks consisting of a set of fractional delays operators, slow A/D converters with different antialiasing filters, digital expanders, and digital synthesis filters (to be designed). The synthesis filters are designed to minimize the maximum gain of a hybrid induced error system. We show that the induced error system is equivalent to a digital system. This digital system enables the design of stable synthesis filters using existing control theory tools such as model-matching and linear matrix inequalities. Moreover, the induced error is robust against delay estimate errors. Numerical experiments show the proposed approach yields better performance compared to existing techniques.",Hybrid Filter Banks With Fractional Delays: Minimax Design and Application to Multichannel Sampling
"['William W. Gaver', 'John Bowers', 'Andy Boucher', 'Hans Gellerson', 'Sarah Pennington', 'A. Schmidt', 'Anthony Steed', 'Nicholas Villars', 'Brendan Walker']","The Drift Table is an electronic coffee table that displays slowly moving aerial photography controlled by the distribution of weight on its surface. It was designed to investigate our ideas about how technologies for the home could support ludic activities-that is, activities motivated by curiosity, exploration, and reflection rather than externally-defined tasks. The many design choices we made, for example to block or disguise utilitarian functionality, helped to articulate our emerging understanding of ludic design. Observations of the Drift Table being used in volunteers' homes over several weeks gave greater insight into how playful exploration is practically achieved and the issues involved in designing for ludic engagement.",The drift table: designing for ludic engagement
"['Taede Tillema', 'Martin Dijst', 'Tim Schwanen']","Using data collected among 742 respondents, this article aims at gaining greater insight into (i) the interaction between face-to-face (F2F) and electronic contacts, (ii) the influence of information content and relational distance on the communication mode/ service choice and (iii) the influence of relational and geographical distance, in addition to other factors, on the frequency of F2F and electronic contacts with relatives and friends. The results show that the frequency of F2F contacts is positively correlated with that for electronic communication, pointing at a complementarity effect.With respect to information content and relational distance, we find, on the basis of descriptive analyses, that synchronous modes/services (F2F and telephone conversations) are used more for urgent matters and that asynchronous modes (in particular email) become more influential as the relational distance increases. Finally, ordered probit analyses confirm that the frequency of both F2F and electronic communication d...",Face-to-face and electronic communications in maintaining social networks: the influence of geographical and relational distance and of information content
['Shou-Chih Lo'],"Existing mobile navigators can readily display any static data related to the area surrounding a user. However, their ability to display any dynamic data is limited. In this paper, we enable the viewing of dynamic data on a navigator using wireless data broadcast. The dynamic data are periodically broadcast via base stations of wireless systems, and can be filtered by navigators fetching desired data. We address several crucial issues related to the design of this application, including data organizing, indexing, caching, and querying. We simulate the performance of the resulting system by measuring the time and energy costs associated with retrieving the dynamic data.",Design of Spatial Data Broadcast for Mobile Navigation Applications
"['Sven Burmester', 'Holger Giese', 'J??rg Niere', 'Matthias Tichy', 'J??rg P. Wadsack', 'Robert Wagner', 'Lothar Wendehals', 'Albert Z?¨ndorf']","TodayÉ??s development processes employ a variety of notations and tools, e.g., the Unified Modeling Language UML, the Standard Description Language SDL, requirements databases, design tools, code generators, model checkers, etc. For better process support, the employed tools may be organized within a tool suite or integration platform, e.g., Rational Rose or Eclipse. While these tool-integration platforms usually provide GUI adaption mechanisms and functional adaption via application programming interfaces, they frequently do not provide appropriate means for data integration at the meta-model level. Thus, overlapping and redundant data from different É??integratedÉ?ù tools may easily become inconsistent and unusable. We propose two design patterns that provide a flexible basis for the integration of different tool data at the meta-model level. To achieve consistency between meta-models, we describe rule-based mechanisms providing generic solutions for managing overlapping and redundant data. The proposed mechanisms are widely used within the Fujaba Tool Suite. We report about our implementation and application experiences .",Tool integration at the meta-model level: the Fujaba approach
"['Susan M. Schweizer', 'Virginia L. Stonick', 'J. L. Evans']","We present a new algorithm for simultaneous filtering and parameter estimation of chaotic time series corrupted by additive measurement noise. This method is based on iteratively minimizing the total least squares (TLS) error in the phase-space using steepest descent. In contrast to prior work, we assume that the dynamic equations modeling the nonlinear time series are known, but the corresponding parameters are not. Specifically, the data is assumed to be generated from time series satisfying a set of coupled logistics equations corrupted by additive white Gaussian noise. This work is motivated in part by language modeling, where the dynamics of a conversation are argued to satisfy the coupled logistics equations and the time series data represents noisy measurements taken from protocols. Accurate estimation of the process parameters is critical, e.g., in diagnosing and treating different types of language disorders and also may prove valuable for testing and improving language understanding systems.",TLS parameter estimation for filtering chaotic time series
['Marc Idelson'],"This paper intends to bear witness on the past, present and future of portals at BNP Paribas with a focus on: 1) the B2E intranet portal (Echo'net), 2) the French B2C home banking portal (BNPPARIBAS.NET). It introduce the challenges that confronted BNP Paribas in each case, reflect on lessons learnt and future trends and finally suggest what financial services firms expect from e-business infrastructure hardware and software vendors and service providers",Industry track portals at BNP Paribas: a brief testimony (April 2005)
"['Naoyuki Takesue', 'Junji Furusho', 'Masamichi Sakaguchi']","Magnetorheological (MR) fluids are substances that respond to an applied magnetic field with a change in their theological behavior. Though they are functionally similar to electrorheological (ER) fluids, MR fluids exhibit much higher yield strengths for the applied magnetic fields than ER fluids for the applied electric fields. The devices using MR fluids have an ability to provide high-torque, low-inertia, a safe device and simple interface. In this study, we report on an actuator developed using the MR fluid, which consists of an input part, an output part and an MR fluid clutch between them. First, the basic experiments are examined to investigate the characteristics of the actuator. Next, the torque control system of the MR-fluid actuator is proposed. Finally, the closed-loop control experiments were carried out and it is confirmed that the torque-feedback control is effective for improving the response properties of MR actuators.",Improvement of response properties of MR-fluid actuator by torque feedback control
"['Youngsoo Shin', 'Dae-Hong Kim', 'Kiyoung Choi']","Providing multiple modes to support dynamically changing environments, standards, and new services is prevalent in embedded systems, especially in mobile radio systems. Because such a system frequently contains time-constrained tasks, it is important to analyze the temporal requirements as well as the functional correctness. This paper presents a method to analyze temporal requirements imposed on an embedded real-time system supporting multiple modes. While most performance analysis methods focus only on testing the feasibility of a task or a system, our method goes further by addressing the problem of locating hot spots of a system thereby helping the designer to choose among alternative designs or architectures. We formally define the analysis problem and show that it is very unlikely to be solved efficiently. We present a heuristic algorithm, which is accurate and fast enough to be used in iterative processes in system-level analysis and design. The analysis problem is extended to accommodate probabilistic behavior exhibited by soft real-time tasks.",Schedulability-driven performance analysis of multiple mode embedded real-time systems
"['Xiaohui Li', 'Xin Xue', 'Guanghui Yu']","In this paper, we focus on the performance analysis of the cognitive coexistence between Bluetooth and WLAN systems with sensing errors. The packet transmission rate and packet error probability are derived corresponding to the sensor operating points (consisting of the false alarm probability and the miss detection probability). Then, the optimization problem is established as maximizing the throughput of cognitive user with the constraint of the colliding probability with primary user. Simulation results illustrate that the proposed error analysis approach can obtain the optimal performance when considering the sensing errors. Moreover, the influence on the throughput of cognitive user caused by different sensor operating points is investigated by simulation.",Performance analysis of cognitive coexistence systems with sensing errors
"['Elaine G. Toms', 'Mark M. Hall']","The interactive task in Cultural Heritage in CLEF 2013 used a standardised interactive protocol, information retrieval system and interface to observe a set of participants remotely via the web as well as in the lab access an English language collection from the Europeana Digital Library. Both user response and log data were collected from the 208 participants.",The CHiC Interactive Task (CHiCi) at CLEF2013
"['Martin Straka', 'Jan Kastil', 'Jaroslav Novotny', 'Zdenek Kotasek']","In the paper, a technique for design of highly dependable communication structure in SRAM-based FPGA is presented. The architecture of the multicore system and the structure of fault tolerant bus with cache memories are demonstrated. The fault tolerant properties are achieved by the replication and utilization of the self checking techniques together with partial dynamic reconfiguration. The experimental results show that presented system has small overhead if the high number of function units are used. All experiments were done on the Virtex5 and Virtex6 platform.",Advanced fault tolerant bus for multicore system implemented in FPGA
"['Valentine A. Aalo', 'Jingjun Zhang']","The effect of cochannel interference on the performance of digital mobile radio systems in a Nakagami (1960) fading channel is studied. The performance of maximal ratio combining (MRC) diversity is analyzed in the presence of multiple equal-power cochannel interferers and additive white Gaussian noise. Closed-form expressions are derived for the average probability of error as well as outage probability of both coherent and noncoherent (differentially coherent) binary frequency-shift keying and binary phase-shift keying schemes in an environment with cochannel interference and noise. The results are expressed in terms of the confluent hypergeometric function of the second kind, a function that can be easily evaluated numerically. The analysis assumes an arbitrary number of independent and identically distributed Nakagami interferers.",Performance analysis of maximal ratio combining in the presence of multiple equal-power cochannel interferers in a Nakagami fading channel
"['Zhe Hou', 'Ranald Clouston', 'Rajeev Gor??', 'Alwen Tiu']","Abstract separation logics are a family of extensions of Hoare logic for reasoning about programs that mutate memory. These logics are ""abstract"" because they are independent of any particular concrete memory model. Their assertion languages, called propositional abstract separation logics, extend the logic of (Boolean) Bunched Implications (BBI) in various ways.   We develop a modular proof theory for various propositional abstract separation logics using cut-free labelled sequent calculi. We first extend the cut-fee labelled sequent calculus for BBI of Hou et al to handle Calcagno et al's original logic of separation algebras by adding sound rules for partial-determinism and cancellativity, while preserving cut-elimination. We prove the completeness of our calculus via a sound intermediate calculus that enables us to construct counter-models from the failure to find a proof. We then capture other propositional abstract separation logics by adding sound rules for indivisible unit and disjointness, while maintaining completeness and cut-elimination. We present a theorem prover based on our labelled calculus for these logics.",Proof search for propositional abstract separation logics via labelled sequents
"['Khai N. Truong', 'Gillian R. Hayes', 'Gregory D. Abowd']","Storyboarding is a common technique in HCI and design for demonstrating system interfaces and contexts of use. Despite its recognized benefits, novice designers still encounter challenges in the creation of storyboards. Furthermore, as computing becomes increasingly integrated into the environment, blurring the distinction between the system and its surrounding context, it is imperative to depict context explicitly in storyboards. In this paper, we present two formative studies designed to uncover the important elements of storyboards. These elements include the use of text, inclusion of people, level of detail, number of panels, and representation of the passage of time. We further present an empirical study to assess the effects of these elements on the understanding and enjoyment of storyboard consumers. Finally, we demonstrate how these guidelines were successfully used in an undergraduate HCI class.",Storyboarding: an empirical determination of best practices and effective guidelines
"['Fuad Abujarad', 'Sandeep S. Kulkarni']","We focus on constraint-based automated addition of nonmasking and stabilizing fault-tolerance to hierarchical programs. We specify legitimate states of the program in terms of constraints that should be satisfied in those states. To deal with faults that may violate these constraints, we add recovery actions while ensuring interference freedom among the recovery actions added for satisfying different constraints. Since the constraint-based{\em manual} design of fault-tolerance is well-known to be applicable in the manual design of nonmasking fault-tolerance, we expect our approach to have a significant benefit in automation of fault-tolerant programs. We illustrate our algorithms with three case studies:stabilizing mutual exclusion, stabilizing diffusing computation, and a data dissemination problem in sensor networks. With experimental results,we show that the complexity of synthesis is reasonable and that it can be reduced using the {\em structure} of the hierarchical systems. To our knowledge, this is the first instance where automated synthesis has been successfully used in synthesizing programs that are correct under fairness assumptions. Moreover, in two of the case studies considered in this paper, the structure of the recovery paths is too complex to permit existing heuristic based approaches for adding recovery.",Constraint Based Automated Synthesis of Nonmasking and Stabilizing Fault-Tolerance
"['Nuno Vieira Lopes', 'P.A. Mogadouro do Couto', 'Humberto Bustince', 'Pedro Melo-Pinto']","In this paper, an automatic histogram threshold approach based on a fuzziness measure is presented. This work is an improvement of an existing method. Using fuzzy logic concepts, the problems involved in finding the minimum of a criterion function are avoided. Similarity between gray levels is the key to find an optimal threshold. Two initial regions of gray levels, located at the boundaries of the histogram, are defined. Then, using an index of fuzziness, a similarity process is started to find the threshold point. A significant contrast between objects and background is assumed. Previous histogram equalization is used in small contrast images. No prior knowledge of the image is required.",Automatic Histogram Threshold Using Fuzzy Measures
"['Alejandro Munoz', 'Armando Ferro', 'Fidel Liberal', 'Javier Ferreiros L??pez']","Traffic monitoring is an increasingly important discipline for nowadays networking, as Accounting, Security and Traffic Engineering lay on it. Besides, traffic bandwidth has increased exponentially in the last few years, and high-speed network monitoring has become a challenging task. Performance requirements are highly relevant for passive QoS monitoring systems. A low-level study of the capturing and processing stages on a traffic analysis system (TAS) has shown room for improvement. We provide an architecture able to cope with high-speed traffic monitoring using commodity hardware. Our system is intended to exploit the parallelism available in up-to- date workstations, which also introduces constraints for multithreaded QoS analysis. This paper presents a kernel-level framework (ksensor) that, keeping the previous requirements, removes some issues from user-level processing and effectively integrates QoS algorithms, improving the overall performance.",Ksensor: Multithreaded kernel-level probe for passive QoS monitoring
"['Rupam Mukherjee', 'Amit Patra', 'Soumitro Banerjee']","Various non-conventional methods have been employed in the past, to reduce the cost and weight of traditional conducted EMI filters and radiation screens for EMI suppression in switching power electronic converters. This paper points out various shortcomings of these methods which are mainly frequency modulation based, and describes the design of a ramp-generator IC based on a modified modulation scheme. This IC can be used on any voltage mode controlled converter and has a feature that enables the user to tune the same converter to various EMC norms. Test results from a prototype showing significant reduction in harmonic power level have been presented. Moreover, this paper discusses a theoretical formulation for calculating the output capacitor size to maintain ripple specifications, when operating under chaotic modulation.",Chaos-Modulated Ramp IC for EMI Reduction in PWM Buck Converters Design and Analysis of Critical Issues
"['Emily M. Craparo', 'Jonathan P. How', 'Eytan Modiano']","This paper presents new algorithms for conducting cooperative sensing using a mobile backbone network. This hierarchical sensing approach combines backbone nodes, which have superior mobility and communication capability, with regular nodes, which are constrained in mobility and communication capability but which can sense the environment. In the framework of a cooperative exploration problem, a technique is developed for simultaneous placement and assignment of regular and mobile backbone nodes. This method, a generalization of existing techniques that only consider stationary regular nodes, optimally solves the simultaneous placement and assignment problem in computationally tractable time for problems of moderate size. For large-scale instances of this problem, a polynomial-time approximation algorithm is developed. This algorithm carries the benefit of a theoretical performance guarantee and also performs well in practice. Finally, the simultaneous placement and assignment technique is incorporated into a cooperative exploration algorithm, and its performance is shown to compare favorably with that of a benchmark based on existing assignment algorithms for mobile backbone networks.",Simultaneous placement and assignment for exploration in mobile backbone networks
['David A. Siegel'],"Quantitative market research and qualitative user-centered design research have long had an uneasy and complex relationship. A trend toward increasingly complex statistical segmentations and associated personas will once again increase the urgency of addressing paradigm differences to allow the two disciplines to collaborate effectively.   We present an instructive case in which qualitative field research helped contribute to abandoning a ""state of the art"" quantitative user segmentation that was used in an attempt to unify both marketing and user experience planning around a shared model of users. This case exposes risks in quantitative segmentation research, common fallacies in the evolving practice of segmentation and use of personas, and the dangers of excessive deference to quantitative research generally.",The mystique of numbers: belief in quantitative approaches to segmentation and persona development
"['Lakshmi N. Bairavasundaram', 'Meenali Rungta', 'Nitin Agrawa', 'Andrea C. Arpaci-Dusseau', 'Remzi H. Arpaci-Dusseau', 'Michael M. Swift']","The long-term availability of data stored in a file system depends on how well it safeguards on-disk pointers used to access the data. Ideally, a system would correct all pointer errors. In this paper, we examine how well corruption-handling techniques work in reality. We develop a new technique called type-aware pointer corruption to systematically explore how a file system reacts to corrupt pointers. This approach reduces the exploration space for corruption experiments and works without source code. We use type-aware pointer corruption to examine Windows NTFS and Linux ext3. We find that they rely on type and sanity checks to detect corruption, and NTFS recovers using replication in some instances. However, NTFS and ext3 do not recover from most corruptions, including many scenarios for which they possess sufficient redundant information, leading to further corruption, crashes, and unmountable file systems. We use our study to identify important lessons for handling corrupt pointers.",Analyzing the effects of disk-pointer corruption
['Sebastian Feller'],"In this paper I argue that dialog facilitates learning. As a consequence teaching and learning should be dialogic in principal. Against this background the question arises what dialog actually is and how it can be implemented for teaching and learning. A pilot corpus study of a dialog and a monolog corpus of selected teacher/learner interactions sheds light on some of the structural and lexical characteristics of dialogic teaching and learning. The analysis of the data discloses five communicative functions of selected keywords of the dialog corpus, which indicate how speakers are affiliated in dialogic interaction.",The nature of dialog: structural and lexical markers of dialogic teacher/learner interactions
"['Wichit Sombat', 'Philipp Rohlfshagen', 'Simon M. Lucas']","The video games industry is one of the fastest-growing industries in the world, bolstered by sophisticated technology in gaming consoles and modern trends such as mobile and social gaming. The goal of most video games is to entertain the gamer and in most games this stems from the interaction between the gamer and the non-player characters (NPCs): it is no longer sufficient for a game to be visually appealing but instead, the gamer must be challenged at the right level of difficulty to be engaged by the game. It is thus necessary to develop suitable NPCs that are fun to play against and the realm of computational intelligence offers a variety of techniques to do so. However, the perception of fun is clearly subjective and indeed, difficult to quantify. In this paper we make use of the Ms Pac-Man vs Ghosts gaming competition to gather and analyse data from human gamers regarding their preference of opponent: each gamer plays two games against different ghost teams, indicating their preference at the end. We subsequently use this data to establish which ghost teams are generally preferred and demonstrate that there are measurable differences between these ghost teams. These differences are sufficient to group the ghosts into different categories with a good degree of accuracy. This work is a first step in better understanding the attributes required by NPCs for players to be engaged.",Evaluating the enjoyability of the ghosts in Ms Pac-Man
"['Verena Rieser', 'Oliver Lemon']","We investigate the use of different machine learning methods in combination with feature selection techniques to explore human multimodal dialogue strategies and the use of those strategies for automated dialogue systems. We learn policies from data collected in a Wizard-of-Oz study where different human É??wizardsÉ?? decide whether to ask a clarification request in a multimodal manner or else to use speech alone. We first describe the data collection, the coding scheme and annotated corpus, and the validation of the multimodal annotations. We then show that there is a uniform multimodal dialogue strategy across wizards, which is based on multiple features in the dialogue context. These are generic features, available at runtime, which can be implemented in dialogue systems. Our prediction models (for human wizard behaviour) achieve a weighted f-score of 88.6 per cent (which is a 25.6 per cent improvement over the majority baseline). We interpret and discuss the learned strategy. We conclude that human wizard behaviour is not optimal for automatic dialogue systems, and argue for the use of automatic optimization methods, such as Reinforcement Learning. Throughout the investigation we also discuss the issues arising from using small initial Wizard-of-Oz data sets, and we show that feature engineering is an essential step when learning dialogue strategies from such limited data.",Learning human multimodal dialogue strategies
"['Hanli Wang', 'Sam Kwong', 'Yaochu Jin', 'Wei Wei', 'Kim-Fung Man']","A new scheme based on multi-objective hierarchical genetic algorithm (MOHGA) is proposed to extract interpretable rule-based knowledge from data. The approach is derived from the use of multiple objective genetic algorithm (MOGA), where the genes of the chromosome are arranged into control genes and parameter genes. These genes are in a hierarchical form so that the control genes can manipulate the parameter genes in a more effective manner. The effectiveness of this chromosome formulation enables the fuzzy sets and rules to be optimally reduced. Some important concepts about the interpretability are introduced and the fitness function in the MOGA will consider both the accuracy and interpretability of the fuzzy model. In order to remove the redundancy of the rule base proactively, we further apply an interpretability-driven simplification method to newborn individuals. In our approach, we first apply the fuzzy clustering to generate an initial rule-based model. Then the multi-objective hierarchical genetic algorithm and the recursive least square method are used to obtain the optimized fuzzy models. The accuracy and the interpretability of fuzzy models derived by this approach are studied and presented in this paper. We compare our work with other methods reported in the literature on four examples: a synthetic nonlinear dynamic system, a nonlinear static system, the Lorenz system and the Mackey-Glass system. Simulation results show that the proposed approach is effective and practical in knowledge extraction.",Multi-objective hierarchical genetic algorithm for interpretable fuzzy rule-based knowledge extraction
"['Terence Blackburn', 'Paul A. Swatman', 'Rudi Vernik']","Results from empirical research in Requirements Engineering lead us to characterise work processes, not as evolutionary and systematic, but as semi structured, emergent and creative. These same properties are found in Situated Action models where work is described as emergent and self defining. At another level within these processes, we use the term ""cognitive dust"" to describe the External and Distributed cognitive representations suspended in a small group workspace. These cognitive representations are components of communicative actions between humans, or between humans and their technological infrastructures, and are part of these creative processes. We use a multi modal sensor infrastructure, which is situated in a ubiquitous workspace, to observe and capture this cognitive dust. We hypothesise that, if an infrastructure can capture enough of these representations and extract enough semantic meaning to ""understand"" the communicative intentions, the infrastructure can dynamically support creative, unstructured activities. This paper sets out the conceptual, theoretical framework for this research by linking known CSCW theories with the emergent, unstructured or semi structured nature of creative work processes within small groups such as designers.",Cognitive Dust: Linking CSCW Theories to Creative Design Processes
"['Sassan Pejhan', 'Mischa Schwartz', 'Dimitris Anastassiou']","We analyze different retransmission (ARQ) schemes for error control in multicast protocols geared toward real-time, multimedia applications. We discuss why retransmission schemes are not inappropriate for such applications, but in fact can be quite effective. We present a quantitative analysis of such schemes, as well as simulation results, taking into account four different parameters (and not just the source throughput): (1) the probability of dropping a packet due to limited time for retransmissions; (2) the average time required to deliver a packet correctly to end receivers; (3) the number of times a packet will be retransmitted; and (4) the cost to the network, in terms of packet duplications, of retransmitting a packet. We reach the counter-intuitive conclusion that the optimum scheme, in terms of all four of the above parameters, in the most general scenarios (where several hosts with widely varying propagation delays and 'quality of connections' are participating in the session) is to immediately retransmit packets-preferably multicast-upon reception of a NACK from any receiver. We also demonstrate, again through quantitative analysis, the circumstances under which it would be beneficial (as well as those under which it would be counter-productive) to multicast control messages in the hope of suppressing duplicates and preventing the source from being overwhelmed by control messages.",Error control using retransmission schemes in multicast transport protocols for real-time media
"['Dorit Nuzman', 'Ayal Zaks']","Vectorization has been an important method of using data-level parallelism to accelerate scientific workloads on vector machines such as Cray for the past three decades. In the last decade it has also proven useful for accelerating multi-media and embedded applications on short SIMD architectures such as MMX, SSE and AltiVec. Most of the focus has been directed at innermost loops, effectively executing their iterations concurrently as much as possible. Outer loop vectorization refers to vectorizing a level of a loop nest other than the innermost, which can be beneficial if the outer loop exhibits greater data-level parallelism and locality than the innermost loop. Outer loop vectorization has traditionally been performed by interchanging an outer-loop with the innermost loop, followed by vectorizing it at the innermost position. A more direct unroll-and-jam approach can be used to vectorize an outer-loop without involving loop interchange, which can be especially suitable for short SIMD architectures.   In this paper we revisit the method of outer loop vectorization, paying special attention to properties of modern short SIMD architectures. We show that even though current optimizing compilers for such targets do not apply outer-loop vectorization in general, it can provide significant performance improvements over innermost loop vectorization. Our implementation of direct outer-loop vectorization, available in GCC 4.3, achieves speedup factors of 3.13 and 2.77 on average across a set of benchmarks, compared to 1.53 and 1.39 achieved by innermost loop vectorization, when running on a Cell BE SPU and PowerPC970 processors respectively. Moreover, outer-loop vectorization provides new reuse opportunities that can be vital for such short SIMD architectures, including efficient handling of alignment. We present an optimization tapping such opportunities, capable of further boosting the performance obtained by outer-loop vectorization to achieve average speedup factors of 5.26 and 3.64.",Outer-loop vectorization: revisited for short SIMD architectures
"['Tetsuya Yamamoto', 'Tsutomu Terada', 'Masahiko Tsukamoto']","In wearable computing environments, people handles various information anytime and anywhere with a wearing computer. In such situation, a gesture is one of powerful methods as input method because it needs no physical devices to touch and a user can input quickly. However, there are various restrictions for gesture input in daily life; gestures must be socially acceptable because a user has to gesture with unusual movements in a crowd, gestures must be flexible because a user cannot gesture when he/she has a bag with his hand that is used for a gesture. In this paper, we clarify the restrictions on gesture interfaces in daily life, then propose practical gestures for selecting simple menu items with hands and feet.",Designing gestures for hands and feet in daily life
"['Sepideh Jabbari', 'Hassan Ghassemian']","Heart murmurs are pathological sounds produced by turbulent blood flow due to certain cardiac defects such as valves disorders. Detection of murmurs via auscultation is a task that depends on the proficiency of physician. There are many cases in which the accuracy of detection is questionable. The purpose of this study is development of a new mathematical model of systolic murmurs to extract their crucial features for identifying the heart diseases. A high resolution algorithm, multivariate matching pursuit, was used to model the murmurs by decomposing them into a series of parametric time-frequency atoms. Then, a novel model-based feature extraction method which uses the model parameters was performed to identify the cardiac sound signals. The proposed framework was applied to a database of 70 heart sound signals containing 35 normal and 35 abnormal samples. We achieved 92.5% accuracy in distinguishing subjects with valvular diseases using a MLP classifier, as compared to the matching pursuit-based features with an accuracy of 77.5%.",Modeling of heart systolic murmurs based on multivariate matching pursuit for diagnosis of valvular disorders
"['Shu-Qing Li', 'Chi Hou Chan', 'Leung Tsang', 'Qin Li', 'Lin Zhou']","Wave scattering from two-dimensional (2-D) random rough surfaces [three-dimensional (3-D) scattering problem] has been previously analyzed using the sparse-matrix/canonical grid (SM/CG) method. The computational complexity and memory requirement of the SM/CG method are O(N log N) per iteration and O(N), respectively, where N is the number of surface unknowns. Furthermore, the SM/CG method is FFT based, which facilitates the implementation on parallel processors. In this paper, we present a cost-effective solution by implementing the SM/CG method on a Beowulf system consisting of PCs (processors) connected by a 100 Base TX Ethernet switch. The workloads of computing the sparse-matrix-vector multiplication corresponding to the near interactions and the fast Fourier transform (FFT) operations corresponding to the far interactions in the SM/CG method can be easily distributed among all the processors. Both perfectly conducting and lossy dielectric surfaces of Gaussian spectrum and ocean spectrum are analyzed thereafter. When possible, speedup factors against a single processor are given. It is shown that the SM/CG method for a single realization of rough surface scattering can be efficiently adapted for parallel implementation. The largest number of surface unknowns solved in this paper is over 1.5 million. On the other hand, a problem of 131072 surface unknowns for a PEC random rough surface of 1024 square wavelengths only requires a CPU time of less than 20 min. We demonstrate that analysis of a large-scale 2-D random rough surface feasible for a single realization and for one incident angle is possible using the low-cost Beowulf system.",Parallel implementation of the sparse-matrix/canonical grid method for the analysis of two-dimensional random rough surfaces (three-dimensional scattering problem) on a Beowulf system
"['Atul Katoch', 'Maurice Meijer', 'Sanjeev Kimar Jain']","As the IC process technology scales the on-chip wiring network becomes denser. Increasing aspect ratios of the on-chip interconnects lead to higher coupling capacitances and ultimately higher cross-talk noise, which degrades signal integrity. In this paper we propose a clamping circuit for on-chip busses, which clamps a victim wire in an on-chip bus based on the states of its immediate aggressors. These clampers help the driver of the victim wire in draining the charge, which is induced due to cross-talk between aggressors and victim wires. This helps in decreasing the cross-talk peak noise and also the delay variability (referred to as delay noise). Simulation results for a 10 mm long communication bus (parallel wires) laid at minimum pitch in 0.13 /spl mu/m CMOS technology show that a reduction of 30%(17.6%), 37%(27.2%) and 26%(65.8%) in cross-talk peak noise amplitude (delay noise) is observed for point to point, parallel repeater inserted and staggered repeater inserted respectively when only immediate neighbours are considered (1/sup st/ order). Furthermore the aggressor-aware clamper is very effective in avoiding glitches, which may occur when more aggressors, in addition to the immediate ones are also switching simultaneously in the same.",Active noise cancellation using aggressor-aware clamping circuit for robust on-chip communication
"['Xian-Da Zhang', 'Jie Cheng']","The authors present a practical algorithm for estimating the power spectrum of a 2-D homogeneous random field based on 2-D autoregressive moving average (ARMA) modeling. This algorithm is a two-step approach: first, the AR parameters are estimated by solving a version of the 2-D modified Yule-Walker equation, for which some existing efficient algorithms are available; then the MA spectrum parameters are obtained by simple computations. The potential capability and the high-resolution performance of the algorithm are demonstrated by using some numerical examples. >",High resolution two-dimensional ARMA spectral estimation
"['John Hannan', 'Frank Pfenning']","A methodology for the verification of compiler correctness based on the LF logical framework as realized within the Elf programming language is presented. This technique is used to specify, implement, and verify a compiler from a simple functional programming language to a variant of the Categorical Abstract Machine (CAM). >",Compiler verification in LF
"['Sami Muhaidat', 'Murat Uysal', 'Raviraj S. Adve']","In this paper, we investigate the performance of a single-relay cooperative scenario where the source, relay, and destination terminals are equipped with multiple transmit/receive antennas. We particularly focus on the so-called blind amplify-and-forward relaying in which the availability of channel state information at the relay terminal is not required. Through the derivation of pairwise error probability, we quantify analytically the impact of multiple antenna deployment assuming various scenarios which involve relay location and power allocation assumptions imposed on the cooperating nodes.",Blind Amplify-and-Forward Relaying in Multiple-Antenna Relay Networks
"['Osamu Mori', 'Toru Omata']","This paper proposes a reconfigurable planar parallel robot by coupling two 2R open kinematic chains (or 2-link robots), the first joints of which are passive. We show that they can reconfigure to a 5R closed kinematic chain which has the same number of actuators as its degrees of freedom. They can also reconfigure to a 4R closed kinematic chain plus one actuated link. The parallel robot has only two actuators but can have multiple functions by reconfigurations. Due to the passive joints, whether or not the 2R open kinematic chains can couple with each other is a non-trivial problem. We propose coupling sequences for forming the 4R and 5R closed kinematic chains and verify those experimentally.",Coupling of two 2-link robots with a passive joint for reconfigurable planar parallel robot
"['N.P. Anthapadmanabhan', 'Alexander Barg', 'Ilya Dumer']","We address the maximum attainable rate of fingerprinting codes under the marking assumption, studying lower and upper bounds on the value of the rate for various sizes of the attacker coalition. Lower bounds are obtained by considering typical coalitions, which represents a new idea in the area of fingerprinting and enables us to improve the previously known lower bounds for coalitions of size two and three. For upper bounds, the fingerprinting problem is modeled as a communications problem. It is shown that the maximum code rate is bounded above by the capacity of a certain class of channels, which are similar to the multiple-access channel (MAC). Converse coding theorems proved in the paper provide new upper bounds on fingerprinting capacity. It is proved that capacity for fingerprinting against coalitions of size two and three over the binary alphabet satisfies and , respectively. For coalitions of an arbitrary fixed size , we derive an upper bound on fingerprinting capacity in the binary case. Finally, for general alphabets, we establish upper bounds on the fingerprinting capacity involving only single-letter mutual information quantities.",On the Fingerprinting Capacity Under the Marking Assumption
"['Elizabeth M. Rudnick', 'Thomas M. Niermann', 'Janak H. Patel']","Methods are investigated for reducing events in sequential circuit fault simulation by reducing the number of faults simulated for each test vector. Inactive faults, which are guaranteed to have no effect on the output or the next state, are identified using local information from the fault-free circuit in one technique. In a second technique, the Star-algorithm is extended to handle sequential circuits and provides global information about inactive faults, based on the fault-free circuit state. Both techniques are integrated into the PROOFS synchronous sequential circuit fault simulator. An average 28% reduction in faulty circuit gate evaluations is obtained for the 19 ISCAS-89 benchmark circuits studied using the first technique, and 33% reduction for the two techniques combined. Execution times decrease by an average of 17% when the first technique is used. For the largest circuits, further improvements in execution time are made when the Star-algorithm is included. >",Methods for reducing events in sequential circuit fault simulation
"['Liyu Liu', 'Moeness G. Amin']","Signal multipath leads to undesirable tracking errors and inaccurate ranging information for GPS receivers. The extent of the tracking error in compromising the receiver discriminator performance depends on the multipath amplitude, delay, and phase relative to the direct path. Compared with the rural area, the GPS receiver in the semi-enclosed area, such as city canyons and building shadows, is subject to much weaker line of sight propagation environment which further compromise its performance. In this paper, we derive analytical expressions of the multipath effect on the GPS tracking errors for both coherent discriminator and noncoherent early-minus-late power discriminators in strong multipath environment.",Tracking performance of the coherent and noncoherent discriminators in strong multipath
"['Jiri Simsa', 'Satnam Singh']","Recent progress in program analysis has produced tools that are able to compute upper bounds on the use of dynamic memory. This opens up a space for the use of dynamic memory abstraction in high-level synthesis. In this paper, we explain how to design hardware using C programs with  malloc () and  free (). A compilation process is outlined for transforming C programs with heap operations into a hardware description language. As demonstrated by our experiments, this approach is feasible. Further, automatic parallelization of the generated circuits improves by a factor up to 1.9 in terms of clock frequency and a factor up to 2.7 in terms of clock cycles over the previous work.",Designing hardware with dynamic memory abstraction
"['Mustafa Karakoc', 'Adnan Kavak']","Orthogonal variable spreading factor (OVSF) codes are widely used to provide variable data rates for supporting different bandwidth requirements in wideband code division multiple access (WCDMA) systems. Many works in the literature have intensively investigated to find an optimal dynamic code assignment scheme for OVSF codes. Unlike earlier studies, which assign OVSF codes using conventional (CCA) or dynamic (DCA) code allocation schemes, in this paper, adaptive simulated annealing genetic algorithm (ASAGA) was applied which population is adaptively constructed according to existing traffic density in the OVSF code-tree. Also, the influences of the ASAGA parameters (selection, crossover and mutation techniques and cooling schedules) were examined on the dynamic OVSF code allocation problem. The simulation results show that the ASAGA provides reduced code blocking probability and improved spectral efficiency in the system when compared to the CCA and DCA schemes. ASAGA is also tested with its components SA and GA.",A New Dynamic OVSF Code Allocation Method based on Adaptive Simulated Annealing Genetic Algorithm (ASAGA)
"['Tina Wong', 'Randy H. Katz']","Scalability of multicast forwarding state is likely to be a major issue facing inter-domain multicast deployment. We present a comprehensive analysis of the multicast forwarding state problem. Our goal is to understand the scaling trends of multicast forwarding state in the Internet, and to explore the intuitions that have motivated state reduction research. We conducted simulation experiments on both real and generated network topologies, with a range of parameters driven by multicast application characteristics. We found that the increase in peering among Internet backbone networks has led to more multicast forwarding state at a handful of core domains, but less state in the rest of the domains. We observed that scalability of multicast forwarding state with respect to session size follows a power law. Our findings show that distribution and concentration of multicast forwarding state in the Internet is significantly, impacted by the application characteristics. We investigated the proposals on non-branching multicast forwarding state elimination, and found substantial reduction is attainable even with very dense multicast sessions.",An analysis of multicast forwarding state scalability
"['Rui Zhou', 'Jinghan Wang', 'Guowei Wang', 'Jing Li']","With the development of Service-Oriented Architecture, more and more researches have provided automatic and semi-automatic approaches to end-user. Users can construct their own applications with web services. However, it is hard for most end-users to customize the interfaces of applications with current service composition methods. To address this issue, an interface generation model was proposed to provide customized user interface and interaction workflow. In this model, knowledge was involved to instruct the workflow of interaction. Templates were adopted to describe the user interface. Some significant points, such as user definition, data profile, user interaction workflow, interface description, were discussed in detail. A prototype system was implemented. Some demos have been shown to verify the customized interface generation model. With this model, end-users can define the interfaces and interaction workflows of web services with rules and templates. It supplies the gap of user interface in service composition. Compared with the current interface generation in service composition, the proposed model is more flexible and more effective for end-users",Customized Interface Generation Model Based on Knowledge and Template for Web Service
"['Pradeepa Thomas', 'Amel Yessad', 'Jean-Marc Labat']","Serious games are now an increasingly used tool in business training. The question of the effectiveness of such devices on learning is a research issue. The indicators provided at the end of a video game are insufficient to understand and follow the path of a learning player. It is therefore necessary to track not only the player's actions but also to provide tools in order to analyze and diagnose the knowledge acquisition of the learner. We developed an approach based on Petri nets, used to model the accurate behavior of the player. We complete this tool with ontology to explain learner's mistakes.","Petri Nets and Ontologies: Tools for the ""Learning Player"" Assessment in Serious Games"
"['Qi Yuan', 'Weidong Zhou', 'Shasha Yuan', 'Xueli Li', 'Jiwen Wang', 'Guijuan Jia']","The automatic identification of epileptic EEG signals is significant in both relieving heavy workload of visual inspection of EEG recordings and treatment of epilepsy. This paper presents a novel method based on the theory of sparse representation to identify epileptic EEGs. At first, the raw EEG epochs are preprocessed via Gaussian low pass filtering and differential operation. Then, in the scheme of sparse representation based classification (SRC), a test EEG sample is sparsely represented on the training set by solving l1-minimization problem, and the represented residuals associated with ictal and interictal training samples are computed. The test EEG sample is categorized as the class that yields the minimum represented residual. So unlike the conventional EEG classification methods, the choice and calculation of EEG features are avoided in the proposed framework. Moreover, the kernel trick is employed to generate a kernel version of the SRC method for improving the separability between ictal and interictal classes. The satisfactory recognition accuracy of 98.63% for ictal and interictal EEG classification and for ictal and normal EEG classification has been achieved by the kernel SRC. In addition, the fast speed makes the kernel SRC suit for the real-time seizure monitoring application in the near future.",Epileptic EEG classification based on kernel sparse representation.
"['Hussam Amrouch', 'Joerg Henkel']","Continuous shrinking in feature size, increasing power density etc. increase the vulnerability of microprocessors against soft errors even in terrestrial applications. The register file is one of the essential architectural components where soft errors can be very mischievous because errors may rapidly spread from there throughout the whole system. Thus, register files are recognized as one of the major concerns when it comes to reliability. This paper introduces Self-Immunity, a technique that improves the integrity of the register file with respect to soft errors. Based on the observation that a certain number of register bits are not always used to represent a value stored in a register. This paper deals with the difficulty to exploit this obvious observation to enhance the register file integrity against soft errors. We show that our technique can reduce the vulnerability of the register file considerably while exhibiting smaller overhead in terms of area and power consumption compared to state-of-the-art in register file protection.",Self-Immunity Technique to Improve Register File Integrity Against Soft Errors
"['Xuan Tian', 'Haihua Li', 'Xiaoyong Du']","In domain ontology, semantic association (SA) is used to depict the correlation between two concepts. In this paper, we define semantic association degree (SAD) for measuring SA in the domain ontology. We first present a method to measure SAD of two direct related concepts by evaluating the semantic relationship between them, and then give another method to measure SAD of two indirect related concepts though SAD of two directed neighboring concepts. A set of comparison experiments show the benefit of our approaches.",Measuring Semantic Associaiton in Domain Ontology
"['Vicen?? Torra', 'Yasuo Narukawa']","In this paper, we review two of the most well-known citation indexes and establish their connections with the Choquet and Sugeno integrals. In particular, we show that the recently established h-index is a particular case of the Sugeno integral, and that the number of citations corresponds to the Choquet integral. In both cases, they use the same fuzzy measure. The results presented here permit one to envision new indexes defined in terms of fuzzy integrals using other types of fuzzy measures. A few considerations in this respect are also included in this paper. Indexes for taking into account recent research and the publisher credibility are outlined.",The $h$ -Index and the Number of Citations: Two Fuzzy Integrals
"['Shubhendu S. Mukherjee', 'Joel S. Emer', 'Steven K. Reinhardt']","Radiation-induced soft errors have emerged as a key challenge in computer system design. If the industry is to continue to provide customers with the level of reliability they expect, microprocessor architects must address this challenge directly. This effort has two parts. First, architects must understand the impact of soft errors on their designs. Second, they must select judiciously from among available techniques to reduce this impact in order to meet their reliability targets with minimum overhead. To provide a foundation for these efforts, this paper gives a broad overview of the soft error problem from an architectural perspective. We start with basic definitions, followed by a description of techniques to compute the soft error rate. Then, we summarize techniques used to reduce the soft error rate. This paper also describes problems with double-bit errors. Finally, this paper outlines future directions for architecture research in soft errors.",The soft error problem: an architectural perspective
"['Chi-Sheng Lin', 'Ting-Hsu Chien', 'Chin-Long Wey']","Operating up to 5.5 GHz with 1-mW power consumption, a 90-nm CMOS programmable frequency divider with eight stages of new static D-flip-flop-based (2/1) divider cells is presented, where the supply voltage of 1.0 V is employed. The divider achieves a full modulus range from 1 to 256 and operates over a wide range maintaining up to 4 GHz with -30-dBm input power. The divider also accomplishes a power efficiency of 12.8 GHz/mW with 0.5-V supply voltage. It is favorable for advanced processes.",A 5.5-GHz 1-mW Full-Modulus-Range Programmable Frequency Divider in 90-nm CMOS Process
"['Q. Tieng', 'Wageeh W. Boles', 'Mohamed Deriche']","A technique for representing and recognising 3-D or space curves is presented. In the proposed algorithm, the space curves are represented by a set of two zero-crossing representations which are constructed based on the dyadic wavelet transform. These representations are then described in the form of an ordered set of complex numbers which is referred to as the compact representation of the space curves. A string-matching technique is adapted for comparing two curves using their compact representations. Experimental results show that the proposed technique can be used for recognising space curves under similarity transformation with and without additive noise.",Space curve recognition based on the wavelet transform and string-matching techniques
"['Kheng Lee Koay', 'Dag Sverre Syrdal', 'Michael L. Walters', 'Kerstin Dautenhahn']","This paper presents five exploratory trials investigating scenarios likely to occur when a personal robot shares a home with a person. The scenarios are: a human and robot working on a collaborative task, a human and robot sharing a physical space in a domestic setting, a robot recording and revealing personal information, a robot interrupting a human in order to serve them, and finally, a robot seeking assistance from a human through various combinations of physical and verbal cues. Findings indicate that participants attribute more blame and less credit to a robot than compared to themselves when working together on a collaborative task. Safety is a main concern when determining participants' comfort when sharing living space with their robot. Findings suggest that the robot should keep its interruption of the user's activities to a minimum. Participants were happy for the robot to store information which is essential for the robot to improve its functionality. However, their main concerns were related to the storing of sensitive information and security measures to safeguard such information.",Five Weeks in the Robot House Exploratory Human-Robot Interaction Trials in a Domestic Setting
"['Robert H. Klenke', 'Lori M. Kaufman', 'James H. Aylor', 'Ronald Waxman', 'Padmini Narayan']","Generation of test vectors for the VLSI devices used in contemporary digital system is becoming much more difficult as these devices increase in size. Automatic Test Pattern Generation (ATPG) techniques are commonly used to generate these tests. Since ATPG is an NP complete problem with complexity exponential to circuit size, the application of parallel processing techniques to accelerate the process of finding test patterns is an active area of research. This paper presents an approach to parallelization of the test generation problem that is targeted to a network-of-workstations environment. The system is based upon partitioning of the fault list across multiple processors and includes enhancements designed to address the main drawbacks of this technique, namely unequal load balancing and generation of redundant vectors. The technique is generalized enough that it can be applied to any test generation system regardless of the ATPG or fault simulation algorithm employed. Results were gathered to determine the impact of workstation processing load and network communications load on the performance of the system. >",Workstation based parallel test generation
"['Rhonda Anderson', 'P. Davis', 'Natalie Linnell', 'Christopher G. Prince', 'V. Razmo', 'Fred Videon']",Classroom Presenter is a Tablet PC-based interaction system that supports the sharing of digital ink on slides between instructors and students. Initial deployments show that using the technology can achieve a wide range of educational goals and foster a more participatory classroom environment.,Classroom Presenter: Enhancing Interactive Education with Digital Ink
"['Zhi-Hong Deng', 'Shiwei Tang', 'Dongqing Yang', 'Ming Zhang', 'Xiao-Bin Wu', 'Meng Yang']","Since 1990's, the exponential growth of theseWeb documents has led to a great deal of interestin developing efficient tools and software toassist users in finding relevant information. Textclassification has been proved to be useful inhelping organize and search text information onthe Web. Although there have been existed anumber of text classification algorithms, most ofthem are either inefficient or too complex. In thispaper we present two Odds-Radio-Based textclassification algorithms, which are called ORand TF*OR respectively. We have evaluated ouralgorithm on two text collections and compared itagainst k-NN and SVM. Experimental resultsshow that OR and TF*OR are competitive withk-NN and SVM. Furthermore, OR and TF*OR ismuch simpler and faster than them. The resultsalso indicate that it is not TF but relevancefactors derived from Odds Radio that play thedecisive role in document categorization.",Two odds-radio-based text classification algorithms
"['Duc A. Tran', 'Hl Raghavendra']","Mobility, channel error, and congestion are the main causes for packet loss in mobile ad hoc networks. Reducing packet loss typically involves congestion control operating on top of a mobility and failure adaptive routing protocol at the network layer. In the current designs, routing is not congestion-adaptive. Routing may let a congestion happen which is detected by congestion control, but dealing with congestion in this reactive manner results in longer delay and unnecessary packet loss and requires significant overhead if a new route is needed. This problem becomes more visible especially in large-scale transmission of heavy traffic such as multimedia data, where congestion is more probable and the negative impact of packet loss on the service quality is of more significance. We argue that routing should not only be aware of, but also be adaptive to, network congestion. Hence, we propose a routing protocol (CRP) with such properties. Our ns-2 simulation results confirm that CRP improves the packet loss rate and end-to-end delay while enjoying significantly smaller protocol overhead and higher energy efficiency as compared to AODV and DSR",Congestion Adaptive Routing in Mobile Ad Hoc Networks
"['Yasumasa Hayashi', 'Takashi Matsubara', 'Yoshiaki Koga']","Bus systems are used in computers as essential architecture, and dependability of bus systems should be accomplished reasonably for various applications. In this paper, we will present dependable bus operations with actual implementation and evaluation by CPLD. Most of the bus systems control transition of some classified phases with synchronous clock or guard time to avoid incorrect phase transition. However, these phase control methods may degrade system performance or cause incorrect operations. We design an asynchronous sequential circuit for bus phase control without clock or guard time. This circuit prevents incorrect phase transition at the time when large input delay or erroneous input occurs. We estimate probability of incorrect phase transition with single stuck-at fault on input signals. From the result of estimation, we also design checking system verifying outputs of initiator and target devices. Incorrect phase transition with single stuck-at fault occurred between both sequential circuits is inhibited completely by implementation of the system.",Implementation and evaluation for dependable bus control using CPLD
"['Ignasi Paredes-Oliva', 'Pere Barlet-Ros', 'Josep Sol??-Pareta']","Sampling techniques are often used for traffic monitoring in high-speed links in order to avoid saturation of network resources. Although there is a wide existing research dealing with anomaly detection, few studies analyzed the impact of sampling on the performance of portscan detection algorithms. In this paper, we performed several experiments on two already existing portscan detection mechanisms to test whether they are robust enough to different sampling techniques. Unlike previous works, we found that flow sampling is not always better than packet sampling to continue detecting portscans reliably.",Portscan Detection with Sampled NetFlow
"['Chunyuan Fu', 'Guohong Fu']","Named entity recognition plays an important role in many natural language processing applications. While considerable attention has been pain in the past to research issues related to named entity recognition, few studies have been reported on the recognition of nested named entities. This paper presents a morpheme-based method to Chinese nested named entity recognition. To approach this task, we first employ the logistic regression model to extract multi-level entity morphemes from an entity-tagged corpus, and thus explore a variety of lexical features under the framework of conditional random fields to perform Chinese nested named entity recognition. Our experimental results on different data set show that our system is effective for most nested named entities under evaluation.",Morpheme-based chinese nested named entity recognition
['Kashif Nisar'],"The Voice over Internet Protocol (VoIP) is a delay sensitive traffic due to real-time applications on networks. The assessment of voice flow quality in the VoIP is an essential requirement for technical and commercial motivation. The packets of VoIP streaming may experience drops because of the competition among the different kinds of traffic flow over the network. A VoIP application is also sensitive to delay and requires the voice packets to arrive on time from the sender to the receiver side without any delay over WLAN. The scheduling system model for VoIP traffic is an unresolved problem. In this research paper, the author proposes a new Voice Priority Queue (VPQ) scheduling system models and algorithms for the VoIP over WLANs to solve scheduling issues over IP-based networks. They present new contributions, through the three stages of the VPQ. The VPQ scheduling algorithm is provided as an essential technique in the VoIP communication networks to guarantee the QoS requirements. The design of the VPQ is managed by the limited bandwidth utilization and has been proven to have an efficient performance over WLANs.",Voice Priority Queue Scheduling System Models for VoIP over WLANs
"['John M. M. Anderson', 'Georgios B. Giannakis', 'Ananthram Swami']","Given a single record, the authors consider the problem of estimating the parameters of a harmonic signal buried in noise. The observed data are modeled as a sinusoidal signal plus additive Gaussian noise of unknown covariance. The authors define novel higher order statistics-referred to as ""mixed"" cumulants-that can be consistently estimated using a single record and are insensitive to colored Gaussian noise. Employing fourth-order mixed cumulants, they estimate the sinusoid parameters using a consistent, nonlinear matching approach. The algorithm requires an initial estimate that is obtained from a consistent, linear estimator. Finally, the authors examine the performance of the proposed method via simulations. >",Harmonic retrieval using higher order statistics: a deterministic formulation
"['Iliano Cervesato', 'Nancy A. Durgin', 'Patrick Lincoln', 'John C. Mitchell', 'Andre Scedrov']","Formal analysis of security protocols is largely based on a set of assumptions commonly referred to as the Dolev-Yao model. Two formalisms that state the basic assumptions of this model are related here: strand spaces and multiset rewriting with existential quantification. Strand spaces provide a simple and economical approach to analysis of completed protocol runs by emphasizing causal interactions among protocol participants. The multiset rewriting formalism provides a very precise way of specifying finite-length protocols with unboundedly many instances of each protocol role, such as client, server, initiator, or responder. A number of modifications to each system are required to produce a meaningful comparison. In particular, we extend the strand formalism with a way of incrementally growing bundles in order to emulate an execution of a protocol with parametric strands. The correspondence between the modified formalisms directly relates the intruder theory from the multiset rewriting formalism to the penetrator strands. The relationship we illustrate here between multiset rewriting specifications and strand spaces thus suggests refinements to both frameworks, and deepens our understanding of the Dolev-Yao model.",Harmonic retrieval using higher order statistics: a deterministic formulation
"['Hideyuki Shimonishi', 'Takashi Yoshikawa', 'Atsushi Iwata']","In this paper, we propose a high-speed and programmable traffic management mechanism to enable easy and timely innovations. A control framework introduced by 4D, Tesseract, or OpenFlow, separates control functions from the switch nodes to a control server so that a variety of network control policies can be implemented outside of the switches. Within this framework, we propose a mechanism to enable flexible flow-based traffic management so that a variety of innovative traffic management schemes can be realized. Per-flow traffic management, however, requires packet-by-packet state updates, which can spoil this control framework. The proposed mechanism consists of a control server that monitors traffic conditions using sampled packets sent from the switches and calculates per-flow packet discarding rate, and switches that discard incoming packets according to the discarding rate. Packet sampling and discarding do not require packet-by-packet state handling at the switches and thus allows controls from a control server. We also propose a mechanism to compress the discarding information using a time series of bloom filters, so that frequent control updates are allowed. We tested the mechanism with per-flow WFQ emulation and the simulation results showed very good per-flow fairness. Furthermore, we found that the flow table is compressed 600 times smaller and that the processing cost at the server and the switches is small enough for use with 10 Gbps links.",Harmonic retrieval using higher order statistics: a deterministic formulation
"['Yi Wang', 'Hongjie Bai', 'Matt Stanton', 'Wen-Yen Chen', 'Edward Yi Chang']","This paper presents PLDA, our parallel implementation of Latent Dirichlet Allocation on MPI and MapReduce. PLDA smooths out storage and computation bottlenecks and provides fault recovery for lengthy distributed computations. We show that PLDA can be applied to large, real-world applications and achieves good scalability. We have released  MPI-PLDA  to open source at http://code.google.com/p/plda under the Apache License.",PLDA: Parallel Latent Dirichlet Allocation for Large-Scale Applications
"['Branislav Kisacanin', 'Dan Schonfeld']","In this correspondence, we present a fast thresholded linear convolution representation of morphological operations. The thresholded linear convolution representation of dilation and erosion is first proposed. A comparison of the efficiency of the direct implementation of morphological operations and the thresholded linear convolution representation of morphological operations is subsequently conducted. Mathematical morphology has emerged as a powerful new tool for image processing. >",A fast thresholded linear convolution representation of morphological operations
"['Ratan Dey', 'Zubin Jelveh', 'Keith W. Ross']","We investigate whether Facebook users have become more private in recent years. Specifically, we examine if there have been any important trends in the information Facebook users reveal about themselves on their public profile pages since early 2010. To this end, we have crawled the public profile pages of 1.4 million New York City (NYC) Facebook users in March 2010 and again in June 2011. We have found that NYC users in our sample have become dramatically more private during this period. For example, in March 2010 only 17.2% of users in our sample hid their friend lists, whereas in June 2011, just 15 months later, 52.6% of the users hid their friend lists. We explore privacy trends for several personal attributes including friend list, networks, relationship, high school name and graduation year, gender, and hometown. We find that privacy trends have become more pronounced for certain demographics. Finally, we attempt to determine the primary causes behind the dramatic decrease in the amount of information Facebook users reveal about themselves to the general public.",Facebook users have become much more private: A large-scale study
"['A.-M. Poussard', 'Christian Olivier', 'Jianhua Wu', 'Christian Chatellier']","The techniques commonly used in image coding (JPEG, MPEG, ...) have as the main objective to compress as much as possible while retaining most of the information. These methods are often based on the use of the discrete cosine transform (DCT) and the wavelet transform (WT). Our purpose is to consider the necessary redundancy to achieve a good reception in the case of heavy interruptions of bits transmission. There is however a contradiction between an optimal compression and the redundancy required. It is thus necessary to master and compress the information to be transmitted as much as possible and to withstand the noise on the transmission channel. This article makes a contribution to this difficult problem: its originality resides in the coupling of orthogonal transforms and a redundant transform. Simulation results are provided using our method and the results are compared with that of DCT and WT based methods for Lena and Mountain images.",Improved compression by coupling of coding techniques and redundant transform
"['Tapani Raiko', 'Alexander Ilin', 'Juha Karhunen']","Principal component analysis (PCA) is a well-known classical data analysis technique. There are a number of algorithms for solving the problem, some scaling better than others to problems with high dimensionality. They also differ in their ability to handle missing values in the data. We study a case where the data are high-dimensional and a majority of the values are missing. In case of very sparse data, overfitting becomes a severe problem even in simple linear models such as PCA. We propose an algorithm based on speeding up a simple principal subspace rule, and extend it to use regularization and variational Bayesian (VB) learning. The experiments with Netflix data confirm that the proposed algorithm is much faster than any of the compared methods, and that VB-PCA method provides more accurate predictions for new data than traditional PCA or regularized PCA.",Principal Component Analysis for Large Scale Problems with Lots of Missing Values
"['Shashi Kiran Chilappagari', 'Dung Viet Nguyen', 'Bane Vasic', 'Michael W. Marcellin']",We investigate the relation between the girth and the guaranteed error correction capability of gamma-left regular LDPC codes when decoded using the bit flipping (serial and parallel) algorithms. A lower bound on the number of variable nodes which expand by a factor of at least 3gamma/4 is found based on the Moore bound. An upper bound on the guaranteed correction capability is established by studying the sizes of smallest possible trapping sets.,On the guaranteed error correction capability of LDPC codes
"['Leonardo Corbal?≠n', 'Germ?≠n Leandro Osella Massa', 'Claudia Russo', 'Laura Cristina Lanzarini', 'A. De Giusti']","This work defines a new nonlinear adaptive filter based on a feed-forward neural network with the capacity of significantly reducing the additive noise of an image. Even though measurements have been carried out using X-ray images with additive white Gaussian noise, it is possible to extend the results to other type of images. Comparisons have been carried out with the Weiner filter because it is the most effective option for reducing Gaussian noise. In most of the cases, image reconstruction using the proposed method has produced satisfactory results. Finally, some conclusions and future work lines are presented",Image recovery using a new nonlinear adaptive filter based on neural networks
"['Shan Lu', 'Gabriel Tsechpenakis', 'Dimitris N. Metaxas', 'Matthew L. Jensen', 'John Kruse']","Behavioral indicators of deception and behavioral state are extremely difficult for humans to analyze. Blob analysis, a method for analyzing the movement of the head and hands based on the identification of skin color is presented. This method is validated with numerous skin tones. A proof-of-concept study is presented that uses blob analysis to explore behavioral state identification in the detection of deception.",Blob Analysis of the Head and Hands: A Method for Deception Detection
"['Ezzaldeen Edwan', 'Stefan Knedlik', 'Otmar Loffeld']","In this paper, we present an extended Kalman filter (EKF)-based solution for the estimation of the angular motion using a gyro-free inertial measurement unit (GF-IMU) built of twelve separate mono-axial accelerometers. Using such a GF-IMU produces a vector, which we call the angular information vector (AIV) that consists of 3D angular acceleration terms and six quadratic terms of angular velocities. We consider the multiple distributed orthogonal triads of accelerometers that consist of three nonplanar distributed triads equally spaced from a central triad as a specific case to solve. During research for the possible filter schemes, we derived equality constraints. Hence we incorporate the constraints in the filter to improve the accuracy of the angular motion estimation, which in turn improves the attitude accuracy (direction cosine matrix (DCM) or quaternion vector).",Constrained Angular Motion Estimation in a Gyro-Free IMU
['Tschera Harkness Connell'],"This paper begins to identify and characterize the knowledge used by experienced librarians while searching for subject information in online catalogs. Ten experienced librarians performed the same set of six subject searches in an online catalog. Investigated was the knowledge used to solve retrieval problems. This knowledge represents expertise in the use of the catalog. Data were collected through the use of think-aloud protocols, transaction logs, and structured interviews. Knowledge was defined as knowledge of objects (factual knowledge), knowledge of events (experiential knowledge), knowledge of performance (process knowledge), and metaknowledge. Metaknowledge is the sense of whole derived from the integration of factual, process, and experiential knowledge about the search and the conditions under which it is performed. The focus of this paper is on metaknowledge. For evidence of metaknowledge the data were examined for explanations that participants gave for their actions and observations, and for ways that participants evaluated their own progress during the process of searching. Reasons and explanations given by searchers were related to all phases of the library information retrieval process from the user's receipt of material to policies for collection development, and not just events directly related to the performance of a particular search task.",Subject searching in online catalogs: metaknowledge used by experienced searchers
"['Lin Zhang', 'Aaron Carpenter', 'Berkehan Ciftcioglu', 'Alok Garg', 'Michael C. Huang', 'Hui Wu']","We propose injection-locked clocking (ILC) to combat deteriorating clock skew and jitter, and reduce power consumption in high-performance microprocessors. In the new clocking scheme, injection-locked oscillators are used as local clock receivers. Compared to conventional clocking with buffered trees or grids, ILC can achieve better power efficiency, lower jitter, and much simpler skew compensation thanks to its built-in deskewing capability. Unlike other alternatives, ILC is fully compatible with conventional clock distribution networks. In this paper, a quantitative study based on circuit and microarchitectural-level simulations is performed. Alpha21264 is used as the baseline processor, and is scaled to 0.13 m and 3 GHz. Simulations show 20- and 23-ps jitter reduction, 10.1% and 17% power savings in two ILC configurations. A test chip distributing 5-GHz clock is implemented in a standard 0.18- m CMOS technology and achieved excellent jitter performance and a deskew range up to 80 ps.",Injection-Locked Clocking: A Low-Power Clock Distribution Scheme for High-Performance Microprocessors
"['Kamalakar Karlapalem', 'Pisipati Radha Krishna']","Modeling and deployment of e-contracts is a challenging task because of the involvement of both technological and business aspects. There are several frameworks and systems available in the literature. Some works mainly deal with the automatic handling of paper contracts and others provide monitoring and enactment of contracts. Because contracts evolve, it is useful to have a system that models and enacts the evolution of e-contracts.",State of the Art in Modeling and Deployment of Electronic Contracts
"['Michael Hartung', 'Anika Gross', 'Erhard Rahm']","Summary: Life science ontologies substantially change over time to meet the requirements of their users and to include the newest domain knowledge. Thus, an important task is to know what has been modified between two versions of an ontology (diff ). This diff should contain all performed changes as compact and understandable as possible. We present CODEX (Complex Ontology Diff Explorer), a tool that allows determining semantic changes between two versions of an ontology which users can interactively analyze in multiple ways. Availability and Implementation: CODEX is available under http: //www.izbi.de/codex and is supported by all major browsers. It is implemented in Java based on Google Web Toolkit technology. Additionally, users can access a web service interface to use the diff functionality in their applications and analyses.",CODEX: Exploration of semantic changes between ontology versions
"['Mao Zheng', 'Olga Ormandjieva']","The increasing trend toward complex software systems has highlighted the need to incorporate quality requirements earlier in the development process. Reliability is one of the important quality indicators of such systems. This paper proposes a reliability analysis approach to measure reliability in the early development of real-time reactive systems (RTRS). The goal is to provide decision support and detect the first signs of low or decreasing reliability as the system design evolves. The analysis is conducted in a formal development environment for RTRS, formalized mathematically and illustrated using a train-gate-controller case study.",Reliability Analysis in the Early Development of Real-Time Reactive Systems
"['David Gareth Evans', 'Paul Blenkhorn']","A suite of tools is described that allow a word-processor operator to produce documents in large print, Braille and on cassette tape so that the documents can subsequently be read by visually impaired or blind users. The tools integrate with Microsoft Word, are written in Visual Basic for Applications and make use of the Word Object Model.",Tools for Creating Documents in 'Preferred Format' for Visually Impaired People
"['Hyeong-Ah Choi', 'Suresh Subramaniam', 'Hongsik Choi']","The stability of the current Internet architecture depends mostly on end-to-end TCP congestion control mechanisms. The Network Congestion Analyzer and Controller (NCAC) is an effort to build a complete user interface application to NS-2 that provides graphical access to most of NS2's functionalities. Besides, the application provides powerful visual tools for monitoring and displaying network performance metrics calculated by the simulation. The application is developed in Java to benefit from Java's portability and platform independence. There are three main parts in NCAC: (1) NS-2 interface, (2) network animation and alarm, and (3) graph plotting application. The functionality of each part is discussed.",NCAC: Network Congestion Analyzer and Controller
"['Ying Bai', 'Hanqi Zhuang']","This paper describes a novel technique for position error compensations of robots based on a fuzzy error interpolation method. A traditional robot calibration implements either model or modelless methods. The compensation of position error in a model-less method is to move the robot's end-effector to a target position in the robot workspace, and to find the target position error online based on the measured neighboring four-point errors around the target position. For this purpose, a stereo camera or other measurement device can be used to measure offline the position errors of the robot's end-effector at predefined grid points. By using the proposed fuzzy error interpolation technique, the accuracy of the position error compensation can be greatly improved, which is confirmed by the simulation results given in this paper. A comparison study among various interpolation methods, such as bilinear, cubic spline, and the fuzzy error interpolation technique is also made via simulation. The simulation results show that more accurate compensation results can be achieved using the fuzzy error interpolation technique compared with its bilinear and cubic spline counterparts.","On the comparison of bilinear, cubic spline, and fuzzy interpolation techniques for robotic position measurements"
"['Lijun Zhang', 'Chun Chen', 'Jiajun Bu', 'Zhengguang Chen', 'Deng Cai', 'Jiawei Han']","Different from traditional one-sided clustering techniques, coclustering makes use of the duality between samples and features to partition them simultaneously. Most of the existing co-clustering algorithms focus on modeling the relationship between samples and features, whereas the intersample and interfeature relationships are ignored. In this paper, we propose a novel coclustering algorithm named Locally Discriminative Coclustering (LDCC) to explore the relationship between samples and features as well as the intersample and interfeature relationships. Specifically, the sample-feature relationship is modeled by a bipartite graph between samples and features. And we apply local linear regression to discovering the intrinsic discriminative structures of both sample space and feature space. For each local patch in the sample and feature spaces, a local linear function is estimated to predict the labels of the points in this patch. The intersample and interfeature relationships are thus captured by minimizing the fitting errors of all the local linear functions. In this way, LDCC groups strongly associated samples and features together, while respecting the local structures of both sample and feature spaces. Our experimental results on several benchmark data sets have demonstrated the effectiveness of the proposed method.",Locally Discriminative Coclustering
"['Tao Meng', 'Mei Ling Shyu']","In current developmental research, one of the challenging tasks is to understand the spatio-temporal gene expression patterns and the relationships among different genes. In situ hybridization (ISH) assay which shows mRNA spatio-temporal expression patterns in cells and tissues directly is currently widely utilized in the bench work. With the increasing of available ISH images, automatic annotation systems are highly demanded. In this paper, an automatic classification system is proposed for annotating the in situ hybridization images with respect to the developmental stages. The embryo is first segmented from the original image, registered and normalized. The segmented embryo image is then divided into 100 blocks from which the pixel intensity and texture features are extracted and discretized. The multiple correspondence analysis (MCA) based association classification approach is proposed to generate classification rules for different stages based on the training data set. The testing instance is classified by applying the rules generated in the training process and a classification coordination module is incorporated to resolve the conflicts utilizing the weights derived from angle values in the MCA procedure. Experimental results show that our proposed method achieves promising results and outperforms other state-of-the-art algorithms.",Automatic annotation of drosophila developmental stages using association classification and information integration
"['Joanna Makowska', 'Mariusz Makowski', 'Adam Liwo', 'Lech Chmurzyè?ski']","The potentials of mean force (PMFs) were determined for systems forming cationic and anionic homocomplexes composed of acetic acid, phenol, isopropylamine, n-butylamine, imidazole, and 4(5)-methylimidazole, and their conjugated bases or acids, respectively, in three solvents with different polarity and hydrogen-bonding propensity: acetonitrile (AN), dimethyl sulfoxide (DMSO), and water (H 2 O). For each pair and each solvent a series of umbrella-sampling molecular dynamics simulations with the AMBER force field, explicit solvent, and counterions added to maintain a zero net charge of a system were carried out and the PMF was calculated by using the Weighted Histogram Analysis Method (WHAM). Subsequently, homoconjugation-equilibrium constants were calculated by numerical integration of the respective PMF profiles. In all cases but imidazole stable homocomplexes were found to form in solution, which was manifested as the presence of contact minima corresponding to hydrogen-bonded species in the PMF curves. The calculated homoconjugation constants were found to be greater for complexes with the OHO bridge (acetic acid and phenol) than with the NHN bridge and they were found to decrease with increasing polarity and hydrogen-bonding propensity of the solvent (i.e., in the series AN > DMSO > H 2 O), both facts being in agreement with the available experimental data. It was also found that interactions with counterions are manifested as the broadening of the contact minimum or appearance of additional minima in the PMF profiles of the acetic acid-acetate, phenol/ phenolate system in acetonitrile, and the 4(5)-methylimidazole/4(5)-methylimidzole cation conjugated base system in dimethyl sulfoxide.",Theoretical calculations of homoconjugation equilibrium constants in systems modeling acid-base interactions in side chains of biomolecules using the potential of mean force.
['Keith S. Reid-Green'],"Computer-aided manufacturing (CAM) can be made cost effective, even for one-of-a-kind jobs. To do so requires reevaluation of Computerized Numerical Control (CNC) from the point of view of a computer system, rather than a glorified machine shop, Good systematization, useful software and cooperative CNC shop staff can improve the productivity of a typical CNC shop by a factor of five.",Cost-Effective Computer-Aided Manufacturing of Prototype Parts
"['Jay J. Lee', 'Jahwan Kim', 'Jin H. Kim']","Although HMM is widely used for online handwriting recognition, there is no simple and well-established method of designing the HMM topology. We propose a data-driven systematic method to design HMM topology. Data samples in a single pattern class are structurally simplified into a sequence of straight-line segments. Then the resulting multiple models of the class are combined to form an architecture of a multiple parallel-path HMM, which behaves as single HMM. To avoid excessive growing of the number of the states, parameter trying is applied such that structural similarity among patterns is reflected. Experiments on online Hangul recognition showed about 19% of error reductions, compared to the intuitive deisgn method.",Data-driven design of HMM topology for online handwriting recognition
"['Qiang Tang 0002', 'Panos Nasiopoulos', 'Rabab K. Ward']","One objective in MPEG-2 to H.264 transcoding is to improve the H.264 compression ratio by using more accurate H.264 motion vectors. Motion re-estimation is by far the most time consuming process in video transcoding, and improving the searching speed is a challenging problem. We introduce a new transcoding scheme that uses the MPEG-2 DCT coefficients to predict the block size partitioning for H.264. Performance evaluations have shown that, for the same rate-distortion performance, our proposed scheme achieves an impressive reduction in the computational complexity of more than 82% compared to the full range motion estimation used by H.264.",Fast block size prediction for MPEG-2 to H.264/AVC transcoding
"['Jong Seung Park', 'Joon Hee Han']","We present a novel method of velocity field estimation for points on moving contours in an image sequence. The method determines the corresponding point in the next image frame by considering curvature changes at each point on a contour. In previous methods, there are errors in estimation for the points which have low curvature variations since those methods compute the solutions by approximating the normal component of optical flow. The proposed method computes optical flow vectors of contour points by minimizing the curvature changes. As a first step, snakes are used to locate smooth curves in 2D imagery. Then, the extracted curves are tracked continuously. We excluded the rearranging process in snakes and allowed the snaxel distance to vary. Each point on a contour has a unique corresponding point in the nest frame. Experimental results showed that the proposed method computes accurate optical flow vectors for various moving contours.",A curvature-based approach to contour motion estimation
"['Edward A. Lee', 'Slobodan Matic', 'Sanjit A. Seshia', 'Jia Zou']","This paper makes the case that the time is right to introduce temporal semantics into programming models for cyber-physical systems. Specifically, we argue for a programming model called PTIDES that provides a coordination language rooted in discreteevent semantics, supported by a lightweight runtime framework and tools for verifying concurrent software components. PTIDES leverages recent innovations in network time synchronization to deliver distributed real-time systems with determinate concurrent semantics, decentralized and robust control, and the potential for rigorous schedulability analysis.",The Case for Timing-Centric Distributed Software Invited Paper
"['Michael D. Powell', 'Ethan Schuchman', 'T. N. Vijaykumar']","Power density is a growing problem in high-performance processors in which small, high-activity resources overheat. Two categories of techniques, temporal and spatial, can address power density in a processor. Temporal solutions slow computation and heating either through frequency and voltage scaling or through stopping computation long enough to allow the processor to cool; both degrade performance. Spatial solutions reduce heat by moving computation from a hot resource to an alternate resource (e.g., a spare ALU) to allow cooling. Spatial solutions are appealing because they have negligible impact on performance, but they require availability of spatial slack in the form of spare or underutilized resource copies. Previous work focusing on spatial slack within a pipeline has proposed adding extra resource copies to the pipeline, which adds substantial complexity because the resources that overheat, issue logic, register files, and ALUs, are the resources in some of the tightest critical paths in the pipeline. Previous work has not considered exploiting the spatial slack already existing within pipeline resource copies. Utilization can be quite asymmetric across resource copies, leaving some copies substantially cooler than others. We observe that asymmetric utilization within copies of three key back-end resources, the issue queue, register files, and ALUs, creates spatial slack opportunities. By balancing asymmetry in their utilization, we can reduce power density. Scheduling policies for these resources were designed for maximum simplicity before power density was a concern; our challenge is to address asymmetric heating while keeping the pipeline simple. Balancing asymmetric utilization reduces the need for other performance-degrading temporal power-density techniques. While our techniques do not obviate temporal techniques in high-resource-utilization applications, we greatly reduce their use, improving overall performance.",Balancing Resource Utilization to Mitigate Power Density in Processor Pipelines
"['Takeshi Kumaki', 'Tetsushi Koide', 'Hans J?¨rgen Mattausch', 'Yasuto Kuroda', 'Hideyuki Noda', 'Katsumi Dosaka', 'Kazutami Arimoto', 'Kazunori Saito']","This paper reports an efficient discrete cosine transform (DCT) processing for the JPEG algorithm using a massive-parallel memory-embedded SIMD matrix processor. The matrix-processing engine has 2,048 2-bit processing elements, which are connected by a flexible switching network, and supports 2-bit 2,048-way bit-serial and word-parallel operations with a single command. For compatibility with this matrix-processing architecture, the conventional DCT algorithm has been improved in arithmetic order and the vertical/horizontal-space 1 dimensional (1D)-DCT processing has been further developed. Evaluation results of the matrix-engine-based DCT processing show that the necessary clock cycles per image blocks can be reduced by 87% in comparison to a conventional DSP architecture. The determined performances in MOPS and MOPS/mm are factors 8 and 5.6 better than with a conventional DSP, respectively. Moreover, the matrix-processing engine can reduce the number of total clock cycles for JPEG application about 49% in comparison to a conventional DSP architecture.",Efficient Vertical/Horizontal-Space 1D-DCT Processing Based on Massive-Parallel Matrix-Processing Engine
"['Yasser Ganji Saffar', 'Kyumars Sheykh Esmaili', 'Mohammad Ghodsi', 'Hassan Abolhassani']","Modern search engines use link structure of the World Wide Web in order to gain better results for ranking the results of users' queries. One of the most popular ranking algorithms which is based on link analysis is HITS. It generates very accurate outputs but because of huge amount of online computations, this algorithm is relatively slow. In this paper we introduce PHITS, a parallelized version of the HITS algorithm that is suitable for working with huge web graphs in a reason- able time. For implementing this algorithm, we use WebGraph framework and we focus on parallelizing access to web graph as the main bottleneck in the HITS algorithm. I. INTRODUCTION Search technology is one of the most important reasons for success of the web. The huge amount of information available on the web, its high growth rate, and its unstructured nature, all increase the need for search engines with high performance and accurate results. One of the major components of each search engine is its ranking algorithm. Traditional Information Retrieval (IR) systems usually use some models like VMS (4) and compute rank of results using content similarity measures between user's query and retrieved documents. But in the context of the web, there are some problems with these approaches. For example, spamming may lead to inefficient ranking. Some methods have been proposed to encounter these problems most of which uses some implicit information which is embedded in the web graph. These methods are known as Link-Analysis based algorithms. PageRank (5) and HITS (Hyperlink Induced Topic Search) (1) are the most well known algorithms in this category. PageRank, which is used by Google for ranking its results, is an offline and query-independent ranking algorithm. This means that the ranking is independent of the specific queries of users and therefore can be done once and used for all of the upcoming queries. On the other hand, HITS is an online and query-dependent algorithm. Being query dependent makes HITS more precise but it has some disadvantages too. In fact, required online computations for this algorithm is too much and the response time of the search engine after submitting queries by users is not acceptable. To overcome this problem, in this paper we will exploit the parallel processing methods to improve the execution performance of the algorithm. The rest of this paper is organized as follows. In section II, link-analysis based algorithms in general and HITS as a special case are discussed. At the end of this section, some of the variations and improvements for the HITS algorithm that are suggested in the literature are also described. Implementing the HITS algorithm and its parallel version, PHITS, are discussed in sections III and IV respectively. Finally, last section of this paper contains conclusion and some ideas for future work in this topic.",Parallel Online Ranking of Web Pages
"['Butler W. Lampson', 'Mart??n Abadi', 'Michael Burrows', 'Edward P. Wobber']","We describe a theory of authentication and a system that implements it. Our theory is based on the notion of principal and a ""speaks for"" relation between principals. A simple principal either has a name or is a communication channel; a compound principal can express an adopted role or delegation of authority. The theory explains how to reason about a principal's authority by deducing the other principals that it can speak for; authenticating a channel is one important application. We use the theory to explain many existing and proposed mechanisms for security. In particular, we describe the system we have built. It passes principals efficiently as arguments or results of remote procedure calls, and it handles public and shared key encryption, name lookup in a large name space, groups of principals, loading programs, delegation, access control, and revocation.",Authentication in distributed systems: theory and practice
"['Dong Xu', 'Shuicheng Yan', 'Dacheng Tao', 'Lei Zhang', 'Xuelong Li', 'Hong-Jiang Zhang']","Human gait is an important biometric feature. It can be perceived from a great distance and has recently attracted greater attention in video-surveillance-related applications, such as closed-circuit television. We explore gait recognition based on a matrix representation in this paper. First, binary silhouettes over one gait cycle are averaged. As a result, each gait video sequence, containing a number of gait cycles, is represented by a series of gray-level averaged images. Then, a matrix-based unsupervised algorithm, namely coupled subspace analysis (CSA), is employed as a preprocessing step to remove noise and retain the most representative information. Finally, a supervised algorithm, namely discriminant analysis with tensor representation, is applied to further improve classification ability. This matrix-based scheme demonstrates a much better gait recognition performance than state-of-the-art algorithms on the standard USF HumanID Gait database",Human Gait Recognition With Matrix Representation
"['Y. Yoshikaw', 'Satoshi Ohtake', 'Michiko Inoue', 'Hideo Fujiwara']","This paper introduces a new concept of hierarchical testability called Single-Port-Change (SPC) two-pattern testability. We propose a non-scan design-for-testability (DFT) method which makes each path that needs to be tested in a data path SPC two-pattern testable. An SPC two-pattern test guarantees robust (resp. non-robust) test if the path is robust (resp. non-robust) testable. Since it is easy to find justification paths for SPC two-pattern tests at register-transfer level, the proposed DFT method can reduce hardware overhead compared to that of our previous DFT method for arbitrary two-pattern tests. Furthermore, we propose a method to reduce test generation effort by removing a subset of sequentially untestable paths from targets of test generation. Experimental results show that the proposed method can reduce hardware overhead without losing the quality of test.",Design for Testability Based on Single-Port-Change Delay Testing for Data Paths
"['Alaa Salman', 'Imad H. Elhajj', 'Ali Chehab', 'Ayman I. Kayssi']","The popularity of mobile devices and the enormous number of third party mobile applications in the market have naturally lead to several vulnerabilities being identified and abused. This is coupled with the immaturity of intrusion detection system (IDS) technology targeting mobile devices. In this paper we propose a modular host-based IDS framework for mobile devices that uses behavior analysis to profile applications on the Android platform. Anomaly detection can then be used to categorize malicious behavior and alert users. The proposed system accommodates different detection algorithms, and is being tested at a major telecom operator in North America. This paper highlights the architecture, findings, and lessons learned.",DAIDS: An Architecture for Modular Mobile IDS
"['C. K. Chua', 'Roger H. L. Chiang', 'Ee Peng Lim']","Many data analysts require tools which can integrate their database management packages (e.g. Microsoft Access) with their data analysis ones (e.g. SAS, SPSS), and provide guidance for the selection of appropriate mining algorithms. In addition, the analysts need to extract and validate statistical results to facilitate data mining. In this paper, we describe an integrated data mining system called the Linear Correlation Discovery System (LCDS) that meets the above requirement. LCDS consists of four major sub-components, two of which, the selection assistant and the statistics coupler, we discussed in this paper. The former examines the scheme and instances to determine appropriate association measurement functions (e.g, chi-square, linear regression, ANOVA). The latter involves the appropriate statistical function on a sample data set, and extracts relevant statistical output such as /spl eta//sup 2/, and R/sup 2/ for effective mining of data. We also describe a new validation algorithm based on measuring the consistency of mining results applied to multiple test sets.",An integrated data mining system to automate discovery of measures of association
"['Roland Thieffry', 'Eric Monacelli', 'Patrick H??naff', 'S. Delaplace']","This article presents a study about a mobility aid system, which integrates adaptive control interface for a robotic walker. The objective is to give to the patient more flexibility in the choice of a technical aid. The specificity of our system is based on an auto-adaptive interface, which improves the configuration choice of the patient's driving commands. The results obtained from experimentations show the capabilities of our method for a technical aid. The learning process is applied on line to a neural network controller. The experiment is focused on sensors configuration and command of a powered walker interface.",Configuring sensors by user learning for a locomotion aid interface
"['David Declercq', 'Patrick Duvaut']","Nous presentons dans cet article une etude complete du test de normalite d'Hermite. Ce test utilise les proprietes des polynomes d'Hermite et une statistique de sphericite modifiee pour decider si un echantillon monodimensionnel, standardise et blanc est gaussien ou non. L'avantage majeur de cette approche est de definir une famille de statistiques qui va permettre d'adapter le choix d'un test particulier aux donnees. Nous avons etabli la distribution asymptotique du test d'Hermite sous l'hypothese nulle et sous l'hypothese alternative et etudie en details le cas particulier de tests a deux polynomes. Nous avons determine les tests asymptotiquement les plus puissants pour quelques distributions alternatives et effectue un grand nombre de simulations afin de comparer le test d'Hermite a trois autres tests. Les bons resultats obtenus nous encouragent a generaliser le test d'Hermite aux cas de donnees colorees et multivariees.",Hermite normality tests
"['Enrique Arriaga-De-Valle', 'Graciano Dieck-Assad']","In this paper we compare and discuss the performance of a boiler evaporator system when the system is controlled by a traditional PID-type strategy and when the system is enhanced by using fuzzy logic blocks to provide set-points for the system. The strategy used in fuzzy logic controllers (FLCs) is called fuzzy supervisory control and it generates set-points for the conventional controllers. The boiler under test is a VU-60 industrial system that produces 180,000 pounds of steam per hour. The mathematical model of the plant is a scaled version model of that obtained for a thermoelectric unit. The new model simplifies the large-scale thermoelectric boiler model to an industrial small-scale type VU-60 boiler model based upon first principle mass and energy balance equations. The main change consists of representing only the behavior of the drumÉ??evaporator system, having a partial model of the combustion process, with a simplified combustion control system and a three-element boiler feed-water controller. The control system for combustion and boiler feed-water receives a supervisory signal (or set-point tracking signal) that comes from the FLC to improve the performance of the overall control system. The behavior of the supervisory controller brings some advantages to the system performance, compared with the traditional control schemes. The comparison reflects fuel improvements from 2.5% to 6.5% depending upon the steam load ramp regime. The simulations are performed using the SIMULINK?? shell running under the MATLAB ?? platform.",Modeling and Simulation of a Fuzzy Supervisory Controller for an Industrial Boiler
"['Avijit Dutta', 'David Z. Pan']","In-place flipping of rectangular blocks/cells can potentially reduce the wirelength of a floorplan/placement solution without changing the chip area, In a recent work [Hao 05], the flipping optimization is solved through a binary decision diagram (BDD) based approach. However, the BDD-based approach is not scalable for large SOC designs with many blocks due to memory and runtime blow-up. This paper presents a new approach using the partitioned ordered partial decision diagrams (POPDD) for wirelength minimization. POPDD is based on a novel compact partial functional representation between flip configurations and corresponding wirelengths. By controlling the number of nodes allowed per POPDD and the iterations, easy trade-off between runtime/memory and accuracy/optimality can be achieved. Experimental results clearly demonstrate the efficiency of the proposed approach.",Partial Functional Manipulation Based Wirelength Minimization
"['Jordi Inglada', 'Marcela Arias', 'Benjamin Tardy', 'Olivier Hagolle', 'Silvia Valero', 'David Morin', 'G. Dedieu', 'Guadalupe Sepulcre', 'Sophie Bontemps', 'Pierre Defourny', 'Benjamin Koetz']","Crop area extent estimates and crop type maps provide crucial information for agricultural monitoring and management. Remote sensing imagery in general and, more specifically, high temporal and high spatial resolution data as the ones which will be available with upcoming systems, such as Sentinel-2, constitute a major asset for this kind of application. The goal of this paper is to assess to what extent state-of-the-art supervised classification methods can be applied to high resolution multi-temporal optical imagery to produce accurate crop type maps at the global scale. Five concurrent strategies for automatic crop type map production have been selected and benchmarked using SPOT4 (Take5) and Landsat 8 data over 12 test sites spread all over the globe (four in Europe, four in Africa, two in America and two in Asia). This variety of tests sites allows one to draw conclusions applicable to a wide variety of landscapes and crop systems. The results show that a random forest classifier operating on linearly temporally gap-filled images can achieve overall accuracies above 80% for most sites. Only two sites showed low performances: Madagascar due to the presence of fields smaller than the pixel size and Burkina Faso due to a mix of trees and crops in the fields. The approach is based on supervised machine learning techniques, which need in situ data collection for the training step, but the map production is fully automatic.",Assessment of an Operational System for Crop Type Map Production Using High Temporal and Spatial Resolution Satellite Optical Imagery
"['Fatih Porikli', 'Huseyin Ozkan']","Nonlinear kernel Support Vector Machines achieve better generalizations, yet their training and evaluation speeds are prohibitively slow for real-time object detection tasks where the number of data points in training and the number of hypotheses to be tested in evaluation are in the order of millions. To accelerate the training and particularly testing of such nonlinear kernel machines, we map the input data onto a low-dimensional spectral (Fourier) feature space using a cosine transform, design a kernel that approximates the classification objective in a supervised setting, and apply a fast linear classifier instead of the conventional radial basis functions. We present a data driven hypotheses generation technique and a LogistBoost feature selection. Our experimental results demonstrate the computational improvements 20É?¨100?? while maintaining a high classification accuracy in comparison to SVM linear and radial kernel basis function classifiers.",Data driven frequency mapping for computationally scalable object detection
"['Elena Ranguelova', 'Mark J. Huiskes', 'Eric J. Pauwels']",This paper describes current work on a photo-id system for humpback whales. Individuals of this species can be uniquely identified by the light and dark pigmentation patches on their tails (flukes). We developed an interface that assists the user in segmenting the animal's tail from the sea and fitting an affine invariant coordinate grid to it. A numerical feature vector capturing the patch-distribution with respect to the grid is then automatically extracted and used to match the individual against a database of similarly processed images.,Towards computer-assisted photo-identification of humpback whales
"['Ming Jin', 'Behrouz Farhang-Boroujeny', 'George Mathew', 'Kalahasthi C. Indukumar']","The study of error-burst statistics is important for all detection systems, and more so for the decision feedback class. In data storage applications, many detection systems use decision feedback in one form or another. Fixed-delay tree search with decision feedback (FDTS/DF) and decision feedback equalization (DFE) are the direct forms, whereas the partial response detectors such as the reduced state sequence estimator (RSSE) and noise predictive maximum likelihood (NPML) detectors are the other forms. Although DF reduces the system complexity, it is inevitably linked with error propagation (EP), which can be quantified using error-burst statistics. Analytical evaluation of these statistics is difficult, if not impossible, because of the complexity of the problem. Hence, the usual practice is to use computer simulations. However, the computational time in traditional bit-by-bit simulations can be prohibitive at meaningful signal-to-noise ratios. In this paper, we propose a novel approach for fast estimation of error-burst statistics in FDTS/DF detectors, which is also applicable to other detection systems. In this approach, error events are initiated more frequently than natural by artificially injecting noise samples. These noise samples are generated using a transformation that results in significant reduction in computational complexity. Simulation studies show that the EP performance obtained by the proposed method matches closely with those obtained by bit-by-bit simulations, while saving as much as 99% of simulation time.",A novel fast approach for estimating error propagation in decision feedback detectors
"['M. Levent Koc', 'Christopher R??']","The proliferation of imprecise data has motivated both researchers and the database industry to push statistical techniques into relational database management systems (RDBMSes). We study strategies to maintain model-based views for a popular statistical technique, classification, inside an RDBMS in the presence of updates (to the set of training examples). We make three technical contributions: (1) A strategy that incrementally maintains classification inside an RDBMS. (2) An analysis of the above algorithm that shows that our algorithm is optimal among all deterministic algorithms (and asymptotically within a factor of 2 of a non-deterministic optimal strategy). (3) A novel hybrid-architecture based on the technical ideas that underlie the above algorithm which allows us to store only a fraction of the entities in memory. We apply our techniques to text processing, and we demonstrate that our algorithms provide an order of magnitude improvement over non-incremental approaches to classification on a variety of data sets, such as the Citeseer and DBLife.",Incrementally maintaining classification using an RDBMS
"['Sivasathivel Kandasamy', 'Audrey Minghelli-Roman', 'Francois Tavin', 'Sandrine Mathieu', 'Fr??d??ric Baret', 'Pierre Gouton']","Hyperspectral imaging systems could be used for identifying the different soil types from the satellites. However, detecting the reflectance of the soils in all the wavelengths involves the use of a large number of sensors with high accuracy and also creates a problem in transmitting the data to earth stations for processing. The current sensors can reach a bandwidth of 20 nm and hence, the reflectance obtained using the sensors are the integration of reflectance obtained in each of the wavelength present in the spectral band. Moreover, not all spectral bands contribute equally to classification and hence, identifying the bands necessary to have a good classification is necessary to reduce sensor cost and problem in data transmission from the satellite. The work presents the spectral bands selected using a PCA-Based Forward Sequential band selection algorithm.",Optimal band selection for future satellite sensor dedicated to soil science
"['Zhengcai Lu', 'Zheng Qin', 'Qiao Jing', 'L. Y. Shan']","Attribute reduction is one of the challenging problems facing the effective application of computational intelligence technology for artificial intelligence. Its task is to eliminate dispensable attributes and search for a feature subset that possesses the same classification capacity as that of the original attribute set. To accomplish efficient attribute reduction, many heuristic search algorithms have been developed. Most of them are based on the model that the approximation of all the target concepts associated with a decision system is dividable into that of a single target concept represented by a pair of definable concepts known as lower and upper approximations. This paper proposes a novel model called macroscopic approximation, considering all the target concepts as an indivisible whole to be approximated by rough set boundary region derived from inconsistent tolerance blocks, as well as an efficient approximation framework called positive macroscopic approximation (PMA), addressing macroscopic approximations with respect to a series of attribute subsets. Based on PMA, a fast heuristic search algorithm for attribute reduction in incomplete decision systems is designed and achieves obviously better computational efficiency than other available algorithms, which is also demonstrated by the experimental results.",Positive Macroscopic Approximation for Fast Attribute Reduction
"['Scott E. Fricke', 'A. J. Shenbar']","Business trends require front-line managers to integrate multiproject concepts with those of traditional single-project management since very rarely can one find major organizations managing just one project. A typical situation entails a limited pool of resources which is applied to the management of several projects, with people moving back and forth among different assignments in different projects. Yet, few studies on project management have started to explore the issue of how to manage an organization with multiple inter- or intradepartmental projects. Using a case study method, our exploratory research investigates the specific problems associated with the management of multiple engineering projects in a manufacturing support environment, with the intent to identify common factors of success. Knowing the factors of success is but the first step toward improving multi-project management. Our findings provide insight into how the most important multiple-project success factors in this environment differ from factors of success in traditional single-project management, and are consistent with other emerging research in product development environments. The differences center on resource allocation and flexibility. Some factors, such as ownership, staff experience, and communication, take on additional dimensions when considered in a multiple-versus a single-project environment. Division and assignment of resources, prioritization, and customized management style, which have little relevance in relation to single projects, are shown to play a major role in the success of multiproject management.",Managing multiple engineering projects in a manufacturing support environment
"['Joao Gomes', 'Victor Barroso']","The super-exponential algorithm is a block-based technique for blind channel equalization and system identification. Due to its fast convergence rate, and no a priori parameterization other than the block length, it is a useful tool for linear equalization of moderately distortive channels. This paper presents a recursive implementation of the super-exponential algorithm for fractionally-sampled PAM signals. Although the resulting algorithm is still block-based, recursive propagation of several key variables allows the block length to be significantly reduced without compromising the algorithm's accuracy or speed, thereby enhancing its ability to track channel variations. The convergence rate is only mildly influenced by specific channel responses, and oversampling provides smaller output variance and almost perfect tolerance to sampling errors. Simulation results demonstrate the effectiveness of the proposed technique.",Performance analysis of a recursive fractional super-exponential algorithm
"['Kengo Ando', 'Norishige Fukushima', 'Tomohiro Yendo', 'Mehrdad Panahpour Tehrani', 'Toshiaki Fujii', 'Masayuki Tanimoto']","The availability of multi-view images of a scene makes new and exciting applications possible, including Free-Viewpoint TV (FTV). FTV allows us to change viewpoint freely in a 3D world, where the virtual viewpoint images are synthesized by Image-Based Rendering (IBR). In this paper, we introduce a FTV depth estimation method for forward virtual viewpoints. Moreover, we introduce a view generation method by using a zoom camera in our camera setup to improve virtual viewpoint-ts' image quality. Simulation results confirm reduced error during depth estimation using our proposed method in comparison with conventional stereo matching scheme. We have demonstrated the improvement in image resolution of virtually moved forward camera using a zoom camera setup.",Free-viewpoint image generation using different focal length camera array
"['Nicos Herodotou', 'Kostas N. Plataniotis', 'Anastasios N. Venetsanopoulos']","A novel technique is introduced to locate and track the facial area in videophone-type sequences. The proposed method essentially consists of two components: (i) a color processing unit, and (ii) a knowledge-based shape and color analysis module. The color processing component utilizes the distribution of skin-tones in the HSV color space to obtain an initial set of candidate regions or objects. The second component in the segmentation scheme, that is, the shape and color analysis module is used to correctly identify and select the facial region in the case where more than one object has been extracted. A number of fuzzy membership functions are devised to provide information about each objectÉ??s shape, orientation, location and average hue. An aggregation operator finally combines these measures and correctly selects the facial area. The suggested approach is robust with regard to diÉÅ?erent skin types, and various types of object or background motion within the scene. Furthermore, the algorithm can be implemented at a low computational complexity due to the binary nature of the operations involved. Experimental results are presented for a series of CIF and QCIF video sequences. ( 1999 Elsevier Science B.V. All rights reserved.",Automatic location and tracking of the facial region in color video sequences
"['Pavel Pacl??k', 'Thomas C. W. Landgrebe', 'David M. J. Tax', 'Robert P. W. Duin']","Unlike fixed combining rules, the trainable combiner is applicable to ensembles of diverse base classifier architectures with incomparable outputs. The trainable combiner, however, requires the additional step of deriving a second-stage training dataset from the base classifier outputs. Although several strategies have been devised, it is thus far unclear which is superior for a given situation. In this paper we investigate three principal training techniques, namely the re-use of the training dataset for both stages, an independent validation set, and the stacked generalization. On experiments with several datasets we have observed that the stacked generalization outperforms the other techniques in most situations, with the exception of very small sample sizes, in which the re-using strategy behaves better. We illustrate that the stacked generalization introduces additional noise to the second-stage training dataset, and should therefore be bundled with simple combiners that are insensitive to the noise. We propose an extension of the stacked generalization approach which significantly improves the combiner robustness.",On deriving the second-stage training set for trainable combiners
"['Sandrine Beauche', 'Pascal Poizat']","Service-Oriented Computing is a cornerstone for the realization of user needs through the automatic composition of services from service descriptions and user tasks, i.e., high-level descriptions of the user needs. Yet, automatic service composition processes commonly assume that service descriptions and user tasks share the same abstraction level, and that services have been pre-designed to integrate. To release these strong assumptions and to augment the possibilities of composition, we add adaptation features into the service composition process using semantic descriptions and adaptive extensions to graph planning.",Automated Service Composition with Adaptive Planning
"['Mohit Agarwal', 'Anuj Puri']",We consider a packet switched wireless network in which a base station serves the mobiles. The packets for the mobiles arrive at the base station and have to be scheduled for transmission to the mobiles. The capacity of the channel from the base station to the mobiles is varying with time due to fading. We assume the mobiles can obtain different types of service based on the prices they are willing to pay. The objective of the base station is to schedule packets for transmission to mobiles to maximize the revenue earned. Our main result is that a simple greedy algorithm does at least 1/2 as good as the optimal offline algorithm that knows the complete future request pattern and channel conditions. We also show that no other online algorithm can do better.,Base station scheduling of requests with fixed deadlines
"['Sachin S. Kajarekar', 'Nicolas Scheffer', 'Martin Graciarena', 'Elizabeth Shriberg', 'Andreas Stolcke', 'Luciana Ferrer', 'Tobias Bocklet']","The SRI speaker recognition system for the 2010 NIST speaker recognition evaluation (SRE) incorporates multiple subsystems with a variety of features and modeling techniques. We describe our strategy for this year's evaluation, from the use of speech recognition and speech segmentation to the individual system descriptions as well as the final combination. Our results show that under most conditions, the cepstral systems tend to perform the best, but that other, non-cepstral systems have the most complementarity. The combination of several subsystems with the use of adequate side information gives a 35% improvement on the standard telephone condition. We also show that a constrained cepstral system based on nasal syllables tends to be more robust to vocal effort variabilities.",THE SRI NIST 2008 speaker recognition evaluation system
"['Jennifer Curtis', 'Loes Ruijs', 'Maartje de Vries', 'Robert Winters', 'Jean-Bernard Martens']","This paper describes an interactive application that aims to support the rehabilitation of handwriting skills in people that suffer from paralysis after a stroke. The purpose of the application is to make the rehabilitation of handwriting skills fun and engaging. Four platform-independent games with adjustable levels of difficulty were created in order to target varying levels of skills. The application also features a performance history, audio-visual feedback, and posture reminders. It was evaluated with medical staff and patients from the Hoensbroeck Rehabilitation Centre in the Netherlands. The initial results indicated that the games are more motivating and fun than traditional pen and paper exercises. The feedback received from therapists supports our claim that the games are a useful addition to the rehabilitation of handwriting.",Rehabilitation of handwriting skills in stroke patients using interactive games: a pilot study
"['Lijun Xu', 'Haili Zhou', 'Zhang Cao', 'Wuqiang Yang']","In this paper, a digital switching demodulator is presented for use in ac-based electrical capacitance tomography systems. Implementing a switching phase-sensitive demodulator (PSD) digitally offers the following advantages: 1) Demodulation can be implemented using a programmable digital device, and hence, CMOS switches, which are used in a conventional switching PSD, are no longer needed; 2) compared with the widely used digital quadrature PSD, this proposed demodulator is simple in configuration because neither a reference signal nor multiplication is required; 3) according to the specific requirements, the new demodulator can be implemented in two operation modes, i.e., the amplitude mode and the phase-sensitive mode; and 4) because only subtractions and accumulations are needed, the proposed demodulator can be easily implemented with low-cost logic devices, e.g., a complex programmable logic device (CPLD). By simulation, the feasibility and effectiveness of the proposed demodulator have been confirmed. CPLD-based and field-programmable-gate-array-based capacitance measurement circuits are constructed, and the performances of different demodulation methods are compared. Both simulation and experiment show that the proposed demodulator can provide demodulation results with high signal-to-noise ratio. The system design can be simplified using the digital switching demodulator.",A Digital Switching Demodulator for Electrical Capacitance Tomography
"['Bing Wu', 'Roberta L. Klatzky', 'Damion Shelton', 'George D. Stetten']","We present a novel psychophysical method for evaluating ultrasonography based on real-time tomographic reflection (RTTR), in comparison to conventional ultrasound (CUS). The method measures the user's perception of the location of an ultrasound-imaged target independently from assessing the action employed to reach it. Three experiments were conducted with the sonic flashlight (SF), an RTTR device, and CUS. The first two experiments determined subjects' perception of target location with a triangulation-by-pointing task. Depth perception with the SF was comparable to direct vision, while CUS caused considerable underestimation of target depth. Binocular depth information in the SF was shown to significantly contribute to its superiority. The third experiment tested subjects in an ultrasound-guided needle insertion task. Because the SF provides visualization of the target at its actual location, subjects performed insertions faster and more accurately by using the SF rather than CUS. Furthermore, the trajectory analysis showed that insertions with the SF generally went directly to the target along the desired path, while CUS often led to a large deviation from the correct path consistent with the observed underestimation of target depth. These findings lend great promise to the use of RTTR-based imaging in clinical practice and provide precise means of assessing efficacy.",Psychophysical evaluation of in-situ ultrasound visualization
"['John K. Heath', 'Marta Z. Kwiatkowska', 'Gethin Norman', 'David Parker', 'Oksana Tymchyshyn']","Probabilistic model checking is a formal verification technique that has been successfully applied to the analysis of systems from a broad range of domains, including security and communication protocols, distributed algorithms and power management. In this paper we illustrate its applicability to a complex biological system: the FGF (Fibroblast Growth Factor) signalling pathway. We give a detailed description of how this case study can be modelled in the probabilistic model checker PRISM, discussing some of the issues that arise in doing so, and show how we can thus examine a rich selection of quantitative properties of this model. We present experimental results for the case study under several different scenarios and provide a detailed analysis, illustrating how this approach can be used to yield a better understanding of the dynamics of the pathway. Finally, we outline a number of exact and approximate techniques to enable the verification of larger and more complex pathways and apply several of them to the FGF case study.",Probabilistic model checking of complex biological pathways
"['Federica Garin', 'Enrico Lovisari', 'Sandro Zampieri']","We study the well known linear consensus algorithm by means of a LQ-type performance cost. We want to understand how the communication topology influences this algorithm. In order to do this, we recall the analogy between Markov Chains and electrical resistive networks. By exploiting this analogy, we are able to rewrite the performance cost as the average effective resistance on a suitable network. We use this result to show that if the communication graph fulfills some local properties, then its behavior can be approximated with that of a suitable grid, over which the behavior of the cost is known.",A resistance-based approach to consensus algorithm performance analysis
"['Yuri Alvarez', 'Miguel Delgado Rodr??guez', 'Fernando Las-Heras', 'Marta M. Hernando']","Electromagnetic interference (EMI) regulations are a very important issue in the design of almost any electronic circuit. Over the years, É??cut-and-tryÉ?ù procedures have been adopted by electronic designers to make circuits comply with these regulations, mainly due to the lack of reliable theoretical models of radiated noise and clear design rules. To gain new insight into this field, a novel approach is presented in this paper based on a well-known technique in the field of antenna design, i.e., the source reconstruction method (SRM). Its application allows a set of equivalent currents to be obtained that behave exactly like the circuit under consideration with regard to radiated noise. From these currents, magnetic and electric radiated fields can be obtained at any point in space, even at 3 or 10 m away from the circuit where the regulations must be met. Moreover, the equivalent currents accurately represent noise sources in the circuit, thus permitting the elements responsible for generating radiated noise to be located. The aforementioned method would enable designers to reduce the use of anechoic-chamber facilities when testing their designs, thereby leaving the chamber only for final certification purposes.",On the Use of the Source Reconstruction Method for Estimating Radiated EMI in Electronic Circuits
"['A. Jardine', 'Stephen McLaughlin', 'John S. Thompson']","This paper considers a fading relay channel where the total transmit power used is constrained to be equal to that of the standard single-hop channel. The relay channel used operates in what is termed as MIMO cooperative diversity mode, where the source transmits to both relay and destination terminals in the first instance. Both the source and relay then transmit to the destination in the second instance. Initially the cooperative diversity framework is introduced to consider system constraints so a direct and fair comparison with the single-hop case can be made. In-particular a power constraint is placed on the system and the optimal transmit power levels are derived and presented. The derived technique for finding the optimal transmit power levels is then used to demonstrate the advantages of using cooperative diversity in a wireless network. The results presented show that MIMO cooperative diversity offers a 3.4 dB increase in spectral efficiency at 5 % outage, with no additional cost incurred in transmit time, power or bandwidth.",MIMO Cooperative Diversity in a Transmit Power Limited Environment
"['Engin Topan', 'Z. Pelin Bayindir', 'Tarkan Tan']","We consider a multi-item two-echelon inventory system in which the central warehouse operates under a (Q,R) policy, and the local warehouses implement basestock policy. An exact solution procedure is proposed to find the inventory control policy parameters that minimize the system-wide inventory holding and fixed ordering cost subject to an aggregate mean response time constraint at each facility.",An exact solution procedure for multi-item two-echelon spare parts inventory control problem with batch ordering in the central warehouse
"['Chiapin Wang', 'Kun Yeh Chan']","This paper presents a novel utility-based connection admission control (CAC) scheme for IEEE 802.16e broadband wireless access networks. We develop specific utility functions for real-time and non-real-time services coupled with a handover process. Given these utility functions we characterize the network utility with respect to the allocated bandwidth, and further propose a CAC algorithm which admits a connection that conducts to the greatest utility so as to maximize the total resource utilization. The simulation results demonstrate the effectiveness of the proposed CAC algorithm in terms of network utility.",Utility-based admission control for mobile WiMAX networks
"['Robert Kooima', 'Jason Leigh', 'Andrew E. Johnson', 'D. A. Roberts', 'Mark SubbaRao', 'Thomas A. DeFanti']","Many interrelated planetary height map and surface image map data sets exist, and more data are collected each day. Broad communities of scientists require tools to compose these data interactively and explore them via real-time visualization. While related, these data sets are often unregistered with one another, having different projection, resolution, format, and type. We present a GPU-centric approach to the real-time composition and display of unregistered-but-related planetary-scale data. This approach employs a GPGPU process to tessellate spherical height fields. It uses a render-to-vertex-buffer technique to operate upon polygonal surface meshes in image space, allowing geometry processes to be expressed in terms of image processing. With height and surface map data processing unified in this fashion, a number of powerful composition operations may be uniformly applied to both. Examples include adaptation to nonuniform sampling due to projection, seamless blending of data of disparate resolution or transformation regardless of boundary, and the smooth interpolation of levels of detail in both geometry and imagery. Issues of scalability and precision are addressed, giving out-of-core access to giga-pixel data sources, and correct rendering at scales approaching one meter.",Planetary-Scale Terrain Composition
"['Kent D. Wilken', 'John Paul Shen']","A low-cost approach to concurrent detection of processor control errors is presented that uses a simple hardware monitor and signatures embedded into the executing program. Existing signature-monitoring techniques detect a large portion of processor control errors at a fraction of the cost of duplication. Analytical methods developed in this study show that the new approach, continuous signature monitoring (CSM), makes major advances beyond existing techniques. CSM reduces the fraction of undetected control-flow errors by orders of magnitude, to less than 10/sup -6/, while the number of signatures reaches a theoretical minimum, being lowered by as much as three times to a range of 4-11%. Signature cost is reduced by placing CSM signatures at locations that minimize performance loss and (for some architectures) memory overhead. CSM exploits the program memory's SEC/DED code to decrease error-detection latency by as much as 1000 times, to 0.016 program memory cycles, without increasing memory overhead. This short latency allows transient faults to be tolerated. >",Continuous signature monitoring: low-cost concurrent detection of processor control errors
"['Ali Maqousi', 'Shalini Tater', 'F. Ball']",In this paper we describe the development of a measurement based flow acceptance control mechanism that will support guaranteed services in multiservice packet switched networks. Using simulation models we present simulation experiments with two monitoring techniques. One that monitors the percentile occupancy of queue and the other monitors the mean and variance of packet inter-arrival times and packet lengths of a continuous media packet stream. Both these techniques are considered in the development of the measurement based flow acceptance mechanism.,Traffic monitoring techniques for measurement based flow acceptance control
"['Shmuel T. Klein', 'Miri Kopel Ben-Nissan']","Recent publications advocate the use of various variable length codes for which each codeword consists of an integral number of bytes in compression applications using large alphabets. This paper shows that another tradeoff with similar properties can be obtained by Fibonacci codes. These are fixed codeword sets, using binary representations of integers based on Fibonacci numbers of order m ges 2. Fibonacci codes have been used before, and this paper extends previous work presenting several novel features. In particular, they compress better and are more robust, at the price of being slower.",Using Fibonacci Compression Codes as Alternatives to Dense Codes
"['Christopher Y. Brown', 'H. Harry Asada']","Human hands employ characteristic patterns of actuation, or synergies, that contain much of the information required to describe an entire hand shape. In some cases, 80% or more of the total information can be described with only two scalar component values. Robotic hands, however, commonly only couple intra-finger joints, and rarely take advantage of this inter-finger coordination. In this paper, real-world data on a variety of human hand postures was collected using a data glove, and principal components analysis was used to calculate these synergies, resulting in what we call eigenpostures. A novel mechanism design is presented to combine the eigenpostures and drive a 17-degree-of-freedom 5-fingered robot hand. The hand uses only 2 DC motors to accurately recreate a wide range of hand shapes. We also present a design improvement that allows us to distinguish between high-precision and low-precision tasks, as well as greatly reduce overall error.",Inter-finger coordination and postural synergies in robot hands via mechanical implementation of principal components analysis
"['Zhangbing Zhou', 'Sami Bhiri', 'Walid Gaaloul', 'Lei Shu', 'Laurentiu Vasiliu', 'Manfred Hauswirth']","Web service interactions lie in the core of SOA. Due to the autonomy, heterogeneity and continuous evolution of Web services, mediators are usually needed to support service interactions to overcome possible mismatches that may exist among business processes. In this paper, we introduce a space-based architecture for process mediator which considers both control-flow and data-flow, present possible mismatch patterns, and suggest how they can be automatically mediated. Our work can be used to perform runtime mediation and thus to facilitate service interactions.",Developing Process Mediator for Web Service Interactions
"['Allen Y. Yang', 'Subhransu Maji', 'C. Mario Christoudias', 'Trevor Darrell', 'Jitendra Malik', 'Shankar Sastry']","In this paper, we study the classical problem of object recognition in low-power, low-bandwidth distributed camera networks. The ability to perform robust object recognition is crucial for applications such as visual surveillance to track and identify objects of interest, and compensate visual nuisances such as occlusion and pose variation between multiple camera views. We propose an effective framework to perform distributed object recognition using a network of smart cameras and a computer as the base station. Due to the limited bandwidth between the cameras and the computer, the method utilizes the available computational power on the smart sensors to locally extract and compress SIFT-type image features to represent individual camera views. In particular, we show that between a network of cameras, high-dimensional SIFT histograms share a joint sparse pattern corresponding to a set of common features in 3-D. Such joint sparse patterns can be explicitly exploited to accurately encode the distributed signal via random projection, which is unsupervised and independent to the sensor modality. On the base station, we study multiple decoding schemes to simultaneously recover the multiple-view object features based on the distributed compressive sensing theory. The system has been implemented on the Berkeley CITRIC smart camera platform. The efficacy of the algorithm is validated through extensive simulation and experiments.",Multiple-view object recognition in band-limited distributed camera networks
"['Erich Cadario', 'Hermann Gross', 'Horst Hammer', 'Karsten Schulz', 'Antje Thiele', 'Ulrich Thoennessen', 'Uwe Soergel', 'Dan Johan Weydahl']","The main advantages of SAR are the capability of imaging large areas in short time and delivering data at any day time and under nearly all weather conditions. This is especially important for disaster management and continuous long term monitoring applications. Key elements of man-made infrastructure are bridges. Especially for bridges over water, the SAR specific side looking imaging geometry can lead to special characteristics in the image. In this paper, the possibilities of extracting bridge features like width and height from SAR data, especially for bridges over water, are discussed. The feature extraction is based on the segmentation of parallel lines in an image. An approach is presented to exploit this feature extraction for change detection. The investigations are supported by SAR simulations, and real airborne and spaceborne data are presented.",Change Detection for Bridges over Water in Airborne and Spaceborne SAR Data
"['Chaomin Shen', 'Yaxin Peng', 'Ling Pi', 'Zhibin Li']","In this paper we present a variational method for synthetic aperture radar (SAR) speckle removal. Variational method is a newly developed technique for the removal of SAR's multiplicative noise. For an image, we could define an energy functional. The energy evolves as the original image changes, and the minimum energy corresponds to the speckle reduced result. Partial differential equation (PDE) technique is used to get the minimal solution. Our energy functional makes use of the statistical information of the multiplicative noise since it follows a Gamma law with mean mu = 1 and variance sigma 2  = 1/M for M-look SAR. Our energy is a regularization term with two constraints. The regularization term is the integral for the norm of image gradient; two constraints are the mean of noise should be 1 and the variance of noise should be 1/M. We use the method of Lagrange multipliers, Euler-Lagrange equation and heat flow method to obtain the minimizer of the energy. ERS Precision Image (PRI) data are to demonstrate our algorithm. Numerical result shows that the speckle reduced image preserves edges and point targets while smoothes homogenous regions in the original image. The algorithm is computationally efficient and easy to implement.",Variational-based speckle noise removal of SAR imagery
"['Wenjun Li', 'Huaiyu Dai']","In this paper, we incorporate clustering techniques into distributed consensus algorithms for faster convergence and better energy efficiency. Together with a simple distributed clustering algorithm, we design cluster-based distributed consensus algorithms in forms of both fixed linear iteration and randomized gossip. The time complexity of the proposed algorithms is presented in terms of metrics of the original and induced graphs, through which the advantage of clustering is revealed. Our cluster-based algorithms are also shown to achieve an Omega(log n) gain in message complexity over the standard ones.",Cluster-based distributed consensus
"['A.R. da Silva', 'J. Saraiva', 'R. Silva', 'Carlos Martins']","The first version of the XIS profile addressed the development of interactive systems by defining models oriented only towards how the system should perform tasks. However, issues such as user-interface layouts, or the capture of interaction patterns, were not addressed by the profile, but only by the source-code generation process. This originated systems that, although functional, were considered by end-users as ""difficult to use"". In this paper we present the second version of the XIS UML profile, which is now a crucial component of the ProjectIT research project. This profile follows the ""separation of concerns"" principle by proposing an integrated set of views that address the various issues detected with the previous version of XIS. In addition, this profile also promotes the usage of extreme modeling, by relying on the extensive use of model-to-model transformation templates that are defined to accelerate the model development tasks",XIS-UML Profile for eXtreme Modeling Interactive Systems
"['M. Mathis', 'Jamshid Mahdavi']","We have developed a Forward Acknowledgment (FACK) congestion control algorithm which addresses many of the performance problems recently observed in the Internet. The FACK algorithm is based on first principles of congestion control and is designed to be used with the proposed TCP SACK option. By decoupling congestion control from other algorithms such as data recovery, it attains more precise control over the data flow in the network. We introduce two additional algorithms to improve the behavior in specific situations. Through simulations we compare FACK to both Reno and Reno with SACK. Finally, we consider the potential performance and impact of FACK in the Internet.",Forward acknowledgement: refining TCP congestion control
"['Mark D. Austin', 'Gordon L. St?¨ber']","In-service interference plus noise power (I+N) and signal-to-interference plus noise power SI(I+N) estimation methods are examined for TDMA cellular systems. A simple (I+N) estimator is developed whose accuracy depends on the channel and symbol estimate error statistics. Improved (I+N) and S/(I+N) estimators are developed whose accuracy depends only on the symbol error statistics. The proposed estimators are evaluated through software simulation with an IS-54 frame structure. For high speed mobiles, it is demonstrated that S/(I+N) can be estimated to within 2 dB in less than a second.",In-service signal quality estimation for TDMA cellular systems
"['Philippa J. Broadfoot', 'A. W. Roscoe']","We carry forward the work described in our previous papers [5,18,20] on the application of data independence to the model checking of security protocols using CSP [19] and FDR [10]. In particular, we showed how techniques based on data independence [12,19] could be used to justify, by means of a finite FDR check, systems where agents can perform an unbounded number of protocol runs. Whilst this allows for a more complete analysis, there was one significant incompleteness in the results we obtained: while each individual identity could perform an unlimited number of protocol runs sequentially, the degree of parallelism remained bounded (and small to avoid state space explosion). In this paper, we report significant progress towards the solution of this problem, by means anticipated in [5], namely by É??internalisingÉ?ù protocol roles within the É??intruderÉ?ù process. The internalisation of protocol roles (initially only server-type roles) was introduced in [20] as a state-space reduction technique (for which it is usually spectacularly successful). It was quickly noticed that this had the beneficial side-effect of making the internalised server arbitrarily parallel, at least in cases where it did not generate any new values of data independent type. We now consider the case where internal roles do introduce fresh values and address the issue of capturing their state of mind (for the purposes of analysis).",Embedding agents within the intruder to detect parallel attacks
"['Helen Kennedy', 'Simon Evans', 'Siobhan Thomas']","This article presents the findings of a research project that aimed to contribute to the social inclusion of people with intellectual disabilities (ID) in the World Wide Web (the Web). The Inclusive New Media Design (INMD) project brought together thirty-one Web designers and developers with twenty-nine people with intellectual disabilities to explore the best practice for building Web sites accessible to the ID community. Specifically, the project took accessibility techniques identified in ID accessibility research, and investigated what would (or would not) make it possible for Web professionals to implement them. This article suggests some tentative answers to the question of whether a fully accessible Web can be built, one that includes people with ID. While the article outlines simple steps that can be taken to facilitate accessibility for people at the mild end of the ID spectrum, it also highlights a number of barriers that exist to implementing ID accessibility guidance, most notably the power holders and decision makers with whom Web designers work, who may not share the designers' commitment to accessibility.",Can the Web Be Made Accessible for People with Intellectual Disabilities
"['Yanhong Yuan', 'Prithviraj Banerjee']","Very fast and accurate 3-D capacitance extraction is essential for interconnect optimization in ultra deep sub-micro designs (UDSM). Parallel processing provides an approach to reducing the simulation turn-around time. This paper examines the parallelization of the well known fast multipole based 3-D capacitance extraction program FASTCAP, which employs new preconditioning and adaptive techniques. To account for the complicated data dependencies in the unstructured problems, we propose a generalized cost function model, which can be used to accurately measure the workload associated with each cube in the hierarchy. We then present two adaptive partitioning schemes, combined with efficient communication mechanisms with bounded buffer size, to reduce the parallel processing overhead. The overall load balance is achieved through balancing the load at each level of the multipole computation. We report detailed performance results using a variety of standard benchmarks on 3-D capacitance extraction, on an IBM SP2.",A parallel implementation of a fast multipole based 3-D capacitance extraction program on distributed memory multicomputers
"['Huimin Lu', 'Hui Zhang', 'Shaowu Yang', 'Zhiqiang Zheng']","How to make vision system work robustly under dynamic light conditions is still a challenging research focus in computer/robot vision community. In this paper, a novel camera parameters auto-adjusting technique based on image entropy is proposed. Firstly image entropy is defined and its relationship with camera parameters is verified by experiments. Then how to optimize the camera parameters based on image entropy is proposed to make robot vision adaptive to the different light conditions. The algorithm is tested by using the omnidirectional vision in indoor RoboCup Middle Size League environment and the perspective camera in outdoor ordinary environment, and the results show that the method is effective and color constancy to some extent can be achieved.",Camera parameters auto-adjusting technique for robust robot vision
['Bela Bonita Chatterjee'],"Considering the criminal uses of encryption, it has been asserted that national security and law enforcement endeavours must not be frustrated by potential evidence being hidden through digital encryption while the encryption key is withheld. The facilitation of state access to encryption keys through the Regulation of Investigatory Powers Act 2000 (É??RIPA 2000É??) was intended to address precisely such a danger. However, since this statuteÉ??s enactment there have been significant shifts in law and policy relating to terrorism and child pornography as well as important technological developments. This article critically examines changes to the RIPA 2000 encryption provisions made by the Terrorism Act 2006 and the Policing and Crime Act 2009. It also considers emerging statistical data and cases including R v S(F) [2008]. It concludes that the underlying premises of the revised provisions are flawed, the provisions themselves ineffectual in practice and in the longer term potentially open to É??mission creepÉ?? to cover lesser offences.",New but not improved : a critical examination of revisions to the Regulation of Investigatory Powers Act 2000 encryption provisions
"['Brygg Ullmer', 'Hiroshi Ishii']","The metaDESK is a user interface platform demonstrating new interaction techniques we call ""tangible user inter- faces."" We explore the physical instantiation of interface elements from the graphical user interface paradigm, giving physical form to windows, icons, handles, menus, and controls. The design and implementation of the metaDESK display, sensor, and software architectures is discussed. A prototype application driving an interaction with geographi- cal space, Tangible Geospace, is presented to demonstrate these concepts.",The metaDESK: models and prototypes for tangible user interfaces
"['Mici Halse', 'Brenda Mallinson']","This paper acknowledges the move from the content-centric to the network-centric approach to teaching and learning using Social Network Services (SNSs) in Higher Education. The South African cultural context is explained with a view to reconciling this with current SNSs. Finally, a way forward is proposed for developing SNSs particular to the Southern African cultural context.","A Motivation for ""Ubuntu"" to Enhance e-Learning Social Network Services in South Africa"
"['Yuki Okajima', 'Kazuo Yoshikawa', 'Takashi Shibayama']","PASCO in approximately 4 years, after the launch of TerraSAR-X (TSX), has successfully carried out a total 22 studies of disaster response for the worldwide and domestic cases. The outline and a part of the conducted cases are explained in detailed in this paper.",Recent experiences utilizing TerraSAR-X for the monitoring of natural disasters in different parts of the world
"['Patroklos G. Argyroudis', ""Donal O'Mahony""]","The ubiquitous computing paradigm suggests that we are going to be surrounded by countless wireless devices capable of providing services trans- parently. By definition, the nature of ubiquitous computing environments is open and extremely dynamic, making difficult the establishment of predefined security relationships between all of the participating entities. Authentication mechanisms can be employed to establish the identity of a pervasive computing entity but they suffer from scalability problems and have limited value in defin- ing authorization decisions among strangers. In this paper we propose AETHER, an authorization management architecture designed specifically to address trust establishment and access control in ubiquitous computing environments. Own- ers define attribute authority sets and access control policy entries that are em- bedded into their devices. Members of the attribute authority sets are trusted to issue credentials for the corresponding attributes that can then be used in order to gain access to protected resources. Our architecture supports dynamic mem- bership in these sets facilitating distributed administration, which is required in the context of the volatile nature of ubiquitous security relationships, and at- tribute mapping to allow roaming among authority domains. Moreover, we pre- sent the foundation of a logic model for our proposed architecture that is used to prove access control decisions.",??THER: an Authorization Management Architecture for Ubiquitous Computing ?Ò
"['S. Kami Makki', 'Stefan Andrei']","Semantic caching is a dynamic caching strategy which deals with not only exact but also inexact similar queries. In this manner, each query will be carefully analyzed by the cache manager to identify the part that can be found in the cache from the part that needs to be retrieved from the server. This trimming process not only speeds up information retrieval but also saves on communication cost especially for mobile and wireless devices. Therefore, query trimming is a key problem in mobile and wireless environment, and devices in this environment have limited connection time, bandwidth, and battery power. However, the existing methods for query trimming have a number of limitations such as, inefficiency in time, space and the complexity of the algorithm used for trimming. These factors restrict the applicability of semantic caching for many applications. In this paper we investigate the shortcomings of query trimming process and propose a new solution to improve this process.",Utilizing semantic caching in ubiquitous environment
['Lennard Kamenski'],"A common approach for generating an anisotropic mesh is the M-uniform mesh approach where an adaptive mesh is generated as a uniform one in the metric specified by a given tensor M. A key component is the determination of an appropriate metric, which is often based on some type of Hessian recovery. Recently, the use of a global hierarchical basis error estimator was proposed for the development of an anisotropic metric tensor for the adaptive finite element solution. This study discusses the use of this method for a selection of different applications. Numerical results show that the method performs well and is comparable with existing metric tensors based on Hessian recovery. Also, it can provide even better adaptation to the solution if applied to problems with gradient jumps and steep boundary layers. For the Poisson problem in a domain with a corner singularity, the new method provides meshes that are fully comparable to the theoretically optimal meshes.",A study on using hierarchical basis error estimates in anisotropic mesh adaptation for the finite element method
"['Ruck Thawonmas', 'Seiji Murakami', 'Takumi Sato']","This paper describes our believable judge bot ICE-CIG2011 that has an ability to learn tactics from a judge player and an ability to judge an opponent character as a human or a bot. We conjecture that a bot with these two abilities should be considered human-like in a competition environment, such as BotPrize, where human players participate to compete not only for being the most human-like player but also the best judge. Main contributions of this work lie in our mechanisms for achieving these two abilities. To achieve the former ability, we develop a system and GUI that allow a selected judge player É?? whose role is to train ICE-CIG2011 É?? to control his or her character by only deciding which tactic to use under a given situation. We then obtain the judge's tactic log and use it for training tactic selection of ICE-CIG2011 with neuro evolution of augmenting topologies. To achieve the latter ability, we acquire additional logs when the judge character interacts with other opponent characters. In order to represent the play of a known (bot or human) character, we train a neural gas É?? a kind of self-organizing neural network É?? from its log. For an unknown character, once its neural gas is trained after a certain period of observation, ICE-CIG2011 decides if it is a human or bot by using the if-nearest-neighbor algorithm; this algorithm considers the majority in the labels of the if-nearest neural gases, of known characters, to the neural gas of that unknown character. Experimental results are given and discussed concerning these two abilities of ICE-CIG2011.",Believable judge bot that learns to select tactics and judge opponents
"['Sheng Chen', 'Martin Erwig']","Previous research on static analysis for program families has focused on lifting analyses for single, plain programs to program families by employing idiosyncratic representations. The lifting effort typically involves a significant amount of work for proving the correctness of the lifted algorithm and demonstrating its scalability. In this paper, we propose a parameterized static analysis framework for program families that can automatically lift a class of type-based static analyses for plain programs to program families. The framework consists of a parametric logical specification and a parametric variational constraint solver. We prove that a lifted algorithm is correct provided that the underlying analysis algorithm is correct. An evaluation of our framework has revealed an error in a previous manually lifted analysis. Moreover, performance tests indicate that the overhead incurred by the general framework is bounded by a factor of 2.",Type-based parametric analysis of program families
"['David Burshtein', 'Alona Strugatski']","A coding scheme for write once memory (WOM) using polar codes is presented. It is shown that the scheme achieves the capacity region of noiseless WOMs when an arbitrary number of multiple writes is permitted. The encoding and decoding complexities scale as O(N log N) where N is the blocklength. For N sufficiently large, the error probability decreases sub-exponentially in N. Some simulation results with finite length codes are presented.",Polar write once memory codes
"['Kok Yew Ng', 'Chee Pin Tan', 'Rini Akmeliawati']",In this paper we present a sensor fault tolerance control scheme that is applied to a double inverted pendulum. Sensor faults will affect the system when it is used in closed-loop feedback. The scheme uses a linear observer reconstruct the sensor fault and to subtract the reconstruction from the faulty sensor. The net result is then used for the closed-loop feedback. It was found that the scheme restored the performance to the fault-free scenario.,Tolerance towards sensor failures: an application to a double inverted pendulum
"['Antonio Brogi', 'Jean-Marie Jacquet']","A number of different coordination models for specifying inter-process communication and synchronisation rely on a notion of shared dataspace. Many of these models are extensions of the Linda coordination model, which includes operations for adding, deleting and testing the presence/absence of data in a shared dataspace.#R##N##R##N#We compare the expressive power of three classes of coordination models based on shared dataspaces. The first class relies on Linda's communication primitives, while a second class relies on the more general notion of multi-set rewriting (e.g., like Bauhaus Linda or Gamma). Finally, we consider a third class of models featuring communication transactions that consist of sequences of Linda-like operations to be executed atomically (e.g., like in Shared Prolog or PoliS).",On the Expressiveness of Coordination Models
"['Hsiuhan Lexie Yang', 'Melba M. Crawford']","While spectral and temporal advantages of multitemporal hyperspectral images provide opportunities for advancing classification of time varying phenomena, significant challenges are associated with high dimensionality and nonstationary signatures. While manifold learning retains critical geometry and develops a low dimension space where class clusters are recovered, spectral changes in temporal imagery impact the fidelity of the geometric representation of class dependent data. In this paper, we investigate a manifold alignment framework that exploits prior information while exploring similar local structures. The aim is to make use of common underlying geometries of two multitemporal images and embed the resemblances in a joint data manifold for classification tasks. Promising results support the advantages of the proposed manifold alignment approach.",Manifold alignment for multitemporal hyperspectral image classification
"['Francisco J. Ferrer-Troyano', 'Jes?ßs S. Aguilar-Ruiz', 'Jos?? C. Riquelme']","Mining data streams is a challenging task that requires online systems based on incremental learning approaches. This paper describes a classification system based on decision rules that may store up-to-date border examples to avoid unnecessary revisions when virtual drifts are present in data. Consistent rules classify new test examples by covering and inconsistent rules classify them by distance as the nearest neighbor algorithm. In addition, the system provides an implicit forgetting heuristic so that positive and negative examples are removed from a rule when they are not near one another.",Incremental rule learning based on example nearness from numerical data streams
['Ferruh ??zbudak'],"Let k É?ù 1 and f1 .... , fr É?? Fqk (x) be a system of rational functions forming a strongly linearly independent set over a finite field Fq. Let ??1 ..... ??r É?? Fq be arbitrarily prescribed elements. We prove that for all sufficiently large extensions Fqkm, there is an element ?? É?? Fqkm of prescribed order such that TrFqkm/Fq (fi (??))= ??i for i = 1, ..., r, where TrFqkm/Fq is the relative trace map from Fqkm onto Fq. We give some applications to BCH codes, finite field arithmetic and ordered orthogonal arrays. We also solve a question of Helleseth et al. (Hypercubic 4 and 5-designs from Double-Error-Correcting codes, Des. Codes. Cryptgr. 28(2003). pp. 265-282) completely.","Elements of prescribed order, prescribed traces and systems of rational functions over finite fields"
"['Nigel Shadbolt', ""Kieron O'Hara"", 'Louise Crow']","The special problems of experimentally evaluating knowledge acquisition and knowledge engineering tools, techniques and methods are outlined, and illustrated in detail with reference to two series of studies. The first is a series of experiments undertaken at Nottingham University under the aegis of the UK Alvey initiative and the ESPRIT project ACKnowledge. The second is the series of Sisyphus benchmark studies. A suggested programme of experimental evaluation is outlined which is informed by the problems with using Sisyphus for evaluation.","The experimental evaluation of knowledge acquisition techniques and methods: history, problems, and new directions"
"['Hyun Cho', 'Jeff Gray']","Domain-Specific Modeling Languages (DSMLs) can offer assistance to domain experts, who may not be computer scientists, by providing notations and semantic constructs that align with abstractions from a particular domain. In this paper, we describe our design and application of a DSML in the area of data composition and interoperability. In particular, we introduce our recent effort to design a DSML to assist with interoperability issues across scientific software applications (e.g., composing scientific data in different file structures and integrating scientific data with data gathering devices). Currently, several different scientific data file specifications have been proposed (e.g., CID, netCDF, and HDF). Each file specification is optimized to manage a specific data type efficiently. Thus, each file specification has evolved with slightly different notions and implementation technologies. These differences led to the need for an environment that provides interoperability among the different specification formats. In this paper, we introduce our framework, supported by a DSML, that provides functionality to visually model the data composition and integration concepts independent from a particular data file specification.",A domain-specific modeling language for scientific data composition and interoperability
"['Tariq M. King', 'Annaji Sharma Ganti']","The cloud computing model continues to gain much attention from software industry practitioners. As such, leading companies are investing in the development, packaging and delivery of cloud services over the Internet. However, although much work is being done to model and build cloud applications and services, there is significantly less research devoted to testing them. In this paper, we describe our research-in-progress towards migrating autonomic self-testing (AST) to the cloud. Our approach combines the development of an automated test harness for a cloud service, with the delivery of test support as-a-service (TSaaS). Both AST and TSaaS are supported by a virtual test environment, which utilizes the power of the cloud to enhance the self-testing process.",Migrating Autonomic Self-Testing to the Cloud
"['Mohamed O. Shaker', 'Soliman A. Mahmoud', 'Ahmed M. Soliman']","In this paper, the design and analysis of a CMOS fifth-order low-pass GM-C filter are presented. It has a cutoff frequency of 4.3 MHz to accommodate the wideband CDMA standard. The transconductor used in this filter is based on a four-transistor cell operating in triode or saturation mode. It achieves high linearity range of /spl plusmn/ 1 V at /spl plusmn/ 1.5 V supply voltages. PSpice simulations show that total harmonic distortion at 1 Vpp and 1 MHZ is equal to 0.1% with 1.234 mW standby power dissipation. The proposed filter and the transconductor are simulated using 0.35 /spl mu/m technology.",A CMOS fifth-order low-pass current-mode filter using a linear transconductor
"['Leonardo F. Peres', 'Carlos C. DaCamara']","The two-temperature method (TTM) allows the separation of land-surface temperature and land-surface emissivity information from radiance measurements, and therefore, the solution can be uniquely determined by the data. However, the inverse problem is still an ill-posed problem, since the solution does not depend continuously on the data. Accordingly, we have used some mathematical tools, which are suited for analyses of ill-posed problems in order to show TTM properties, evaluate it, and optimize its estimations. Related to this last point, we have shown that it is necessary to constrain the problem, either by defining a region of physically admissible solutions and/or by using regularization methods, in order to obtain stable results. Besides, the results may be improved by using TTM with systems that possess a high temporal resolution, as well as by acquiring observations near the maximum and minimum of the diurnal temperature range.",Inverse problems theory and application: analysis of the two-temperature method for land-surface temperature and emissivity estimation
"['Xiao-lin Wang', 'Zheng-Jie Yin']","Genetic algorithm-based learning classifier system (LCS) is a massively parallel, message-passing and rule-based machine learning system. But its potential self-adaptive learning capability has not been paid enough attention in reservoir operation research. In this paper, an operating rule classification system based on LCS , which learns through credit assignment (the bucket brigade algorithm) and rule discovery (the genetic algorithm), is established to extract water-supply reservoir operating rules. The proposed system acquires the online identification rate 95% for training samples and offline rate 85% for testing samples in a case study, and further discussions are made about the impacts on the performances or behaviors of the rule classification system from three aspects of obtained rules, training or testing samples and the comparisons between the rule classification system and the artificial neural network (ANN). The results indicate the learning classifier system is feasible and effective for the system to obtain the reservoir supply operating rules.",Operating Rules Classification System of Water Supply Reservoir Based on LCS
"['Johann Knechtel', 'Igor L. Markov', 'Jens Lienig']","Three-dimensional ICs promise to significantly extend the scale of system integration and facilitate new-generation electronics. However, progress in commercial 3D ICs has been slow. In addition to technology-related difficulties, industry experts cite the lack of a commercial 3D EDA tool-chain and design standards, high risk associated with a new technology, and high cost of transition from 2D to 3D ICs. To streamline the transition, we explore design styles that reuse existing 2D Intellectual Property (IP) blocks in 3D ICs. Currently, these design styles severely limit the placement of Through-Silicon Vias (TSVs) and constrain the reuse of existing 2D IP blocks in 3D ICs. To overcome this problem, we develop a methodology for using TSV islands and novel techniques for clustering nets to connect 2D IP blocks through TSV islands. Our empirical validation demonstrates 3D integration of traditional 2D circuit blocks without modifying their layout for this context.",Assembling 2D blocks into 3D chips
"['Ryan Eccles', 'Deborah A. Stacey']","The availability of low-cost commodity multiprocessor machines change the nature of mainstream programming. This discipline is required to include small-scale, dual and quadruple processor machines, to remain competitive. These small-scale parallel systems require software engineering principles capable of encapsulating the complex parallel programming issues. This paper discusses a technique that provides a simple model for incorporating parallel programming in a scheduler. This model can dynamically adjust to single and small-scale multiple processor environments.",Software engineering issues for small-scale parallelism
"['Dong-Uno period after U Lee', 'Hyungjin Kim', 'Chris Jones', 'John D. Villasenor']",We present a pilotless frame synchronization approach that exploits feedback from a low-density parity-check (LDPC) code decoder. The synchronizer is based on syndrome checks using hard decisions from the channel observations. The bandwidth overhead associated with pilot symbols in conventional receiver architectures is eliminated while providing sufficient synchronization performance. An LDPC decoder coupled with our synchronizer exhibits negligible frame error rate degradation over a system with perfect synchronization. The complexity of the frame synchronizer is kept relatively low due to its XOR-based approach.,Pilotless Frame Synchronization for LDPC-Coded Transmission Systems
"['S. De Gennaro', 'Krieg K', 'Louis D. Braida', 'N. I. Durlach']","Multiband amplitude compression has been studied as a compensation for the reduced auditory dynamic range often associated with sensorineural hearing loss, since it is capable of altering the dynamic range of speech as a function of frequency. Compression systems are generally characterized by their response to steady state tones and to simple dynamic stimuli, such as tone bursts. Level distributions of unprocessed and compressed materials are presented to demonstrate that these descriptions are inadequate to predict the processing of speech. A simple analysis of multiband compression which incorporates the interactions of static and dynamic properties is presented.",Third-octave analysis of multichannel amplitude compressed speech
"['Yun Li', 'M?ùrten Sj??str??m', 'Ulf Jennehag', 'Roger Olsson']","Three-dimensional (3D) movies in theaters have become a massive commercial success during recent years, and it is likely that, with the advancement of display technologies and the production of 3D contents, TV broadcasting in 3D will play an important role in home entertainments in the not too distant future. 3D video contents contain at least two views from different perspectives for the left and the right eye of viewers. The amount of coded information is doubled if these views are encoded separately. Moreover, for multi-view displays (i.e. different perspectives of a scene in 3D are presented to the viewer at the same time through different angles), either video streams of all the required views must be transmitted to the receiver, or the displays must synthesize the missing views with a subset of the views. The latter approach has been widely proposed to reduce the amount of data being transmitted. The virtual views can be synthesized by the Depth Image Based Rendering (DIBR) approach from textures and associated depth images. However it is still the case that the amount of information for the textures plus the depths presents a significant challenge for the network transmission capacity. An efficient compression will, therefore, increase the availability of content access and provide a better video quality under the same network capacity constraints.In this thesis, the compression of depth images is addressed. These depth images can be assumed as being piece-wise smooth. Starting from the properties of depth images, a novel depth image model based on edges and sparse samples is presented, which may also be utilized for depth image post-processing. Based on this model, a depth image coding scheme that explicitly encodes the locations of depth edges is proposed, and the coding scheme has a scalable structure. Furthermore, a compression scheme for block-based 3D-HEVC is also devised, in which diffusion is used for intra prediction. In addition to the proposed schemes, the thesis illustrates several evaluation methodologies, especially, the subjective test of the stimulus-comparison method. It is suitable for evaluating the quality of two impaired images, as the objective metrics are inaccurate with respect to synthesized views.The MPEG test sequences were used for the evaluation. The results showed that virtual views synthesized from post-processed depth images by using the proposed model are better than those synthesized from original depth images. More importantly, the proposed coding schemes using such a model produced better synthesized views than the state of the art schemes. As a result, the outcome of the thesis can lead to a better quality of 3DTV experience.",Depth image post-processing method by diffusion
"['Mark S. Ackerman', 'Eric Mandel']","Many forms of organizational memory must exist embedded within the organizational processes and tasks. This paper argues that ""memory-in-the small"", memory utilized in the performance of an organizational task, can serve as an effective performance support mechanism. By basing organizational memory upon organizational tasks (and basing task support upon organizational memory), organizational memory systems can provide additional and necessary support services for organizations and communities. As an example of memory-in-the-small, this paper describes a software application called the ASSIST, that combines organizational memory with task performance for a scientific community. The ASSIST utilizes and stores the collective memory of astrophysicists about data analysis and is used world-wide by astrophysicists. The paper also considers the theoretical and architectural issues involved when combining organizational memory with task performance. >",Memory in the small: an application to provide task-based organizational memory for a scientific community
"['Eckart Bindewald', 'Tanner Kluth', 'Bruce A. Shapiro']","Computational RNA secondary structure prediction approaches differ by the way RNA pseudoknot interactions are handled. For reasons of computational efficiency, most approaches only allow a limited class of pseudoknot interactions or are not considering them at all. Here we present a computational method for RNA secondary structure prediction that is not restricted in terms of pseudoknot complexity. The approach is based on simulating a folding process in a coarse-grained manner by choosing helices based on established energy rules. The steric feasibility of the chosen set of helices is checked during the folding process using a highly coarse-grained 3D model of the RNA structures. Using two data sets of 26 and 241 RNA sequences we find that this approach is competitive compared to the existing RNA secondary structure prediction programs pknotsRG, HotKnots and UnaFold. The key advantages of the new method are that there is no algorithmic restriction in terms of pseudoknot complexity and a test is made for steric feasibility. Availability: The program is available as web server at the site: http://cylofold.abcc.ncifcrf.gov.",CyloFold: secondary structure prediction including pseudoknots
"['Sakshi Pahwa', 'Amelia Hodges', 'Caterina M. Scoglio', 'Sean Wood']","This paper presents a complex systems overview of a power grid network. In recent years, concerns about the robustness of the power grid have grown because of several cascading outages in different parts of the world. In this paper, cascading effect has been simulated on three different networks, the IEEE 300 bus test system, the IEEE 118 bus test system, and the WSCC 179 bus equivalent model. Power Degradation has been discussed as a measure to estimate the damage to the network, in terms of load loss and node loss. A network generator has been developed to generate graphs with characteristics similar to the IEEE standard networks and the generated graphs are then compared with the standard networks to show the effect of topology in determining the robustness of a power grid. Three mitigation strategies, Homogeneous Load Reduction, Targeted Range-Based Load Reduction, and Use of Distributed Renewable Sources in combination with Islanding, have been suggested. The Homogeneous Load Reduction is the simplest to implement but the Targeted Range-Based Load Reduction is the most effective strategy.",Topological analysis of the power grid and mitigation strategies against cascading failures
"['Chih-Wen Hsueh', 'Tien-Fu Chen', 'Rong-Guey Chang', 'Shi-Wu Lo']","For the success of an SoC design, a design platform and some key design technologies are needed. For this purpose, a research project in ""Technology Development Program for Academia"" was granted recently in Taiwan to develop the following technologies toward a high-performance low-power SoC design platform. First, a soft intellectual property (soft IP) and related RTOS, compiler, and integrated design environment (IDE) software of an Advanced Taiwan VLTW DSP Processor Core, which can be used as the Star IP of an SoC design platform, will be developed. Second, low-power and low-voltage digital and mixed-signal circuit design technologies will be developed based on the advanced MTCMOS process. Third, some key multimedia and communication soft or hard IPs will be developed. The developed advanced key technologies can be used as the technology driver to facilitate the design of SoC-based products, and they will also help to enhance the design capability of the Taiwan SoC industry. In this paper, we introduce the research project and focus on architecture and software technologies developing in one of the subitems.",Development of architecture and software technologies in high-performance low-power SoC design
"['David Brown', 'Andrea Kirk']","Previous authors have identified mechanical, human and work organisation factors that may contribute to accident occurrences and have noted the value of site-specific data in accident analysis. However, major criticisms of site specific safety information systems have focused on the difficulties using traditional approaches for the identification of critical paths involving processes related to injury and non-injury incidents. This paper presents an idiographic approach to the study of accidents and through contextual analysis develops maps of work processes related to injury incidents in mining. The information used in the contextual maps includes data related to work area, activity and work role being performed, and equipment being used at the time of the injury incident. The computer algorithm consists of a series of contextual conditionals where the incidents and lost days recorded in the final category of each pathway meet all of the conditionals of the previous categories. The order of selection indicates the criticality of the path. The analysis resolves the processes related to the incident into their constitutive components, and then redescribes these processes as paths in order to reveal associations. These contextual paths illustrate the processes which are part of an established pattern of recurring regularities associated with injury incidents.",Using information systems to structurally map workplace injury
['Martti Forsell'],"It is possible to implement the parallel random access machine (PRAM) on a chip multiprocessor (CMP) efficiently with an emulated shared memory (ESM) architecture to gain easy parallel programmability crucial to wider penetration of CMPs to general purpose computing. This implementation relies on exploitation of the slack of parallel applications to hide the latency of the memory system instead of caches, sufficient bisection bandwidth to guarantee high throughput, and hashing to avoid hot spots in intercommunication. Unfortunately this solution can not handle workloads with low thread-level parallelism (TLP) efficiently because then there is not enough parallel slackness available for hiding the latency. In this paper we show that integrating nonuniform memory access (NUMA) support to the PRAM implementation architecture can solve this problem. The obtained PRAM-NUMA hybrid model is described and architectural implementation of it is outlined on our Eclipse ESM CMP framework.",A PRAM-NUMA model of computation for addressing low-TLP workloads
"['Wouter Horr??', 'Sam Michiels', 'Wouter Joosen', 'Pierre Verbaeten']",Middleware services facilitate sensor-network application development. DAVIM is adaptable middleware that enables dynamic management of services and isolation between simultaneously running applications.,DAVIM: Adaptable Middleware for Sensor Networks
"['Ilhem Boussa??d', 'Julien Lepagnot', 'Patrick Siarry']","Metaheuristics are widely recognized as efficient approaches for many hard optimization problems. This paper provides a survey of some of the main metaheuristics. It outlines the components and concepts that are used in various metaheuristics in order to analyze their similarities and differences. The classification adopted in this paper differentiates between single solution based metaheuristics and population based metaheuristics. The literature survey is accompanied by the presentation of references for further details, including applications. Recent trends are also briefly discussed.",A survey on optimization metaheuristics
"['J??r??my Besson', 'C??line Robardet', 'Jean-Fran??ois Boulicaut']","We are designing new data mining techniques on boolean contexts to identify a priori interesting bi-sets (i.e., sets of objects or transactions associated to sets of attributes or items). A typical important case concerns formal concept mining (i.e., maximal rectangles of true values or associated closed sets by means of the so-called Galois connection). It has been applied with some success to, e.g., gene expression data analysis where objects denote biological situations and attributes denote gene expression properties. However in such real-life application domains, it turns out that the Galois association is a too strong one when considering intrinsically noisy data. It is clear that strong associations that would however accept a bounded number of exceptions would be extremely useful. We study the new pattern domain of ?Ò/?˝ concepts, i.e., consistent maximal bi-sets with less than ?Ò false values per row and less than ?˝ false values per column. We provide a complete algorithm that computes all the ?Ò/?˝ concepts based on the generation of concept unions pruned thanks to anti-monotonic constraints. An experimental validation on synthetic data is given. It illustrates that more relevant associations can be discovered in noisy data. We also discuss a practical application in molecular biology that illustrates an incomplete but quite useful extraction when all the concepts that are needed beforehand can not be discovered.",Mining formal concepts with a bounded number of exceptions from transactional data
"['Magdalena Feldhahn', 'Philipp Thiel', 'Mathias M. Schuler', 'Nina Hillen', 'Stefan Stevanovic', 'Hans-Georg Rammensee', 'Oliver Kohlbacher']","Predicting the T-cell-mediated immune response is an important task in vaccine design and thus one of the key problems in computational immunomics. Various methods have been developed during the last decade and are available online. We present EpiToolKit, a web server that has been specifically designed to offer a problem-solving environment for computational immunomics. EpiToolKit offers a variety of different prediction methods for major histocompatibility complex class I and II ligands as well as minor histocompatibility antigens. These predictions are embedded in a user-friendly interface allowing refining, editing and constraining the searches conveniently. We illustrate the value of the approach with a set of novel tumor-associated peptides. EpiToolKit is available online at www. epitoolkit.org.",EpiToolKitÉ??a web server for computational immunomics
"['Dongmin Guo', 'David Zhang', 'Naimin Li', 'Lei Zhang', 'Jianhua Yang']","Certain gases in the breath are known to be indicators of the presence of diseases and clinical conditions. These gases have been identified as biomarkers using equipments, such as gas chromatography and electronic nose (e-nose). GC is very accurate but is expensive, time consuming, and nonportable. E-nose has the advantages of low cost and easy operation, but is not particular for analyzing breath odor, and hence, has a limited application in diseases diagnosis. This paper proposes a novel system that is special for breath analysis. We selected chemical sensors that are sensitive to the biomarkers and compositions in human breath, developed the system, and introduced the odor signal preprocessing and classification method. To evaluate the system performance, we captured breath samples from healthy persons and patients known to be afflicted with diabetes, renal disease, and airway inflammation, respectively, and conducted experiments on medical treatment evaluation and disease identification. The results show that the system is not only able to distinguish between breath samples from subjects suffering from various diseases or conditions (diabetes, renal disease, and airway inflammation) and breath samples from healthy subjects, but in the case of renal failure is also helpful in evaluating the efficacy of hemodialysis (treatment for renal failure).",A Novel Breath Analysis System Based on Electronic Olfaction
"['Abbas Keramati', 'Hamid Reza Golian', 'Masoud Afshari-Mofrad']","The first purpose of this paper is to improve business processes in an industrial firm with business process modelling notation (BPMN) and business process execution language (BPEL). The second purpose is to provide a scientific approach for bringing business analysts and IT professionals together in a framework of an action research (AR). The research is conducted in one of the biggest distribution companies in Iran. BPMN is applied to model as-is and to-be situations of sale and distribution process. Both as-is and to-be models were simulated to compare their performance indexes. Moreover, models were implemented using BPEL, so that the model can be used for automating and improving the process. In this study, AR methodology was used to find a resolution of an organisational issue with those who experience this issue directly and to improve scientific knowledge and real-life contexts simultaneously.",Improving business processes with business process modelling notation and business process execution language: an action research approach
"['A. Bar-Lev', 'Alfred M. Bruckstein', 'Gershon Elber']","This paper describes a computer graphics system that enables users to define virtual marionette puppets, operate them using relatively simple hardware input devices, and display the scene from a given viewpoint on the computer screen. This computerized marionette theater has the potential to become a computer game for children, an interaction tool over the Internet, enabling the creation of simultaneously viewed and operated marionette show by users on the World Wide Web, and, most importantly, a versatile and efficient professional animation system.",Virtual marionettes: a system and paradigm for real-time 3D animation
"['Zhen-Yu Na', 'Zhenyong Wang', 'Qing Guo', 'Mingchuan Yang']","A fair and adaptive call admission control algorithm for multimedia low Earth orbit (LEO) satellite networks was proposed. Based on current call dropping probability of destination cell, this algorithm reserves bandwidth for handoff calls using double threshold method. To avoid the discrimination of quality of service (QoS) caused by the allocation based on fair bandwidth, this algorithm adopts the bandwidth allocation rule based on fair QoS. Simulation results show that the proposed algorithm can accurately and adaptively reserve bandwidth, present satisfactory call blocking probability and greatly reduce handoff call dropping probability, while guarantees the high bandwidth utilization.",A Call Admission Control Algorithm Based on Utility Fairness for Low Earth Orbit Satellite Networks
"['Minmin Chen', 'Kilian Q. Weinberger', 'John Blitzer']","Domain adaptation algorithms seek to generalize a model trained in a source domain to a new target domain. In many practical cases, the source and target distributions can differ substantially, and in some cases crucial target features may not have support in the source domain. In this paper we introduce an algorithm that bridges the gap between source and target domains by slowly adding to the training set both the target features and instances in which the current algorithm is the most confident. Our algorithm is a variant of co-training [7], and we name it CODA (Co-training for domain adaptation). Unlike the original co-training work, we do not assume a particular feature split. Instead, for each iteration of co-training, we formulate a single optimization problem which simultaneously learns a target predictor, a split of the feature space into views, and a subset of source and target features to include in the predictor. CODA significantly out-performs the state-of-the-art on the 12-domain benchmark data set of Blitzer et al. [4]. Indeed, over a wide range (65 of 84 comparisons) of target supervision CODA achieves the best performance.",Co-Training for Domain Adaptation
"['Alexandru Gherega', 'Valentin Pupezescu']","Distributed computing systems provide a highly dynamic behavior which originates from heterogeneous computing and storage resources, heterogeneous users and the variety of submitted applications and finally from the heterogeneous communication that takes part among the systems entities. As such applying global optima oriented allocation algorithms usually produces poor results and heuristics are used instead. We concentrated our experiments around the Sufferage heuristic and its adaptive cluster-aware version XSufferage. Both Sufferage and XSufferage use a centralized design and produce good results for low levels of dynamism and deterministic environments. In real life distributed environments, both heuristics produce poor results. We expose the Sufferage heuristic through a distributed architecture based on a cooperative set of entities, which form a Multi-Agent System, such that the results could be improved. We implemented a new algorithm, based on this architecture, called Distributed XSufferage. In order to test the new algorithm, a series of experiments were developed by simulating two real life Grid environments. A complex set of performance metrics were collected -- flow time, make span, throughput -- both resource and cluster level, utilization -- both resource and cluster level and resources and clusters mean loads. Algorithms produce their allocation solution based on estimates and modeling of system's resources and as such are sensitive to estimation errors. Throughout our experiments DX Sufferage was more robust to such errors compared to the original Sufferage and, respectively, XSufferage heuristics.",Multi-agent Resource Allocation Algorithm Based on the XSufferage Heuristic for Distributed Systems
"['Adam Pauls', 'Daniel N. Klein']","Hierarchical A* (HA*) uses of a hierarchy of coarse grammars to speed up parsing without sacrificing optimality. HA* prioritizes search in refined grammars using Viterbi outside costs computed in coarser grammars. We present Bridge Hierarchical A* (BHA*), a modified Hierarchial A* algorithm which computes a novel outside cost called a bridge outside cost. These bridge costs mix finer outside scores with coarser inside scores, and thus constitute tighter heuristics than entirely coarse scores. We show that BHA* substantially outperforms HA* when the hierarchy contains only very coarse grammars, while achieving comparable performance on more refined hierarchies.",Hierarchical A* Parsing with Bridge Outside Scores
['Tristan Freiberg'],"Fix ã®´> 0, and let p1 =2 ,p 2 =3 ,... be the sequence of all primes. We prove that if (q, a )=1 , then there are infinitely many pairs pr ,p r+1 such that pr É?≠ pr+1 É?≠ a mod q and pr+1 É?? pr <ã®´ log pr. ã®´ ã®´ ã®´ ã®´",Strings of congruent primes in short intervals
"['Juan Carlos Martinez Santos', 'Yunsi Fei']","Program execution can be tampered with by malicious attackers through exploiting software vulnerabilities. Changing the program behavior by compromising control data and decision data has become the most serious threat in computer system security. Although several hardware approaches have been presented to validate program execution, they either incur great hardware overhead or introduce false alarms. We propose a new hardware-based approach by leveraging the existing speculative architectures for runtime program validation. The on-chip branch target buffer (BTB) is utilized as a cache of the legitimate control flow transfers stored in a secure memory region. In addition, the BTB is extended to store the correct program path information. At each indirect branch site, the BTB is used to validate the decision history of previous conditional branches and monitor the following execution path at runtime. Implementation of this approach is transparent to the upper operating system and programs. Thus, it is applicable to legacy code. Because of good code locality of the executable programs and effectiveness of branch prediction, the frequency of control-flow validations against the secure off-chip memory is low. Our experimental results show a negligible performance penalty and small storage overhead.",Leveraging speculative architectures for runtime program validation
"['Toshihiko Komine', 'Toshiaki Yamamoto', 'Satoshi Konishi']","A large number of cells will be deployed to provide high speed services in any places using the Long-Term Evolution (LTE) system. The management of such a large number of cells increases the operating expenditure (OPEX). Self-organizing networks (SON) attracted interest as an effective way to reduce OPEX. One of the main targets in SON is the self-optimization of handover (HO) that realizes mobility robustness. HO optimization algorithms adjust HO parameters between the serving and the reconnected cells based on the HO failure logs and cell selection, which is the procedure used to select a suitable reconnected cell, is very important for HO optimization algorithms. In this paper, we propose a cell selection scheme to enhance the performance of HO optimization. In the proposed scheme, both the uplink and downlink channel quality is considered when selecting a suitable reconnected cell. Through the computer simulation, we can see that the proposed scheme reduces the HO failure rate and the number of HO failures by 3 percentage points and 38%, respectively, compared to the conventional scheme based solely on downlink channel quality.",A proposal of cell selection algorithm for LTE handover optimization
"['Mohammad Masud Hasan', 'Jason P. Jue']","For a large-scale mesh network with dynamic traffic, maintaining the global state information in a centralized fashion is impractical. Hence, distributed schemes are needed to organize nodes and to manage state information in a more localized manner. One such effective scheme for organizing nodes is to cluster the nodes into a hierarchical structure. In this paper, we address the problem of determining the appropriate clustering of nodes for providing scalability in wavelength division multiplexed (WDM) optical networks with dynamic traffic. We present an on-line load-based (or bandwidth availability based) clustering technique that determines clusters adaptively in response to current network conditions. We also consider the problem of dynamic multicast on clustered networks with wavelength conversion capability. We introduce a heuristic using an auxiliary graph model to address routing, wavelength assignment, and traffic grooming jointly. Simulation results demonstrate the feasibility of our approach.",Clustering Large Optical Networks for Distributed and Dynamic Multicast
"['Kaidi Zhao', 'Bing Liu', 'Thomas M. Tirpak', 'Andreas Schaller']","Analyzing data to find trends, correlations, and stable patterns is an important problem for many industrial applications. We propose a new technique based on parallel coordinates visualization. Previous work on parallel coordinates method has shown that they are effective only when variables that are correlated and/or show similar patterns are displayed adjacently. Although current parallel coordinates tools allow the user to manually rearrange the order of variables, this process is very time-consuming when the number of variables is large. Automated assistance is needed. We propose an edit-distance based technique to rearrange variables so that interesting patterns can be easily detected. Our system, V-Miner, includes both automated methods for visualizing common patterns and a query tool that enables the user to describe specific target patterns to be mined/displayed by the system. Following an overview of the system, a case study is presented to explain how Motorola engineers have used V-Miner to identify significant patterns in their product test and design data.",Detecting patterns of change using enhanced parallel coordinates visualization
"['Seungheon Hyeon', 'Yusuk Yun', 'Hyeongdong Kim', 'Seungwon Choi']","This paper presents a simple procedure of obtaining a diversity gain in an antenna-array system with a short interelement separation, typically less than the carrier wavelength. The new technique provides a diversity gain through a noncoherent combination of received signals at each antenna element. The diversity gain arises because, as the number of signal components of the received signal at each antenna element becomes large enough and as the arrival angle of each signal component is distinct from one another, which is a general signal circumstance in most practical code division multiple access (CDMA) signal environments, the amplitudes of the received signals become nearly independent due to the phase difference among the received signals. The diversity gain was referred to as ldquophase diversityrdquo in this paper. The proposed technique is first theoretically analyzed to estimate the performance in terms of pseudorandom-noise-code acquisition, which is verified through extensive computer simulations. Then, through the experimental results that are obtained from a CDMA array-antenna base station system, it has been shown that the performance of noncoherent detection is proportionally improved to the number of antenna elements.",Phase Diversity for an Antenna-Array System With a Short Interelement Separation
"['Yin-Leng Theng', 'Joanna Sin']","Using an e-learning system as a case example of complex information systems, we tested a conceptual framework that expands upon the Technology Acceptance Model (TAM) and incorporate measures of self-efficacy. A survey instrument was developed to gather large samples of users' perceptions on system usability and usefulness. Advanced statistical tests such as structural equation modeling were carried out. The paper concludes with a discussion on efficacy of evaluation techniques and design implications for e-learning systems.","Evaluating Usability and Efficaciousness of an E-learning System: A Quantitative, Model-Driven Approach"
"['Felix Petngang', 'Nadia Baaziz']","Le standard de compression JPEG2000 se caracterise par la diversite des options d'encodage menant a de bons compromis compression/qualite. Cependant, une extension integrant des elements de securite est fortement attendue. Le marquage numerique (watermarking, de l'expression anglaise) est une technologie prometteuse pour securiser la circulation des contenus multimedias. L'incorporation d'un marquage dans la chaine de compression presente l'avantage de prise en compte de la distorsion introduite par la marque dans le processus global de compression, en plus de la possibilite de reduire la complexite d'implementation en exploitant les traitements deja effectues pour la compression. Le travail realise consiste a la conception et au developpement d'un environnement sous MATLAB offrant differentes ouvertures sur l'architecture JPEG2000 et permettant d'incorporer une ou plusieurs methodes de marquage. L'environnement a ete developpe a partir de JJ2000, implementation du standard en langage JAVA. Comme premiere experimentation, un marquage robuste a ete incorpore au module de transformee en ondelettes du codeur et du decodeur JPEG2000.",Conception D'un Environnement Sous Matlab Pour le Marquage D'images en Jpeg2000
"['Vincent W. Freeh', 'David K. Lowenthal', 'Feng Pan', 'Nandini Kappiah', 'Robert Springer', 'Barry Rountree', 'Mark Edward Femal']","Although users of high-performance computing are most interested in raw performance both energy and power consumption has become critical concerns. One approach to lowering energy and power is to use high-performance cluster nodes that have several power-performance states so that the energy-time trade-off can be dynamically adjusted. This paper analyzes the energy-time trade-off of a wide range of applications-serial and parallel-on a power-scalable cluster. We use a cluster of frequency and voltage-scalable AMD-64 nodes, each equipped with a power meter. We study the effects of memory and communication bottlenecks via direct measurement of time and energy. We also investigate metrics that can, at runtime, predict when each type of bottleneck occurs. Our results show that, for programs that have a memory or communication bottleneck, a power-scalable cluster can save significant energy with only a small time penalty. Furthermore, we find that, for some programs, it is possible to both consume less energy and execute in less time by increasing the number of nodes while reducing the frequency-voltage setting of each node",Analyzing the Energy-Time Trade-Off in High-Performance Computing Applications
"['Maynard Falconer', 'Kiran Kumar Kamisetty', 'Adam Norman', 'Konika Ganguly', 'Kristina Morgan', 'Garrison W. Greenwood']","Today's high performance computer systems must have fast, reliable access to memory and I/O devices. Unfortunately, inter-symbol interference, transmission line effects and other noise sources can distort data transfers. Engineers must therefore determine if bus designs have signal integrity  . e., the bus can transfer data with minimal amplitude or timing distortion. One method of determining signal quality on buses is to conduct a set of data transfers and measure various signal parameters at the receiver end. But the tests must be conducted with stressful test patterns that maximize inter-symbol interference to help identify any potential problems. In this paper we describe how an evolutionary algorithm was used to evolve such test patterns. All test results were obtained intrinsically",Using Evolutionary Algorithms for Signal Integrity Checks of High-Speed Data Buses
"['Libo Zhong', 'Fady Alajaji', 'Glen Takahara']","A model for a binary additive noise communication channel with memory is introduced. The channel noise process, which is generated according to a ball sampling mechanism involving a queue of finite length M, is a stationary ergodic Mth-order Markov source. The channel properties are analyzed and several of its statistical and information-theoretical quantities (e.g., block transition distribution, autocorrelation function (ACF), capacity, and error exponent) are derived in either closed or easily computable form in terms of its four parameters. The capacity of the queue-based channel (QBC) is also analytically and numerically compared for a variety of channel conditions with the capacity of other binary models, such as the well-known Gilbert-Elliott channel (GEC), the Fritchman channel, and the finite-memory contagion channel. We also investigate the modeling of the traditional GEC using this QBC model. The QBC parameters are estimated by minimizing the Kullback-Leibler divergence rate between the probability of noise sequences generated by the GEC and the QBC, while maintaining identical bit-error rates (BER) and correlation coefficients. The accuracy of fitting the GEC via the QBC is evaluated in terms of ACF, channel capacity, and error exponent. Numerical results indicate that the QBC provides a good approximation of the GEC for various channel conditions; it thus offers an interesting alternative to the GEC while remaining mathematically tractable.",A Binary Communication Channel With Memory Based on a Finite Queue
"['Eric Saux', 'R??my Thibaud', 'Ki-Joune Li', 'Minhwan Kim']","Triangular Irregular Network (TIN) and Regular Square Grid (RSG) are widely used for representing 2.5 dimensional spatial data. However, these models are not defined from the topographic properties of the terrain (i.e., ridge lines, valley lines, saddle points, etc.). This paper introduces a three-step feature-based approach for topographic properties extraction on scattered elevation data modeled by a TIN. Firstly, a segmentation process extracts homogeneous morphological areas bounded by critical lines and points. Secondly, these lines and points are displaced using a deformable process in order to derive the terrain feature points, lines and areas. Thirdly, a classification process labels any topographic feature. This three-step approach relies on the definition of an adapted model of representation (SPIN) and data structure (DCFL2). The proposed approach is validated on a real case study (Seolak mountain in South Korea). Consistent results with the morphology of terrain are displayed.",A new approach for a topographic feature-based characterization of digital elevation data
"['Jianqing Fan', 'Richard J. Samworth', 'Yichao Wu']","Variable selection in high-dimensional space characterizes many contemporary problems in scientific discovery and decision making. Many frequently-used techniques are based on independence screening; examples include correlation ranking (Fan & Lv, 2008) or feature selection using a two-sample t-test in high-dimensional classification (Tibshirani et al., 2003). Within the context of the linear model, Fan & Lv (2008) showed that this simple correlation ranking possesses a sure independence screening property under certain conditions and that its revision, called iteratively sure independent screening (ISIS), is needed when the features are marginally unrelated but jointly related to the response variable. In this paper, we extend ISIS, without explicit definition of residuals, to a general pseudo-likelihood framework, which includes generalized linear models as a special case. Even in the least-squares setting, the new method improves ISIS by allowing feature deletion in the iterative process. Our technique allows us to select important features in high-dimensional classification where the popularly used two-sample t-method fails. A new technique is introduced to reduce the false selection rate in the feature screening stage. Several simulated and two real data examples are presented to illustrate the methodology.",Ultrahigh Dimensional Feature Selection: Beyond The Linear Model
"['Shuji Suwa', 'Nariyoshi Yamai', 'Kiyohiko Okayama', 'Motonori Nakamura']","In recent years, spam mails intending for ``One-click fraud"" or ``Phishing"" have become increasing. As one anti-spam technology, DNSBL based on the URLs or their corresponding IP addresses in the messages is well used. However, some spam mails that cannot be filtered by conventional DNSBLs get appearing since the spammers create websites using various techniques such as botnet, fast-flux and Wildcard DNS record. To improve the accuracy of filtering spam mails using these techniques, we analyzed DNS record features corresponding to the domain name from the URLs in actual spam mails. According to the result of this analysis, we confirmed that abuse of Wildcard DNS record is one effective criterion for spam filtering.",DNS Resource Record Analysis of URLs in E-Mail Messages for Improving Spam Filtering
"['Babita Majhi', 'Ganapati Panda']",Transmission and storing of high density digital information plays an important role in the present age of information technology. These binary data are distorted while reading out of the recording medium or arriving at the receiver end due to inter symbol interference in the channel. The adaptive channel equalizer alleviates this distortion and reconstructs the transmitted data faithfully. The bacterial foraging optimization (BFO) is a recently developed efficient and derivative free evolutionary computing tool used for optimization purpose. In the present paper we propose a novel nonlinear channel equalizer using BFO algorithm. The recovery performance of the new equalizer is obtained through computer simulation study using nonlinear channels. It is shown that the proposed equalizer offers superior performance both in terms of bit-error-rate and convergence speed compared to the GA based equalizers. In addition it requires substantially less computation during training.,Recovery of Digital Information Using Bacterial Foraging Optimization Based Nonlinear Channel Equalizers
"['Erica Y. Yang', 'Jie Xu', 'Keith H. Bennett']","Several private information retrieval (PIR) schemes were proposed to protect users' privacy when sensitive information stored in database servers is retrieved. However, existing PIR schemes assume that any attack to the servers does not change the information stored and any computational results. We present a novel fault-tolerant PIR scheme (called FT-PIR) that protects users' privacy and at the same time ensures service availability in the presence of malicious server faults. Our scheme neither relies on any unproven cryptographic assumptions nor the availability of tamper-proof hardware. A probabilistic verification function is introduced into the scheme to detect corrupted results. Unlike previous PIR research that attempted mainly to demonstrate the theoretical feasibility of PIR, we have actually implemented both a PIR scheme and our FT-PIR scheme in a distributed database environment. The experimental and analytical results show that only modest performance overhead is introduced by FT-PIR while comparing with PIR in the fault-free cases. The FT-PIR scheme tolerates a variety of server faults effectively. In certain fail-stop fault scenarios, FT-PIR performs even better than PIR. It was observed that 35.82% less processing time was actually needed for FT-PIR to tolerate one server fault.",A fault-tolerant approach to secure information retrieval
"['Steve Clark', 'Hugh F. Durrant-Whyte']",This paper discusses the use of a 77 GHz millimeter wave radar as a guidance sensor for autonomous land vehicle navigation. A test vehicle has been fitted with a radar and encoders that give steer angle and velocity. An extended Kalman filter optimally fuses the radar range and bearing measurements with vehicle control signals to give estimated position and variance as the vehicle moves around a test site. The effectiveness of this data fusion is compared with encoders alone and with a satellite positioning system. Consecutive scans have been combined to give a radar image of the surrounding environment. Data in this format are invaluable for future work on collision detection and map building navigation.,Autonomous land vehicle navigation using millimeter wave radar
"['Jung-Hyun Kim', 'Hyeong-Joon Kwon', 'Kwang-Seok Hong']","In this paper, we suggest and implement an enhanced Wireless Broadband (WiBro) Net.-based five senses multimedia technology using Web-map-oriented mobile Mash-up. The WiBro Net. in this applicative technology supports and allows Web 2.0-oriented various issues such as user-centric multimedia, individual multimedia message exchange between multi-users, and a new media-based information and knowledge sharing / participation without spatiotemporal-dependency. To inspect applicability and usability of the technology, we accomplish various experiments, which include 1) WiBro Net.-based real-time field tests and 2) ISO 9241/11 and /10-based surveys on the user satisfaction by relative experience in comparison with the AP-based commercialized mobile service. As a result, this application provides higher data rate transmission in UP-DOWN air-link and wider coverage region. Also, its average System Usability Scale (SUS) scores estimated at 83.58%, and it relatively held competitive advantage in the specific item scales such as system integration, individualization, and conformity with user expectations.",WiBro Net.-Based Five Senses Multimedia Technology Using Mobile Mash-Up
"['Daniel Bartz', 'Klaus Robert M?¨ller']","Analytic shrinkage is a statistical technique that offers a fast alternative to cross-validation for the regularization of covariance matrices and has appealing consistency properties. We show that the proof of consistency requires bounds on the growth rates of eigenvalues and their dispersion, which are often violated in data. We prove consistency under assumptions which do not restrict the covariance structure and therefore better match real world data. In addition, we propose an extension of analytic shrinkage -orthogonal complement shrinkage- which adapts to the covariance structure. Finally we demonstrate the superior performance of our novel approach on data from the domains of finance, spoken letter and optical character recognition, and neuroscience.",Generalizing Analytic Shrinkage for Arbitrary Covariance Structures
"['Ines M?¨nch', 'Gabriela Lindemann von Trzebiatowski']","Conceptual modeling is an important basis for developing general purpose systems for information and service management. In this paper, we present concepts for the analysis and design of a system of agents in which agents represent interests of individuals or groups. In particular, we develop a semantic model of agent domain knowledge by introducing analytical concepts for service scheduling which is the envisioned application of our prototypical system ChariTime.",ChariTime É?? Concepts of Analysis and Design of an Agent-Oriented System for Appointment Management
"['Kumar Vijay Mishra', 'Anton Kruger', 'Witold F. Krajewski']","The sun is a convenient and frequently employed external radiation source for calibrating weather radar antenna and receiver characteristics. However, changes in solar activity can be a major source of error in interpreting the results of solar calibration for lower frequency bands. The cross-correlation of horizontal and vertical polarization signals, which is zero for perfectly unpolarized electromagnetic radiation, could give non-zero estimates if a quiet sun is not observed by the radar. In this paper, solar scan measurements are made at X-band to detect the effect of solar activity on this cross-correlation coefficient. To facilitate mitigation of instrument-wide errors, we employ multiple XPOL radars to simultaneously observe the sun. Though our experiments during a limited period show that the cross-channel correlation estimates obtained by X-band weather radars remain relatively unaffected by the variations in solar flux, the paper makes suggestions on improving the results.",Monitoring cross-channel correlation solar scan measurements using the Iowa X-band polarimetric radars
['Kevin Deeb'],"Several research projects have recently surfaced to automate the discovery and acquisition of knowledge from information and to facilitate information sharing and reusability in the global network. However, some of these research attempts are limited in scope with respect to proper identification and delivery of customized information. This paper addresses some of these challenges by proposing a high-level ontological and context-based architecture that enables information customization and knowledge organization. The proposed model aims at managing knowledge items through the use of stand-alone computational layers. Ontology is used in this research to describe knowledge representation and structure, whereas context reflects knowledge adaptability to its hosting environment.",A Context-based Ontological Structure for Knowledge Sharing and Customization
"['A.R. Rizwana Shaikh', 'Satish Devane']","Emerging e-commerce activity is giving scope for the design of many new protocols, and to gain confidence, these protocol need to be verified for its designed properties. Specifically protocol used in ecommerce transactions needs to be verified for their security properties. Verification of these protocols is done using the formal verification tools. AVISPA is one of the evolving tools used mainly for verifying security properties. A newly designed electronic payment protocol is verified for its correctness and security properties. This paper presents the use of AVISPA for verifying the security properties of the newly evolved electronic transaction protocol.",Verification of security properties of payment protocol using AVISPA
"['Kurt Plarre', 'Francesco Bullo']","We consider the problem of Kalman filtering when observations are available according to a Bernoulli process. It is known that there exists a critical probability  pc  such that, if measurements are available with probability greater than  pc , then the expected prediction covariance is bounded for all initial conditions; otherwise, it is unbounded for some initial conditions. We show that, when the system observation matrix restricted to the observable subspace is invertible, the known lower bound on  pc  is, in fact, the exact critical probability. This result is based on a novel decomposition of positive semidefinite matrices.",On Kalman Filtering for Detectable Systems With Intermittent Observations
"['Alex Dekhtyar', 'Robert B. Ross', 'V. S. Subrahmanian']","Dyreson and Snodgrass have drawn attention to the fact that, in many temporal database applications, there is often uncertainty about the start time of events, the end time of events, and the duration of events.  When the granularity of time is small (e.g., milliseconds), a statement such as É??Packet  p  was shipped sometime during the first 5 days of January, 1998É?ù leads to a massive amount of uncertainty (5??24??60??60??1000) possibilities.  As noted in Zaniolo et al. [1997], past attempts to deal with uncertainty in databases have been restricted to relatively small amounts of uncertainty in attributes.  Dyreson and Snodgrass have taken an important first step towards solving this problem.  In this article, we first introduce the   syntax of Temporal-Probabilistic (TP) relations and then show how they can be converted to an explicit, significantly more space-consuming form, called Annotated Relations.  We then present a  theoretical annotated temporal algebra  (TATA).  Being explicit, TATA is convenient for specifying how the algebraic operations should behave, but is impractical to use because annotated relations are overwhelmingly large.  Next, we present a  temporal probabilistic algebra  (TPA).  We show that our definition of the TP-algebra provides a correct implementation of TATA despite the fact that it operates on implicit, succinct TP-relations instead of overwhemingly large annotated relations. Finally, we report on timings for an implementation of the TP-Algebra built on   top of ODBC.","Probabilistic temporal databases, I: algebra"
"['Obinna Anya', 'Hissam Tawfik', 'Atulya K. Nagar']","Emerging work models increasingly take the form of loosely structured, often self-organising networks of nimble and virtual knowledge work teams within and between organisations. To model such work patterns requires a different approach from that of traditional workflow management systems. This paper presents the conceptual design of a prototype adaptive and collaborative e-Work environment - e-Workbench, which we are currently developing to enable future collaborative workspaces to adapt to emerging knowledge work models. We argue that with appropriate knowledge of tasks, workspaces will be able to adapt to work, and automatically retrieve contextually relevant knowledge elements from the Web in order to contribute creatively to problem solving and semantically manage shared information among collaborating workers. Our goal is to enable e-Workbench to become, not only a working environment but also, a collaborator and a co-worker as a result of its knowledge of work and creative participation in problem solving",A Conceptual Design of an Adaptive and Collaborative E-Work Environment
"['Heesub Lee', 'Yongmin Kim', 'Alan H. Rowberg', 'Eve A. Riskin']","Displacement estimated interframe (DEI) coding, a coding scheme for 3-D medical image data sets such as X-ray computed tomography (CT) or magnetic resonance (MR) images, is presented. To take advantage of the correlation between contiguous slices, a displacement-compensated difference image based on the previous image is encoded. The best fitting distribution functions for the discrete cosine transform (DCT) coefficients obtained from displacement compensated difference images are determined and used in allocating bits and optimizing quantizers for the coefficients. The DEI scheme is compared with 2-D block discrete cosine transform (DCT) as well as a full-frame DCT using the bit allocation technique of S. Lo and H.K. Huang (1985). For X-ray CT head images, the present bit allocation and quantizer design, using an appropriate distribution model, resulted in a 13-dB improvement in the SNR compared to the full-frame DCT using the bit allocation technique. For an image set with 5-mm slice thickness, the DEI method gave about 5% improvement in the compression ratio on average and less blockiness at the same distortion. The performance gain increases to about 10% when the slice thickness decreases to 3 mm. >",Statistical distributions of DCT coefficients and their application to an interframe compression algorithm for 3-D medical images
"['Blair Neate', 'Warwick Irwin', 'Neville Churcher']","The concept of pagerank has proved successful in allowing search engines to identify important pages in the World Wide Web. In this paper, we describe the application of the pagerank concept to the domain of software in order to derive a new family of metrics, CodeRank, which captures aspects of software not readily obtainable from other metrics. We have implemented a tool, CODERANKER, to compute values of CodeRank metrics using a full semantic model which we have developed. We present some results and discuss the use of CodeRank metrics in their interpretation.",CodeRank: a new family of software metrics
"['Giuseppe Ateniese', 'Michael Steiner', 'Gene Tsudik']","Many modern computing environments involve dynamic peer groups. Distributed simulation, multiuser games, conferencing applications, and replicated servers are just a few examples. Given the openness of today's networks, communication among peers (group members) must be secure and, at the same time, efficient. This paper studies the problem of authenticated key agreement in dynamic peer groups with the emphasis on efficient and provably secure key authentication, key confirmation, and integrity. It begins by considering two-party authenticated key agreement and extends the results to group Diffie-Hellman (1976) key agreement. In the process, some new security properties (unique to groups) are encountered and discussed.",New multiparty authentication services and key agreement protocols
"['Andreas Schlapbach', 'Horst Bunke']","In this paper, we present a new approach to improving the performance of a writer identification system by fusing asynchronous feature streams. Different feature streams are extracted from on-line handwritten text acquired from a whiteboard. The feature streams are used to train a text and language independent writer identification system based on Gaussian mixture models (GMMs). From a stroke consisting of n points, n point-based feature vectors and one stroke-based feature vector are extracted. The resulting feature streams thus have an unequal number of feature vectors. We evaluate different methods to directly fuse the feature streams and show that, by means of feature fusion, we can improve the performance of the writer identification system on a data set produced by 200 different writers.",Fusing Asynchronous Feature Streams for On-line Writer Identification
"['Tielei Zhang', 'Yong Cui', 'Youjian Zhao', 'Lizheng Fu', 'Turgay Korkmaz']","Being an important and yet a challenging problem, the QoS-based routing in the converging Internet has received significant attention from the research community. However, most of the QoS-based routing research is conducted in the context of intra-domain routing, leaving QoS-based inter-domain routing relatively open. In this paper, we specifically investigate how to extend the current inter-domain routing protocol (BGP) to support multiple QoS metrics. With multiple metrics, a BGP speaker has to advertise multiple routes for each destination to its peer. This will increase the routing message overhead and make QoS-aware BGP unscalable to large networks. Therefore, our focus in this paper is to bring QoS extensions to the original BGP while simultaneously providing high performance and scalability. In response to this, we propose path reduction algorithms, i.e., Contribution Based Reduction (CBR) algorithms, to reduce the number of routes advertised by BGP speakers while maximizing the routing success ratio. Extensive simulations show that our schemes achieve high performance with low complexity in terms of message overhead and computation, making the QoS extension to BGP scalable.",Scalable BGP QoS Extension with Multiple Metrics
"['Alexander Wentzel', 'Per Bruheim', 'Anders ??verby', '??yvind M. Jakobsen', 'H?ùvard Sletta', 'Walid Omara', 'David A. Hodgson', 'Trond E. Ellingsen']","Background: Systems biology approaches to study metabolic switching in Streptomyces coelicolor A3(2) depend on cultivation conditions ensuring high reproducibility and distinct phases of culture growth and secondary metabolite production. In addition, biomass concentrations must be sufficiently high to allow for extensive time-series sampling before occurrence of a given nutrient depletion for transition triggering. The present study describes for the first time the development of a dedicated optimized submerged batch fermentation strategy as the basis for highly time-resolved systems biology studies of metabolic switching in S. coelicolor A3(2). Results: By a step-wise approach, cultivation conditions and two fully defined cultivation media were developed and evaluated using strain M145 of S. coelicolor A3(2), providing a high degree of cultivation reproducibility and enabling reliable studies of the effect of phosphate depletion and L-glutamate depletion on the metabolic transition to antibiotic production phase. Interestingly, both of the two carbon sources provided, D-glucose and L-glutamate, were found to be necessary in order to maintain high growth rates and prevent secondary metabolite production before nutrient depletion. Comparative analysis of batch cultivations with (i) both L-glutamate and D-glucose in excess, (ii) L-glutamate depletion and D-glucose in excess, (iii) L-glutamate as the sole source of carbon and (iv) D-glucose as the sole source of carbon, reveal a complex interplay of the two carbon sources in the bacterium's central carbon metabolism. Conclusions: The present study presents for the first time a dedicated cultivation strategy fulfilling the requirements for systems biology studies of metabolic switching in S. coelicolor A3(2). Key results from labelling and cultivation experiments on either or both of the two carbon sources provided indicate that in the presence of D-glucose, L-glutamate was the preferred carbon source, while D-glucose alone appeared incapable of maintaining culture growth, likely due to a metabolic bottleneck at the oxidation of pyruvate to acetyl-CoA.",Optimized submerged batch fermentation strategy for systems scale studies of metabolic switching in Streptomyces coelicolor A3(2)
"['Matthew E. Antone', 'Seth J. Teller']","We describe a linear-time algorithm that recovers absolute camera positions for networks of thousands of terrestrial images spanning hundreds of meters in outdoor urban scenes, under uncontrolled lighting. The algorithm requires no human input or interaction. For real data, it recovers camera pose globally consistent on average to roughly five centimeters, or about four pixels of epipolar alignment. The paper's principal contributions include an extension of Markov chain Monte Carlo estimation techniques to the case of unknown numbers of feature points, unknown occlusion and deocclusion, large scale (thousands of images, and hundreds of thousands of point features), and large dimensional extent (tens of meters of inter-camera baseline, and hundreds of meters of baseline overall). Also, a principled method is given to manage uncertainty on the sphere; a new use of the Hough transform is proposed; and a method for aggregating local baseline constraints into a globally consistent pose set is described.","Scalable, absolute position recovery for omni-directional image networks"
"['Parijat Dube', 'Zhen Liu', 'Sambit Sahu', 'Jeremy I. Silber']","Performance of overlay networks is dependent on last-mile connections, since they require that data traverse these last-mile bottlenecks at each forwarding step. This requires several times more upstream bandwidth than downstream, further exaggerating the asymmetry between down-stream and upstream bandwidth in last-mile technologies. This imbalance can cause packet queuing at the outgoing network interface of forwarding nodes, increasing latency and causing packet losses. We describe a model of a last-mile constrained overlay network and formulate and use it to solve a simplified latency- and bandwidth-bounded overlay construction problem. We observe that queueing delay may be a significant component of the end-to-end delay and approaches ignoring this may potentially result in an overlay network violating the delay and/or loss bounds. We observe that allowing a small amount of loss, it is possible to support a significantly large number of nodes. For a given end to end delay and loss bound we identify feasible degree (fan out) of each nodes. Our study sheds insights which provide engineering guidelines for designing overlays accounting for last mile problem in the Internet.",Last mile problem in overlay design
"['Matthew Butler', 'Dimitar Kazakov']","This paper introduces a novel forecasting algorithm that is a blend of micro and macro modelling perspectives when using Artificial Intelligence (AI) techniques. The micro component concerns the fine-tuning of technical indicators with population based optimization algorithms. This entails learning a set of parameters that optimize some economically desirable fitness function as to create a dynamic signal processor which adapts to changing market environments. The macro component concerns combining the heterogeneous set of signals produced from a population of optimized technical indicators. The combined signal is derived from a Learning Classifier System (LCS) framework that combines population based optimization and reinforcement learning (RL). This research is motivated by two factors, that of non-stationarity and cyclical profitability (as implied by the adaptive market hypothesis [10]). These two properties are not necessarily in contradiction but they do highlight the need for adaptation and creation of new models, while synchronously being able to consult others which were previously effective. The results demonstrate that the proposed system is effective at combining the signals into a coherent profitable trading system but that the performance of the system is bounded by the quality of the solutions in the population.",A learning adaptive Bollinger band system
"['Dominik Joho', 'Maren Bennewitz', 'Sven Behnke']","We present an algorithm for estimating the fundamental frequency in speech signals. Our approach incorporates models of voiced speech on three levels. First, we estimate the pitch for each time frame based on its harmonic structure using non-negative matrix factorization. The second level utilizes temporal pitch continuity to extract partial pitch contours. Thirdly, we incorporate statistics of the succession of voiced segments to aggregate partial contours to the final contour of an utterance. We evaluate our approach on the Keele database. The experimental results show the robustness of our method for noisy speech, and the good performance for clean speech in comparison with state-of-the-art algorithms.",Pitch Estimation using Models of Voiced Speech on Three Levels
"['Peter J. Sherman', 'Kang-Ning Lou']","A recently developed point spectrum identification procedure based on a family of AR and ML spectral estimates is exploited to arrive at a mixed spectrum identification procedure. To this end, a variety of properties of the AR and ML estimates as a function of model order are described. These properties relate to amplitude convergence, resolution and a characterization of the AR spectral artifact which is used to arrive at improved continuous spectral estimates. A variety of examples are presented. >",On the family of ML spectral estimates for mixed spectrum identification
"['Robert A. Whiteside', 'Jerrold S. Leichter']","A distributed parallel processing system based on the LINDA programming constructs has been implemented on a local area network of computers. This system allows a single application program to utilize many machines on the network simultaneously. Several applications have been implemented on the network at Sandia National Laboratories and have achieved performances considerably faster than that of a Cray-1S. Several collections of machines have been used including up to eleven DEC VAXes, three Sun/3 workstations, and a PC.",Using Linda for supercomputing on a local area network
"['Michael Redeker', 'Bruce F. Cockburn', 'Duncan G. Elliott', 'Yunan Xiang', 'Sue Ann Ung']","Multilevel dynamic random-access memory (MLDRAM) attempts to increase the storage density of semiconductor memory without further reducing the lithographic dimensions. It does so by using more than two possible signal voltages on each cell capacitor thus permitting more than one bit to be stored in each cell. Birk's MLDRAM scheme has several promising properties, including robust locally-generated data signal and reference signal generation, and fast flash-conversion sensing. This paper describes a fault model for Birk's MLDRAM that was developed by considering the behaviors produced by likely defects at the schematic level. The resulting behaviors include faults that are detectable as observable logical errors, faults that can be detected by current measurements, and faults that, in the worst case, can only be detected by testing for degraded noise margins. All Boolean faults in the fault model can be detected by an efficient test whose length grows linearly in the number of cells. The narrower noise margins in MLDRAM will make it more vulnerable to pattern sensitivities. We also developed a linear test that evaluates worst-case sensing conditions.",Fault modeling and pattern-sensitivity testing for a multilevel DRAM
"['Li-Chun Wang', 'Wei-Cheng Liu']","In this paper, we present a computable bit error rate (BER) expression for the binary signals in the IEEE 802.15.3a ultra-wideband (UWB) channel. In the literature, the impacts of the RAKE receiver's finger numbers and lognormal shadowing on the BER performance have not been reported yet. We propose a characteristic function (CF) based BER formula to overcome the convergence problem of the existing moment generating function (MGF) approach when the BER calculation takes account of the shadowing effect. Simulation results demonstrate that the proposed CF-based computable BER formula can accurately estimate the complete effects of the cluster and ray arrival processes, the lognormal fading as well as shadowing, and the finger numbers at RAKE receivers.",Bit error rate analysis in IEEE 802.15.3a UWB channels
"['Dipanjan Sengupta', 'Resve Saleh']","Among the different methods of reducing power for core-based system-on-chip (SoC) designs, the  voltage-island   technique  has gained in popularity. Assigning cores to the different supply voltages and floorplanning to create contiguous voltage islands are two important steps in the design process. We propose a new application-driven approach to voltage partitioning and island creation with the objective of reducing overall SoC power, area, and floorplanner runtime. Given an application power-state machine (PSM), we first identify the suitable range of supply voltages for each core. Then, we generate the discrete voltage assignment table using a heuristic technique. Next, we describe a methodology of reducing the large number of available choices from the voltage assignment table down to a useful set using the application PSM. We partition the cores into islands, using a cost function that gradually shifts from a power-based assignment to a connectivity-based one. Compared with previously reported techniques, a 9.4% reduction in power and 8.7% reduction in area are achieved using our approach, with an average runtime improvement of 2.4 times.",Application-Driven Voltage-Island Partitioning for Low-Power System-on-Chip Design
"['Amit Gupta', 'S. Sudarshan', 'Sundar Vishwanathan']","Complex queries are becoming commonplace, with the growing use of decision support systems. Decision support queries often have a lot of common sub-expressions within each query, and queries are often run as a batch. Multi query optimization aims at exploiting common sub-expressions, to reduce the evaluation cost of queries, by computing them once and then caching them for future use, both within individual queries and across queries in a batch. In case cache space is limited, the total size of sub-expressions that are worth caching may exceed available cache space. Prior work in multi query optimization involves choosing a set of common sub-expressions that fit in available cache space, and once computed, retaining their results across the execution of all queries in a batch. Such optimization algorithms do not consider the possibility of dynamically changing the cache contents. This may lead to sub-expressions occupying cache space even if they are not used by subsequent queries. The available cache space can be best utilized by evaluating the queries in an appropriate order and changing the cache contents as queries are executed. We present several algorithms that consider these factors, in order to reduce the cost of query evaluation.",Query scheduling in multi query optimization
"['Yi Chi Wu', 'Huei Ru Tseng', 'Wuu Yang', 'Rong Hong Jan']","In Distributed Denial-of-Service (DDoS) Attack, an attacker breaks into many innocent computers (called zombies). Then, the attacker sends a large number of packets from zombies to a server, to prevent the server from conducting normal business operations. We design a DDoS-detection system based on a decision-tree technique and, after detecting an attack, to trace back to the attacker's locations with a traffic-flow pattern-matching technique. Our system could detect DDoS attacks with the false positive ratio about 1.2-2.4%, false negative ratio about 2-10%, and find the attack paths in traceback with the false negative rate 8-12% and false positive rate 12-14%.",DDoS detection and traceback with decision tree and grey relational analysis
"['Daeui Park', 'Semin Lee', 'Dan M. Bolser', 'Michael Schroeder', 'Michael Lappe', 'Donghoon Oh', 'Jong Bhak']","Motivation: Many genomes have been completely sequenced. However, detecting and analyzing their protein--protein interactions by experimental methods such as co-immunoprecipitation, tandem affinity purification and Y2H is not as fast as genome sequencing. Therefore, a computational prediction method based on the known protein structural interactions will be useful to analyze large-scale protein--protein interaction rules within and among complete genomes.#R##N##R##N#Results: We confirmed that all the predicted protein family interactomes (the full set of protein family interactions within a proteome) of 146 species are scale-free networks, and they share a small core network comprising 36 protein families related to indispensable cellular functions. We found two fundamental differences among prokaryotic and eukaryotic interactomes: (1) eukarya had significantly more hub families than archaea and bacteria and (2) certain special hub families determined the topology of the eukaryotic interactomes. Our comparative analysis suggests that a very small number of expansive protein families led to the evolution of interactomes and seemed tohave played a key role in species diversification.#R##N##R##N#Contact: jong@kribb.re.kr#R##N##R##N#Supplementary information: http://interactomics.org",Comparative interactomics analysis of protein family interaction networks using PSIMAP (protein structural interactome map)
['Taro Narahara'],"This talk presents results of a case study from a course called ""History of Games"" offered at the School of Art + Design at New Jersey Institute of Technology. After analyzing various traditional board games and their mechanics, students explore the possibility of producing their own original board games by altering various existing game structures through application of new technologies such as digital prototyping, including laser cutting and 3-D printing, and microcontroller technologies. In principle, we can fully emulate the playing of a board game such as Monopoly inside a computer, digitally. However, there is a certain quality of physicality with traditional board games that cannot be experienced through games in fully digital environments. The existence of tangible game pieces, boards, and real human players can produce cooperation, engagement, and tensions unlike those in video games and AR-based applications. Through this project-based course, students further explore how new technologies can help in developing new types of games.",Exploring board game design using digital technologies
"['Dmytro Dyachuk', 'Ralph Deters']","By defining workflows, existing services are put into novel contexts and exposed to different workloads, which in turn can result in unexpected behaviors. This paper examines the chaotic behavior of sequential workflows in overload situations and discusses the use of call-contexts as a means of avoiding them.",Using Call-Context to Prevent the Emergence of Chaotic Workflow Behaviors in Overload Situations
"['Ganapathy Naganathan', 'A. H. Soni']","A critical modeling of a flexible manipulator will involve a thorough understanding of the issues specific to flexible manipulator performance and an experimental investigation of a physical system. A comprehensive dynamic model of a flexible manipulator has been presented in this paper together with a preliminary experimental investigation that examines the performance of this comprehensive model. The analytical model recognizes in particular, the coupling characteristics of the deformations in a flexible manipulator configuration and realizes an efficient and dedicated finite element scheme to model general spatial manipulator configurations with revolute and prismatic pairs. The experimental results indicate a favorable level of performance for the application of the special finite element for a practical manipulator.",An analytical and experimental investigation of flexible manipulator performance
"['Weng-Fai Wong', 'Chung-Kwong Yuen']","The authors propose BIDDLE, a direct execution architecture for Lisp based on both data- and demand-driven principles. A priority queuing mechanism is used to control the parallelism and the workload of the processing elements. Also introduced is a novel mechanism whereby the sequential semantics of Lisp can be enforced in such a way as not to reduce the parallelism too drastically. BIDDLE is, therefore, aimed at achieving a balance between eager and lazy evaluation, sequential semantics, and parallelism. At present, BIDDLE exists only on paper and is a long way from hard ware implementation. >",BIDDLE: a bidirectional data driven Lisp engine
"['Marco Dorigo', 'Gianni A. Di Caro', 'Luca Maria Gambardella']","This article presents an overview of recent work on ant algorithms, that is, algorithms for discrete optimization that took inspiration from the observation of ant colonies' foraging behavior, and introduces the ant colony optimization (ACO) metaheuristic. In the first part of the article the basic biological findings on real ants are reviewed and their artificial counterparts as well as the ACO metaheuristic are defined. In the second part of the article a number of applications of ACO algorithms to combinatorial optimization and routing in communications networks are described. We conclude with a discussion of related work and of some of the most important aspects of the ACO metaheuristic.",Ant algorithms for discrete optimization
"['Sanket Gupte', 'Mohamed F. Younis']","Traffic congestion has become a daily problem that most people suffer. This not only impacts the productivity of the population but also poses a safety risk. Most of the technologies for intelligent highways focus on safety measures and increased driver awareness, and expect a centralized management for the traffic flow. This paper presents a new approach for enabling autonomous and adaptive traffic management through vehicular networks. By allowing data exchange between vehicles about route choices, congestions and traffic alerts, a vehicle makes a decision on the best course of action. Unlike centralized schemes that provide recommendations, our VANET-based Autonomous Management (VAM) approach factors in the destination and routes of nearby vehicles in deciding on whether rerouting is advisable. In addition, VAM leverages the presence of smart traffic lights and enables coordination between vehicles and lightcontrollers in order to ease congestion. The collective effect of all vehicles will be an autonomous reshape of the traffic pattern based on their destinations and road conditions. The simulation results demonstrate the advantage of VAM.",Vehicular networking for intelligent and autonomous traffic management
"['Richard Suselbeck', 'Gregor Schiele', 'Christian Becker']","Cloud gaming has recently been proposed as an alternative to traditional video game distribution. With this approach, the entire game is stored, run and rendered on a remote server. Player input is forwarded to the server via the Internet and the game's output is returned as a video stream. This adds network delay, which can negatively impact the gameplay. The delay is acceptable as long as the user is located geographically close to the cloud servers. However, for Massively Multiplayer Online Games (MMOGs), this delay is added on top of the existing delay between MMOG client and server. As MMOGs are highly delay-sensitive, this can significantly degrade their playability. To deal with this issue, we propose to use peer-to-peer techniques to distribute the MMOG server functionality and place it at the cloud server centers. This allows us to reduce the additional delay introduced by running the MMOG clients in the cloud.",Peer-to-peer support for low-latency Massively Multiplayer Online Games in the cloud
"['Lorenz Schauer', 'Martin Werner', 'Philipp Marcus']","The rapid deployment of smartphones as all-purpose mobile computing systems has led to a wide adoption of wireless communication systems such as Wi-Fi and Bluetooth in mobile scenarios. Both communication systems leak information to the surroundings during operation. This information has been used for tracking and crowd density estimations in literature. However, an estimation of pedestrian flows has not yet been evaluated with respect to a known ground truth and, thus, a reliable adoption in real world scenarios is rather difficult. With this paper, we fill in this gap. Using ground truth provided by the security check process at a major German airport, we discuss the quality and feasibility of pedestrian flow estimations for both Wi-Fi and Bluetooth captures. We present and evaluate three approaches in order to improve the accuracy in comparison to a naive count of captured MAC addresses. Such counts only showed an impractical Pearson correlation of 0.53 for Bluetooth and 0.61 for Wi-Fi compared to ground truth. The presented extended approaches yield a superior correlation of 0.75 in best case. This indicates a strong correlation and an improvement of accuracy. Given these results, the presented approaches allow for a practical estimation of pedestrian flows.",Estimating crowd densities and pedestrian flows using wi-fi and bluetooth
"['Linda Court Salisbury', 'Fred M. Feinberg']","Analysts often rely on methods that presume constant stochastic variance, even though its degree can differ markedly across experimental and field settings. This reliance can lead to misestimation of effect sizes or unjustified theoretical or behavioral inferences. Classic utility-based discrete-choice theory makes sharp, testable predictions about how observed choice patterns should change when stochastic variance differs across items, brands, or conditions. We derive and examine the implications of assuming constant stochastic variance for choices made under different conditions or at different times, in particular, whether substantive effects can arise purely as artifacts. These implications are tested via an experiment designed to isolate the effects of stochastic variation in choice behavior. Results strongly suggest that the stochastic component should be carefully modeled to differ across both available brands and temporal conditions, and that its variance may be relatively greater for choices made for the future. The experimental design controls for several alternative mechanisms (e.g., flexibility seeking), and a series of related models suggest that several econometrically detectable explanations like correlated error, state dependence, and variety seeking add no explanatory power. A series of simulations argues for appropriate flexibility in discrete-choice specification when attempting to detect temporal stochastic inflation effects.","Alleviating the Constant Stochastic Variance Assumption in Decision Research: Theory, Measurement, and Experimental Test"
"['Yijun Liu', 'Pinghua Chen', 'Guobo Xie', 'Zhusong Liu', 'Zhenkun Li']","Sensor network nodes have a very tight power budget and the power efficiency is the biggest design concern in sensor network circuits. A general-purpose processor (e.g. an ARM processor) is not efficient to execute encryption algorithms because it has no special instructions to support encryption operations, for example very often-used permutation operations. In the paper, we propose a low-power ASIC encryption coprocessor for sensor network nodes. A DES algorithm is used because the algorithm does not include power-hungry and complex mathematic operations, such as multiplication, division and addition. An asynchronous logic style is used to design the coprocessor. With an asynchronous controller, a global clock is not necessary when idle, resulting in zero standby dynamic power. Using the DES coprocessor, the power consumed by encryption can be saved by 4 orders of magnitude than a pure software calculation.",The Design of a Low-Power Asynchronous DES Coprocessor for Sensor Network Encryption
"['Ruth I. Murrugarra', 'William A. Wallace']","Engineering ethics education is taking on increasing importance worldwide, but in Chile the percentage of universities that have a mandatory course concerning ethics is still small. Traditionally, Chilean universities with existing ethics courses teach them using a philosophical or theological perspective, limited to occidental theories, and usually from a Christian point of view. This article studies the impact of a new methodology and technique to teach ethics in Chile: case-based, non-normative, and with a critical-descriptive approach. An empirical study is conducted to assess the relative impact of an ethics class on students individual and inherent moral values and attitudes, and understand the factors that contribute to this impact. Results indicate that even though the importance of religion in Chile is decreasing, it is still a major source of studentsã®´ ethical principles and moral values. In addition, results suggest that a change in moral values develops when discussions among groups with different points of view occur.",The effect of a stand-alone ethics course in Chilean engineering students' attitudes
"['Jan Hlavicka', 'Stanislav Racek', 'Pavel èˇmrha']","The paper presents an alternative approach to the formal specification and validation of distributed asynchronous algorithms. It begins with a syntactically correct description of the algorithm whose correctness is then to be validated. The validation of the algorithm is based on the process-oriented discrete simulation and permits a partial correctness validation of the algorithm implemented by a program. The suggested method enables to model independent activity of several processors (using pseudo-parallel processes) in simulation time and to model communication channels with defined time behavior and failure semantics. Using the approach it is easy to add other processes like model of system's environment, fault injector and state observer. The method is described with the aid of a simple C-based validation tool called C-Sim. The utilization of C-Sim requires only slight changes in C-coded implementation of the verified algorithm. An example of validation of distributed election algorithm with the presence of faults is presented.",Functional validation of fault-tolerant asynchronous algorithms
"['Jiyoung Woo', 'Hsinchun Chen']","Social media is being increasingly used as a communication channel. Among social media, web forums, where people in online communities disseminate and receive information by interaction, provide a good environment to examine information diffusion. In this research, we aim to understand the mechanisms and properties of the information diffusion in the web forum. For that, we model topic-level information diffusion in web forums using the baseline epidemic model, the SIR(Susceptible, Infective, and Recovered) model, frequently used in previous research to analyze disease outbreaks and knowledge diffusion. In addition, we propose an event-driven SIR model that reflects the event effect on information diffusion in the web forum. The proposed model incorporates the effect of news postings on the web forum. We evaluate two models using a large longitudinal dataset from the web forum of a major company. The event-SIR model outperforms the SIR model in fitting on major spikey topics that have peaks of author participation.",An event-driven SIR model for topic diffusion in web forums
['Kokichi Sugihara'],A robust method for finding points of intersection of line segments in a 2-D plane is presented. The plane is subdivided by Delaunay triangulation to localize areas where points of intersection exist and to guarantee the topological consistency of the resulting arrangement. The subdivision is refined by inserting midpoints recursively until the areas containing points of intersection are sufficiently localized. The method is robust in the sense that it does not miss points of intersection that are easily detectable when costly line-pair checking is performed. The algorithm is adaptive in the sense that most of the computational cost is incurred for the areas where finding points of intersection is difficult. >,An intersection algorithm based on Delaunay triangulation
"['James Frew', 'Rajendra Bose']","The Earth System Science Workbench (ESSW) is a non-intrusive data management infrastructure for researchers who are also data publishers. An implementation of ESSW to track the processing of locally received satellite imagery is presented, demonstrating the Workbench's transparent and robust support for archiving and publishing data products. ESSW features a Lab Notebook metadata service, an ND-WORM (No Duplicate-Write Once Read Many) storage service, and Web user interface tools. The Lab Notebook logs processes (experiments) and their relationships via a custom API to XML documents stored in a relational database. The ND-WORM provides a managed storage archive for the Lab Notebook by keeping unique file digests and name-space meta-data, also in a relational database. ESSW Notebook tools allow project searching and ordering, and file and meta-data management.",Earth System Science Workbench: a data management infrastructure for earth science products
"['Navin Budhiraja', 'Keith Marzullo']",The authors derive lower bounds and the corresponding optimal protocols for three parameters for synchronous primary-backup systems. They compare their results with similar results for active replication in order to determine whether the common folklore on the virtues of the two approaches can be shown formally. They also extend some of their results to asynchronous primary-backup systems. They implement an important subclass of primary-backup protocols that they call 0-blocking. These protocols are interesting because they introduce no additional protocol related delay into a failure-free service request. Through implementing these protocols the authors hope to determine the appropriateness of their theoretical system model and uncover other practical advantages or limitations of the primary-backup approach. >,Highly-available services using the primary-backup approach
"['Yun-Ju Lee', 'Tae-Beom Lim', 'Kyung Won Kim', 'Jae Won Moon']","In an audiovisual scene consisting of high capacity multimedia objects, when a scene change, which an object is inserted, deleted, or replaced in real time by user interaction, is required, the object priority order compositor for an multimedia player according to the present invention performs rendering of objects based on priority order of objects. The compositor searches priority order of objects, and makes it possible to arbitrarily change order of objects and further to perform rendering of only objects requiring reconstruction. Thus, the object priority order compositor provides re-usability and availability of multimedia data. Further, the object priority order compositor renders efficient multimedia data processing possible by providing a user with a dynamic scene.",Efficient composition of media object for multimedia scene rendering
"['Roni Potasman', 'Joseph Lis', 'Alexandru Nicolau', 'Daniel D. Gajski']","A new approach called Percolation Based Synthesis for the scheduling phase of High Level Synthesis (HLS) is presented. We discuss some new techniques (which are implemented in our tools) for compaction of flow graphs beyond basic blocks limits, which can produce order of magnitude speed ups versus serial execution. Our algorithm applies to programs with  conditional jumps, loops  and  multicycle pipelined operations . In order to schedule under resource constraints we start by first finding the  optimal schedule  (without constraints) and then add heuristics to map the optimal schedule onto the given system. We argue that starting from an optimal schedule is one of the most important factors in scheduling because it offers the user flexibility to tune the heuristics and gives him a good bound for the resource constrained schedule. This scheduling algorithm is integrated with synthesis tool which uses VHDL as input description and produces a  structural netlist  of generic register-transfer components and a  unit based control table  as output. We show that our algorithm obtains better results than previously published algorithms.",Percolation based synthesis
"['Marko H??nnik??inen', 'J. Knuutila', 'Timo D. H??m??l??inen', 'J. Saarinen']","Specification and Description Language (SDL) is a high abstraction level system design language with a clear graphical notation. Because of formal presentation, an SDL model can be automatically converted into source C code for implementation. However, the high abstraction level creates a conceptual gap between a general SDL model and its implementation in a final operational platform. The paper studies the SDL development of an embedded Medium Access Control (MAC) protocol for a wireless local area network (WLAN) demonstrator. The SDL design flow for the protocol is first started by architectural design without target platform dependencies. Functionality is added to the model using the top-down design approach. Functional simulations are used for verifying the operation of the protocol. Next, the performance is estimated using performance simulations in a workstation environment. Performance improvements can be achieved by optimising the SDL model.",Using SDL for implementing a wireless medium access control protocol
"['Ana Cristina', 'Bicharra Garcia']","All I know is that I know nothing (Socratic Ignorance). The world is an increasingly complex with problems that require swift resolution. Although knowledge is widely available, be it stored in companiesÉ?? databases or spread over the Internet, humans have intrinsic limitations for handling very large volumes of information or keeping track of frequent updates in a constantly changing world. Moreover, human reasoning is highly intuitive and potentially biased. Decision-making is often based on rules of thumb instead of systematic analysis with full understanding of decisionsÉ?? context. Computer systems that manage knowledge to thoroughly explore the context and the range of alternatives may improve human decision-making by making people aware of possible misconceptions and biases. Computer systems are also limited in their potential usage due to the frame problem. Systems are not aware of their ignorance, thus they cannot surplus human intelligence, but they may be useful complement to humanÉ??s intelligence. The objective of this paper is to present the AGUIA model for amplifying human intelligence, based on agentÉ??s technology, for task-oriented contexts. AGUIA uses domain ontology and task scripts for handling formal and semiformal knowledge bases, thereby helping to systematically (1) explore the range of alternatives, (2) interpret the problem and the context, and (3) maintain É??awarenessÉ?ù of the problem. As for humans, knowledge is a fundamental resource for AGUIA performance. AGUIAÉ??s knowledge base keeps updating its content, in the background, during interaction with humans, either through identified individuals or through anonymous mass contribution. The feasibility and benefits of AGUIA were demonstrated in many different fields, such as engineering design, fault diagnosis, accident investigation and online interaction with the government. The experiments considered a set of criteria including: product cost, number of explored alternatives, usersÉ?? problem understanding and usersÉ?? awareness of problem context changes. Results indicate AGUIA can actually improve human problem solving capacity in many different areas.",AGUIA: Agents Guidance for Intelligence Amplification in Goal Oriented Tasks
"['Pedro Tiago Martins Batista', 'Carlos Silvestre', 'Paulo Jorge Ramalho Oliveira']","This paper presents a set of filters with globally asymptotically stable error dynamics for source localization and navigation, in 3-D, based on direction measurements from the agent (or vehicle) to the source, in addition to relative velocity readings of the agent. Both the source and the agent are allowed to have constant unknown drift velocities and the relative drift velocity is also explicitly estimated. The observability of the system is studied and realistic simulation results are presented, in the presence of measurement noise, that illustrate the performance of the achieved solutions. Comparison results with the Extended Kalman Filter are also provided and similar performances are achieved.",Globally asymptotically stable filters for source localization and navigation aided by direction measurements
"['Alex Shye', 'Berkin ??zisikyilmaz', 'Arindam Mallik', 'Gokhan Memik', 'Peter A. Dinda', 'Robert P. Dick', 'Alok N. Choudhary']","The ultimate goal of computer design is to satisfy the end-user. In particular computing domains, such as interactive applications, there exists a variation in user expectations and user satisfaction relative to the performance of existing computer systems. In this work, we leverage this variation to develop more efficient architectures that are customized to end-users. We first investigate the relationship between microarchitectural parameters and user satisfaction. Specifically, we analyze the relationship between hardware performance counter (HPC) readings and individual satisfaction levels reported by users for representative applications. Our results show that the satisfaction of the user is strongly correlated to the performance of the underlying hardware. More importantly, the results show that user satisfaction is highly user-dependent. To take advantage of these observations, we develop a framework called Individualized Dynamic Voltage and Frequency Scaling (iDVFS). We study a group of users to characterize the relationship between the HPCs and individual user satisfaction levels. Based on this analysis, we use artificial neural networks to model the function from HPCs to user satisfaction for individual users. This model is then used online to predict user satisfaction and set the frequency level accordingly. A second set of user studies demonstrates that iDVFS reduces the CPU power consumption by over 25% in representative applications as compared to the Windows XP DVFS algorithm.",Learning and Leveraging the Relationship between Architecture-Level Measurements and Individual User Satisfaction
"['Michael Massoth', 'Thomas Bingel']","The paper compares the performance of different traditional mobile payment service concepts with a state of the art NFC-based mobile payment solution. The goal is to evaluate the different mobile payment concepts, not their software implementation, from a performance and end-to-end service duration time point of view. Overall, there have been five different mobile payment services developed, implemented and benchmarked for the concept comparison, namely: Interactive Voice Response, Short Message Service, Wireless Application Protocol, One Time Password Generator as well as a solution based on Near Field Communication.",Performance of Different Mobile Payment Service Concepts Compared with a NFC-Based Solution
"['Moi-Tin Chew', 'Tatt-Huong Tham', 'Ye Chow Kuang']","Maintaining quality and reliability of operation of remotely located electrical machines as well as power supply equipment by monitoring their load is an important task in modern practical electrical engineering. This paper presents a low-cost efficient, robust and relatively simple load monitoring electronic system based on the use of special encapsulated temperature sensors-data loggers (so-called Thermochron iButtons) combined with application of 1-wire data communications implemented on the power supply line, dedicated data communication line with off-line data transfer.",Electrical Power Monitoring System Using Thermochron Sensor and 1-Wire Communication Protocol
['A. Lee'],"The worldwide concern about the gender gap in information technology and the lack of woman participation in computer science has been attributed to the different cultural influences to which boys and girls are subject. In The University of Hong Kong, girls achieved greater improvements in their computer skills than their male counterparts after completing one year of studies. Recognising their own progress has, in turn, boosted their confidence in using IT. The young women's estimates of their skill levels have doubled over the years from 1998 to 2000. Despite this recorded acceleration at the end of the academic years, girls were less confident of their abilities and possessed lower IT skill levels than boys before starting their university education, as found in surveys of freshmen's computer skills. This study compares the responses of student participants of the HKU/IBM Notebook Computer Programme, which started in 1998, in the self-reported IT skills and attitudes of male and female students, in surveys conducted both at the beginning and again at the end of the freshman year. It also examines the achievement scores of the IT Proficiency Tests and the 'Foundations to Information Technology' courses administered for the student IT requirement for graduation.",Undergraduate students' gender differences in IT skills and attitudes
"['Mario Latendresse', 'Markus Krummenacker', 'Miles Trupp', 'Peter D. Karp']","Motivation: Flux balance analysis (FBA) is a well-known technique for genome-scale modeling of metabolic flux. Typically, an FBA formulation requires the accurate specification of four sets: biochemical reactions, biomass metabolites, nutrients and secreted metabolites. The development of FBA models can be time consuming and tedious because of the difficulty in assembling completely accurate descriptions of these sets, and in identifying errors in the composition of these sets. For example, the presence of a single non-producible metabolite in the biomass will make the entire model infeasible. Other difficulties in FBA modeling are that model distributions, and predicted fluxes, can be cryptic and difficult to understand.#R##N##R##N#Results: We present a multiple gap-filling method to accelerate the development of FBA models using a new tool, called MetaFlux, based on mixed integer linear programming (MILP). The method suggests corrections to the sets of reactions, biomass metabolites, nutrients and secretions. The method generates FBA models directly from Pathway/Genome Databases. Thus, FBA models developed in this framework are easily queried and visualized using the Pathway Tools software. Predicted fluxes are more easily comprehended by visualizing them on diagrams of individual metabolic pathways or of metabolic maps. MetaFlux can also remove redundant high-flux loops, solve FBA models once they are generated and model the effects of gene knockouts. MetaFlux has been validated through construction of FBA models for Escherichia coli and Homo sapiens.#R##N##R##N#Availability: Pathway Tools with MetaFlux is freely available to academic users, and for a fee to commercial users. Download from: biocyc.org/download.shtml.#R##N##R##N#Contact: mario.latendresse@sri.com#R##N##R##N#Supplementary information:Supplementary data are available at Bioinformatics online.",Construction and completion of flux balance models from pathway databases
"['Vikas Kumar', 'Rajkumar Venkatesan', 'Tim Bohling', 'Denise Beckmann']","Customer management activities at firms involve making consistent decisions over time, about: a which customers to select for targeting, b determining the level of resources to be allocated to the selected customers, and c selecting customers to be nurtured to increase future profitability. Measurement of customer profitability and a deep understanding of the link between firm actions and customer profitability are critical for ensuring the success of the above decisions. We present the case study of how IBM used customer lifetime value CLV as an indicator of customer profitability and allocated marketing resources based on CLV. CLV was used as a criterion for determining the level of marketing contacts through direct mail, telesales, e-mail, and catalogs for each customer. In a pilot study implemented for about 35,000 customers, this approach led to reallocation of resources for about 14% of the customers as compared to the allocation rules used previously which were based on past spending history. The CLV-based resource reallocation led to an increase in revenue of about $20 million a tenfold increase without any changes in the level of marketing investment. Overall, the successful implementation of the CLV-based approach resulted in increased productivity from marketing investments. We also discuss the organizational and implementation challenges that surrounded the adoption of CLV in this firm.",Practice Prize Report---The Power of CLV: Managing Customer Lifetime Value at IBM
"['Kazuki Takeda', 'Fumiyuki Adachi']","Uplink multi-user direct sequence-code division multi-access (DS-CDMA) suffers from strong multi-user interference (MUI) and self inter-chip interference (ICI) caused by severe frequency-selective fading. In this paper, we propose a joint Tx/iterative Rx frequency-domain equalization (FDE) based on minimum mean square error (MMSE) criterion and successive MUI cancellation (MUIC) for DS-CDMA uplink. In the proposed scheme, each user applies one-tap Tx FDE before transmitting signal. At the base station, joint one-tap Rx FDE and successive MUIC is iteratively performed. The FDE weights of users and base station are jointly optimized based on the MMSE criterion in order to reduce MUI and ICI while exploiting channel frequency-selectivity. Computer simulation results show that the proposed scheme provides much improved bit error rate (BER) performance than the conventional iterative Rx MMSE-FDE with successive MUIC.",Multi-User Joint Tx/Iterative Rx MMSE-FDE and Successive MUI Cancellation for Uplink DS-CDMA
['Alexander M. Clark'],"A collection of primitive operations for molecular diagram sketching has been developed. These primitives compose a concise set of operations which can be used to construct publication-quality 2 D coordinates for molecular structures using a bare minimum of input bandwidth. The input requirements for each primitive consist of a small number of discrete choices, which means that these primitives can be used to form the basis of a user interface which does not require an accurate pointing device. This is particularly relevant to software designed for contemporary mobile platforms. The reduction of input bandwidth is accomplished by using algorithmic methods for anticipating probable geometries during the sketching process, and by intelligent use of template grafting. The algorithms and their uses are described in detail.",Basic primitives for molecular diagram sketching
"['Wen-Gong Shieh', 'Wen-Bing Horng']","A complete remote authentication scheme should provide the following security properties: (1) mutual authentication, (2) session key exchange, (3) protection of user anonymity, (4) support of immediate revocation capability, (5) low communication and computation cost, (6) resistance to various kinds of attacks, (7) freely choosing and securely changing passwords by users, and (8) without storing password or verification tables in servers. However, none of the existing schemes meets all the requirements. In this paper, along the line of cost effective approach using hash functions for authentication, we propose an efficient and practical remote user authentication scheme with smart cards to support the above complete security properties.",Efficient and complete remote authentication scheme with smart cards
"['Adriana Brancaccio', 'Giovanni Leone']","The inverse problem of reconstructing the shape of dielectric cylinders by aspect-limited multimonostatic multifrequency electromagnetic scattering data is dealt with. The problem is formulated as a linear one by means of the physical-optics approximation distributional approach. The difference with respect to the case of perfectly electrical conducting scatterers is pointed out, since the penetrability of the scatterers is taken into account by considering the contribution of the É??shadowedÉ?ù side to the local reflection coefficient. The adopted model allows one to predict that both the É??illuminatedÉ?ù and É??shadowedÉ?ù sides of the scatterer provide contribution to the reconstructed image but with a delocalization depending on the relative dielectric permittivity. The numerical results confirm this expectation and show the effectiveness of the approach.",Multimonostatic Shape Reconstruction of Two-Dimensional Dielectric Cylinders by a Kirchhoff-Based Approach
"['Wen Fan', 'Chiu-Sing Choy', 'Ka Nang Leung']","Multiband orthogonal frequency division multiplexing (MB-OFDM) ultra wideband (UWB) systems have drawn much attention for its high spectrum efficiency and multiple access capability. However, its large throughput requirement and low power spectral density result in high hardware complexity and high power consumption, which are challenges of designing the packet detector. In this paper, a novel detection method is proposed with very good detection performance in low SNR. Low cost and low power schemes are also introduced in circuit design to save 70% area and 71% power. The proposed packet detector is synthesized with UMC 0.13?Êm library at 132MHz clock frequency. The hardware cost is 56.9K gates and the power consumption is only 11.7mW.",Robust and low complexity packet detector design for MB-OFDM UWB
"['Ciprian Iliescu', 'Elena Barbarini', 'Marioara Avram', 'Guolin Xu', 'Andrei Avram']","This paper presents a microfluidic device for magnetophoretic separation red blood cells from blood under continuous flow. The separation method consist of continuous flow of a blood sample (diluted in PBS) through a microfluidic channel which presents on the bottom ldquodotsrdquo of ferromagnetic layer. By applying a magnetic field perpendicular on the flowing direction, the ferromagnetic ldquodotsrdquo generate a gradient of magnetic field which amplifies the magnetic force. As a result, the red blood cells are captured on the bottom of the microfluidic channel while the rest of the blood is collected at the outlet. Experimental results show that an average of 95 % of red blood cells is trapped in the device.",Microfluidic device for continuous magnetophoretic separation of red blood cells
"['Jiaqing Huang', 'Liang Wang', 'Wenqing Cheng', 'Hui Li']","Network coding in cyclic networks meets more problems than in acyclic networks. Recently, S.-Y.R.Li et al.proposed a framework of convolutional network coding (CNC) as well as its four properties for cyclic networks with theoretic fundamentals of discrete valuation ring (DVR). The four properties, Convolutional Multicast (CM), Convolutional Broadcast (CB), Convolutional Dispersion (CD) and Basic Convolutional NetworkCode (BCNC), are notions of increasing strength in this order with regard to linear independence among the global encoding kernels. The existence of a BCNC then implies the existence ofthe rest. That is, BCNC is the best convolutional network code in terms of linear independence. However, the code construction algorithm of BCNC was not presented explicitly. To the best of our knowledge, this is the first paper to propose a polynomial time construction algorithm of BCNC for network coding in cyclic networks, which can deal with different characteristics of cycles in terms of topology, including link cycles but flow acyclic, simpleflow cycles and knots. Finally, polynomial time complexity of the algorithm was proved as well as its effectiveness.",Polynomial Time Construction Algorithm of BCNC for Network Coding in Cyclic Networks
"['Robert Eigner', 'Christoph Mair']","This paper presents a new way of addressing and routing for mobile ad hoc networks on the basis of contextual information such as air pressure, brightness, wind direction and strength, or GPS position. The most common use case of context-based addressing is group communication: A participant sends a message to an a priori unspecified set of recipients, but indicates the context in which the message could be useful for a potential receiver. In contrast to infrastructure networks the sender no longer designates the receiver of its message with a distinct identifier. Instead, each recipient using his local context decides by himself, whether the message is useful for him and whether it should be sent out again. The modeling of the necessary application knowledge is done as ontologies in OWL (Web Ontology Language). As an example scenario, a wind gust warning on highways using a vehicular ad hoc network (VANET) is described: the warning message should be sent to all vehicles on the same route containing the place where the wind was detected. The models are applied in a prototypical example scenario in order to show the performance of the approach through a simulation, using the JiST/SWANS simulator for mobile ad hoc networks. The results show that the number of messages that are necessary to warn all vehicles in a given environment of the wind danger can be reduced by half É?? as opposed to a simple flooding of the network.",Using Context Ontologies for Addressing and Routing in Mobile Ad Hoc Networks
"['Bineng Zhong', 'Hongxun Yao', 'Shaohui Liu', 'Xiaotong Yuan']","We propose a novel feature, local histogram of figure/ground segmentations, for robust and efficient background subtraction (BGS) in dynamic scenes (e.g., waving trees, ripples in water, illumination changes, camera jitters, etc.). We represent each pixel as a local histogram of figure/ground segmentations, which aims at combining several candidate solutions that are produced by simple BGS algorithms to get a more reliable and robust feature for BGS. The background model of each pixel is constructed as a group of weighted adaptive local histograms of figure/ground segmentations, which describe the structure properties of the surrounding region. This is a natural fusion because multiple complementary BGS algorithms can be used to build background models for scenes. Moreover, the correlation of image variations at neighboring pixels is explicitly utilized to achieve robust detection performance since neighboring pixels tend to be similarly affected by environmental effects (e.g., dynamic scenes). Experimental results demonstrate the robustness and effectiveness of the proposedmethod by comparing with four representatives of the state of the art in BGS.",Local histogram of figure/ground segmentations for dynamic background subtraction
"['Shigeyoshi Tsutsui', 'Devid E. Goldberg']","In real-coded genetic algorithms (GAs), some crossover operators do not work well on functions which have their optimum at the corner of the search space. To cope with this problem, we have proposed a boundary extension method which allows individuals to be located within a limited space beyond the boundary of the search space. In this paper, we give an analysis of the boundary extension methods from the viewpoint of sampling bias and perform a comparative study on the effect of applying two boundary extension methods, namely the boundary extension by mirroring (BEM) and the boundary extension with extended selection (BES). We were able to confirm that to use sampling methods which have smaller sampling bias had good performance on both functions which have their optimum at or near the boundaries of the search space, and functions which have their optimum at the center of the search space. The BES/SD/A (BES by shortest distance selection with aging) had good performance on functions which have their optimum at or near the boundaries of the search space. We also confirmed that applying the BES/SD/A did not cause any performance degradation on functions which have their optimum at the center of the search space.",Search space boundary extension method in real-coded genetic algorithms
"['Said Esmael El-Khamy', 'Noha Ossama El-Ganainy']",This paper introduces an implementation of a halfduplex decode-and-forward cooperative algorithm using the complete complementary code (CCC) sets. These code sets have an impulsive autocorrelation sum among each set and a cross correlation sum along the set size that vanishes for all shifts. Each user is assigned a set of spreading codes to spread his data and send each of the resulting signals on a different sub-band. The decode-and-forward cooperative procedure is applied and a receiver consisting of parallel branches matched filter followed by a ?Ø-MRC combiner is used for detection. It is demonstrated that the probability of error performance of the proposed algorithm is very close to the performance of the analytical closed-form probability of error in similar transmission conditions. The algorithm is also compared with a noncooperative multiband direct-sequence code division multiple access (MB DS-CDMA) algorithm using the complete complementary sets. The simulation results illustrate the enhanced performance of the proposed algorithm under different channel assumptions.,A New Implementation of a Half-Duplex Decode-and-Forward Cooperative Algorithm Using Complete Complementary Code Sets.
"['Teodor Gabriel Crainic', 'Guido Perboli', 'Walter Rei', 'Roberto Tadei']","We consider the variable cost and size bin packing problem, a generalization of the well-known bin packing problem, where a set of items must be packed into a set of heterogeneous bins characterized by possibly different volumes and fixed selection costs. The objective of the problem is to select bins to pack all items at minimum total bin-selection cost. The paper introduces lower bounds and heuristics for the problem, the latter integrating lower and upper bound techniques. Extensive numerical tests conducted on instances with up to 1000 items show the effectiveness of these methods in terms of computational effort and solution quality. We also provide a systematic numerical analysis of the impact on solution quality of the bin selection costs and the correlations between these and the bin volumes. The results show that these correlations matter and that solution methods that are un-biased toward particular correlation values perform better.",Efficient lower bounds and heuristics for the variable cost and size bin packing problem
"['Feel-soon Kang', 'Seok-Gyu Oh']","A new sustain driver employing a step-up function is presented to achieve the faster rise-time of sustain voltage, which Ls suitable for widely used address-and-display-period-separated (ADS) driving method, and most of all for cost saving. The proposed sustain driver can reduce the number of switching devices by 25 (%) compared to the prior approach improving the overall system efficiency about 10 (%). And brightness decrease problem resulted from the lack of the number of the sustain pulse can be solved without a limitation to sustain pulse width. Operational principle and its features are illustrated comparing with conventional approaches. The validity of the proposed sustain driver is verified through experiments using a prototype equipped with a 7.5 (inch) diagonal panel, which is operated at 200 (kHz) switching frequency.",An efficiently revised sustain driver for AC plasma display
"['Ted Lewis', 'Pierre Francus', 'Raymond S. Bradley', 'Kinuyo Kanamaru']","A macro has been developed that allows for automated statistical analyses of particles. It can be used with binary images from any source, and is developed for use with lacustrine, marine, Aeolian, and marine sediments. The macro code is freely available, and runs on open-source software. It has been specially designed to accommodate very small regions of interest relative to particle size, and to produce continuous downcore stratigraphies. The macro runs quickly, requires no user-interaction, is easily modifiable, saves 'raw data' for algorithm checking and further analyses, and can run in a mode that quantifies cracks and other disturbances on images. The macro allows for rapid analyses of relatively long sedimentary sequences, and provides relevant sedimentary interpretations of transport and depositional processes. Examples of output from two lacustrine sediment records and one marine record are shown.",Short note: An automated system for the statistical analysis of sediment texture and structure at the micro scale
"['Ching-Lung Lin', 'Yuan-Chuen Hwang', 'Ching-Feng Lin']","Saving power is the major goal in every area either in industry or in campus. However, there are more uncertainties in campus. For instances, the usages of air conditioners in the classroom are irregular events. In the other hand, the lighting in the parking area is the regular event. For the regular event, we use the unstable green energy like wind or solar to save the electric cost. For the irregular event, we use the stable electricity but managed by dynamic power dispatch system to efficiently handle usages of air conditioners in the classroom in order to reduce cost. This dispatch system is the integration of Internet, network, coded radio frequency, database, interfacing technology, and campus administration computerization servers. By using the existing media and devices, the set-up of this system is without extra expensive cost. This hybrid strategy is simulated in a University with about 15000 students and more than 100 classrooms equipped with air conditioners. We can estimate to save more than 10% power than usual.",Development of A Hierarchical Saving Power System for Campus
"['Xiaofeng Cui', 'Yanchun Sun', 'Gang Huang', 'Hong Mei']","Mission-critical software claims safe and robust adaptations that comply with rigorous criteria of multiple critical quality attributes. Existing adaptation approaches pay little attention to comprehensively capture mission goals and explicitly specify adaptation requirements. We propose an approach to using scenario-based analysis to elicit and specify the criteria of multiple quality attributes as adaptation invariants, and design corresponding architecture variants as facilities implementing adaptations. We also present how to make adaptation decisions at runtime.",Architectural Adaptation Addressing the Criteria of Multiple Quality Attributes in Mission-Critical Systems
"['S. Jayanthy', 'M. C. Bhuvaneswari', 'M. Prabhu']","A new multi-objective genetic algorithm has been proposed for testing crosstalk delay faults in asynchronous sequential circuits that reduces average power dissipation during test application. The proposed Elitist Non-dominated Sorting Genetic Algorithm ENGA-based Automatic Test Pattern Generation ATPG for crosstalk induced delay faults generates test pattern set that has high fault coverage and low switching activity. Redundancy is introduced in ENGA-based ATPG by modifying the fault dropping phase and hence a very good reduction in transition activity is achieved. Tests are generated for several asynchronous SIS benchmark circuits. Experimental results demonstrate that ENGA gives higher fault coverage, reduced transitions and compact test vectors for most of the asynchronous benchmark circuits when compared with those generated by Weighted Sum Genetic Algorithm WSGA.",Simulation-based ATPG for low power testing of crosstalk delay faults in asynchronous circuits
"['Stephen A. Wilkus', 'Jeffrey D. Bailey', 'Daniel G. Brown', 'Ratna Dav??', 'Robert L. Dorn', 'Jean-Marc Hanriot', 'Mark Hoffman', 'Anoop R. Kulkarni', 'Choong S. Lee', 'Lyla R. Meader', 'John M. Polakovic', 'John Sullivan']","Field measurements of a DVB-SH network with both satellite and terrestrial transmitters are presented. The system is a Single Frequency Network transmitting video streams to vehicular terminals with up to 4 branches of receiver antenna diversity. The signal is transmitted in the S-band at 2.1859 GHz with both time and frequency synchronization of the terrestrial repeaters with the satellite's signal. The time and frequency variations are cancelled out at the satellite gateway, but because these can not be canceled at all locations, and because the satellite's position in space is variable (in an inclined orbit) the terrestrial repeaters are made to cancel the residual time and frequency variation. The field measurements include data taken in late 2008 through 2009 with multiple repeaters located in several cities including Las Vegas, NV, Raleigh and Durham, NC, as well as various morphologies, and with elevation angles to the satellite ranging from 25 ?¯  to 52 ?¯  . This paper presents coverage data for these various morphologies, modulation and code rates, as well as various MPE-iFEC interleaver settings used to ameliorate the effects of long shadowed intervals such as when the vehicle goes under highways or bridges where there signal is obscured for several seconds. We observed excellent performance in hybrid mode with better than 99% of the measured seconds error free. In satellite-only mode, the MPE-iFEC interleaver raised the performance from 81% to 91% averaged over all environments, including dense urban.",Field Measurements of a Hybrid DVB-SH Single Frequency Network With an Inclined Satellite Orbit
"['Tomas Eklund', 'Barbro Back', 'Hannu Vanharanta', 'Ari Visa']","Analyzing financial performance in todayÉ??s information-rich society can be a daunting task. With the evolution of the Internet, access to massive amounts of financial data, typically in the form of financial statements, is widespread. Managers and stakeholders are in need of a data-mining tool allowing them to quickly and accurately analyze this data. An emerging technique that may be suited for this application is the self-organizing map. The purpose of this study was to evaluate the performance of self-organizing maps for analyzing financial performance of international pulp and paper companies. For the study, financial data, in the form of seven financial ratios, was collected, using the Internet as the primary source of information. A total of 77 companies, and six regional averages, were included in the study. The time frame of the study was the period 1995-00. An example analysis was performed, and the results analyzed based on information contained in the annual reports. The results of the study indicate that self-organizing maps can be feasible tools for the financial analysis of large amounts of financial data.",ASSESSING THE FEASIBILITY OF SELF-ORGANIZING MAPS FOR DATA MINING FINANCIAL INFORMATION
"['Morihiko Tamai', 'Naoki Shibata', 'Keiichi Yasumoto', 'Minoru Ito']","In this demonstration, we show an energy-aware video streaming system which allows users to play back video for the specified duration within the remaining battery amount. In the system, we execute a proxy server on an intermediate node in the network. It receives the video stream from a content server, transcodes it to the videos with appropriate quality, and forwards it to a PDA or a laptop PC. Here, suitable parameter values of the video (such as picture size, frame rate and bitrate) which enable playback for the specified duration are automatically calculated on the proxy using our battery consumption model. The system also allows users to play back video segments with different qualities based on the importance specified to each video segment.",An Energy-Aware Video Streaming System for Portable Computing Devices
['Thomas Ahlswede'],"This paper describes a set of interactive routines that can be used to create, maintain, and update a computer lexicon. The routines are available to the user as a set of commands resembling a simple operating system. The lexicon produced by this system is based on lexical-semantic relations, but is compatible with a variety of other models of lexicon structure. The lexicon builder is suitable for the generation of moderate-sized vocabularies and has been used to construct a lexicon for a small medical expert system. A future version of the lexicon builder will create a much larger lexicon by parsing definitions from machine-readable dictionaries.",A TOOL KIT FOR LEXICON BUILDING
"['Liang Li', 'Tian Liu', 'Ke Xu']","Backtracking is a basic strategy to solve constraint satisfaction problems (CSPs). A satisfiable CSP instance is backtrack-free if a solution can be found without encountering any dead-end during a backtracking search, implying that the instance is easy to solve. We prove an exact phase transition of backtrack-free search in some random CSPs, namely in Model RB and in Model RD. This is the first time an exact phase transition of backtrack-free search can be identified on some random CSPs. Our technical results also have interesting implications on the power of greedy algorithms, on the width of superlinear dense random hypergraphs and on the exact satisfiability threshold of random CSPs.",Exact phase transition of backtrack-free search with implications on the power of greedy algorithms
"['Stefan Osswald', 'Attila G??r??g', 'Armin Hornung', 'Maren Bennewitz']","In this paper, we present an approach to enable a humanoid robot to autonomously climb up spiral staircases. This task is substantially more challenging than climbing straight stairs since careful repositioning is needed. Our system globally estimates the pose of the robot, which is subsequently refined by integrating visual observations. In this way, the robot can accurately determine its relative position with respect to the next step. We use a 3D model of the environment to project edges corresponding to stair contours into monocular camera images. By detecting edges in the images and associating them to projected model edges, the robot is able to accurately locate itself towards the stairs and to climb them. We present experiments carried out with a Nao humanoid equipped with a 2D laser range finder for global localization and a low-cost monocular camera for short-range sensing. As we show in the experiments, the robot reliably climbs up the steps of a spiral staircase.",Autonomous climbing of spiral staircases with humanoids
"['Zhuoling Xiao', 'Hongkai Wen', 'Andrew Markham', 'Niki Trigoni']","Indoor tracking and navigation is a fundamental need for pervasive and context-aware smartphone applications. Although indoor maps are becoming increasingly available, there is no practical and reliable indoor map matching solution available at present. We present MapCraft, a novel, robust and responsive technique that is extremely computationally efficient (running in under 10 ms on an Android smartphone), does not require training in different sites, and tracks well even when presented with very noisy sensor data. Key to our approach is expressing the tracking problem as a conditional random field (CRF), a technique which has had great success in areas such as natural language processing, but has yet to be considered for indoor tracking. Unlike directed graphical models like Hidden Markov Models, CRFs capture arbitrary constraints that express how well observations support state transitions, given map constraints. Extensive experiments in multiple sites show how MapCraft outperforms state-of-the art approaches, demonstrating excellent tracking error and accurate reconstruction of tortuous trajectories with zero training effort. As proof of its robustness, we also demonstrate how it is able to accurately track the position of a user from accelerometer and magnetometer measurements only (i.e. gyro- and WiFi-free). We believe that such an energy-efficient approach will enable always-on background localisation, enabling a new era of location-aware applications to be developed.",Lightweight map matching for indoor localisation using conditional random fields
"['George D. Stetten', 'Stephen M. Pizer']","Segmentation is generally regarded as partitioning space at the boundary of an object so as to represent the object's shape, pose, size, and topology. Some images, however, contain so much noise that distinct boundaries are not forthcoming even after the object has been identified. We have used statistical methods based on medial features in Real Time 3D echocardiography to locate the left ventricular axis, even though the precise boundaries of the ventricle are simply not visible in the data. We then produce a fuzzy labeling of ventricular voxels to represent the shape of the ventricle without any explicit boundary. The fuzzy segmentation permits calculation of total ventricular volume as well as determination of local boundary equivalencies, both of which are validated against manual tracings on 155 left ventricles. The method uses a medial-based compartmentalization of the object that is generalizable to any shape.",Medial-Guided Fuzzy Segmentation
"['Darko Kirovski', 'Nebojsa Jojic']","We present FACECERTS, a simple, inexpensive, and cryptographically secure identity certification system. A FACECERT is a printout of person's portrait photo, an arbitrary textual message, and a 2D color bar-code which encodes an RSA signature of the message hash and the compressed representation of the face encompassed by the photo. The signature is created using the private key of the party issuing the ID. Verification is performed by a simple, intelligent, and off-line scanning device that contains the public key of the issuer. The system does not require smart cards. More interestingly, the ID does not need to be printed by a high-end printer, it can be printed anywhere. We present a novel algorithm for compressing faces and investigate the reliability of the crucial components of the system.",Cryptographically secure identity certificates
"['Kyung-Mook Oh', 'Jack Meadows']","South Korean universities are currently at various stages of advancement in their use of electronic communication. Networking provision for universities is expected to improve considerably over the next few years. There is therefore a need to examine their present usage of communication technologies, how this depends on the level of facilities available and what the implications are for their future development. The study described here investigates usage at a stratified sample of South Korean universities via a questionnaire survey. It also examines the factors affecting use via a series of interviews. The results suggest that, though infrastructural limitations are important, optimal deployment of communication technologies will require organisational changes within the university system.",Use of communication technologies in South Korean universities
"['Mao Nakajima', 'Minoru Watanabe']","To date, holographic configuration speeds have remained limited to 16 mus because of issues related to the architecture of optically reconfigurable gate array VLSIs (ORGA-VLSIs). Therefore, to improve the issue, optically differential reconfigurable gate array VLSIs (ODRGA-VLSIs) have been developed and have achieved zero-overhead and nanosecond optical reconfiguration. Moreover, the architecture of an ODRGA-VLSI has the advantage of accelerating the reconfiguration speed compared to that of other ORGAs. However, nanosecond holographic configurations and, in particular, rapid holographic reconfiguration exploiting the advantages of ODRGA-VLSIs have not been reported. Therefore, this paper presents results of the world's fastest 62.5 ns holographic reconfiguration, exploiting advantages of the ODRGA-VLSI.",A 62.5 ns holographic reconfiguration of an optically differential reconfigurable gate array
"['Ozan K. Tonguz', 'Okan M. Tanrikulu', 'Leonid G. Kazovsky']","The impact of finite intermediate frequency (IF) on the performance of heterodyne ASK lightwave systems is examined and quantified in the presence of laser phase noise and shot noise. For negligible linewidths, it is shown that certain finite choices of IF (R/sub b/,3R/sub b//2,2R/sub b/,5R/sub b//2, etc.) lead to the same ideal bit-error-rate (BER) performance as infinite choices of IF. Results indicate that for negligible linewidths the worst case sensitivity penalty is 0.9 dB for proper heterodyne detection and occurs when f/sub IF/=1.25 R/sub b/. For nonnegligible linewidths (e.g., when /spl Delta//spl nu/T/spl ges/0.04) the sensitivity penalty is always less than 0.9 dB for finite choices of IF. The analysis presented does lead to a closed-form signal-to-noise ratio (SNR) expression at the decision gate of the receiver which can readily be used for BER and sensitivity penalty computations. The SNR expression provided includes all the key system parameters of interest such as system bit rate (R/sub b/), the peak IF SNR (/spl xi/), laser linewidth (/spl Delta//spl nu/), and the IF filter expansion factor (/spl alpha/). The findings of this work suggest that the number of channels in a multichannel heterodyne ASK lightwave system can be increased substantially by properly choosing a small value for the IF at the expense of a small penalty <1 dB. On the negative side, IF frequency stabilization becomes a more critical requirement in multichannel systems employing small values of IF.",Performance of coherent ASK lightwave systems with finite intermediate frequency
"['Mauro Pelosi', 'Ondrej Franek', 'Mikael B. Knudsen', 'Gert Fr??lund Pedersen']",In this paper both normal-band and narrow-band PIFA antennas for small terminals are compared through numerical simulations and measurements for different UMTS bandwidths. It is found that using different antennas for the transmitting and receiving regions of the frequency duplex it is possible to achieve a significant improvement in terms of isolation. Measurement results show also that ohmic losses lower the total efficiency in the narrow-band case especially at low frequencies.,Simulation and Measurement of Narrow-Band Antennas for Small Terminals
['Santosh Nagaraj'],"In this letter, we propose an extension to the ordered subcarrier selection algorithm (OSSA) for orthogonal frequency division multiplexing (OFDM) systems. The result is a simple algorithm for minimizing the bit error rate of the OFDM system at a fixed throughput. The proposed algorithm employs multiple modulations (non-uniform bit loading) within an OFDM symbol. However, unlike existing bit loading algorithms that have a very high computational complexity, the proposed algorithm is based only on the ordered statistics of the subcarrier gains and is consequently very simple. After ordering the subcarriers based on their gains, progressively higher order modulations are used with increasing gains. The key aspect here that greatly simplifies the algorithm is that the modulation used on a subcarrier depends only on the position of its gain in the ordered set and not on the actual values of the gains. We show an analytical approach for determining the parameters of the algorithm.",An extension to the ordered subcarrier selection algorithm (OSSA)
"['Mithun Das Gupta', 'Shyamsundar Rajaram', 'Nemanja Petrovic', 'Thomas S. Huang']","In this paper, we present a supervised learning based approach for sub-pixel motion estimation. The novelty of this work is the learning based method itself which tries to learn the shifts from a large training database. Integer pixel shift is sub-divided and discretized to levels in both the horizontal and vertical direction. We pose the problem of motion estimation in a polar coordinate system. Shift estimation in the x and y direction has been posed as a problem of estimating r and thetas. The ordinal property of r has been used, and consequently, we employ a ranking based approach for estimating r. For thetas estimation we employ multi-class classification techniques. We demonstrate how very simplistic features can be used to differentiate between different sub-pixel shifts",Classifiers for Motion
"['Daojun Xue', 'Yang Qin', 'Chee Kheong Siew']","This paper considers the scheduling problem in a new slotted optical network called Time-Domain Wavelength Interleaved network (TWIN). The TWIN architecture possesses interesting properties, which may offer solutions for next-generation optical networks. Besides, better Quality of Service (QoS) could be achieved in TWIN by minimizing two parameters: queueing delay and delay variance. However, to the best of our knowledge, most of the existing scheduling algorithms in TWIN ignored consideration of QoS and focused mainly on maximizing the throughput. In this paper, we formulate the scheduling problem into an Integer Linear Programming (ILP) problem and propose a novel heuristic - Destination Slot Set (DSS) algorithm to solve it fast and efficiently. Besides, we derive an analytical model for TWIN and investigate the performance of DSS in it. By means of simulations, we demonstrate that our analytical model approximates the TWIN network very well; moreover, DSS incurs smaller queueing delay and delay variance, which ensures better QoS.",Performance analysis of a novel traffic scheduling algorithm in slotted optical networks
"['Dariusz Blasiak', 'Wai-Yip Chan']","Motion-compensated prediction of video is formulated as a novel vector quantization scheme called motion filter vector quantization (MFVQ). In MFVQ, the motion vector and the pixel-intensity interpolation filter are combined into a motion filter and the entire filter is vector quantized. A codebook design algorithm is proposed for designing unit gain and entropy constrained MFVQ codebooks. The algorithm is tested under two application configurations, MFVQ with static codebook and MFVQ with forward-adaptive codebook, and is shown to furnish up to a dB of PSNR gain.",Motion filter vector quantization
"['Martin David Adams', 'Penelope Probert']","Describes the design of a real-time obstacle avoidance and navigation strategy for a mobile robot, using simulated time of flight infrared data. Algorithms have been developed in order to overcome the undesirable effect of potential traps within the field, and a new approach for dealing with sensing whilst moving is demonstrated. The authors show simulated results of a vehicle moving under artificial potential force equations, which is designed to achieve modification of behaviour at a speed close to the normal operational speed of a real mobile. >",Towards a real-time navigation strategy for a mobile robot
"['Larry Li', 'Brian Cox', 'Myron A. Diftler', 'Susan Shelton', 'Barry Rogers']","A master-slave system is developed to evaluate the effectiveness of telepresence in space telerobotics applications. An operator uses the master's telepresence and virtual reality equipment to control the slave. The slave is a dual-arm, dual-hand robot equipped with a stereo camera platform designed to provide an operator-centered perspective of the remote environment. The initial integration and tests of the system presented several operational issues that are resolved through a variety of shared control techniques and optimized algorithms. The resulting system provides a very flexible capability and is used to perform a range of tasks: grasping and handling toots, manipulating electronics controls, manipulating soft flexible material, and performing planetary geology tasks that involve a variety of manipulation and tool-use skills. Using this system an operator is able to complete most of these tasks in less than 2 minutes.",Development of a telepresence controlled ambidextrous robot for space applications
"['Lizhong Gu', 'Jianbo Su']","View-independence and user-independence are two fundamental requirements for hand posture recognition during natural human-robot interaction. However only a few research concerns on the two issues simultaneously. The difficulty for natural gesture-based human-robot interaction lies in that appearances of the same hand posture vary with different users from different viewing directions. In this paper, we propose a systematic feature selection approach based on Zernike moments and Isomap dimensionality reduction. A hierarchical classifier based on multivariate decision tree and piecewise linearization is developed to deal with the irregular distribution of the same hand postures. The proposed method is compared with other commonly used ones in hand posture recognition. Experimental results indicate that the proposed method can effectively identify different hand postures, irrespective of viewing directions and users.",Natural hand posture recognition based on Zernike moments and hierarchical classifier
"['Binyam Gebrekidan Gebre', 'O.A. Crasborn', 'Peter Wittenburg', 'Sebastian Drude', 'Tom Heskes']","Prior research on language identification focused primarily on text and speech. In this paper, we focus on the visual modality and present a method for identifying sign languages solely from short video samples. The method is trained on unlabelled video data (unsupervised feature learning) and using these features, it is trained to discriminate between six sign languages (supervised learning). We ran experiments on short video samples involving 30 signers (about 6 hours in total). Using leave-one-signer-out cross-validation, our evaluation shows an average best accuracy of 84%. Given that sign languages are underresourced, unsupervised feature learning techniques are the right tools and our results indicate that this is realistic for sign language identification.",Unsupervised Feature Learning for Visual Sign Language Identification
"['Minkyong Kim', 'Zhen Liu', 'Srinivasan Parthasarathy', 'Dimitrios Pendarakis', 'Hao Yang']","As mobile nodes roam in a wireless network, they continuously associate with different access points and perform handoff operations. However, frequent handoffs can potentially incur unacceptable delays and even interruptions for interactive applications. To alleviate these negative impacts, we present novel association control algorithms that can minimize the frequency of handoffs occurred to mobile devices. Specifically, we show that a greedy LookAhead algorithm is optimal in the offline setting, where the user's future mobility is known. Inspired by such optimality, we further propose two online algorithms, namely LookBack and Track, that operate without any future mobility information. Instead, they seek to predict the lifetime of an association using randomization and statistical approaches, respectively. We evaluate the performance of these algorithms using both analysis and trace-driven simulations. The results show that the simple LookBack algorithm has surprisingly a competitive ratio .of (log k + 2), where k is the maximum number of APs that a user can hear at any time, and the Track algorithm can achieve near-optimal performance in practical scenarios.",Association Control in Mobile Wireless Networks
"['Patrick Eugster', 'Vinaitheerthan Sundaram', 'Xiangyu Zhang']","The Internet of Things (IoT) has the strong potential to support a human society interacting more symbiotically with its physical environment. Indeed, the emergence of tiny devices that sense environmental cues and trigger actuators after consulting logic and human preferences promises a more environmentally aware and less wasteful society. However, the IoT inherently challenges software development processes, particularly techniques for ensuring software reliability. Researchers have developed debugging tools for wireless sensor networks (WSNs), which can be viewed as the enablers of perception in the IoT. These tools gather run-time information on individual sensor node executions and node interactions and then compress that information.",Debugging the Internet of Things: The Case of Wireless Sensor Networks
"['Kim Issroff', 'Eileen Scanlon', 'Ann Jones']","In this paper the results and implications of two studies of computer-supported collaborative learning are presented and implications discussed. The first study was an experimental study in a British secondary school, while the second study followed a group of primary school children in a naturalistic context. Assessing learning situations is discussed with an emphasis on the affective factors. The differences between the products, the interactions and the outcomes of learning situations are discussed along with the research methodology. There is an emphasis on pre- and post-testing, naturalistic and experimental studies and time-based analyses.",Two empirical studies of computer-supported collaborative learning in science: methodological and affective implications
"['H. Dharma Kwon', 'Steven A. Lippman']","We study the impact of learning on the optimal policy and the time-to-decision in an infinite-horizon Bayesian sequential decision model with two irreversible alternatives: exit and expansion. In our model, a firm undertakes a small-scale pilot project to learn, via Bayesian updating, about the project's profitability, which is known to be in one of two possible states. The firm continuously observes the project's cumulative profit, but the true state of the profitability is not immediately revealed because of the inherent noise in the profit stream. The firm bases its exit or expansion decision on the posterior probability distribution of the profitability. The optimal policy is characterized by a pair of thresholds for the posterior probability. We find that the time-to-decision does not necessarily have a monotonic relation with the arrival rate of new information.",Acquisition of Project-Specific Assets with Bayesian Updating
"['Sidney Viana', 'Jorge Rady de Almeida', 'Judith Pav??n']","Active Database Systems (ADBSs) provides a good infrastructure to define and execute active rules. Nevertheless, this infrastructure offered by ADBSs does not completely satisfy the necessities of rules management that demands current business applications. Rules also need to be stored in appropriate structures to facilitate their management, as the existing structures for data in these systems. This work proposes a rule repository, composed by structures that allow the storage and organization of rules, in order to facilitate their management. For this purpose, a rule classification with the main rule types existing in the literature is presented, and then, it represents the characteristics and anatomy of each type in a meta-model, with the goal of analyzing the data that must be stored about rules. The rule repository, proposed in this paper, has been built based on this meta-model.",A RULE REPOSITORY FOR ACTIVE DATABASE SYSTEMS
"['Anna Chmielowiec', 'Spyros Voulgaris', 'Maarten van Steen']","Imagine a network of entities, being it replica servers aiming to minimize the probability of data loss, players of online team-based games and tournaments, or companies that look into co-branding opportunities. The objective of each entity in any of these scenarios is to find a few suitable partners to help them achieve a shared goal: replication of the data for fault tolerance, winning the game, successful marketing campaign. All information attainable by the entities is limited to the profiles of other entities that can be used to assess the pairwise fitness. How can they create teams without help of any centralized component and without going into each otherÉ??s way? We propose a decentralized algorithm that helps nodes in the network to form groups of a specific size. The protocol works by finding an approximation of a weighted k-clique matching of the underlying graph of assessments. We discuss the basic version of the protocol, and explain how dissemination via gossiping helps in improving its scalability. We evaluate its performance through extensive simulations.",Decentralized group formation
"['Saiprasad Ravishankar', 'Yoram Bresler']","Natural signals and images are well-known to be approximately sparse in transform domains such as Wavelets and DCT. This property has been heavily exploited in various applications in image processing and medical imaging. Compressed sensing exploits the sparsity of images or image patches in a transform domain or synthesis dictionary to reconstruct images from undersampled measurements. In this work, we focus on blind compressed sensing, where the underlying sparsifying transform is a priori unknown, and propose a framework to simultaneously reconstruct the underlying image as well as the sparsifying transform from highly undersampled measurements. The proposed block coordinate descent type algorithms involve highly efficient optimal updates. Importantly, we prove that although the proposed blind compressed sensing formulations are highly nonconvex, our algorithms are globally convergent (i.e., they converge from any initialization) to the set of critical points of the objectives defining the formulations. These critical points are guaranteed to be at least partial global and partial local minimizers. The exact point(s) of convergence may depend on initialization. We illustrate the usefulness of the proposed framework for magnetic resonance image reconstruction from highly undersampled k-space measurements. As compared to previous methods involving the synthesis dictionary model, our approach is much faster, while also providing promising reconstruction quality.",Efficient Blind Compressed Sensing Using Sparsifying Transforms with Convergence Guarantees and Application to MRI
"['Weiwei Chen', 'Jie Song', 'Leyuan Shi']","The local pickup and delivery problem (LPDP) is an essential operational problem in intermodal industry. While the problem with deterministic settings is already difficult to solve, in reality, there exist a set of loads, called uncertain loads, which are unknown at the beginning of the day. But customers may call in during the day to materialize these loads. In this paper, we call the LPDP considering these uncertain loads as the stochastic LPDP. The problem description and the mathematical modeling of stochastic LPDP are discussed. Then, a simulation-based optimization approach is proposed to solve the problem, which features in a fast solution generation procedure and an intelligent simulation budget allocation framework. The numerical examples show the best strategy to consider the stochastic loads in the planning process and validate the benefits compared to its deterministic counterpart.",Optimizing local pickup and delivery with uncertain loads
"['King To Ng', 'Qing Wu', 'Shing-Chow Chan', 'Heung-Yeung Shum']","A new object-based coding system for a class of dynamic image-based representations called plenoptic videos (PVs) is proposed. PVs are simplified dynamic light fields, where the videos are taken at regularly spaced locations along line segments instead of a 2-D plane. In the proposed object-based approach, objects at different depth values are segmented to improve the rendering quality. By encoding PVs at the object level, desirable functionalities such as scalability of contents, error resilience, and interactivity with an individual image-based rendering (IBR) object can be achieved. Besides supporting the coding of texture and binary shape maps for IBR objects with arbitrary shapes, the proposed system also supports the coding of grayscale alpha maps as well as depth maps (geometry information) to respectively facilitate the matting and rendering of the IBR objects. Both temporal and spatial redundancies among the streams in the PV are exploited to improve the coding performance, while avoiding excessive complexity in selective decoding of PVs to support fast rendering speed. Advanced spatial/temporal prediction methods such as global disparity-compensated prediction, as well as direct prediction and its extensions are developed. The bit allocation and rate control scheme employing a new convex optimization-based approach are also introduced. Experimental results show that considerable improvements in coding performance are obtained for both synthetic and real scenes, while supporting the stated object-based functionalities.",Object-Based Coding for Plenoptic Videos
"['Takahiro Ogawa', 'Miki Haseyama']","This paper presents an adaptive reconstruction method of missing textures based on an inverse projection via sparse representation. The proposed method approximates original and corrupted textures in lower-dimensional subspaces by using the sparse representation technique. Then, this approach effectively solves problems of not being able to directly estimate an inverse projection for reconstructing missing textures. Furthermore, even if target textures contain missing areas, the proposed method enables adaptive generation of the subspaces by monitoring errors caused in their known neighboring textures by the estimated inverse projection. Consequently, since the optimal inverse projection is adaptively estimated for each texture, successful reconstruction of the missing areas can be expected. Experimental results show impressive improvement of the proposed reconstruction technique over previously reported reconstruction techniques.",Adaptive reconstruction method of missing textures based on inverse projection via sparse representation
"['Alexander Zien', 'Joaquin Qui?Òonero Candela']","It is common in classification methods to first place data in a vector space and then learn decision boundaries. We propose reversing that process: for fixed decision boundaries, we ""learn"" the location of the data. This way we (i) do not need a metric (or even stronger structure) - pairwise dissimilarities suffice; and additionally (ii) produce low-dimensional embeddings that can be analyzed visually. We achieve this by combining an entropy-based embedding method with an entropy-based version of semi-supervised logistic regression. We present results for clustering and semi-supervised classification.",Large margin non-linear embedding
['Marcoen J. T. F. Cabbolet'],"Scientific misconduct is usually assumed to be self-serving. This paper, however, proposes to distinguish between two types of scientific misconduct: É??type one scientific misconductÉ?? is self-serving and leads to falsely positive conclusions about oneÉ??s own work, while É??type two scientific misconductÉ?? is other-harming and leads to falsely negative conclusions about someone elseÉ??s work. The focus is then on the latter type, and three known issues are identified as specific forms of such scientific misconduct: biased quality assessment, smear, and officially condoning scientific misconduct. These concern the improper ways how challenges of the prevailing opinion are thwarted in the modern world. The central issue is pseudoskepticism: uttering negative conclusions about someone elseÉ??s work that are downright false. It is argued that this may be an emotional response, rather than a calculated strategic action. Recommendations for educative and punitive measures are given to prevent and to deal with these three forms of scientific misconduct.",Scientific Misconduct: Three Forms that Directly Harm Others as the Modus Operandi of MillÉ??s Tyranny of the Prevailing Opinion
"['Martin Keller-Ressel', 'Thomas Steiner']","We consider a model for interest rates where the short rate is given under the risk-neutral measure by a time-homogeneous one-dimensional affine process in the sense of Duffie, Filipovic, and Schachermayer. We show that in such a model yield curves can only be normal, inverse, or humped (i.e., endowed with a single local maximum). Each case can be characterized by simple conditions on the present short rate rt. We give conditions under which the short rate process converges to a limit distribution and describe the risk-neutral limit distribution in terms of its cumulant generating function. We apply our results to the Vasicek model, the CIR model, a CIR model with added jumps, and a model of OrnsteinÉ??Uhlenbeck type.",Yield Curve Shapes and the Asymptotic Short Rate Distribution in Affine One-Factor Models
"['Mohamed Abdel-Mottaleb', 'Santhana Krishnamachari']","The amount of digital multimedia content available to consumers is growing because of the existence of digital capturing devices such as digital cameras, camcorders and the advent of digital video broadcast. With this increase in content, it becomes important for users to be able to browse and search for content in a timely manner. Descriptions and annotations of the content are needed to enable searching and browsing of content. MPEG-7 is a recent ISO standard for multimedia content description. In this paper, we present three descriptors, which we had proposed to MPEG-7, and have been accepted to the standard. In addition, we describe algorithms for automatically extracting these descriptors. We also present the indexing and retrieval algorithms that we developed for these descriptors; these algorithms are fast and scalable for large databases. The results presented in the paper for image and video segment matching using these descriptors show their usefulness in real applications.",Multimedia descriptions based on MPEG-7: extraction and applications
"['Jonathan C. Carr', 'W. Richard Fright', 'Andrew H. Gee', 'Richard W. Prager', 'Kevin J. Dalton']","Volume intersection algorithms are used to reconstruct incomplete objects from their silhouettes. An imagined light source is moved about the data and the cumulative amount of ""light"" seen at each point an space is interpreted as indicating the likelihood that the point is inside the object. The object data need not be uniformly distributed nor exclusively surface data. Explicit distinction between noise, surface and interior data is avoided. The novel concept of a localised viewing region is introduced to overcome the inherent inability of volume intersection algorithms to reconstruct concave surfaces. Algorithms for 2D pixel and 3D voxel data are described and applied to 3D ultrasound data.",3D shape reconstruction using volume intersection techniques
"['Srinivasan Seetharaman', 'Mostafa H. Ammar']","The Internet is a complex structure arising from the interconnection of numerous autonomous systems (AS), each exercising its own administrative policies to reflect the commercial agreements behind the interconnection. However, routing in service overlay networks is quite capable of violating these policies to its advantage. To prevent these violations, we see an impending drive in the current Internet to detect and filter overlay traffic. In this paper, we first present results from a case study overlay network, constructed on top of Planetlab, that helps us gain insights into the frequency and characteristics of the different inter-domain policy violations. We further investigate the impact of two types of overlay traffic filtering that aim to prevent these routing policy violations: blind filtering and policy- aware filtering. We show that such filtering can be detrimental to the performance of overlay routing. We next consider two approaches that allow the overlay network to realize the full advantage of overlay routing in this context. In the first approach, overlay nodes are added so that good overlay paths do not represent inter-domain policy violations. In the second approach, the overlay acquires transit permits from certain ASes that allow certain policy violations to occur. We develop a single cost-sharing framework that allows the incorporation of both approaches into a single strategy. We formulate and solve an optimization problem that aims to determine how the overlay network should allocate a given budget between paying for additional overlay nodes and paying for transit permits to ASes. We illustrate the use of this approach on our case study overlay network and evaluate its performance under varying network characteristics.",Characterizing and Mitigating Inter-domain Policy Violations in Overlay Routes
"['Li Bai', 'Yan Wang']","This paper presents a sensor-fusion framework for video-based navigation. Video-based navigation offers the advantages over existing approaches. With this type of navigation, road signs are directly superimposed onto the video of the road scene, as opposed to those superimposed onto a 2-D map, as is the case with conventional navigation systems. Drivers can then follow the virtual signs in the video to travel to the destination. The challenges of video-based navigation require the use of multiple sensors. The sensor-fusion framework that we propose has two major components: (1) a computer vision module for accurately detecting and tracking the road by using partition sampling and auxiliary variables and (2) a sensor-fusion module using multiple particle filters to integrate vision, Global Positioning Systems (GPSs), and Geographical Information Systems (GISs). GPS and GIS provide prior knowledge about the road for the vision module, and the vision module, in turn, corrects GPS errors.",A Sensor Fusion Framework Using Multiple Particle Filters for Video-Based Navigation
"['Runwei Cheng', 'Mitsuo Gen']","This paper describes an implementation of an evolution programme for the resource-constrained project scheduling problem. In essentials, the problem consists of two issues; (a) to determine the order of activities without violating precedence constraints and (b) subsequently to determine earliest start time for each activity according to available resources. How to determine the order of activation is critical to the problem, because if the order is determined, a schedule can be easily constructed with some determining procedures. The basic ideas of the proposed approach are; (a) using an evolution programme to evolve an appropriate order of activities and (b) using a fit-in-best procedure to calculate the earliest start times of activities. A new approach is addressed to guide how to design genetic operators; one operator is designed to perform a wide spread search to try to explore the area beyond local optima, whereas the other is designed to perform an intensive search to try to find an improved solut...",An evolution programme for the resource-constrained project scheduling problem
"['Antonio Canclini', 'Paolo Annibale', 'Fabio Antonacci', 'Augusto Sarti', 'Rudolf Rabenstein', 'Stefano Tubaro']","In this paper we propose a methodology for assessing the accuracy of techniques of wave field rendering through loudspeaker arrays. In order to measure the rendered wave field we adopt a solution based on a circular harmonic analysis of the sound field captured by a virtual microphone array. As a result of this analysis stage, we are able to compare the target, the theoretical and the measured wave fields, which may differ due to the non-ideality in the loudspeaker array or in the environment that generates some spurious reverberations. Moreover, in order to quantify the error between target, theoretical and measured wave fields, we define some evaluation metrics, based on RMSE and modal analysis of the acquired wave fields. We show some experimental results on real data.",A methodology for evaluating the accuracy of wave field rendering techniques
['Gregory M. Nielson'],"This article describes and presents examples of some techniques for the representation and interactive design of surfaces based on a parametric surface representation that user v-spline curves. These v-spline curves, similar in mathematical structure to v-splines, were developed as a more computationally efficient alternative to splines in tension. Although splines in tension can be modified to allow tension to be applied at each control point, the procedure is computationally expensive. The v-spline curve, however, uses more computationally tractable piecewise cubic curves segments, resulting in curves that are just as smoothly joined as those of a standard cubic spline. After presenting a review of v-splines and some new properties, this article extends their application to a rectangular grid of control points. Three techniques and some application examples are presented.",Rectangular v-Splines
"['Keivan Navaie', 'Halim Yanikomeroglu']","In this paper using a utility-based approach, down-link packet transmission in a CDMA/TDMA cellular network is formulated as an optimization problem. A utility function corresponds to each packet served by a base-station that is an increasing function of the packet experienced delay and the channel gain, and a decreasing function of the base-station load. Unlike previous works, in this paper, the optimization objective is to maximize the total network utility instead of the base-station utility. We show that this optimization results in joint base-station assignment and packet scheduling. Therefore, in addition to multi-user diversity, the proposed method also exploits multi-access-point diversity and soft capacity. A polynomial time heuristic algorithm is then proposed to solve the optimization problem. Simulation results indicate a significant performance improvement in terms of packet-drop-ratio and achieved throughput.",Downlink Joint Base-station Assignment and Packet Scheduling Algorithm for Cellular CDMA/TDMA Networks
"['Farid Nait-Abdesselam', 'Hend Koubaa']","In wireless mobile ad-hoc networks, data packets have to be relayed hop by hop from a given source node to a destination node. This means that some or all of the mobile nodes must accept to forward information for the benefit of other nodes. It has been shown by F. Nait-Abdesselam, et al., (2003) that this ability of forwarding packets leads to a new unfairness problem in wireless ad-hoc networks, where a node which is forwarding other node's packets gets less bandwidth, for its own use, than a node which is not participating to the routing service. The proposed RAMAC scheme by F. Nait-Abdesselam, et al., (2003), has shown all its effectiveness to cope with this unfairness problem. However, an extra bandwidth is sometimes allowed to the routing node's own traffic comparing to other nonrouting node's own traffics, and the routing node's routed traffic gets much less bandwidth. This is explained by the fact that at the upper layer (for instance the IP layer) does not differentiate between the routing and the own traffics, and that the multiplicative factor used to compute the new contention window is too aggressive. In this paper, an enhanced RAMAC scheme is proposed, by taking into account the differentiation on top of the MAC layer, between the and routed traffics within a routing node, and by smoothing the multiplicative factor used to compute the new contention window. The simulation results obtained showed a good improvement of the original RAMAC scheme, leading to better approximate equal bandwidth share among all the mobile nodes in the wireless ad-hoc network.",Enhanced routing-aware adaptive MAC with traffic differentiation and smoothed contention window in wireless ad-hoc networks
"['Shusaku Tsumoto', 'Shoji Hirano']","Marginal distributions play an central role in statistical analysis of a contingency table. However, when the number of partition becomes large, the contribution from marginal distributions decreases. This paper focuses on a formal analysis of marginal distributions in a contingency table. The main approach is to take the difference between two matrices with the same sample size and the same marginal distributions, which we call difference matrix. The important nature of the difference matrix is that the determinant is equal to 0: when the rank of a matrix is r, the difference between a original matrix and the expected matrix will become r - 1 at most. Since the sum of rows or columns of the will become zero, which means that the information of one rank corresponds to information on the frequency of a contingency matrix. Interestingly, if we take an expected matrix whose elements are the expected values based on marginal distributions, the difference between an original matrix and expected matrix can be represented by linear combination of determinants of 2 times 2 submatrices.",Meaning of Pearson Residuals Linear Algebra View
"['M. Shel Swenson', 'Rahul Suri', 'C. Randal Linder', 'Tandy J. Warnow']","Background#R##N#Supertree methods represent one of the major ways by which the Tree of Life can be estimated, but despite many recent algorithmic innovations, matrix representation with parsimony (MRP) remains the main algorithmic supertree method.",An experimental study of Quartets MaxCut and other supertree methods
"['Lissette R. Gonzalez', 'Marelys L. Garcia', 'Martha A. Centeno']","Simulation modeling has been widely used to study many aspects of FMS design, planing and control. Yet, simulation modeling still offers other capabilities that are proven to be effective for the on-line control of FMS. An example of it is the training of neural nets off-line, so that it will later control the decision making process when the FMS is in operation. if the neural net runs out of knowledge, it turns back to a simulation model to learn new situations. In this paper, we review the various ways in which on-line knowledge-based simulation for FMS has been approached. This paper represents the early stages of on-going research efforts at Florida International University.",On-line knowledge-based simulation for FMS: a state of the art survey
"['Fefei Shan', 'Yanhai Zhao']","Urban multi-ethnic community is a special type of community which has a high degree of heterogeneity. Thus a study on the management of these communities will be of theoretical and practical significance. This paper took an urban multi-ethnic community of Lanzhou as example, investigating on the management of multi-ethnic communities in northwestern cities. Based on the theory of social relationship, using qualitative and case study methods, this paper has probed into the management models, causes, further development and other issues s in multi-ethnic communities in the Northwestern cities of China.",Study on the Management Models of Urban Multi-Ethnic Community in Northwestern Cities of China - The Case of Lanzhou
"['James M. Okech', 'Yskandar Hamam', 'Anish Mathew Kurien']","The deployment of wireless mesh paradigm was meant to extend Internet access without a consideration of delay sensitive applications. None the less, since voice over IP (VoIP) services are rapidly increasing in popularity, IEEE 802.11 based wireless mesh networks are challenged with the provision of guaranteed quality VoIP calls. In this paper, the disquiet on VoIP systems caused by physical (PHY) and medium access control (MAC) anomaly in the current wireless mesh deployment is addressed through a cross-layer scheme. The scheme is aimed at enhancing VoIP call capacity by mitigating PHY and MAC overheads through aggregation of packets of the same next hop. Through simulations, it is shown that the proposed scheme has significant performance improvements while leaving the IEEE 802.11 standard intact.",A Cross-Layer Adaptation for VoIP over Infrastructure Mesh Network
"['Angelo Oddi', 'Amedeo Cesta', 'Nicola Policella', 'Stephen F. Smith']","Iterative flattening search (ifs) is an iterative improvement heuristic schema for makespan minimization in scheduling problems. Given an initial solution, ifsiteratively interleaves a relaxation-step, which randomly retracts some search decisions, and an incremental solving step (or flattening-step) to recompute a new solution. The process continues until a stop condition is met and the best solution found is returned. In recent work we have created a uniform software framework to analyze component techniques that have been proposed in ifsapproaches. In this paper we combine basic components to obtain hybrid variants and perform a detailed experimental evaluation of their performance. Specifically, we examine the utility of: (1) operating with different relaxation strategies and (2) using different searching strategies to built a new solution. We present a two-step experimental evaluation: (a) an extensive explorative evaluation with a spectrum of parameter combination; (b) a time-intensive evaluation of the best ifscombinations emerged from the previous. The experimental results shed light on weaknesses and strengths of the different variants improving the current understanding of this family of meta-heuristics.",Combining variants of iterative flattening search
"['C.R. King', 'Martin O. Culjat', 'Miguel L. Franco', 'James W. Bisley', 'Gregory P. Carman', 'Erik Dutson', 'Warren S. Grundfest']","A multi-element tactile feedback (MTF) system has been developed to translate the force distribution, in magnitude and position, from 3times2 sensor arrays on surgical robotic end-effectors to the fingers via 3times2 balloon tactile displays. High detection accuracies from perceptual tests (> 96%) suggest that MTF may be an effective means to improve robotic control.",A Multielement Tactile Feedback System for Robot-Assisted Minimally Invasive Surgery
['Sridhar Moorthy'],"This paper asks whether brand extension can serve as a signal of product quality given that it costs less than a new brand. (Existing literature has assumed either that brand extension is cost-neutral or that it costs more.) I show that it can as a perfect Bayesian equilibrium, but the argument is unconvincing. For one thing, the separating equilibrium is not unique; a pooling equilibrium also exists in which brand extension signals nothing. For another, the separating equilibrium relies on off-equilibrium beliefs that are poorly motivated in the model. I propose a refinement of the perfect Bayesian equilibrium that resolves both issues. Empirical off-equilibrium beliefs require that consumers' off-equilibrium beliefs be justifiable on the basis of their prior beliefs and product performance observations. With empirical off-equilibrium beliefs, two necessary conditions for brand extension to signal product quality are identified: (i) consumers must perceive old and new products of the firm to be positively correlated in quality, and (ii) at least some consumers must identify with brands and not the firm behind the brands. Even with these conditions in place, the signaling argument is fragile: firm observability of past performance diminishes brand extension's signaling capability; an arbitrarily small probability of failure for good products eliminates it. My results suggest that, going forward, the case for brand extension must rest on foundations other than signaling product quality.",Can Brand Extension Signal Product Quality
"['Jierui Xie', 'Boleslaw K. Szymanski']","Studies of community structure and evolution in large social networks require a fast and accurate algorithm for community detection. As the size of analyzed communities grows, complexity of the community detection algorithm needs to be kept close to linear. The Label Propagation Algorithm (LPA) has the benefits of nearly-linear running time and easy implementation, thus it forms a good basis for efficient community detection methods. In this paper, we propose new update rule and label propagation criterion in LPA to improve both its computational efficiency and the quality of communities that it detects. The speed is optimized by avoiding unnecessary updates performed by the original algorithm. This change reduces significantly (by order of magnitude for large networks) the number of iterations that the algorithm executes. We also evaluate our generalization of the LPA update rule that takes into account, with varying strength, connections to the neighborhood of a node considering a new label. Experiments on computer generated networks and a wide range of social networks show that our new rule improves the quality of the detected communities compared to those found by the original LPA. The benefit of considering positive neighborhood strength is pronounced especially on real-world networks containing sufficiently large fraction of nodes with high clustering coefficient.",Community detection using a neighborhood strength driven Label Propagation Algorithm
"['Luis A. A. Meira', 'Fl?≠vio Keidi Miyazawa']","In this paper we analyze a known relaxation for the Spars- est Cut problem based on positive semidefinite constraints, and we present a branch and bound algorithm and heuristics based on this relaxation. The relaxed formulation and the algorithms were tested on small and moderate sized instances. It leads to values very close to the optimum solution values. The exact algorithm could obtain solutions for small and moderate sized instances, and the best heuristics obtained optimum or near optimum solutions for all tested instances. The semidefinite relaxation gives a lower bound C/W and each heuristic produces a cut S with a ratio c S /w S , where either c S  is at most a factor of C or w S  is at least a factor of W. We solved the semidefinite relaxation using a semi-infinite cut generation with a commercial linear programming package adapted to the sparsest cut problem. We showed that the proposed strategy leads to a better performance compared to the use of a known semidefinite programming solver.",Semidefinite Programming Based Algorithms for the Sparsest Cut Problem
"['Venkatesh Akella', 'N.H. Vaidya', 'G.R. Redinbo']","A comparison-based decoder detects the arrival of a code word by comparing the received checkbits with the checkbits computed using the received data. Implementation issues underlying comparison-based decoders for systematic delay-insensitive (DI) or unordered codes is the subject of this paper. We show that if the decoder is to be implemented using asynchronous logic, i.e., if the gate and wire delays are arbitrary (unbounded but finite), then it is impossible to design a comparison-based decoder for any code that is more efficient than a dual-rail code. In other words, the encoded word must contain at least twice as many bits as the data. In addition, the codes should satisfy two other properties, called the initial condition and the all-zero lower triangle (AZLT) property, for the realization of a delay-insensitive comparison-based decoder. The paper shows that comparison-based decoders for codes that have the requisite level of redundancy and that satisfy the two properties can be implemented using asynchronous logic.",Asynchronous comparison-based decoders for delay-insensitive codes
"['Youngkwon Cho', 'Jae Hong Lee']","A new multiuser detection scheme is proposed which employs adaptive minimum mean square error (MMSE) detection in combination with successive interference cancellation (SIC). Through theoretical analysis and numerical examples, it is shown that the proposed detector provides superior performance to existing ones in terms of asymptotic multiuser efficiency (AME) and bit error rate (BER).",Analysis of an adaptive SIC for near-far resistant DS-CDMA
"['Lin X. Cai', 'Lin Cai', 'Xuemin Shen', 'Jon W. Mark']","Millimeter-wave (mmWave) transmissions are promising technologies for high data rate (multi-Gbps) Wireless Personal Area Networks (WPANs). In this paper, we first introduce the concept of exclusive region (ER) to allow concurrent transmissions to explore the spatial multiplexing gain of wireless networks. Considering the unique characteristics of mmWave communications and the use of omni-directional or directional antennae, we derive the ER conditions which ensure that concurrent transmissions can always outperform serial TDMA transmissions in a mmWave WPAN. We then propose REX, a randomized ER based scheduling scheme, to decide a set of senders that can transmit simultaneously. In addition, the expected number of flows that can be scheduled for concurrent transmissions is obtained analytically. Extensive simulations are conducted to validate the analysis and demonstrate the effectiveness and efficiency of the proposed REX scheduling scheme. The results should provide important guidelines for future deployment of mmWave based WPANs.",Rex: A randomized EXclusive region based scheduling scheme for mmWave WPANs with directional antenna
"['John Trushell', 'Clare Burrell', 'Amanda Maitland']","The use of É??interactive storybooksÉ?ù in the primary classroom may facilitate small group and individual reading with minimal teacher intervention. This small-scale study examines whether small groups of Year 5 pupils, without teacher supervision, progress linearly through an É??interactive storybookÉ?ù and whether such diversions as cued animations affect pupil comprehension. The study finds that more intensive choice of diversions affects some pupils' comprehension.",Year 5 pupils reading an interactive storybook on CD-ROM: losing the plot?
"['J. L. Solomon', 'Mark Horowitz']","This paper presents a method of using texture mapping with mipmapping to render a VLSI layout.  Texture mapping is used to save already rasterized areas of the layout from frame to frame, and to take advantage of any hardware accelerated capabilities of the host platform. Mipmapping is used to select which textures to display so that the amount of information sent to the display is bounded, and the image rendered on the display is filtered correctly.  Additionally, two caching schemes are employed.  The first, used to bound memory consumption, is a general purpose cache that holds textures spatially close to the user's current viewpoint. The second, used to speed up the rendering process, is a cache of heavily used sub-designs that are precomputed so rasterization on the fly is not necessary.  An experimental implementation shows that real-time navigation can be achieved on arbitrarily large designs. Results also show how this technique ensures that image quality does not degrade as the number of polygons drawn increases, avoiding the aliasing artifacts common in other layout systems.",Using texture mapping with mipmapping to render a VLSI layout
"['Peter Benner', 'Pablo Ezzatti', 'Enrique S. Quintana-Ort??', 'Alfredo Rem??n']","In this paper we tackle the inversion of large-scale dense matrices via conventional matrix factorizations (LU, Cholesky, LDL T ) and the Gauss-Jordan method on hybrid platforms consisting of a multi-core CPU and a many-core graphics processor (GPU). Specifically, we introduce the different matrix inversion algorithms using a unified framework based on the notation from the FLAME project; we develop hybrid implementations for those matrix operations underlying the algorithms, alternative to those in existing libraries for singleGPU systems; and we perform an extensive experimental study on a platform equipped with state-of-the-art general-purpose architectures from Intel and a É??FermiÉ?ù GPU from NVIDIA that exposes the efficiency of the different inversion approaches. Our study and experimental results show the simplicity and performance advantage of the GJE-based inversion methods, and the difficulties associated with the symmetric indefinite case.",Matrix inversion on CPUÉ??GPU platforms with applications in control theory
"['Andrea M. Tonello', 'Fabio Versolatto', ""Salvatore D'Alessandro""]","We consider the use of a relay to provide capacity improvements and range extension for in-home power line communication networks. In particular, we focus on opportunistic relaying where the relay is exploited only if it provides improved capacity w.r.t. the use of direct transmission between the source and the destination. The relay applies a decode and forward scheme and the channel is shared in a time division multiple access mode. The performance is studied in statistically representative in-home power line communication (PLC) networks via the use of a statistical topology model together with the application of transmission line theory for the computation of the channel transfer function among network nodes. The statistical topology model allows determining the capacity improvements as a function of the relay position. Furthermore, we determine the optimal time slot duration for each considered relay configuration, as well as we propose the use of a globally optimal time slot duration that maximizes the average network capacity. The numerical results show that significant capacity improvement can be obtained via opportunistic relaying in in-home PLC networks. The gains are more significant for low SNR scenarios and for networks composed by sub-networks each connected to the main panel via a circuit breaker that introduces signal attenuation.",Opportunistic Relaying in In-Home PLC Networks
"['Ciprian Chelba', 'Alex Acero']","The paper presents the Position Specific Posterior Lattice (PSPL), a novel lossy representation of automatic speech recognition lattices that naturally lends itself to efficient indexing and subsequent relevance ranking of spoken documents.In experiments performed on a collection of lecture recordings --- MIT iCampus data --- the spoken document ranking accuracy was improved by 20% relative over the commonly used baseline of indexing the 1-best output from an automatic speech recognizer.The inverted index built from PSPL lattices is compact --- about 20% of the size of 3-gram ASR lattices and 3% of the size of the uncompressed speech --- and it allows for extremely fast retrieval. Furthermore, little degradation in performance is observed when pruning PSPL lattices, resulting in even smaller indexes --- 5% of the size of 3-gram ASR lattices.",SPEECH OGLE: Indexing Uncertainty for Spoken Document Search
"['Mazen Al-Khatib', 'M.Z. Huq', 'Rhishikesh Muley']","In this paper we suggest and compare the performance of different systems that can be used for wireless asynchronous transfer mode (WATM) to improve performance in terms of cell loss ratio and throughput under a certain fading wireless channel for a certain period of time. The type of fading considered in this paper is a result of a sudden and a sharp change in the signal level due to a sudden change in the weather condition (e.g. lightning) or due to a multipath effect caused by a mobile obstacle. It was found that compressing the ATM cell header and then applying the Bose-Chaudhuri-Hocquenghem BCH (i+18,i,3) code to the compressed header during the fading period, when the bit error rate ranges between 10/sup -3/ and 10/sup -1/, results in an optimum performance in terms of cell loss ratio and throughput.",Performance evaluation for different suggested wireless ATM systems deployed at a wireless channel with abrupt fading conditions
['Ajith Abraham'],"Recently Web mining has become a hot research topic, which combines two of the prominent research areas comprising of data mining and the World Wide Web (WWW). Web usage mining attempts to discover useful knowledge from the secondary data obtained from the interactions of the users with the Web. Web usage mining has become very critical for effective Web site management, business and support services, personalization, network traffic flow analysis and so on. Our previous study on Web usage mining using a concurrent neuro-fuzzy approach has shown that the usage trend analysis very much depends on the performance of the clustering of the number of requests. In this paper, a novel approach 'intelligent-miner' (i-Miner) is introduced to optimize the concurrent architecture of a fuzzy clustering algorithm (to discover data clusters) and a fuzzy inference system to analyze the trends. In the concurrent neuro-fuzzy approach, self-organizing maps were used to cluster the Web user requests. A hybrid evolutionary FCM approach is proposed in this paper to optimally segregate similar user interests. The clustered data is then used to analyze the trends using a Takagi-Sugeno fuzzy inference system learned using a combination of evolutionary algorithm and neural network learning. Empirical results clearly shows that the proposed technique is efficient with lesser number of if-then rules and improved accuracy at the expense of complicated algorithms and extra computational cost.",i-Miner: a Web usage mining framework using hierarchical intelligent systems
"['John Klein', 'Christele Lecomte', 'Pierre Miche']","This article presents a preceding car rear view tracking algorithm which utilizes a particle filter and belief function data fusion. Most of tracking applications resort to only one source of information, making the system dependent on the source reliability. To achieve more robust and longer tracking, multiple source data fusion is a solution. Belief functions are a powerful tool for data fusion. Using bridges between probability theory and belief function theory, data fusion information can be incorporated inside a particle filter. The efficiency of the proposed method is demonstrated on natural on-road sequences.",Preceding car tracking using belief functions and a particle filter
"['Alireza Kenarsari-Anhari', 'Lutz Lampe']","Bit-interleaved coded modulation (BICM) has been adopted in many systems and standards for spectrally efficient coded transmission. The analytical evaluation of BICM performance parameters, in particular bit-error rate (BER), has received considerable attention in the recent past. In this paper, we derive BER approximations for BICM transmission over general fading channels impaired by Gaussian mixture noise (GMN). To this end, we build upon the saddlepoint approximation of the pairwise error probability (PEP) and a recently established approximation for the probability density function (PDF) of bit-wise reliability metrics for nonfading additive white Gaussian noise (AWGN) channels. We extend this PDF approximation to the case of GMN, and obtain closed-form expressions for its Laplace transform for fading GMN channels. The latter allows us to express the PEP and thus BER via the saddlepoint approximation. For the special case of fading AWGN channels the presented approximations are closed form, since the saddlepoint is well approximated by 1/2 for BICM decoding. Furthermore, we derive closed-form PEP expressions also for GMN channels in the high signal-to-noise ratio regime and establish the diversity and coding gain for BICM transmission over fading GMN channels. Selected numerical results for the BER of convolutional coded BICM highlight the usefulness of the proposed approximations and the differences between AWGN and GMN channels.",Performance Analysis for BICM Transmission over Gaussian Mixture Noise Fading Channels
"['Hang Liu', 'Mingquan Wu', 'Dekai Li', 'Saurabh Mathur', 'Kumar Ramaswamy', 'Liqiao Han', 'Dipankar Raychaudhuri']","We report the implementation experience and experimental evaluation of a staggered adaptive forward error correction (FEC) system for video multicast over wireless LANs. In the system, the parity packets generated by a cross-packet FEC code are transmitted at a time delay from the original video packets, i.e. staggercasting video stream and FEC stream in different multicast groups. The delay provides temporal diversity to improve the robustness of video multicast, especially to enable the clients to correct burst packet loss using FEC and to achieve seamless handoff. A wireless client dynamically joins the FEC multicast groups based upon its channel conditions and handoff events. We have implemented the system including the streaming server and client proxy. A novel software architecture is designed to integrate the FEC functionality in the clients without requirement for changing the existing video player software. We conduct extensive experiments to investigate the impact of FEC overhead and the delay between the video stream and FEC stream to the video quality under different interference levels and mobile handoff durations. The efficacy of staggered adaptive FEC system on improving video multicast quality is demonstrated in real system implementation.",A Staggered FEC System for Seamless Handoff in Wireless LANs: Implementation Experience and Experimental Study
"['Ioan Dor?? Landau', 'Marouane Alma']",Adaptive feedforward broadband vibration (or noise) compensation is currently used when an image of the disturbance is available. However in most of the systems there is a É??positiveÉ?ù feedback coupling between the compensator system and the measurement of the image of the disturbances. The paper proposes a new algorithm taking in account this coupling effect and the corresponding analysis. The algorithm has been applied to an active vibration control (AVC) system and real time results are presented.,An adaptive feedforward compensation algorithm for active vibration control
"['Khaled Belahcene', 'Christophe Labreuche', 'Nicolas Maudet', 'Vincent Mousseau', 'Wassila Ouerdane']","As decision-aiding tools become more popular everydayÉ??but at the same time more sophisticatedÉ??it is of utmost importance to develop their explanatory capabilities. Some decisions require careful explanations, which can be challenging to provide when the underlying mathematical model is complex. This is the case when recommendations are based on incomplete expression of preferences, as the decision-aiding tool has to infer despite this scarcity of information. This step is key in the process but hardly intelligible for the user. The robust additive utility model is a necessary preference relation which makes minimal assumptions, at the price of handling a collection of compatible utility functions, virtually impossible to exhibit to the user. This strength for the model is a challenge for the explanation. In this paper, we come up with an explanation engine based on sequences of preference swaps, that is, pairwise comparison of alternatives. The intuition is to confront the decision maker with É??elementaryÉ?ù comparisons, thus building incremental explanations. Elementary here means that alternatives compared may only differ on two criteria. Technically, our explanation engine exploits some properties of the necessary preference relation that we unveil in the paper. Equipped with this, we explore the issues of the existence and length of the resulting sequences. We show in particular that in the general case, no bound can be given on the length of explanations, but that in binary domains, the sequences remain short.",Explaining robust additive utility models by sequences of preference swaps
['Tobias Keim'],"Recommender systems (RS) so far have been applied to many fields of e-commerce in order to assist users in finding the products that best meet their preferences. However, while the application of RS to the search for objects is well established, this is not the case for the search for subjects. This is astonishing as a growing number of people make personal and professional information digitally available to others by managing profiles in CV databases, social networking platforms and other online services. In order to address this new field of application for RS, we integrate own prior research into a unified multilayer framework supporting the matching of individuals for recruitment and team staffing processes. By this means we enhance RS research and make a next step towards the development of empirically and theoretically grounded decision support for the human resources function",Extending the Applicability of Recommender Systems: A Multilayer Framework for Matching Human Resources
"['Torsten Braun', 'Linqing Liu']",This paper describes a concept to support scalable multicast communications for small audio/video conferencing groups on the Internet. The solution presented in this paper is based on extensions of IPv6 and the session description protocol (SDP). A goal of the concept called multicast for small conferences (MSC) is the smooth deployment in the Internet.,Multicast for small conferences
['Thorsten Ehm'],This paper shows how to use the transformation of Paterson and Hewitt to improve the memory and operations used in a pointer algorithm. That transformation scheme normally is only of theoretical interest because of the inefficient performance of the transformed function. However we present a method how it can be used to decrease the amount of selective updates in memory while preserving the original runtime performance. This leads to a general transformation framework for the derivation of a class of pointer algorithms.,Transformational Construction of Correct Pointer Algorithms
"['Sergey Boldyrev', 'Ian Justin Oliver', 'Ronald Brown', 'Juha-Matti Tuupola', 'Arto Palin', 'Antti Lappetel??inen']","The presented approach addresses the problem of query and persistent query (subscription) resolution, taking into consideration distribution across multi-domains, network infrastructure and content management. This approach is particularly suitable for information-centric and cloud computing applications based around a mobile-device infrastructure.",Network and content aware information management
"['Tommi Kramer', 'Lars Klimpke', 'Armin Heinzl']","Outsourcing of software development tasks has become a major issue for large software enterprises over the last decades. Nowadays, small and medium-sized enterprises (SMEs) follow this trend and outsource parts of their software development as well. However, most of the existing literature deals with large enterprises whereas the situation of SMEs is being neglected. Especially sourcing decisions and the organizational as well as operational setup may differ between large enterprises and SMEs. We choose an exploratory multiple-case study approach focusing on the German software market in order to shed light on these aspects of the sourcing behavior of SMEs. This paper addresses three complementing research questions. Besides the question of which phases of the software development process qualify for outsourcing, we explore the organizational setup of SMEs' outsourcing scenarios. In addition, we seek to find out which characteristics a software component has to fulfill in order to qualify for outsourcing.",Outsourcing Decisions of Small and Medium-Sized Enterprises: A Multiple-Case Study Approach in the German Software Industry
"['Pei Sun', 'Xin Li', 'Ming Yuan Ting']","In this paper, a new sparse approximation technique is proposed for incremental power grid analysis. Our proposed method is motivated by the observation that when a power grid network is locally updated during circuit design, its response changes locally and, hence, the incremental ""change"" of the power grid voltage is almost zero at many internal nodes, resulting in a unique sparse pattern. An efficient Orthogonal Matching Pursuit (OMP) algorithm is adopted to solve the proposed sparse approximation problem. In addition, several numerical techniques are proposed to improve the numerical stability of the proposed solver, while simultaneously maintaining its high efficiency. Several industrial circuit examples demonstrate that when applied to incremental power grid analysis, our proposed approach achieves up to 130?? runtime speed-up over the traditional Algebraic Multi-Grid (AMG) method, without surrendering any accuracy.",Efficient incremental analysis of on-chip power grid via sparse approximation
"['Andreas Heil', 'Iman Moradi', 'Torben Weis']","In this paper, we present a high-level graphical language to develop pervasive applications based on a unique interface design. The language supports a wide range of programming constructs. Its graphical notation is based on the LCARS design, which is appealing to different target groups, based on their specific interests and requirements. We show that users can easily create pervasive applications using an LCARS-based user interface. The first step is to describe the technical context in which the application will execute. Based on this technical context, the UI offers a context-specific set of visual primitives. By composing these visual primitives on the screen, the user can specify the behavior of the application.",LCARS: the next generation programming context
"['Zhe Lin', 'Larry S. Davis', 'David S. Doermann', 'Daniel DeMenthon']","Local part-based human detectors are capable of handling partial occlusions efficiently and modeling shape articulations flexibly, while global shape template-based human detectors are capable of detecting and segmenting human shapes simultaneously. We describe a Bayesian approach to human detection and segmentation combining local part-based and global template-based schemes. The approach relies on the key ideas of matching a part-template tree to images hierarchically to generate a reliable set of detection hypotheses and optimizing it under a Bayesian MAP framework through global likelihood re-evaluation and fine occlusion analysis. In addition to detection, our approach is able to obtain human shapes and poses simultaneously. We applied the approach to human detection and segmentation in crowded scenes with and without background subtraction. Experimental results show that our approach achieves good performance on images and video sequences with severe occlusion.",Hierarchical Part-Template Matching for Human Detection and Segmentation
"['Michael Rowe', 'Vivienne Bozalek', 'Jose M. Frantz']","Abstract#R##N##R##N#While technology has the potential to create opportunities for transformative learning in higher education, it is often used to merely reinforce didactic teaching that aims to control access to expert knowledge. Instead, educators should consider using technology to enhance communication and provide richer, more meaningful platforms for the social construction of knowledge. By using technology to engage in shared learning experiences that extend beyond the walls of the classroom, we can create opportunities to develop the patterns of thinking that students need to participate in complex, real world situations. We used authentic learning as a framework to guide the implementation of a case-based, blended module in a South African physiotherapy department. Google Drive was used as a collaborative online authoring environment in which small groups of students used clinical cases to create their own content, guided by a team of facilitators. This paper describes an innovative approach to clinical education using authentic learning as a guiding framework, and Google Drive as an implementation platform. We believe that this approach led to the transformation of student learning practices, altered power relationships in the classroom and facilitated the development of critical attitudes towards knowledge and authority.",Using Google Drive to facilitate a blended approach to authentic learning
"['Manuel Barranco', 'Julian Proenza', 'Luis Almeida']","There has been an increasing interest in using star topologies in field-bus communications, e.g., in Time Triggered Protocol for SAE classC applications (TTP/C), FlexRay, or controller area network (CAN), due to increased fault resilience and potential error-containment advantages. In this context, an innovative CAN-compliant star topology, CANcentrate, has been developed, whose hub includes enhanced fault-treatment mechanisms. However, despite this interest toward stars, it is still necessary to quantify their real dependability benefits. For this purpose and for the particular case of CAN, this paper presents models for the dependability features of CAN and CANcentrate using Stochastic Activity Networks (SANs). It quantitatively compares their reliability and error-containment capabilities under permanent hardware faults. These models rely on assumptions that ensure that results are not biased toward CANcentrate, which, in some cases, is too detrimental for it. Thus, despite not reflecting the full CANcentrate potential, results quantitatively confirm the improvement of error-containment it achieves over CAN. Additionally, the way in which the nodes' ability to contain their own errors affects the relevance of using a star topology has been quantified. Although this paper refers to the case of CAN, conclusions regarding the justification of using a star topology depending on this ability can be extrapolated to other field-bus technologies.",Quantitative Comparison of the Error-Containment Capabilities of a Bus and a Star Topology in CAN Networks
"['Shih-Yu Chen', 'Zhu Rong Li', 'Chern-Lin Chen']","Analysis and design of a single-stage  LLC  resonant converter are proposed. A single-stage converter uses only one control signal to drive two power converters, a power factor corrector (PFC) converter and a dc/dc converter, for reducing the cost of the system. However, this simplicity induces power imbalance between two converters, and then, the bus voltage between two converters drifts and becomes unpredictable. To ensure that the bus capacitor voltage can be kept in a tolerable region, the characteristics of a PFC converter and an  LLC  tank are investigated, and then, a design procedure is proposed correspondingly. Finally, a single-stage  LLC  resonant converter is implemented to verify the analysis.",Analysis and Design of Single-Stage AC/DC $LLC$ Resonant Converter
"['Sajid Hussain', 'Md. Rafiqul Islam', 'Elhadi M. Shakshuki', 'M. S. Zaman']","This paper investigates the architecture and design of agent-based sensor networks for petroleum offshore monitoring. A few challenges to monitor the reservoir, wellbore and wellhead are identified. Moreover, the necessary components for a reliable, precise, and accurate monitoring are suggested. The paper describes the architecture of the routing agent and discusses the cross layer optimization issues for query processing. The paper also provides the software design and components for a web-based continuous monitoring application.",Agent-Based Petroleum Offshore Monitoring Using Sensor Networks
"['Huiyan Jiang', 'Yudong Zhao', 'Ning Li']","This article carries on a new method which can improve quality and speed of 3D visualization. This method, first based on dynamic threshold method, divides region of interest ROI from the original image; then extends bounding box algorithm to 3D space, and combines it with ray casting algorithm. We validate the validity and robustness of this method in 3D reconstruction, experimenting with 3D lung parenchyma's image, vascular image and bones' image, which used to be 2D medical images about chest CT.",The Study of 3D Reconstruction Method Based on Dynamic Threshold Method and Improved Ray Casting Algorithm
"['Krishna Chandramouli', 'Craig D. Stewart', 'Tim J. Brailsford', 'Ebroul Izquierdo']","The presentation of learning materials in Adaptive Education Hypermedia is influenced by several factors such as learning style, background knowledge and cultural background, to name a few. In this paper, we introduce the notion of the CAE-L Ontology for modelling stereotype cultural artefacts in adaptive education. The Ontology design is based on the user study gathered from the respondents to the CAE questionnaire which determines the cultural artefacts that influence a learner?s behaviour within an educational environment. We present a brief overview of the implementation and discuss the stereotype presentation styles from three different countries, namely China, Ireland and UK.",CAE-L: An Ontology Modelling Cultural Behaviour in Adaptive Education
"['Scott Morton', 'Luke Scharber', 'Nikolaos Papanikolopoulos']","An aircraft that is capable of continuous flight offers a new level of autonomous capacity for unmanned aerial vehicles. We present an overview of the components and concepts of a small scale unmanned aircraft that is capable of sustaining powered flight without a theoretical time limit. We then propose metrics that quantify the robustness of continuous flight achieved and optimization criteria to maximize these metrics. Finally, the criteria are applied to a fabricated and flight tested small scale high efficiency aircraft prototype to determine the optimal battery and photovoltaic array mass for robust continuous flight.",Solar powered unmanned aerial vehicle for continuous flight: Conceptual overview and optimization
"['Merc?? Teixid??', 'Tom?ˇs Pallej?ˇ', 'Marcel Tresanchez', 'Miquel Nogu??s', 'Jordi Palac??n']",This work describes the analysis of different walking paths registered using a Light Detection And Ranging (LIDAR) laser range sensor in order to measure oscillating trajectories during unsupervised walking. The estimate of the gait and trajectory parameters were obtained with a terrestrial LIDAR placed 100 mm above the ground with the scanning plane parallel to the floor to measure the trajectory of the legs without attaching any markers or modifying the floor. Three different large walking experiments were performed to test the proposed measurement system with straight and oscillating trajectories. The main advantages of the proposed system are the possibility to measure several steps and obtain average gait parameters and the minimum infrastructure required. This measurement system enables the development of new ambulatory applications based on the analysis of the gait and the trajectory during a walk.,Measuring Oscillating Walking Paths with a LIDAR
"['Paul M. J. Van den Hof', 'Ruud J.P. Schrama', 'Raymond A. de Callafon', 'O.H. Bosgra']",Recently introduced methods of iterative identification and control design are directed towards the design of high performing and robust control systems. These methods show the necessity of identifying approximate models from closed loop plant experiments. In this paper a method is proposed to approximately identify normalized coprime plant factors from closed loop data. The fact that normalized plant factors are estimated gives specific advantages both from an identification and from a robust control design point of view. It will be shown that the proposed method leads to identified models that are specifically accurate around the bandwidth of the closed loop system. The identification procedure fits very naturally into a recently developed the iterative identification/control design scheme based on HÉ?? robustness optimization.,Identification of Normalised Coprime Plant Factors from Closed-loop Experimental Data
['Nobuhiro Sawano'],"Automatic Identification System (AIS) has to be on board in every ship over 500 gross tonnages (GT) in Japan from July 1, 2008. This is a response of amendment of maritime relating laws in accordance with SOLAS (International Convention for the Safety of Life at Sea) Convention and IMO (International Maritime Organization) requires all ships over 500 GT or upward are fitted with ship borne AIS. AIS is a system which makes it possible to get precise on-line information from a large area about ships and their movements. AIS is based on a ship borne radio wave of VHF, it continuously and automatically transmits fixed, dynamic and voyage-related information and receives corresponding information from other ships near by. VTS (Vessel Transport Service) is provided by collection of AIS information from ships. Most of the VTS stations around the area of Gulf of Finland have equipped with coastal surveillance system with radar for early warnings to endangering ships. Combinations of AIS and VTS, in addition, practical lane-separation system have also been activated under international agreement of Russia, Estonia and Finland. These three systems are well combined and organised then significant effects of reducing risks of ship borne accidents have been observed. Practices of Gulf of Finland should be introduced other congested sea lanes.",Current Situation of Digitalized Ship Navigation System for Safety
"['Pey-Chang Kent Lin', 'Sunil P. Khatri']","The inference of gene predictors in the gene regulatory network (GRN) has become an important research area in the genomics and medical disciplines. Accurate predicators are necessary for constructing the GRN model and to enable targeted biological experiments that attempt to validate or control the regulation process. In this paper, we implement a SAT-based algorithm to determine the gene predictor set from steady state gene expression data (attractor states). Using the attractor states as input, the states are ordered into attractor cycles. For each attractor cycle ordering, all possible predictors are enumerated and a conjunctive normal form (CNF) expression is generated which encodes these predictors and their biological constraints. Each CNF is solved using a SAT solver to find candidate predictor sets. Statistical analysis of the resulting predictor sets selects the most likely predictor set of the GRN, corresponding to the attractor data. We demonstrate our algorithm on attractor state data from a melanoma study [1] and present our predictor set results.",Inference of gene predictor set using Boolean satisfiability
"['Wei Li', 'Joseph E. Sutton', 'Yunyi Li']","This paper develops new multiple sensor-based algorithms for identifying a chemical odor source in a near-shore and ocean environment via an autonomous underwater vehicle (AUV). Those algorithms implement two modules: source declaration and source verification, which are embedded in a subsumption architecture for chemical plume tracing (CPT). The source declaration module is based on chemical events detected by a chemical sensor, in combination with measured vehicle locations and fluid flow directions, while the source verification module uses a fuzzy color segmentation algorithm to process an image taken when the odor source is declared.",Integration of Chemical and Visual Sensors for Identifying an Odor Source in Near Shore Ocean Conditions
"['Luciano Vieira de Ara?ßjo', 'Sabri Saeed Sanabani', 'Ester C. Sabino', 'Jo?úo Eduardo Ferreira']",An automated web based tool for assigning HIV-1 pure and recombinant subtypes within unaligned sequences is presented. The system combines the BLAST search algorithm and the recombination identification program for genetic subtyping of HIV-1. The software was validated through combined analysis of simulated and other HIV-1 real data.,HIVSetSubtype: software for subtype classification of HIV-1 sequences
['Guoyan Zheng'],"The widely used procedure of evaluation of cup orientation following THA using single standard anteroposterior radiograph is known inaccurate, largely due to the wide variability in individual pelvic position relative to X-ray plate. 3D-2D image registration methods have been introduced to estimate the transformation between a CT volume and the radiograph for an accurate estimation of the cup orientation relative to an anatomical reference extracted from the CT data. However, the robustness of these methods is questionable. This paper presents a robust image similarity measure which is derived from a variational approximation to mutual information and allows for incorporation of spatial information via energy minimization. Experimental results on estimating cup alignment from single X-ray radiograph with gonadal shielding demonstrate the robustness and the accuracy of the present approach.",Robust intensity-based 3D-2D registration of ct and X-ray images for precise estimation of cup alignment after total hip arthroplasty
"['Michael Elad', 'Boaz Matalon', 'Michael Zibulevsky']","Shrinkage is a well known and appealing denoising technique. The use of shrinkage is known to be optimal for Gaussian white noise, provided that the sparsity on the signal??s representation is enforced using a unitary transform. Still, shrinkage is also practiced successfully with nonunitary, and even redundant representations. In this paper we shed some light on this behavior. We show that simple shrinkage could be interpreted as the first iteration of an algorithm that solves the basis pursuit denoising (BPDN) problem. Thus, this work leads to a novel iterative shrinkage algorithm that can be considered as an effective pursuit method. We demonstrate this algorithm, both on synthetic data, and for the image denoising problem, where we learn the image prior parameters directly from the given image. The results in both cases are superior to several popular alternatives.",Image Denoising with Shrinkage and Redundant Representations
"['Eduardo V. L. Nunes', 'Liu Hsu']","This paper shows that a well-known causal PD controller plus feedforward solves the global output feedback tracking control problem of robot manipulators, by requiring only the existence of the robot natural damping, no matter how small. To this end, we first demonstrate that a robot controlled by a causal PD is globally input-to-state stable (ISS) with respect to a bounded input disturbance. Then, we prove that the addition of a feedforward compensation renders the error system uniformly globally asymptotically stable. Furthermore, we present a possible extension to more general nonlinear systems and also to uncertain systems.",Global tracking for robot manipulators using a simple causal pd controller plus feedforward
"['George R. R. Justo', 'P. Vekariya', 'T. Delaitre', 'Jamal Zemerly', 'Stephen Winter']","We discuss the problem of developing performance-oriented software and the need for methodologies. We then present the EDPEPPS (Environment for Design and Performance Evaluation of Portable Parallel Software) approach to the problem of designing and evaluating high-performance (parallel) applications. The EDPEPPS toolset is based on a rapid prototyping philosophy, where the designer synthesises a model of the intended software which may be simulated, and the performance is subsequently analysed using visualisation. The toolset combines a graphical design tool, a simulation facility, and a visualisation tool. The same design is used to produce a code suitable for simulation and real execution.",Prototype-oriented development of high-performance systems
"['Nicholas A. Lynch', 'Cagdas D. Onal', 'Eugenio Schuster', 'Metin Sitti']","In this paper, a strategy for controlled pushing is presented for microassembly of 4.5 mum polystyrene particles on a flat glass substrate using an atomic force microscope probe tip. Real-time vision based feedback from a CCD camera mounted to a high resolution optical microscope is used to track particle positions relative to the tip and target position. Tip-particle system is modeled in 2D as a nonholonomic differential drive robot. Effectiveness of the controller is demonstrated through experiments performed using a single goal position as well as linking a series of target positions to form a single complex trajectory. Cell decomposition and wavefront expansion algorithms are implemented to autonomously locate a navigable path to a specified target position. Control strategy alleviates problem of slipping and spinning during pushing.",A Strategy for Vision-Based Controlled Pushing of Microparticles
"['Yasser Hamed', 'John Kahwaty', 'Andy Lin', 'Evan Goldberg', 'Lawrence Chai']","On Disney's  Big Hero 6 , we needed to create the city of  San Fransokyo  with unparalleled levels of visual complexity. The cityscape has more buildings and more geometry than any prior Disney film. Inhabiting this city are hundreds of unique characters, each performing a high caliber of animation individually and as a group. These challenges prompted a major upgrade to our existing crowd pipeline and the development of several new technologies in authoring crowd characters, generating crowd animation cycles, and instancing crowds for rendering.",Crowd character complexity on Big Hero 6
"['Angelo Corsaro', 'Douglas C. Schmidt']","This paper provides two contributions to the study of programming languages and middleware for real-time and embedded applications. First, we present the empirical results from applying the RTJPerf benchmarking suite to evaluate the efficiency and predictability of several implementations of the real-time specification for Java (RTSJ). Second, we describe some of the techniques used to develop jRate, which is an open-source ahead-of-time-compiled implementation of RTSJ we are developing. Our results indicate that RTSJ implementations are maturing to the point where they can be applied to a variety of real-time embedded applications.",Evaluating real-time Java features and performance for real-time embedded systems
"['Melanie E. Moses', 'Soumya Banerjee']","Distributed search problems are ubiquitous in Artificial Life (ALife). Many distributed search problems require identifying a rare and previously unseen event and producing a rapid response. This challenge amounts to finding and removing an unknown needle in a very large haystack. Traditional computational search models are unlikely to find, nonetheless, appropriately respond to, novel events, particularly given data distributed across multiple platforms in a variety of formats and sources with variable and unknown reliability. Biological systems have evolved solutions to distributed search and response under uncertainty. Immune systems and ant colonies efficiently scale up massively parallel search with automated response in highly dynamic environments, and both do so using distributed coordination without centralized control. These properties are relevant to ALife, where distributed, autonomous, robust and adaptive control is needed to design robot swarms, mobile computing networks, computer security systems and other distributed intelligent systems. They are also relevant for searching, tracking the spread of ideas, and understanding the impact of innovations in online social networks. We review design principles for Scalable Robust, Adaptive, Decentralized search with Automated Response (Scalable RADAR) in biology. We discuss how biological RADAR scales up efficiently, and then discuss in detail how modular search in the immune system can be mimicked or built upon in ALife. Such search mechanisms are particularly useful when components have limited capacity to communicate and when physical distance makes communication more costly.","Biologically inspired design principles for Scalable, Robust, Adaptive, Decentralized search and automated response (RADAR)"
"['Brian Amedro', 'Fran??oise Baude', 'Fabrice Huet', 'Elton N. Mathias']","Distributed computing environments have evolved from in-house clusters to Grids and now Cloud platforms. We, as others, provide HPC benchmarks results over Amazon EC2 that show a lower performance of Cloud resources compared to private resources., So, it is not yet clear how much of impact Clouds will have in high performance computing (HPC). But hybrid Grid/Cloud computing may offer opportunities to increase overall applications performance, while benefiting from in-house computational resources extending them by Cloud ones only whenever needed. In this paper, we advocate the usage of Proactive, a well established middleware in the grid community, for mixed Grid/Cloud computing, extended with features to address Grid/Cloud issues with little or, no effort for application developers. We also introduce a framework, developed in the context of the Disco Grid project, based upon the Proactive middleware to couple HPC domain-decomposition SPMD applications in heterogeneous multi-domain environments. Performance results, coupling Grid and Cloud resources for the execution of such, kind of highly communicating and processing intensive applications, have shown an overhead of about 15%, which is a non-negligible value, but lower enough to consider using such environments to achieve a better cost-performance trade-off than using exclusively Cloud resources.",Combining Grid and Cloud Resources by Use of Middleware for SPMD Applications
"['Chuck Baldwin', 'Tina Eliassi-Rad', 'Ghaleb Abdulla', 'Terence Critchlow']","As scientific data sets grow exponentially in size, the need for scalable algorithms that heuristically partition the data increases. In this paper, we describe the three-step evolution of a hierarchical partitioning algorithm for large-scale spatio-temporal scientific data sets generated by massive simulations. The first version of our algorithm uses a simple top-down partitioning technique, which divides the data by using a four-way bisection of the spatio-temporal space. The shortcomings of this algorithm lead to the second version of our partitioning algorithm, which uses a bottom-up approach. In this version, a partition hierarchy is constructed by systematically agglomerating the underlying Cartesian grid that is placed on the data. Finally, the third version of our algorithm utilizes the intrinsic topology of the data given in the original scientific problem to build the partition hierarchy in a bottom-up fashion. Specifically, the topology is used to heuristically agglomerate the data at each level of the partition hierarchy. Despite the growing complexity in our algorithms, the third version of our algorithm builds partition hierarchies in less time and is able to build trees for larger size data sets as compared to the previous two versions.",The evolution of a hierarchical partitioning algorithm for large-scale scientific data: three steps of increasing complexity
"['Jarosè?aw Buczyè?ski', 'Adam Ginensky', 'J. M. Landsberg']","We address special cases of a question of Eisenbud on the ideals of secant varieties of Veronese re-embeddings of arbitrary varieties. Eisenbud's question generalizes a conjecture of Eisenbud, Koh and Stillman (EKS) for curves. We prove that set-theoretic equations of small secant varieties to a high degree Veronese re-embedding of a smooth variety are determined by equations of the ambient Veronese variety and linear equations. However this is false for singular varieties, and we give explicit counter-examples to the EKS conjecture for singular curves. The techniques we use also allow us to prove a gap and uniqueness theorem for symmetric tensor rank. We put Eisenbud's question in a more general context about the behaviour of border rank under specialisation to a linear subspace, and provide an overview of conjectures coming from signal processing and complexity theory in this context.",Determinantal equations for secant varieties and the Eisenbud-Koh-Stillman conjecture
"['Martin Hillenbrand', 'Klaus D. M?¨ller-Glaser']","TodayÉ??s vehicles include a complex network of programmableelectronic control units with software components. AvehicleÉ??s electric and electronic (EE) architecture has to bemodeled in an early design phase to evaluate design alternatives.The tool PREEvision offers possibilities to modelEE-architectures considering feature function networks,function networks, component networks as well as wiringharness and the respective mappings.The software architecture specified by AUTOSAR separateshardware dependent and hardware independent softwaremodules. This allows the mapping of hardware independentsoftware applications to different hardware platforms.Hardware-in-the-Loop (HiL) is an established technologyfor testing electronic control units (ECU) and to assurequality. HiL-test-systems (HiL-TS) simulate the ECUÉ??sfunctional environment (car, driver, road, tires, etc.) andadditionally offers the possibility to insert logical faults aswell as electrical faults (short circuit, open load, etc.).Mostly, this HiL-simulation is individually engineered forevery single ECU.This paper introduces a concept for the automated supportof such simulations. This includes the derivation of relevantinformation from the model of the EE-architecture as wellas the portation of the AUTOSAR software architecture tothe HiL-TS. Following this concept, engineering costs canbe reduced and the quality and correctness of the simulationincreased.",An Approach to Supply Simulations of the Functional Environment of ECUs for Hardware-in-the-Loop Test Systems Based on EE-architectures Conform to AUTOSAR
"['Yanlong Bu', 'Shuangchun Peng', 'Nan Wang', 'Hui Peng', 'Lincheng Shen']","Mutual information based matching-suitable features are studied in this paper, for the selection of good matching areas with high success probability in SAR matching aided navigation system. Based on the analysis of SAR imaging and multi-source matching, four feature constructing guide lines are proposed first. Then ten candidate features are designed under mutual information measurement. Effectiveness of these features is tested and compared through experiments under real SAR data. And features, such as reference image complexity and absolute value roughness et.al., which show stable monotony and good convergence, can be use as good matching-suitable features in practices.",Constriction of Mutual Information Based Matching-Suitable Features for SAR Image Aided Navigation
"['Zhihai He', 'Tian-Hu Yu', 'Sanjit K. Mitra']","A coding algorithm of low addressing and implementation complexity is proposed. It is based on partitioning uniformly quantized wavelet coefficients into multiscale blocks with each block classified as either an all-zero block or a non-zero block. The non-zero blocks are coded by a new method called zero-mapping whose outputs are further compressed by a first-order arithmetic coder. The performance of the proposed coding algorithm compares favorably with that of some well-known coding algorithms, particularly for images with considerable high frequency components.",Blockwise zero mapping image coding
"['M. Jacobs', 'Nikos Deligiannis', 'Frederik Verbist', 'J. Slowack', 'Joeri Barbarien', 'Rik Van de Walle', 'Peter Schelkens', 'Adrian Munteanu']","Novel distributed video coding (DVC) architectures developed by the IBBT DVC group realize state-of-the-art video coding efficiency under stringent energy restrictions, while supporting error-resilience and scalability. Therefore, these architectures are particularly attractive for application scenarios involving low-complexity energy-constrained wireless visual sensors. This demo presents the scenarios, which are considered to be the most promising areas of integration for IBBT's DVC systems, considering feasibility and commercial applicability.",Demo: Distributed video coding applications in wireless multimedia sensor networks
"['Ariel Oleksiak', 'Alisdair Tullo', 'Paul Graham', 'Tomasz Kuczynski', 'Jarek Nabrzyski', 'Dawid Szejnfeld', 'Terry Sloan']","One of the goals of the HPC-Europa project is to provide users with a Single Point of Access (SPA) to the resources of HPC centers in Europe. To this end, the HPC-Europa Portal is being built to provide transparent, uniform, flexible and intuitive user access to HPC-Europa resources. In this paper, we present a mechanism that enables end-users to transparently access the diverse services available in the HPC-Europa environment. The uniform job submission interface that uses this mechanism, utilizing the job specification description language (JSDL), is described. We also present the architecture of the SPA, based on the GridSphere portal framework. Finally, we discuss the various interoperability problems encountered, in particular those concerning job submission, security and accounting.",HPC-Europa: towards uniform access to European HPC infrastructures
"['Zeynab Mirzadeh', 'Jean-Fran??ois Boland', 'Yvon Savaria']","Cosmic rays lead to soft errors and faulty behavior in electronic circuits. Knowing about their faulty behavior before fabrication would be helpful. This research proposes an approach for modeling the faulty behaviour of digital circuits. It could be applied in a design flow before circuit fabrication. This is achieved by extracting information about faulty behaviour of circuits from low-level models expressed in the VHDL language. Afterwards the extracted information is used to train high-level artificial neural networks models expressed in C/C++ or MATLAB TM  languages. The trained neural network models are able to replicate the behaviour of circuits in presence of faults. The methodology is based on experiments done with two benchmarks, the ISCAS-C17 and a 4-bit multiplier. Results show that the neural network approach leads to models that are more accurate than a previously reported signature generation method. For the C17, using only 30% of the dataset generated with the LIFTING fault simulator, the neural network is able to replicate the output of the circuit in presence of faults with a mean absolute modeling error below 6%.",Modeling the faulty behaviour of digital designs using a feed forward neural network approach
"['Jun Fang', 'Hongbin Li']","We consider distributed estimation of a deterministic vector parameter from noisy sensor observations in a wireless sensor network (WSN). The observation noise is assumed uncorrelated across sensors. To meet stringent power and bandwidth budgets inherent in WSNs, local data dimensionality reduction is performed at each sensor to reduce the number of messages sent to a fusion center (FC). The problem of interest is to jointly design the compression matrices associated with those sensors, aiming at minimizing the estimation error at the FC. Such a dimensionality reduction problem is investigated in this paper. Specifically, we study a homogeneous environment where all sensors have identical noise covariance matrices and an inhomogeneous environment where the noise covariance matrices across the sensors have the same correlation structure but with different scaling factors. Given a total number of messages sent to the FC, theoretical lower bounds on the estimation error of any compression strategy are derived for both cases. Compression strategies are developed to approach or even attain the corresponding theoretical lower bounds. Performance analysis and simulations are carried out to illustrate the optimality and effectiveness of the proposed compression strategies.",Optimal/Near-Optimal Dimensionality Reduction for Distributed Estimation in Homogeneous and Certain Inhomogeneous Scenarios
"['Fu-Yun Tsuo', 'Jen-Ping Huang', 'Chun-Han Ko', 'Hung-Yu Wei']","Multicast and broadcast service (MBS) is a service offered by the base station (BS) to multiple receivers requesting the same information. Such a BS must be kept updated with the receivers' feedback information (e.g., packet loss rates) to configure the MBS. We propose MBS operation schemes that are dominant-strategy incentive compatible in game theory, i.e., the schemes induce dominant-strategy equilibria, where all selfish receivers reveal their true information. Moreover, the induced equilibria are Pareto efficient and max-min fair. To conclude, the proposed schemes can elicit true feedback information from the receivers, avoiding any manipulation and, thereby, ensuring an efficient and fair system operation.",Incentive Compatible Configuration for Wireless Multicast: A Game Theoretic Approach
"['Xusheng Sun', 'Edward J. Coyle']","In clustered networks of wireless sensor motes, each mote collects noisy observations of the environment, quantizes these observations into a local estimate of finite length, and forwards them through one or more noisy wireless channels to the Cluster Head (CH). The measurement noise is assumed to be zero-mean and have finite variance. Each wireless hop is assumed to be a Binary Symmetric Channel (BSC) with a known crossover probability. We propose a novel scheme that uses dithered quantization and channel compensation to ensure that each motes' local estimate received by the CH is unbiased. The CH then fuses these unbiased local estimates into a global one using a Best Linear Unbiased Estimator (BLUE). The energy allocation problem at each mote and among different sensor motes are also discussed. Simulation results show that the proposed scheme can achieve much smaller mean square error (MSE) than two other common schemes while using the same amount of energy. The sensitivity of the proposed scheme to errors in estimates of the crossover probability of the BSC channel is studied by both analysis and simulation.","Quantization, channel compensation, and energy allocation for estimation in wireless sensor networks"
['Sergei A. Abramov'],We present an algorithm to compute rational function solutions to a first order system of linear q-difference equations with rational coefficients. We make use of the fact that q-difference equations bear similarity to differential equations at the point 0 and to difference equations at other points. This allows the combining of known algorithms for the differential and the difference cases. This algorithm does not require preliminary uncoupling of the given system.,A direct algorithm to compute rational solutions of first order linear q-difference systems
"['Rasha Tawhid', 'Dorina C. Petriu']","Product derivation is an essential part of the Software Product Line (SPL) development process. The paperproposes a model transformation for deriving automatically a UML model of a specific product from the UML model of a product line. This work is a part of a larger project aiming to integrate performance analysis in the SPL model-driven development. The SPL source model is expressed in UML extended with two separate profiles: a ""product line"" profile from literature for specifying the commonality and variability between products, and the MARTE profile recently standardized by OMG for performance annotations. The automatic derivation of a concrete product model based on a given feature configuration is enabled through the mapping between features from the feature model and their realizations in the design model. The paper proposes an efficient mapping technique that aims to minimize the amount of explicit feature annotations in the UML design model of SPL. Implicit feature mapping is inferred during product derivation from the relationships between annotated and non-annotated model elements as defined in the UML metamodel and well formedness rules. The transformation is realized in the Atlas Transformation Language (ATL) and illustrated with an ecommerce case study that models structural and behavioural SPL views.",Product Model Derivation by Model Transformation in Software Product Lines
"['Michael Grace-Martin', 'Geri Gay']","Students in two different courses at a major research university (one a Communication course, the other a Computer Science course) were given laptop computers with wireless network access during the course of a semester. StudentsÉ?? Web browsing on these laptops (including: URLs, dates, and times) was recorded 24 hours/day, 7 days/week in a log file by a proxy server during most of a semester (about 15 weeks). For each student, browsing behavior was quantified and then correlated with academic performance. The emergence of statistically significant results suggests that quantitative characteristics of browsing behaviorÉ??even prior to examining browsing contentÉ??can be useful predictors of meaningful behavioral outcomes. Variables such as Number of browsing sessions and Length of browsing sessions were found to correlate with studentsÉ?? final grades; the valence and magnitude of these correlations were found to interact with Course (i.e., whether student was enrolled in the Communication or Computer Science course), Browsing Context (i.e., setting in which browsing took place: during class, on the wireless network between classes, or at home) and Gender. The implications of these findings in relation to previous studies of laptop use in education settings are discussed.","Web Browsing, Mobile Computing and Academic Performance"
"['Chien-Chang Li', 'Yuan-Pei Lin', 'Shang-Ho Tsai', 'P. P. Vaidyanathan']","There have been many results on designing transceivers for MIMO channels. In early results, the transceiver is designed for a given bit allocation. In this paper we will jointly design the transceiver and bit allocation for maximizing bit rate. By using a high bit rate assumption, we will see that the optimal transceiver and bit allocation can be obtained in a closed form using simple Hadamard inequality and the Poincare separation theorem. In the simulation, we will demonstrate the usefulness of the joint design. Simulation results, in which a high bit rate assumption is not used in allocating bits, show that a higher bit rate can be achieved compared to previously reported methods.",Optimization of transceivers with bit allocation to maximize bit rate for MIMO transmission
"['Drew Davidson', 'Matthew Fredrikson', 'Benjamin Livshits']","Privacy and personalization of mobile experiences are inherently in conflict: better personalization demands knowing more about the user, potentially violating user privacy. A promising approach to mitigate this tension is to migrate personalization to the client, an approach dubbed  client-side personalization . This paper advocates for  operating system support  for client-side personalization and describes MoRePriv, an operating system  service  implemented in the Windows Phone OS. We argue that personalization support should be as ubiquitous as location support, and should be provided by a unified system within the OS, instead of by individual apps.   We aim to provide a solution that will stoke innovation around mobile personalization. To enable easy application personalization, MoRePriv approximates users' interests using  personae  such as technophile or business executive. Using a number of case studies and crowd-sourced user studies, we illustrate how more complex personalization tasks can be achieved using MoRePriv.   For privacy protection, MoRePriv distills sensitive user information to a coarse-grained  profile , which limits the potential damage from information leaks. We see MoRePriv as a way to increase end-user privacy by enabling client-side computing, thus minimizing the need to share user data with the server. As such, MoRePriv shepherds the ecosystem towards a better privacy stance by  nudging  developers away from today's privacy-violating practices. Furthermore, MoRePriv can be  combined  with privacy-enhancing technologies and is complimentary to recent advances in data leak detection.",MoRePriv: mobile OS support for application personalization and privacy
"['Yang Xiao', 'Yuguang Fang', 'Yi-Bing Lin']","Deregistration due to the departures of mobile users from their current visiting registration area may cause significant traffic in the wireless cellular networks. In this paper, we propose a hierarchical implicit deregistration scheme with forced registration in third-generation wireless cellular networks to reduce the remote/international roaming signaling traffic when home-location registers (HLRs), gateway-location registers (GLRs), and the visitor-location registers (VLRs) form a three-level database hierarchy. In this scheme, if a mobile phone arrives and the GLR/VLR is full, a random record is deleted and the reclaimed storage is reassigned to the new arriving mobile phone. When a call arrives and the callee's record is missing in the GLR/VLR, forced registration is executed to restore the GLR/VLR record before the call-setup operation proceeds. An analytic model is proposed to carry out the performance evaluation for the proposed scheme. Our results show that the proposed scheme not only reduces the local deregistration traffic between the GLR and the VLR, but also reduces the remote/international deregistration traffic between the HLR and the GLR, especially when the ratio of the cost of the remote/international traffic between GLR and HLR to the cost of local traffic between the VLR and the GLR is high.",Hierarchical implicit deregistration with forced registrations in 3G wireless networks
"['Thomas Ruland', 'Tomas Pajdla', 'Lars Kr?¨ger']",This paper introduces simultaneous global optimization of both camera orientation and vehicle wheel circumference without requiring any information about the translations in the system. The main contribution are new objective function bounds to integrate this problem into a branch-and-bound parameter space search. The presented method constitutes the first guaranteed globally optimal estimator for both components of the problem with respect to a cost function based on reprojection errors. The algorithm operates directly on image measurements and does not depend on any structure and motion preprocessing to estimate camera poses. The complete system is implemented and validated on both synthetic and real automotive datasets.,Global optimization of extended hand-eye calibration
"['Chih-Lu Lin', 'Hung-Yu Kao']","Analyzing interconnections among blog communities reveals blogger behaviors that could help in assessing blog quality. A new approach to ranking blogs uses a social blog network model based on blogs' interconnection structure and a popularity ranking method, called BRank. Experiments show that the proposed method can discriminate blogs with various degrees of popularity in the blogosphere.",Blog Popularity Mining Using Social Interconnection Analysis
['Raoul M. Bongers'],"In a recent study, Cardinali et al. (2009) showed that training the use of a tool affected kinematic characteristics of subsequent free movements (i.e., movement were slower, for instance), which they interpreted as that the use of a tool affects the body schema. The current study examined whether these results can also be explained in terms of motor learning where movement characteristics during tool use persist in the free movements. Using a different tool we replicated parts of the study of Cardinali et al: As did Cardinali et al. we found that tool use after-effects can be found in subsequent free movements. Importantly, we showed that the tooling movement was very slow compared to the free hand movement. We concluded that it can not be ruled out yet that after-effects of tool use originate from a general slowing down of movement speed that persists in free hand movements.",Do changes in movements after tool use depend on body schema or motor learning
"['Dongsong Zhang', 'Lina Zhou']","With the increase of economic globalization and evolution of information technology, financial data are being generated and accumulated at an unprecedented pace. As a result, there has been a critical need for automated approaches to effective and efficient utilization of massive amount of financial data to support companies and individuals in strategic planning and investment decision-making. Data mining techniques have been used to uncover hidden patterns and predict future trends and behaviors in financial markets. The competitive advantages achieved by data mining include increased revenue, reduced cost, and much improved marketplace responsiveness and awareness. There has been a large body of research and practice focusing on exploring data mining techniques to solve financial problems. In this paper, we describe data mining in the context of financial application from both technical and application perspectives. In addition, we compare different data mining techniques and discuss important data mining issues involved in specific financial applications. Finally, we highlight a number of challenges and trends for future research in this area.",Discovering golden nuggets: data mining in financial application
['Apostolos P. Fournaris'],"Side channel attacks and more specifically fault, simple power attacks, constitute a pragmatic, potent mean of braking a cryptographic algorithm like RSA. For this reason, many researchers have proposed modifications on the arithmetic operation functions required for RSA in order to thwart those attacks. However, these modifications are applied on theoretic É?? algorithmic level and do not necessary result in high performance RSA designs. This paper constitute the first complete attempt for an efficient design approach on a fault and simple power attack resistant RSA based on the well known, for its high performance, Montgomery multiplication algorithm. To achieve this, a fault and simple power attack resistant modular exponentiation algorithm is proposed that is based on the Montgomery modular multiplication. In order to optimize this algorithm's performance we also propose a modified version of Montgomery modular multiplication algorithm that employs value precomputation and carry save logic in all input, output and intermediate values. We introduce a hardware architecture based on the proposed Montgomery modular multiplication algorithm and use it as a building block for the design of a fault and simple power attack resistant modular exponentiation unit. This unit is optimized by taking advantage of the inherit parallelism in the proposed fault and simple power attack resistant modular exponentiation algorithm. Realizing the proposed unit in FPGA technology very advantageous results are found when compared against other well known designs even though our design bears an extra computation cost due to its fault and simple power attack resistance characteristic.",Fault and simple power attack resistant RSA using Montgomery modular multiplication
"['Ari Tryggvason', 'Caterina Melchiorre', 'Kerstin Johansson']","We present an algorithm developed for GIS-applications in order to produce maps of landside susceptibility in postglacial and glacial sediments in Sweden. The algorithm operates on detailed topographic and Quaternary deposit data. We compare our algorithm to two similar computational schemes based on a global visibility operator and a shadow-casting algorithm. We find that our algorithm produces more reliable results in the vicinity of stable material than the global visibility algorithm. We also conclude that our algorithm is more computationally efficient than the other two methods, which is important when we may want to assess the effects of uncertainty in the data by evaluating many different models. Our method also provides the possibility to take other data into account. We show how different soil types with different geotechnical properties may be modelled. Our algorithm may also take depth information, i.e. the thicknesses of the deposits into account. We thus propose that our method may be used to provide more refined maps than the overview maps in areas where more detailed geotechnical/geological data have been acquired. The efficiency of our algorithm suggests that it may replace any global visibility operators used in other applications or processing schemes of gridded map data.",A fast and efficient algorithm to map prerequisites of landslides in sensitive clays based on detailed soil and topographical information
['Denis Royer'],"The introduction of enterprise identity management systems (EldMS) in organisations is a costly and challenging endeavour, where organisations have to face various costs for the planning, the implementation, and the operation of such systems. In the planning phase it is important that organisational aspects are incorporated into the development of an enterprise identity management (EldM) solution, instead of purely focussing on the technological or financial issues. Indeed, without a proper assessment of the costs and the organisational settings (such as stakeholders, processes), companies will not see the benefit for introducing EldM as additional layer into their IT infrastructure and their business processes. This paper proposes initial ideas for a generic approach, based on the balanced scorecard, for assessing the value of investing in the introduction of EldMS. In the decision process, such an instrument can be used for decision support purposes and the planning phase on a tactical level. Furthermore, the organisational aspects are discussed and possible solutions for integrating all relevant parties into the planning process are presented.",Assessing the Value of Enterprise Identity Management (EIdM) Towards a Generic Evaluation Approach
"['Babak Falsafi', 'David A. Wood']","This paper proposes and evaluates a new approach to directory-based cache coherence protocols called  Reactive NUMA  (R-NUMA). An R-NUMA system combines a conventional CC-NUMA coherence protocol with a more-recent Simple-COMA (S-COMA) protocol. What makes R-NUMA novel is the way it dynamically reacts to program and system behavior to switch between CC-NUMA and S-COMA and exploit the best aspects of both protocols. This reactive behavior allows each node in an R-NUMA system to independently choose the best protocol for a particular page, thus providing much greater performance stability than either CC-NUMA or S-COMA alone. Our evaluation is both qualitative and quantitative. We first show the theoretical result that R-NUMA's worst-case performance is bounded within a small constant factor (i.e., two to three times) of the best of CC-NUMA and S-COMA. We then use detailed execution-driven simulation to show that, in practice, R-NUMA usually performs better than either a pure CC-NUMA or pure S-COMA protocol, and no more than 57% worse than the best of CC-NUMA and S-COMA, for our benchmarks and base system assumptions.",Reactive NUMA: a design for unifying S-COMA and CC-NUMA
"['Quanxin Zhu', 'Jinde Cao']","This paper is concerned with the problem of exponential stability for a class of Markovian jump impulsive stochastic Cohen-Grossberg neural networks with mixed time delays and known or unknown parameters. The jumping parameters are determined by a continuous-time, discrete-state Markov chain, and the mixed time delays under consideration comprise both time-varying delays and continuously distributed delays. To the best of the authors' knowledge, till now, the exponential stability problem for this class of generalized neural networks has not yet been solved since continuously distributed delays are considered in this paper. The main objective of this paper is to fill this gap. By constructing a novel Lyapunov-Krasovskii functional, and using some new approaches and techniques, several novel sufficient conditions are obtained to ensure the exponential stability of the trivial solution in the mean square. The results presented in this paper generalize and improve many known results. Finally, two numerical examples and their simulations are given to show the effectiveness of the theoretical results.",Robust Exponential Stability of Markovian Jump Impulsive Stochastic Cohen-Grossberg Neural Networks With Mixed Time Delays
"['Jean-Pierre Talpin', 'Pierre Jouvelot']","The type and effect discipline, a framework for reconstructing the principal type and the minimal effect of expressions in implicitly typed polymorphic functional languages that support imperative constructs, is introduced. The type and effect discipline outperforms other polymorphic type systems. Just as types abstract collections of concrete values, effects denote imperative operations on regions. Regions abstract sets of possibly aliased memory locations. Effects are used to control type generalization in the presence of imperative constructs while regions delimit observable side effects. The observable effects of an expression range over the regions that are free in its type environment and its type; effects related to local data structures can be discarded during type reconstruction. The type of an expression can be generalized with respect to the variables that are not free in the type environment or in the observable effect. >",The type and effect discipline
"['S??bastien Demange', 'Slim Ouni']","This paper presents a new acoustic-to-articulatory inversion method-based on an episodic memory, which is an interesting model for two reasons. First, it does not rely on any assumptions about the mapping function but rather it relies on real synchronized acoustic and articulatory data streams. Second, the memory structurally embeds the naturalness of the articulatory dynamics. In addition, we introduce the concept of generative episodic memory, which enables the production of unseen articulatory trajectories according to the acoustic signals to be inverted. The proposed memory is evaluated on the MOCHA corpus. The results show its effectiveness and are very encouraging since they are comparable to those of recently proposed methods.",Acoustic-to-articulatory inversion using an episodic memory
"['Eun Ae Cho', 'Chang Joo Moon', 'Dae Ha Park', 'Doo Kwon Baik']","It becomes increasingly common to use distributed services according to development of wire/wireless network. Home network area is also based on distributed environment which use a lot of services. Various technologies already have been used in home network. However, the research of information security part is insufficient. Thus, in this paper, we enhance our previous work by applying SSL component as a bundle format. This framework includes efficient user access control based on OSGi service platform from the former researches. This paper can cover the venerability between home gateway and authorization server in existing OSGi service platform. Therefore we provide the advantages of pre-studied framework and user information protection simultaneously.",Home Network Framework Based on OSGi Service Platform Using SSL Component Bundle
"['Timothy Bourke', 'Arcot Sowmya']","An infrared sensor is modeled and analyzed in Uppaal. The sensor typifies the sort of component that engineers regularly integrate into larger systems by writing interface hardware and software.   In all, three main models are developed. In the first model, the timing diagram of the sensor is interpreted and modeled as a timed safety automaton. This model serves as a specification for the complete system. A second model that emphasizes the separate roles of driver and sensor is then developed. It is validated against the timing diagram model using an existing construction that permits the verification of timed trace inclusion, for certain models, by reachability analysis (i.e., model checking). A transmission correctness property is also stated by means of an auxiliary automaton and shown to be satisfied by the model.   A third model is created from an assembly language driver program, using a direct translation from the instruction set of a processor with simple timing behavior. This model is validated against the driver component of the second timing diagram model using the timed trace inclusion validation technique. The approach and its limitations offer insight into the nature and challenges of programming in real time.",Analyzing an embedded sensor with timed automata in uppaal
"['Juho Hamari', 'Jonna Koivisto']","This paper investigates how social factors predict attitude toward gamification and intention to continue using gamified services, as well as intention to recommend gamified services. The paper employs structural equation modelling for analyses of data (n=107) gathered through a survey that was conducted among users of one of the worldÉ??s largest gamification applications for physical exercise. The results indicate that social factors are strong predictors for attitudes towards gamification, and, further, continued use intentions and intentions to recommend the related service.",Social Motivations To Use Gamification: An Empirical Study Of Gamifying Exercise
"['Jelle J. Goeman', 'Sara van de Geer', 'F. Kort', 'Hans C. van Houwelingen']","Motivation: This paper presents a global test to be used for the analysis of microarray data. Using this test it can be determined whether the global expression pattern of a group of genes is significantly related to some clinical outcome of interest. Groups of genes may be any size from a single gene to all genes on the chip (e.g. known pathways, specific areas of the genome or clusters from a cluster analysis).#R##N##R##N#Result: The test allows groups of genes of different size to be compared, because the test gives one p-value for the group, not a p-value for each gene. Researchers can use the test to investigate hypotheses based on theory or past research or to mine gene ontology databases for interesting pathways. Multiple testing problems do not occur unless many groups are tested. Special attention is given to visualizations of the test result, focussing on the associations between samples and showing the impact of individual genes on the test result.#R##N##R##N#Availability: An R-package globaltest is available from http://www.bioconductor.org",A global test for groups of genes: testing association with a clinical outcome
"['Rossouw von Solms', 'Kerry-Lynn Thomson', 'Prosecutor Mvikeli Maninjwa']","Information Security Governance has become one of the key focus areas of strategic management due to its importance in the overall protection of the organization's information assets. A properly implemented Information Security Governance framework should ideally facilitate the implementation of (directing), and compliance to (control), Strategic level management directives. These Strategic level management directives are normally interpreted, disseminated and implemented by means of a series of information security related policies. These policies should ideally be disseminated and implemented from the Strategic management level, through the Tactical level to the Operational level where eventual execution takes place. Control is normally exercised by capturing data at the lowest levels of execution and measuring compliance against the Operational level policies. Through statistical and summarized analyses of the Operational level data into higher levels of extraction, compliance at the Tactical and Strategic levels can be facilitated. This scenario of directing and controlling defines the basis of sound Information Security Governance. Unfortunately, information security policies are normally not disseminated onto the Operational level. As a result, proper controlling is difficult and therefore compliance measurement against all information security policies might be problematic. The objective of this paper is to argue towards a more complete information security policy architecture that will facilitate complete control, and therefore compliance, to ensure sound Information Security Governance.",Information Security Governance control through comprehensive policy architectures
"['Prashant R. Chandra', 'Allan L. Fisher', 'Corey Kosak', 'Tze Sing Eugene Ng', 'Peter Steenkiste', 'E. Takahashi', 'Hui Zhang']","The Internet is rapidly changing from a set of wires and switches that carry packets into a sophisticated infrastructure that delivers a set of complex value-added services to end users. Services can range from bit transport all the way up to distributed value-added services like video teleconferencing, data mining, and distributed interactive simulations. Before such services can be supported in a general and dynamic manner we have to develop appropriate resource management mechanisms. These resource management mechanisms must make it possible to identify and allocate resources that meet service or application requirements, support both isolation and controlled dynamic sharing of resources across organizations sharing physical resources, and be customizable so services and applications can tailor resource usage to optimize their performance. The Darwin project is developing a set of customizable resource management mechanisms that support value-added services, In this paper we present these mechanisms, describe their implementation in a prototype system, and describe the results of a series of proof-of-concept experiments.",Darwin: customizable resource management for value-added network services
"['Bin Zhao', 'Fei Wang', 'Changshui Zhang']","Support vector ordinal regression (SVOR) is a recently proposed ordinal regression (OR) algorithm. Despite its theoretical and empirical success, the method has one major bottleneck, which is the high computational complexity. In this brief, we propose a both practical and theoretical guaranteed algorithm, block-quantized support vector ordinal regression (BQSVOR), where we approximate the kernel matrix  K  with [( K )\tilde] that is composed of  k   2  constant blocks. We provide detailed theoretical justification on the approximation accuracy of BQSVOR. Moreover, we prove theoretically that the OR problem with the block-quantized kernel matrix [( K )\tilde] could be solved by first separating the data samples in the training set into  k  clusters with kernel  k -means and then performing SVOR on the  k  cluster representatives. Hence, the algorithm leads to an optimization problem that scales only with the number of clusters, instead of the data set size. Finally, experiments on several real-world data sets support the previous analysis and demonstrate that BQSVOR improves the speed of SVOR significantly with guaranteed accuracy.",Block-Quantized Support Vector Ordinal Regression
"['Jonathan P. Bowen', 'Mike Hinchey', 'Helge Janicke', 'M. Ward', 'Hussein Zedan']",Combining formal and agile techniques in software development has the potential to minimize change-related problems.,"Formality, Agility, Security, and Evolution in Software Development"
"['Maya Kallas', 'Paul Honeine', 'Cedric Richard', 'Clovis Francis', 'Hassan Amoud']","Moreover, in order to have a physical interpretation, some constraints should be incorporated in the signal or image processing technique, such as the non-negativity of the solution. This paper deals with the non-negative pre-image problem in kernel machines, for nonlinear pattern recognition. While kernel machines operate in a feature space, associated to the used kernel function, a pre-image technique is often required to map back features into the input space. We derive a gradient-based algorithm to solve the pre-image problem, and to guarantee the non-negativity of the solution. Its convergence speed is significantly improved due to a weighted stepsize approach. The relevance of the proposed method is demonstrated with experiments on real datasets, where only a couple of iterations are necessary.",Non-negative pre-image in machine learning for pattern recognition
"['Isael Diaz', 'Leif Wilhelmsson', 'Joachim Neves Rodrigues', 'Johan L??fgren', 'Thomas Olsson', 'Viktor ??wall']","This paper presents an architecture of an autocorrelator for Orthogonal Frequency Division Multiplexing systems. The received signal is quantized to only the sign-bit, which dramatically simplifies the frequency offset estimation. Hardware cost is reduced under the assumption that synchronization during acquisition does not have to be very accurate, but sufficient for coarse estimation. The architecture is synthesized towards a 65nm low-leakage high threshold standard cell CMOS library. The proposed architecture results in area reduction of 93% if compared to typical 8-bit implementation. The area occupied by the architecture is 0.063 mm 2 . The architecture is evaluated for WLAN, LTE and DVB-H. Power simulations for DVB-H transmission shows a power consumption of 4.8?ÊW per symbol.",A sign-bit auto-correlation architecture for fractional frequency offset estimation in OFDM
"['Vincent Kin Nang Lau', 'Meilong Jiang']","In this paper, the downlink rate quantization and cross-layer scheduling design are investigated in multiuser multiple-input single-output (MISO) systems with imperfect channel state information at transmitter (CSIT). We shall propose a systematic analytical design framework based on information theoretical approach. To capture the effect of the potential packet outage, we introduce the average system goodput, which measures the average b/s/Hz delivered to the mobiles successfully, as the system performance objective. Numerical results demonstrate that, by considering the statistics of CSIT errors into the design, the proposed scheduling scheme provides significant performance enhancement.",Rate Quantization and Cross-Layer Design of Multiple-Antenna Base Stations with Transmit MMSE and Imperfect CSIT
"['Wei Liu', 'Yuguang Duan', 'Wei Du']","Huge energy consumption is increasingly become a major obstacle factor to the development of cluster technology, therefore, how to effectively reduce energy consumption in the cluster system is a problem very worthy of study. To address this problem, an algorithm called an Energy Efficient Clustering-based Scheduling Algorithm for Parallel Tasks on Homogeneous DVS-Enabled Clusters (ECSTD) is proposed in this paper. This novel algorithm comprehensively considers the energy consumption of the duplication activities in the scheduling process, and two contributions have been made in this algorithm: (1) delay of the network is taken into account during the scheduling for achieving a more realistic simulation environment; (2) energy consumption is taken into account when a duplication activity is being considered, that is, a proper energy threshold is chosen as a selection criteria of copy tasks, comprehensively considering whether the duplication activity will be proceeded or not. In addition, Dynamic Voltage Scaling (DVS) has been demonstrated as one of the most effective low-power system design techniques, so the DVS technology is used in the algorithm proposed in this paper for obtaining a better energy-saving effect. The final results show that this work is fruitful.",An energy efficient clustering-based scheduling algorithm for parallel tasks on homogeneous DVS-enabled clusters
"['Yao Xiao', 'Gang Zhao', 'Chunhua Yin']","The bullwhip effect brings higher inventory cost and customer service level reduction, which is an amplification of the demand variability because of information distortion in the transmission process. In this paper, for reducing the effect, we analyze comparatively how three information sharing models affect the bullwhip effect, such as order information, demand information, inventory information. And, we also analyze related parameters and present some measures to reduce bullwhip effect.",Analysis of the bullwhip effect based on different information sharing models
"['Hong Shao', 'Wencheng Cui', 'Hong Zhao']","Automatic analysis of brain pathology is an important research subject in the computer-aided diagnosis field and a hotspot with a bright future in medical image diagnosis technology. This work presents a survey on publications concerning this subject. The methods on automatic segmentation and recognition of brain tumor are introduced, which are mainly brought forward by Howard Medical School. Medical image registration and segmentation are basis technologies and important problems in pathological analysis, which are briefly reviewed. At last, an approach on automatic analysis of brain pathology based on atlas and template is presented according to the recent developments. The steps and technological problems are introduced.",Automatic analysis of brain pathology based on image content
"['Duy-Phuong Pham', 'Chia Feng Lin', 'Shyan-Ming Yuan', 'Emery Jou']","In the tide called Cloud Computing, people move their applications previously running on on-premise servers to the Cloud. However, this migration is not a comfortable trip. Compatibility is a critical issue of the new environment. It is reasonable for the enterprises to decide to move certain portions of their IT infrastructure to the Cloud. And database is one important piece that may be migrated. In this article, a database model backed by cloud data-store which can be consumed by on-premise application is introduced. This works addresses several issues to provide the on-premise applications access to data-store in the cloud while guarantees efficiency and compatibility.",Database Backed by Cloud Data Store for On-premise Applications
"['Ernestina Cianca', 'S. De Fina', 'A. De Luise', 'Marina Ruggieri', 'R. Prasad']","This paper aims at clarifying the role of the outer loop power control for CDMA satellite systems with on-board power constraints. If the inner loop of the power control is perfect, the channel turns into a AWGN channel and there is no need of the outer loop. In satellite CDMA systems, due to the longer propagation delay with respect to a terrestrial system, the inner loop of power control is only partly able to track power variations due to fast fading. Moreover, the Rice factor, which characterizes the channel statistics, can widely vary even if the user does not move but just because of the change of the elevation angle. Because of that, a wide range of target SNIR (and larger than in typical terrestrial systems) may be necessary to get the same BER performance. Therefore, the outer loop power control turns out to be essential to minimize the dynamic of the power link margins and avoid capacity degradations induced by the systematic use of static link margins. A semi-analytical model for the capacity evaluation has been developed, which is specifically intended for the power-limited satellite-to-mobile link with multi satellite reception. We found that the capacity gain with respect to a pure SNIR-based strategy (i.e., only inner loop) can reach the 40% of the total capacity in a single reception scheme. A smaller, but still noticeable capacity gain of the order of 20 - 30% is observed in presence of satellite diversity. Therefore, any dimensioning of CDMA satellite systems should not neglect this component of the power control.",Outer loop power control in CDMA satellite systems with on-board power constraints
"['Kenneth Lo', 'Raphael Gottardo']","Motivation: Inference about differential expression is a typical objective when analyzing gene expression data. Recently, Bayesian hierarchical models have become increasingly popular for this type of problem. The two most common hierarchical models are the hierarchical Gamma--Gamma (GG) and Lognormal--Normal (LNN) models. However, to facilitate inference, some unrealistic assumptions have been made. One such assumption is that of a common coefficient of variation across genes, which can adversely affect the resulting inference.#R##N##R##N#Results: In this paper, we extend both the GG and LNN modeling frameworks to allow for gene-specific variances and propose EM based algorithms for parameter estimation. The proposed methodology is evaluated on three experimental datasets: one cDNA microarray experiment and two Affymetrix spike-in experiments. The two extended models significantly reduce the false positive rate while keeping a high sensitivity when compared to the originals. Finally, using a simulation study we show that the new frameworks are also more robust to model misspecification.#R##N##R##N#Availability: The R code for implementing the proposed methodology can be downloaded at http://www.stat.ubc.ca/~c.lo/FEBarrays#R##N##R##N#Contact: c.lo@stat.ubc.ca#R##N##R##N#Supplementary information: The supplementary material is available at http://www.stat.ubc.ca/~c.lo/FEBarrays/supp.pdf",Flexible empirical Bayes models for differential gene expression
"['Jeffrey E. Boyd', 'James J. Little']","Psychological studies indicate that people have a small but statistically significant ability to recognize the gaits of individuals that they know. Recently, there has been much interest in machine vision systems that can duplicate and improve upon this human ability for application to biometric identification. While gait has several attractive properties as a biometric (it is unobtrusive and can be done with simple instrumentation), there are several confounding factors such as variations due to footwear, terrain, fatigue, injury, and passage of time. This paper gives an overview of the factors that affect both human and machine recognition of gaits, data used in gait and motion analysis, evaluation methods, existing gait and quasi gait recognition systems, and uses of gait analysis beyond biometric identification. We compare the reported recognition rates as a function of sample size for several published gait recognition systems.",Biometric gait recognition
"['Pablo Montero', 'Javier Taibo', 'V??ctor M. Gul??as', 'Samuel Rivas']","GPUs excel in parallel computations, so they are very efficient calculating the discrete cosine transform of spatial domain images, as required for video encoding. The last steps of MPEG-2 compression, however, are inherently sequential since they require a serial processing of the resulting DCT coefficients. As that can easily become a bottleneck in GPUbased video encoders, in this paper we analyze the problem of computing the zigzag scan and Huffman encoding of a MPEG- 2 coefficient block in a GPU. We observed that simply optimizing the parallelism of the serialization and compression algorithm is not enough, and it can actually lead to worse results than a simple approach with no parallelism because of inefficient memory usage, since memory accesses can dramatically slow down the computation. This paper describes three different techniques to calculate the final bit stream for a MPEG-2 quantized coefficient matrix: a simple serial implementation, a fully parallel implementation, and a combination that beats them both when considering the cost of transferring the result to the CPU.",Parallel Zigzag Scanning and Huffman Coding for a GPU-based MPEG-2 Encoder
"['Dana Sinno', 'Douglas Cochran']","This paper is concerned with a class of dynamic estimation problems in which the estimator has the ability to dynamically select, from among a temporally evolving set of possibilities, the source of the data on which the estimate will be based. After motivating and formulating this class of ""attentive estimation"" problems in some generality, the paper focuses on the special case in which the state of a linear discrete-time dynamical system driven by gaussian noise is to be estimated using linear measurements corrupted by additive gaussian noise. This differs from the standard Kalman filtering problem in that the measurement map at each time step is selectable from a pre-determined set of such maps. When the system dynamics and noise statistics are known, the problem admits a ""sensor scheduling"" solution i.e., a criterion for measurement selection that can be used to determine an optimal sequence of output functions in an open-loop fashion prior to the onset of estimation. When the noise statistics or other parameters are unknown, however, closed-loop adaptive strategies for measurement selection can improve estimator performance.",Dynamic estimation with selectable linear measurements
"['Vincent Dor??', 'Mohamed Cheriet']","Most denoising methods require that some smoothing parameters be set manually to optimize their performance. Among these methods, a new filter based on nonlocal weighting (NL-means filter) has been shown to have a very attractive denoising capacity. In this paper, we propose fixing the smoothing parameter of this filter automatically. The smoothing parameter corresponds to the bandwidth h of a local constant regression. We use the C p  statistic embedded in Newton's method to optimize  h  in a point-wise fashion. This statistic also has the advantage of being a reliable measure of the quality of the denoising process for each pixel. In addition, we introduce a robust regression in the NL-means filter designed to greatly reduce the blur yielded by the weighting. Finally, we show how the automatic denoising model can be extended to images degraded by multiplicative noise. Experiments conducted on images with additive and multiplicative noise demonstrate a high denoising power with a degree of detail preservation...",Robust NL-Means Filter With Optimal Pixel-Wise Smoothing Parameter for Statistical Image Denoising
"['Marko Horvat', 'Dujo Duvnjak', 'Davor Jug']","The Geneva Affective Picture Database WordNet Annotation Tool (GWAT) is a user-friendly web application for manual annotation of pictures in Geneva Affective Picture Database (GAPED) with WordNet. The annotation tool has an intuitive interface which can be efficiently used with very little technical training. A single picture may be labeled with many synsets allowing experts to describe semantics with different levels of detail. Noun, verb, adjective and adverb synsets can be keyword-searched and attached to a specific GAPED picture with their unique identification numbers. Changes are saved automatically in the tool's relational database. The attached synsets can be reviewed, changed or deleted later. Additionally, GAPED pictures may be browsed in the tool's user interface using simple commands where previously attached WordNet synsets are displayed alongside the pictures. Stored annotations can be exported from the tool's database to different data formats and used in 3 rd party",GWAT: The Geneva Affective Picture Database WordNet Annotation Tool
"['Adan Cabello', 'Lars Eirik Danielsen', 'Antonio J. Lopez-Tarrida', 'Jos?? Ram??n Portillo']","We introduce a physical approach to social networks (SNs) in which each actor is characterized by a yesÉ??no test on a physical system. This allows us to consider SNs beyond those originated by interactions based on pre-existing properties, as in a classical SN (CSN). As an example of SNs beyond CSNs, we introduce quantum SNs (QSNs) in which actor i is characterized by a test of whether or not the system is in a quantum state |??iã®´ . We show that QSNs outperform CSNs for a certain task and some graphs. We identify the simplest of these graphs and show that graphs in which QSNs outperform CSNs are increasingly frequent as the number of vertices increases. We also discuss more general SNs and identify the simplest graphs in which QSNs cannot be outperformed.",Quantum social networks
['Xin Li'],Order of approximating functions and their derivatives by radial bases on arbitrarily scattered data is derived. And then radial bases are used to construct solutions of biharmonic equations that approximate potential integrals for the exact solutions with the order of approximation derived.,Radial basis approximation and its application to biharmonic equation
"['M. de Bakker', 'P.W. Verbeek', 'Gijs K. Steenvoorden']","In the present paper we introduce a range image sensor, the PSD-chip, designed for sheet of light range imaging. The image sensor consists of an array of 128 Position Sensitive Detector (PSD)-strips with a 20 mm length and a 28 /spl mu/m pitch. Design considerations for the image sensor are discussed, as well as the on-chip electronics. The on-chip electronics consists of an analog part and a digital part. The analog preamplifiers deal with the low-pass filtering of the sensor-signals in order to reduce the noise bandwidth. The digital part consists of an analog current multiplexer, implemented in ECL technology. Our goal is to achieve a 2 MHz range/frequency at 12 bits resolution.",Design considerations for a range image sensor containing a PSD-array and an on-chip multiplexer
"['Rui Jos??', 'Jorge Cardoso', 'Florian Alt', 'Sarah Clinch', 'Nigel Davies']","Mobile devices can be a powerful tool for interaction with public displays, but mobile applications supporting this form of interaction are not yet part of our everyday reality. There are no widely accepted abstractions, standards, or practices that may enable systematic interaction between mobile devices and public displays. We envision public displays to move away from a world of closed display networks to scenarios where mobile applications could allow people to interact with the myriad of displays they might encounter during their everyday trips. In this research, we study the key processes involved in this collaborative interaction between public shared displays and mobile applications. Based on the lessons learned from our own development and deployment of 3 applications, and also on the analysis of the interactive features described in the literature, we have identified 8 key processes that may shape this form of interaction: Discovery, Association, Presence Management, Exploration, Interface Migration, Controller, Media Upload and Media Download. The contribution of this work is the identification of these high-level processes and an elicitation of the main design considerations for display networks.",Mobile applications for open display networks: common design considerations
"['Daphne Ruth Raban', 'Inbal Ronen', 'Ido Guy']","Social technologies tend to attract research on social structure or interaction. In this paper we analyze the individual use of a social technology, specifically an enterprise people-tagging application. We focus on active participants of the system and distinguish between users who initiate activity and those who respond to activity. This distinction is situated within the preferential attachment theory in order to examine which type of participant contributes more to the process of tagging. We analyze the usage of the people-tagging application in a snapshot representing 3 years of activity, focusing on self-tagging compared to tagging by and of others. The main findings are: (1) People who tag themselves are the most productive contributors to the system. (2) Preferential attachment saturation is reached at 12É??14 tags per user. (3) The nature of participation is more significant than the number of participants for system growth. The paper concludes with theoretical and practical implications. ?? 2011 Wiley Periodicals, Inc.",Acting or reacting? Preferential attachment in a people-tagging system
"['Qi Wang', 'Phillip C.-Y. Sheu']","Although the service composition problem has been widely addressed, automatically composing a service from existing services is still very difficult. This paper focuses on the composition problem for relational web services. It considers answering complex queries using a set of available relational services collectively. The method we present is guaranteed to find a solution if there is one.",Relational Service Composition
"['Simon Lindgren', 'Ragnar Lundstr??m']",This article uses the case of Twitter activity under the #WikiLeaks hashtag to address issues of social movements online. The aim is to analyze the potential of elusive web spaces as sites of mobil ...,Pirate culture and hacktivist mobilization: The cultural and social protocols of #WikiLeaks on Twitter
"['Gustavo Percio Zimmermann Montesdioca', 'Antonio Carlos Gastaud Ma??ada']","User satisfaction is the most widely measure used to assess information system success in Delone-McLean model. Using quality dimensions as antecedents to user satisfaction, this model is largely accepted in the information systems field. However, there are some contexts and relationships among variables that still unexplored in this model. In this sense, we proposed empirically measure user satisfaction using the quality dimensions of the Delone-McLean model on the information security context, and test the relationship among quality variables and user satisfaction variable. To do that, we performed a survey with 176 information system users about their satisfaction with information security practices. The results demonstrated that the information quality was positive associated with user satisfaction, the system quality was negative associated with user satisfaction, and the service quality was not associated with user satisfaction. The quality variables relationship presented positive relationship among them, unless when information quality pointed to service quality.",Quality Dimensions of the DeLone-McLean Model to Measure User Satisfaction: An Empirical Test on the Information Security Context
"['Yang Xiao', 'Haizhon Li']","Best-effort data control and admission control are vital to guarantee quality of service for real-time (voice and video) transmissions in the IEEE 802.11e wireless LANs. In this paper, we propose and study a global data parameter control scheme integrated with a measurement-based admission control scheme for the IEEE 802.11e enhanced distributed channel access. In the proposed global data control scheme, the access point dynamically controls best-effort data parameters of stations globally based on traffic condition. Such a global/centralized data parameter control mechanism provides the best fairness for data transmissions among stations. In the proposed centrally-assisted distributed admission control scheme for voice and video transmissions, stations listen to available budgets from the access point to make decisions on acceptance or rejection of a voice or video stream. Such a scheme provides good differentiation among different access categories and provides good fairness among real-time streams within the same access category. The proposed mechanisms are evaluated via extensive simulations. Studies show that, with the proposed global data control scheme and the admission control scheme, quality of service can be greatly improved while maintaining a good utilization.",Voice and video transmissions with global data parameter control for the IEEE 802.11e enhance distributed channel access
"['Philippe Cerfontaine', 'Marc Schirski', 'Daniel B?¨ndgens', 'Torsten Kuhlen']","We propose a method to determine the optimal camera alignment for a tracking system with multiple cameras by specifying the volume to be tracked and an initial camera setup. We use optimization strategies based on methods usually employed for solving nonlinear systems of equations. All approaches are fully automatic and take advantage of modern graphics hardware since we also implement a GPU-based, accelerated visibility test. The algorithm automatically optimizes the whole setup by adjusting the given set of camera parameters. We can steer the optimization towards different goals depending on the desired application, e.g. the widest possible volume coverage or maximum camera visibility to overcome heavy occlusion problems during the tracking process. We also consider parameter constraints that the user may specify according to restrictions in the local environment where the cameras have to be mounted. This allows for a convenient definition of higher level constraints for the camera setup.",Automatic Multi-Camera Setup Optimization for Optical Tracking
"['Benjamin Belzer', 'John D. Villasenor']","We present here design techniques for trellis-coded vector quantizers with symmetric codebooks that facilitate low-complexity quantization as well as partitioning into equiprobable sets for trellis coding. The quantization performance of this coder on the independently identically distributed (i.i.d.) Laplacian source matches the performance of trellis-based scalar-vector quantization (TB-SVQ), but requires less computational complexity.",Symmetric trellis-coded vector quantization
"['Antonio Bernini', 'Stefano Bilotta', 'Renzo Pinzani', 'Vincent Vajnovszki']",A cross-bifix-free set of words is a set in which no prefix of any length of any word is the suffix of any other word in the set. A construction of cross-bifix-free sets has recently been proposed by Chee {\it et al.} in 2013 within a constant factor of optimality. We propose a \emph{trace partitioned} Gray code for these cross-bifix-free sets and a CAT algorithm generating it.,A Gray Code for cross-bifix-free sets
"['Rujiang Wang', 'Gilles Y. Delisle']",A joint optimization of the decision feedback equalizer (DFE) filter weights and an erasure slicer is proposed in this letter. The feedback weights of the DFE are constrained with an accurate mean square error (MSE) model as the criterion. An erasure slicer is adopted to further decrease error propagation. Two consecutive 1-D optimizations are shown as good as that of a single 2-D optimization. Simulation results verify the performance improvements of the proposed scheme.,Mitigating error propagation of MMSE-DFE by joint parameter optimization
"['Malakondaiah Naidu', 'Suresh Gopalakrishnan', 'Thomas Wolfgang Nehl']","Future automobiles will be equipped with by-wire systems to improve reliability, safety and performance. The fault tolerant capability of these systems is crucial due to their safety critical nature. Three fault tolerant inverter topologies for permanent magnet brushless dc motor drives suitable for automotive x-by-wire systems are analyzed. A figure of merit taking into account both cost and post-fault performance is developed for these drives. Simulation results of the two most promising topologies for various inverter faults are presented. The drive topology with the highest post-fault performance and cost effectiveness is built and evaluated experimentally.",Fault Tolerant Permanent Magnet Motor Drive Topologies for Automotive X-By-Wire Systems
"['Hong Chen', 'Gongde Guo', 'Yu Huang', 'Tianqiang Huang']","A novel similarity measure based on spatial overlapping relation is proposed in this paper, which calculates the similarity between a pair of data points by using the mutual overlapping relation between them in a multi-dimensional space. A spatial overlapping based hierarchical clustering method SOHC was also developed and implemented aimed to justify the effectiveness of the proposed similarity measure. SOHC works well both in low-dimensional and high-dimensional datasets, and is able to cluster arbitrary shape of clusters. Moreover, it can work for both numerical and categorical attributes in a uniform way. Experimental results carried out on some public datasets collected from the UCI machine learning repository and predictive toxicology domain show that SOHC is a promising clustering method in data mining.",A Spatial Overlapping Based Similarity Measure Applied to Hierarchical Clustering
"['Can Li', 'Weihua Deng']","Levy flight models whose jumps have infinite moments are mathematically used to describe the superdiffusion in complex systems. Exponentially tempering Levy measure of Levy flights leads to the tempered stable Levy processes which combine both the ?-stable and Gaussian trends; and the very large jumps are unlikely and all their moments exist. The probability density functions of the tempered stable Levy processes solve the tempered fractional diffusion equation. This paper focuses on designing the high order difference schemes for the tempered fractional diffusion equation on bounded domain. The high order difference approximations, called the tempered and weighted and shifted Grunwald difference (tempered-WSGD) operators, in space are obtained by using the properties of the tempered fractional calculus and weighting and shifting their first order Grunwald type difference approximations. And the Crank-Nicolson discretization is used in the time direction. The stability and convergence of the presented numerical schemes are established; and the numerical experiments are performed to confirm the theoretical results and testify the effectiveness of the schemes.",High order schemes for the tempered fractional diffusion equations
"['Ho Chul Lee', 'Jae Weon Choi', 'Taek Lyul Song', 'Chan Ho Song']","This paper is concerned with control allocation strategies with two-time scale dynamic inversion which generate nominal control input trajectories. In addition, a robust flight control design method is proposed by using a time-varying control technique which is time-varying version of the pole placement of linear time-invariant system for an agile missile with aerodynamic fin, thrust vectoring control, and side-jet thruster. The control allocation algorithms proposed in this paper are capable of extracting the maximum performance by combining each control effector. The time-varying control technique for the autopilot design enhances the robustness of the tracking performance for the wide angle of attack range. The main results are validated through the nonlinear simulations with aerodynamic data.",Agile missile autopilot design via time-varying eigenvalue assignment
"['Nemanja Petrovic', 'Nebojsa Jojic', 'Thomas S. Huang']","We present a novel generative model for video that models video as mixture of transformed video scenes. The learning procedure automatically clusters video frames into video scenes and objects. The learning algorithm is based on a hierarchical, on-line EM algorithm. Fast Fourier transform (FFT) is used for rapid computations in E and M step of the EM algorithm. We use the model to: 1. perform video clustering by grouping similar (up to translation and scale) video frames into clusters; 2. robustly stabilize video by inferring translation and scale intensity for each frame. We believe that video scene modeling of this kind is essential to bridge the ""semantic gap"" in video understanding. We illustrate this with several excellent results, both in terms of speed and accuracy.",Hierarchical video clustering
"['Hiroshi Shimodaira', 'Takashi Sudo', 'Mitsuru Nakai', 'Shigeki Sagayama']","This paper proposes a novel handwriting recognition interfacefor wearable computing where users write characterscontinuously without pauses on a small single writingbox. Since characters are written on the same writingarea, they are overlaid with each other. Therefore thetask is regarded as a special case of the continuous characterrecognition problem. In contrast to the conventionalcontinuous character recognition problem, location informationof strokes does not help very much in the proposedframework. To tackle the problem, substroke based hiddenMarkov models (HMMs) and a stochastic bigram languagemodel are employed. Preliminary experiments were carriedout on a dataset of 578 handwriting sequences with acharacter bigram consisting of 1,016 Japanese educationalKanji and 71 Hiragana characters. The proposed methoddemonstrated promising performance with 69.2% of hand-writingsequences beeing correctly recognized when differentstroke order was permitted, and the rate was improvedup to 88.0% when characters were written with fixed strokeorder.",On-line overlaid-handwriting recognition based on substroke HMMs
"['Esra Erdin', 'Christopher Zachor', 'Mehmet Hadi Gunes']","Communication privacy has been a growing concern, particularly with the Internet becoming a major hub of our daily interactions. Revelations of government tracking and corporate profiling have resulted in increasing interest in anonymous communication mechanisms. Several systems have been developed with the aim of preserving communication privacy via unlinkability within a public network environment such as Tor and I2P. As the anonymity networks cannot guarantee perfect anonymity, it is important for users to understand the risks they might face when utilizing such technologies. In this paper, we discuss potential attacks on the anonymity networks that can compromise user identities and communication links. We also summarize protection mechanisms against such attacks. Many attacks against anonymity networks are well studied, and most of the modern systems have built-in mechanisms to prevent these attacks. Additionally, some of the attacks require considerable resources to be effective and hence are very unlikely to succeed against modern anonymity networks.",How to Find Hidden Users: A Survey of Attacks on Anonymity Networks
['Saleh Al-Jufout'],"Abstract#R##N##R##N#Reliability of protection and apparatus selection of the electrical power systems depend on the correct and accurate calculation of short-circuit currents. The simulation of the electrical power systems is one of the methods that is used for the determination of the short-circuit currents. In this paper, the model of the electrical power system has been represented by a system of differential equations for current determination. These differential equations have been expressed in Cartesian system of coordinates that decreased the number of the equations by one-third. This paper compares two approaches of short-circuit simulation in electrical power systems and evaluates the error in the calculated short-circuit current during both transient and steady-state conditions. These approaches are as follows: first, the replacement of the load parameters by zero and second, the introduction of fault simulation switch. A fault has been simulated in a study electrical power system and the factors on which the error depends have been determined. In this paper, the envelope of the short-circuit current has been considered as a comparative criterion. Copyright ?? 2008 John Wiley & Sons, Ltd.",Evaluating the error caused by load ignorance through simulation of short circuit in electrical power systems
"['Yonggang Chen', 'Yunrui Guo', 'Wenlin Li']","This paper considers the robust stability analysis problem for a class of uncertain stochastic neural networks with time-varying delay. Based on the Lyapunov functional method, and by resorting to the new technique for estimating the upper bound of the stochastic derivative of Lyapunov functionals, the novel asymptotic stability criteria are obatined in terms of Linear matrix inequalities (LMIs). Two numerical examples are presented to show the effectiveness and the less conservativeness of the proposed method.",Novel Robust Stability Criteria for Uncertain Stochastic Neural Networks with Time-Varying Delay
"['Giovanni Aloisio', 'Massimo Cafaro', 'Sandro Fiore', 'Maria Mirto']","Today many data grid applications need to manage and process a very large amount of data distributed across multiple grid nodes and stored in relational databases. The Grid Relational Catalog Project (GRelC) developed at the CACT/ISUFI of the University of Lecce, represents an attempt to design and deploy a grid-DBMS for the Globus Community. In this paper, after defining the grid-DBMS concept, we describe the GRelC library which is layered on top of the Globus Toolkit. The user can build client applications on top of it that can easily get access to and interact with data resources.",The GRelC library: a basic pillar in the grid relational catalog architecture
"['Jia-Bin Huang', 'Chu-Song Chen']","Cast shadows induced by moving objects often cause serious problems to many vision applications. We present in this paper an online statistical learning approach to model the background appearance variations under cast shadows. Based on the bi-illuminant (i.e. direct light sources and ambient illumination) dichromatic reflection model, we derive physics-based color features under the assumptions of constant ambient illumination and light sources with common spectral power distributions. We first use one Gaussian mixture model (GMM) to learn the color features, which are constant regardless of the background surfaces or illuminant colors in a scene. Then, we build up one pixel based GMM for each pixel to learn the local shadow features. To overcome the slow convergence rate in the conventional GMM learning, we update the pixel-based GMMs through confidence-rated learning. The proposed method can rapidly learn model parameters in an unsupervised way and adapt to illumination conditions or environment changes. Furthermore, we demonstrate that our method is robust to scenes with few foreground activities and videos captured at low or unsteady frame rates.",Moving cast shadow detection using physics-based features
"['Shaobo Hou', 'Aphrodite Galata']","We propose a variational Bayes approach to the problem of robust estimation of Gaussian mixtures from noisy input data. The proposed algorithm explicitly takes into account the uncertainty associated with each data point, makes no assumptions about the structure of the covariance matrices and is able to automatically determine the number of the Gaussian mixture components. Through the use of both synthetic and real world data examples, we show that by incorporating uncertainty information into the clustering algorithm, we get better results at recovering the true distribution of the training data compared to other variational Bayesian clustering algorithms.",Robust estimation of gaussian mixtures from noisy input data
"['Thomas Kurner', 'D.J. Cichon', 'Werner Wiesbeck']","Network planning in mobile communications requires a realistic description of the propagation phenomena. Common wave propagation models for rural areas consider mainly the influence of topography. However, shadowing effects of the morphography in the vicinity of the mobile cause additional path loss. Usually this path loss is accounted for either by empirical corrections or by additional knife edges. In this paper new ray optical approaches for the calculation of the additional path loss for mobile locations within forested or urban environments are presented. For these situations typical macro situations are defined. In the case of urban environments the additional path loss is determined by means of the Uniform Theory of Diffraction whereas in forested areas a lateral wave propagation approach is used. The model is applied to an existing GSM-network. The results achieved by this model show good agreement with both wideband and narrowband measurements. >",Influence of the receiver near range in urban and forested areas in land mobile radio systems
"['Hui Won Je', 'Dong Hyun Kim', 'Kwang Bok Lee']","In this paper, we propose a joint precoding scheme for both the base station (BS) and relay station (RS) to increase the ergodic capacity of downlink non-regenerative multiple-input multiple-output(MIMO)-relay systems. Different from previous work, we assume only the mean and the covariance matrix are known for the channel state information (CSI) of the RS-mobile station(MS) link. Since the exact solution of this problem is difficult to be found because the RS-MS channel is only partially known, we employ precoding matrices that maximize the upper bound on the ergodic capacity. Based on this approach, the proposed scheme uses an iterative method to determine both BS and RS precoding matrices simultaneously. In numerical results, the proposed joint precoding scheme is shown to outperform the water-filling and amplify-and-forward (WF-AF) scheme in high signal-to-noise ratio (SNR) regions. It is also shown to outperform the spatial multiplexing and RS precoding (SM-RP) scheme in low SNR regions.",Joint Precoding for MIMO-Relay Systems with Partial Channel State Information
['Sahra Sedigh'],"""Smart"" technology spans a broad array of application domains, from critical infrastructure systems such as smart power grids to intelligent healthcare and personalized education. The common thread among these applications is their reliance on computing and communication for ""intelligence"" -- accurately characterizing their operating environment and intended use, adapting their operation based on this characterization, and efficiently delivering the services expected in a dependable fashion.","Panel III ""Smart Technology"" -- Applications and Infrastructure"
['Leslie Lamport'],"The formal correspondence between an implementation and its specification is examined. It is shown that existing specifications that claim to describe priority are either vacuous or else too restrictive to be implemented in some reasonable situations. This is illustrated with a precisely formulated problem of specifying a first-come-first-served mutual exclusion algorithm, which it is claimed cannot be solved by existing methods.",What it means for a concurrent program to satisfy a specification: why no one has specified priority
"['Hidekazu Murata', 'Susumu Yoshida']","Cochannel interference is a major impairment that limits the capacity of microcellular radio systems. This paper proposes a trellis-coded cochannel interference canceller, which uses trellis-coded modulation with maximum likelihood sequence estimation of both the desired signal and cochannel interference. This technique permits operation even when the signal-to-interference ratio is 0 dB. Computer simulation results show that this technique significantly reduces the bit-error rate in microcellular systems with Nakagami-Rice fading, particularly when the average signal-to-interference ratio is 0 dB. However, the technique requires: (1) synchronization of the interference and the desired signal and (2) higher computational complexity than standard techniques.",Trellis-coded cochannel interference canceller for microcellular radio
"['Ali Asgar Sohanghpurwala', 'Peter M. Athanas', 'Tannous Frangieh', 'Aaron Wood']","The Xilinx Partial Reconfiguration Early Access Software Tools for ISE 9.2i has been an instrumental package for performing a wide variety of research on Xilinx FPGAs, and is now superseded with the corresponding non-free add-on for ISE 12.3. The original package was developed and offered by Xilinx as a downloadable add-on package to the Xilinx ISE 9.2 tools. The 9.2i toolkit provided a methodology for creating rectangular partial reconfiguration modules that could be swapped in and out of a static baseline design with one or more PR slots. This paper presents a new PR toolkit called Open PR that, for starters, provides similar functionality as the Xilinx PR Toolkit, yet is extendable to explore other modes of partial reconfiguration. The distinguishing feature of this toolkit is that it is being released as open source, and is intended to extend to the needs of individual researchers.",OpenPR: An Open-Source Partial-Reconfiguration Toolkit for Xilinx FPGAs
"['Jason Baldridge', 'Miles Osborne']","For complex tasks such as parse selection, the creation of labelled training sets can be extremely costly. Resource-efficient schemes for creating informative labelled material must therefore be considered. We investigate the relationship between two broad strategies for reducing the amount of manual labelling necessary to train accurate parse selection models: ensemble models and active learning. We show that popular active learning methods for reducing annotation costs can be outperformed by instead using a model class which uses the available labelled data more efficiently. For this, we use a simple type of ensemble model called the Logarithmic Opinion Pool (LOP). We furthermore show that LOPs themselves can benefit from active learning. As predicted by a theoretical explanation of the predictive power of LOPs, a detailed analysis of active learning using LOPs shows that component model diversity is a strong predictor of successful LOP performance. Other contributions include a novel active learning method, a justification of our simulation studies using timing information, and cross-domain verification of our main ideas using text classification.",Active learning and logarithmic opinion pools for hpsg parse selection
"['Qinghua Ling', 'Max Q.-H. Meng', 'Tao Mei', 'Haoran Lu']","This paper introduces and analyses a three dimensional real-time kinematic simulation system for four-legged robot. The effective method of constructing interface between 3D geometrical modeling software and simulation system in OpenGL is applied in the geometrical modeling module for the robot. The construction of kinematic model and the demonstration of animation are analyzed emphatically. Further more, the structure of the functional modules of simulation system is discussed in detail",3D simulation design based on openGL for four-legged robot
"['Won-Ju Yoon', 'Sang-Hwa Chung']","The standard tag collection algorithm (TCA) in ISO/IEC 18000-7 has difficulty in collecting data from a massive number of active radio frequency identification (RFID) tags in a timely manner, so it should be improved to allow successful application of active RFID systems to a wide variety of industrial fields. We propose a novel TCA, which is an identified slot scan-based TCA (ISS-TCA), to improve the performance of tag collection (TC) in active RFID systems. In the proposed ISS-TCA, an active RFID reader differentiates identified slots from collided and empty slots via an ISS phase, in which active RFID tags send minimal responses. The reader then avoids collided and empty slots in the following TC phase, in which it collects the actual data from the tags. This reduces the cost of collided and empty slots during TC, resulting in improved TC performance. We carried out simulations to evaluate and compare the performances of the standard TCA and the proposed ISS-TCA in TC. The simulation results with up to 500 tags showed that the proposed ISS-TCA reduced the average time required for TC by 33.2% or 27.3% when the reader collected 50 or 100 bytes of additional data from the tags, respectively. We also experimentally evaluated the performance improvement of TC by the proposed ISS-TCA using an active RFID reader and 60 tags that were implemented for this paper. The experimental results confirmed that the proposed ISS-TCA could improve the TC performance in practice, confirming the simulation results, and showed that it could also decrease the average battery consumption of the tags by 39.3%.",ISS-TCA: An Identified Slot Scan-Based Tag Collection Algorithm for Performance Improvement in Active RFID Systems
"['Damjan Cicin-Sain', 'Antonio Hermoso Pulido', 'Anton Crombach', 'Karl R. Wotton', 'Eva Jim??nez-Guri', 'Jean-Fran??ois Taly', 'Guglielmo Roma', 'Johannes Jaeger']","We present SuperFly (http://superfly.crg.eu), a relational database for quantified spatio-temporal expression data of segmentation genes during early development in different species of dipteran insects (flies, midges and mosquitoes). SuperFly has a special focus on emerging non-drosophilid model systems. The database currently includes data of high spatio-temporal resolution for three species: the vinegar fly Drosophila melanogaster, the scuttle fly Megaselia abdita and the moth midge Clogmia albipunctata. At this point, SuperFly covers up to 9 genes and 16 time points per species, with a total of 1823 individual embryos. It provides an intuitive web interface, enabling the user to query and access original embryo images, quantified expression profiles, extracted positions of expression boundaries and integrated datasets, plus metadata and intermediate processing steps. SuperFly is a valuable new resource for the quantitative comparative study of gene expression patterns across dipteran species. Moreover, it provides an interesting test set for systems biologists interested in fitting mathematical gene network models to data. Both of these aspects are essential ingredients for progress toward a more quantitative and mechanistic understanding of developmental evolution.",SuperFly: a comparative database for quantified spatio-temporal gene expression patterns in early dipteran embryos
"['Gunnar Braun', 'Achim Nohl', 'Weihua Sheng', 'Jianjiang Ceng', 'Manuel Hohenauer', 'Hanno Scharw??chter', 'Rainer Leupers', 'Heinrich Meyr']","Architecture description languages (ADL) have been established to aid the design of application-specific instruction-set processors (ASIP). Their main contribution is the automatic generation of a software toolkit, including C compiler, assembler, linker, and instruction-set simulator. Hence, the challenge in the design of such ADLs is to unambiguously capture the architectural information required for the toolkit generation in a single model. This is particularly difficult for C compiler and simulator, as both require information about the instructions' semantics, however, while the C compiler needs to know  what  an instructions does, the simulator needs to know  how . Existing ADLs solve this problem by either introducing redundancy or by limiting the language's flexibility.This paper presents a novel, mixed-level approach for ADL-based instruction-set description, which offers maximum flexibility while preventing from inconsistencies. Moreover, it enables capturing instruction- and cycle-accurate descriptions in a single model. The feasibility and design efficiency of our approach is demonstrated with a number of contemporary, real-world processor architectures.",A novel approach for flexible and consistent ADL-driven ASIP design
"['Irith Pomeranz', 'Sudhakar M. Reddy']","The use of three-valued logic for the fault simulation of synchronous sequential circuits may incur a loss of accuracy that would cause the fault coverage to be underestimated. In addition, loss of fault coverage may occur due to the test strategy employed. These problems were previously alleviated at the cost of a high computational complexity. We present an observation that allows us to alleviate loss of fault coverage in many cases, at a computational cost similar to conventional three-value fault simulation. Based on this observation, we propose a fault simulation procedure that uses a conventional fault simulation procedure enhanced by a simple implication procedure. The proposed fault simulation procedure identifies faults that are detected under the multiple observation time approach and under a special case of this approach, called the restricted multiple observation time approach. The results of the proposed simulation procedure are compared to the results of a previously proposed procedure to demonstrate its effectiveness. Heuristics to guide a test generation procedure whose test sequences are effective for faults that can only be detected under the multiple observation time approach are also described.",Low-complexity fault simulation under the multiple observation time and the restricted multiple observation time testing approaches
"['Xuan-Tu Tran', 'Yvain Thonnart', 'Jean Durupt', 'Vincent Beroulle', 'Chantal Robach']","Asynchronous design offers an attractive solution to overcome the problems faced by networks-on-chip (NoC) designers such as timing constraints. Nevertheless, post-fabrication testing is a big challenge to bring the asynchronous NoCs to the market due to a lack of testing methodology and support. This paper first presents the design and implementation of a design-for-test (DfT) architecture, which improves the testability of an asynchronous NoC architecture. Then, a simple method for generating test patterns for network routers is described. Test patterns are automatically generated by a custom program, given the network topology and the network size. Finally, we introduce a testing strategy for the whole asynchronous NoC. With the generated test patterns, the testing methodology presents high fault coverage (99.86%) for  single   stuck-at   fault   models .",A Design-for-Test Implementation of an Asynchronous Network-on-Chip Architecture and its Associated Test Pattern Generation and Application
"['Adriana Melo Leite', 'Rosario Girardi', 'Uiratan Cavalcante']","The application of ontologies in Software Engineering presents several advantages. Ontologies allow for the formalized and integrated representation of models and their documentation in a same knowledge base, making the selection of these artifacts more precise through semantics inferences. They are also easily expandable and integrable. This paper introduces ONTORMAS, an ontology-driven tool for multi-agent systems development which integrates concepts from Multi-agent Domain and Application Engineering following the guidelines of the MADEM and MAAEM methodologies.",An ontology for Multi-Agent Domain and Application Engineering
['Deirdre Hogan'],"In this paper we present methods for improving the disambiguation of noun phrase (NP) coordination within the framework of a lexicalised history-based parsing model. As well as reducing noise in the data, we look at modelling two main sources of information for disambiguation: symmetry in conjunct structure, and the dependency between conjunct lexical heads. Our changes to the baseline model result in an increase in NP coordination dependency f-score from 69.9% to 73.8%, which represents a relative reduction in f-score error of 13%.",Coordinate Noun Phrase Disambiguation in a Generative Parsing Model
"['Ben F. Barton', 'Marthalee S. Barton']","Violations of published strictures on password use have led to widespread unauthorized access to computer systems. The problem may compound as inexpert users, handicapped by inadequate guidance and ignorance of computers, are increasingly involved on networked, supposedly ''user-friendly'' workstations. The literature on password methods reflects a technocentric focus emphasizing security without due regard for user comfort, i.e., a ''user-hostile'', system perspective. We present a ''user-friendly'' model for the password selection and re-creation processes rooted in cognitive psychology. The model suggests two approaches to password selection - one rooted in a nomothetic, or particularized, the other in an idiographic, or generalized, treatment of experience - that exploit principles of recall, memory aids and simple formal transformations. A third approach, exploiting environmental cues - hence recognition rather than recall - is also considered. Intermediate approaches enable tradeoffs between password security and memorability appropriate to the context and cognitive style of the user. The reduction of the approaches to practice is illustrated in numerous examples. The approaches yield passwords more vulnerable to discovery than those envisioned in system-oriented theory, yet operationally superior to many prompted by strictures reflecting a technocentric system perspective. We recommend that guidance materials on password use be made available on systems.",User-friendly password methods for computer-mediated information systems
"['C. Ma', 'J.Q. Feng', 'Z. Yang', 'Q.H. Wu']","Scientific researchers usually need to deal with a large number of scientific and technical articles. A handy toolkit to facilitate retrieval and identification of these articles is vital. This paper presents an agent-based personal article citation assistant (APACA). As an autonomous agent, it provides extensive and efficient assistance for users to identify most related citations in a local article repository in collaboration and interaction with other distributed personal citation assistants. Furthermore, this paper proposes an article ontology and an optimised Bayesian network method to infer the most relevant article annotation in the local article ontology using a given keywords set K.",Agent-based personal article citation assistant
"['Adam P. Davies', 'Richard A. Watson', 'Rob Mills', 'Christopher L. Buckley', 'Jason Noble']","Simple distributed strategies that modify the behavior of selfish individuals in a manner that enhances cooperation or global efficiency have proved difficult to identify. We consider a network of selfish agents who each optimize their individual utilities by coordinating (or anticoordinating) with their neighbors, to maximize the payoffs from randomly weighted pairwise games. In general, agents will opt for the behavior that is the best compromise (for them) of the many conflicting constraints created by their neighbors, but the attractors of the system as a whole will not maximize total utility. We then consider agents that act as creatures of habit by increasing their preference to coordinate (anticoordinate) with whichever neighbors they are coordinated (anticoordinated) with at present. These preferences change slowly while the system is repeatedly perturbed, so that it settles to many different local attractors. We find that under these conditions, with each perturbation there is a progressively higher chance of the system settling to a configuration with high total utility. Eventually, only one attractor remains, and that attractor is very likely to maximize (or almost maximize) global utility. This counterintuitive result can be understood using theory from computational neuroscience; we show that this simple form of habituation is equivalent to Hebbian learning, and the improved optimization of global utility that is observed results from well-known generalization capabilities of associative memory acting at the network scale. This causes the system of selfish agents, each acting individually but habitually, to collectively identify configurations that maximize total utility.","if you can't be with the one you love, love the one you're with: How individual habituation of agent interactions improves global utility"
"['??agri Tekinay', 'Mamadou D. Seck', 'Alexander Verbraeck']","The choice of resolution for a simulation model at a given scale is a trade-off between the level of accuracy offered by the model and the computational cost of its execution. The understanding of this trade-off requires insight in how model resolution and system scale influence accuracy and computational cost. This paper examines performance and accuracy measurements obtained from models of a simple scenario simulated at different spatial resolutions and different scales. The base model under consideration consists of a battalion formed by four tanks moving towards a fixed point on a two dimensional lattice with a certain height profile. Changes in computational cost and prediction accuracy are studied for different levels of spatial aggregation and model variations studying individual tanks and aggregated battalions. The findings are explained based on model specification choices and the adopted aggregation mechanisms. From this analysis, general propositions are derived which improve our understanding of multi-resolution modeling.",Exploring multi-level model dynamics: performance and accuracy (WIP)
"['Yu-Hong Dai', 'Roger Fletcher']","This paper studies projected Barzilai-Borwein (PBB) methods for large-scale box-constrained quadratic programming. Recent work on this method has modified the PBB method by incorporating the Grippo-Lampariello-Lucidi (GLL) nonmonotone line search, so as to enable global convergence to be proved. We show by many numerical experiments that the performance of the PBB method deteriorates if the GLL line search is used. We have therefore considered the question of whether the unmodified method is globally convergent, which we show not to be the case, by exhibiting a counter example in which the method cycles. A new projected gradient method (PABB) is then considered that alternately uses the two Barzilai-Borwein steplengths. We also give an example in which this method may cycle, although its practical performance is seen to be superior to the PBB method. With the aim of both ensuring global convergence and preserving the good numerical performance of the unmodified methods, we examine other recent work on nonmonotone line searches, and propose a new adaptive variant with some attractive features. Further numerical experiments show that the PABB method with the adaptive line search is the best BB-like method in the positive definite case, and it compares reasonably well against the GPCG algorithm of More and Toraldo. In the indefinite case, the PBB method with the adaptive line search is shown on some examples to find local minima with better solution values, and hence may be preferred for this reason.",Projected Barzilai-Borwein methods for large-scale box-constrained quadratic programming
"['Alvin C. Valera', 'Winston Khoon Guan Seah', 'S. V. Rao']","A mobile ad hoc network is an autonomous system of infrastructure-less, multihop, wireless mobile nodes. Reactive routing protocols perform well in this environment due to their ability to cope quickly against topological changes. This paper proposes a new routing protocol named CHAMP (caching and multiple path) routing protocol. CHAMP uses cooperative packet caching and shortest multipath routing to reduce packet loss due to frequent route failures. We show through extensive simulation results that these two techniques yield significant improvement in terms of packet delivery, end-to-end delay and routing overhead. We also show that existing protocol optimizations employed to reduce packet loss due to frequent route failures, namely local repair in AODV and packet salvaging in DSR, are not effective at high mobility rates and high network traffic.",Improving protocol robustness in ad hoc networks through cooperative packet caching and shortest multipath routing
"['Zhijun Xie', 'Guangyan Huang', 'Roozbeh Zarei', 'Jing He', 'Yanchun Zhang', 'HongWu Ye']","Deformation is the direct cause of heritage object collapse. It is significant to monitor and signal the early warnings of the deformation of heritage objects. However, traditional heritage object monitoring methods only roughly monitor a simple-shaped heritage object as a whole, but cannot monitor complicated heritage objects, which may have a large number of surfaces inside and outside. Wireless sensor networks, comprising many small-sized, low-cost, low-power intelligent sensor nodes, are more useful to detect the deformation of every small part of the heritage objects. Wireless sensor networks need an effective mechanism to reduce both the communication costs and energy consumption in order to monitor the heritage objects in real time. In this paper, we provide an effective heritage object deformation detection and tracking method using wireless sensor networks (EffeHDDT). In EffeHDDT, we discover a connected core set of sensor nodes to reduce the communication cost for transmitting and collecting the data of the sensor networks. Particularly, we propose a heritage object boundary detecting and tracking mechanism. Both theoretical analysis and experimental results demonstrate that our EffeHDDT method outperforms the existing methods in terms of network traffic and the precision of the deformation detection.",Wireless sensor networks for heritage object deformation detection and tracking algorithm.
"['Thomas Erneux', 'Albert Goldbeter']","A three-variable model describing the oscillatory activity of a cascade of enzyme reactions is analyzed. A quasi-steady-state approximation reduces the three equations to a system of two equations which admits only a stable steady state. This apparent failure of the quasi-steady- state approximation to describe the limit-cycle oscillations observed in the full, three-variable system is analyzed in detail. We first show that the oscillations occur in the full system provided the Michaelis constants are sufficiently small. We then develop a method for determining the correct limit for application of the quasi-steady-state approximation. The leading problem consists of two equations for a conservative oscillator, and a higher order analysis is required in order to determine the amplitude of the limit-cycle oscillations. Finally, we observe a good agreement when comparing exact numerical and approximate bifurcation diagrams.",Rescue of the QuasiÉ?êSteadyÉ?êState Approximation in a Model for Oscillations in an Enzymatic Cascade
"['Yi Li', 'Wing-Kin Sung', 'Lance D. Miller']","One important way that gene expression data are often analysed in an unsupervised way is to cluster the samples without reference to any annotations about them. Before clustering, the data are often subjected to a feature selection preprocessing step, in which a subset of genes are chosen for further analysis. We examine the use of multimodality as a criterion for choosing genes in feature selection, and also propose a novel measure of pairwise dissimilarity to cluster the genes that have survived the preprocessing step. The resulting multiple gene subsets usually contain those that are more strongly correlated with the sample annotations of interest than those obtained through variance-based feature selection. Class discovery may be facilitated when gene expression data are analysed using the proposed method.",Multimodality as a criterion for feature selection in unsupervised analysis of gene expression data
"['Timothy A. Davis', 'Ekanathan Palamadai Natarajan']","KLU is a software package for solving sparse unsymmetric linear systems of equations that arise in circuit simulation applications. It relies on a permutation to Block Triangular Form (BTF), several methods for finding a fill-reducing ordering (variants of approximate minimum degree and nested dissection), and Gilbert/PeierlsÉ?? sparse left-looking LU factorization algorithm to factorize each block. The package is written in C and includes a MATLAB interface. Performance results comparing KLU with SuperLU, Sparse 1.3, and UMFPACK on circuit simulation matrices are presented. KLU is the default sparse direct solver in the Xyce TM circuit simulation package developed by Sandia National Laboratories.","Algorithm 907: KLU, A Direct Sparse Solver for Circuit Simulation Problems"
"['Christopher L. Brown', 'Abdelhak M. Zoubir']","Parametric methods of clutter characterisation require assumptions and models that may only be approximations or may change over time. Nonparametric methods are free from these constraints and can, therefore, be used in a wider range of situations. However, most, including the Wdcoxon test, require independent, identically distributed data - a condition that may not be met in the time domain. In this paper, a new scheme for detecting targets in clutter in the frequency domain, based on the Conditional Wdcoxon test, is proposed. Simulation results are compared to a standard Wilcoxon test-based detector and reveal a large improvement in performance. Previously, conditional tests have only been used to reduce computational costs. We can now demonstrate that conditional tests are capable of greatly increasing detection performance.",Nonparametric Detection in the Frequency Domain Using the Conditional Wilcoxon Test
['Hubie Chen'],"We systematically investigate the complexity of model checking the existential positive fragment of first-order logic. In particular, for a set of existential positive sentences, we consider model checking where the sentence is restricted to fall into the set; a natural question is then to classify which sentence sets are tractable and which are intractable. With respect to fixed-parameter tractability, we give a general theorem that reduces this classification question to the corresponding question for primitive positive logic, for a variety of representations of structures. This general theorem allows us to deduce that an existential positive sentence set having bounded arity is fixed-parameter tractable if and only if each sentence is equivalent to one in bounded-variable logic. We then use the lens of classical complexity to study these fixed-parameter tractable sentence sets. We show that such a set can be NP-complete, and consider the length needed by a translation from sentences in such a set to bounded-variable logic; we prove superpolynomial lower bounds on this length using the theory of compilability, obtaining an interesting type of formula size lower bound. Overall, the tools, concepts, and results of this article set the stage for the future consideration of the complexity of model checking on more expressive logics.",On the complexity of existential positive queries
"['Tangwen Xu', 'Jianhua Ge', 'Haiyang Ding']","Proposed is a distributed link selection scheme for cognitive selection amplify-and-forward (AF) relaying networks. For such a scheme, by approximating the instantaneous signal-to-noise ratio of dual-hop relaying link by its tight bound, a closed-form outage lower bound expression for the considered system is derived. Also, the average feedback overhead is evaluated. Both analytical and numerical results show that no matter whether the channel state information (CSI) knowledge of the interference links is perfect or not, the proposed scheme incurs extremely low signaling overhead to achieve almost the same outage performance with that of the centralized scheme. In addition, for both schemes, the influence of relay placement on the average feedback overhead is investigated and some useful conclusions are drawn as well.",An Efficient Distributed Link Selection Scheme for AF-Based Cognitive Selection Relaying Networks
"['Michal Kosinski', 'Yoram Bachrach', 'Pushmeet Kohli', 'David Stillwell', 'Thore Graepel']","Individual differences in personality affect users' online activities as much as they do in the offline world. This work, based on a sample of over a third of a million users, examines how users' behaviour in the online environment, captured by their website choices and Facebook profile features, relates to their personality, as measured by the standard Five Factor Model personality questionnaire. Results show that there are psychologically meaningful links between users' personalities, their website preferences and Facebook profile features. We show how website audiences differ in terms of their personality, present the relationships between personality and Facebook profile features, and show how an individual's personality can be predicted from Facebook profile features. We conclude that predicting a user's personality profile can be applied to personalize content, optimize search results, and improve online advertising.",Manifestations of user personality in website choice and behaviour on online social networks
"['X. Rong Li', 'Ryan R. Pitre', 'Vesselin P. Jilkov', 'Huimin Chen']","The joint optimization of conflicting objectives is usually challenging because there is no clear-cut way to do so. When planning for search and track missions, one must choose between the conflicting objectives of detecting new targets and tracking previously detected targets. This paper proposes a novel metric that integrates the objectives of target detection, target tracking, and vehicle survivability nicely into an integrated single scalar index that can be used to optimize paths for joint detection and tracking. The cornerstone is the introduction of a new performance index, that is, information gain, which permits joint optimization of the objectives for search and track missions. Several examples are provided to illustrate how to use our proposed metric.",A new performance metric for search and track missions
"['Xiang Fan', 'Ralph C. Smith']","Undesired hysteresis and constitutive nonlinearities are present to varying degrees in all smart material based transducers when they are driven at high levels. This motivates the development of adaptive inverse compensation techniques that can approximately linearize the transducer response and also are sufficiently efficient to accommodate model uncertainties and the error introduced by inexact inverse algorithms. In this paper, we employ the homogenized energy model for describing hysteresis, and we incorporate the corresponding inverse filter in L 1  control design to develop a robust adaptive inverse control approach. Asymptotic tracking properties of the proposed algorithm are established, and for periodic reference trajectories, the parameter convergence behavior is characterized. Simulation results are provided to illustrate the effectiveness of the proposed algorithm.",Model-based L 1 adaptive control of hysteresis in smart materials
['Didar Zowghi'],"Academics and practitioners alike have long recognized the key challenges of requirements engineering education and training. This panel attempts to discuss these key challenges in details and provide an opportunity for a meaningful dialogue between academics and practitioners for identifying effective pedagogical paradigms, practical techniques and tools for teaching and learning requirements engineering principles.",Requirements Engineering Education and Training: Key Challenges and Practical Solutions
"['Guoli Yang', 'Baoxin Xiu', 'Wei Ming Zhang', 'Yong Zhang']","Operation System of Systems (OSoS) as a new form of modern warfare is proposed, aiming at designing the military organization as a whole to implement the mission. This paper puts forward the organization model of OSoS which including organization task, organization entity and organization relationship. As for the uncertainty of battlefield, this paper makes the structure reorganization analysis and proposes the physical cost and efficiency cost of structure reorganization. Based on the two costs, we design the dynamic evolution process of the OSoS, meanwhile provide the effective solution using the genetic algorithm. What's more, a dynamic case analysis is presented to validate the effectiveness of our model.",Dynamic OSoS analysis using structure reorganization methodology
"['Artur Janicki', 'Wojciech Mazurczyk', 'Krzysztof Szczypiorski']","Transcoding steganography (TranSteg) is a fairly new IP telephony steganographic method that functions by compressing overt (voice) data to make space for the steganogram by means of transcoding. It offers high stega- nographic bandwidth, retains good voice quality, and is generally harder to detect than other existing VoIP stegano- graphic methods. In TranSteg, after the steganogram reaches the receiver, the hidden information is extracted, and the speech data is practically restored to what was originally sent. This is a huge advantage compared with other existing VoIP steganographic methods, where the hidden data can be extracted and removed, but the original data cannot be re- stored because it was previously erased due to a hidden data insertion process. In this paper, we address the issue of steganalysis of TranSteg. Various TranSteg scenarios and possibilities of warden(s) localization are analyzed with regards to the TranSteg detection. A novel steganalysis method based on Gaussian mixture models and mel- frequency cepstral coefficients was developed and tested for various overt/covert codec pairs in a single warden sce- nario with double transcoding. The proposed method allowed for efficient detection of some codec pairs (e.g., G.711/G.729), while some others remained more resistant to detection (e.g., iLBC/AMR).",Steganalysis of transcoding steganography
"['Costas S. Xydeas', 'Thomas M. Chapman']","This paper presents a novel and efficient variable bit rate LPC quantization approach. The proposed MCVQ framework allows a dynamic programming based minimum quantization distortion partitioning and quantization process to be performed on input LSP vector tracks in time. Variable duration segments of LSP vector tracks are classified into one of a finite number of language related events. Specific codebooks, designed optimally for each event type, are then employed to vector quantize the individual LSP vectors of a given segment. ""high quality"" LSP quantization can be easily achieved at an average of 700 bits/sec while ""transparent"" performance is obtained at an average rate of 800 bits/sec.",Multicodebook vector quantization of LPC parameters
"['Athina P. Petropulu', 'Chrysostomos L. Nikias']","An iterative algorithm for reconstructing a finite impulse response (FIR) signal from only the phase of its bispectrum is introduced. The algorithm is based on the key observation that the differences of the bicepstrum coefficients contain all the information concerning the phase of the signal, whereas their sums contain the magnitude information. Analysis and simulation examples are presented to demonstrate the algorithm's convergence properties. It is also demonstrated that imposing an energy constraint causes the convergence rate of the algorithm to improve dramatically. >",Signal reconstruction from the phase of the bispectrum
"['Cyril S. Ku', 'Heung Dae Kim', 'Lawrence J. Henschen']","We introduce an inference scheme, based on the compilation approach, that can answer ""true,"" provable-false,"" ""indefinite,"" or ""assumable-false"" to a closed query in an indefinite deductive database under the generalized closed world assumption. The inference scheme proposed in this paper consists of a representation scheme and an evaluation process that uses one of two groups of positive indefinite ground clauses (PIGC's) derivable from the database for a given query. These two groups of PIGC's are base-PIGC's and descendants of base-PIGC's. We prove that the set of base-PIGC's derivable from the database is sufficient to infer the indefiniteness of a query. This newly proposed method offers many advantages over the existing compilation method. This new method outperforms the existing one in terms of the cost of compilation, ease in the handling of updates, and efficiency in query evaluation. >",An efficient indefiniteness inference scheme in indefinite deductive databases
"['Aydin Sezgin', 'Oliver Henkel']","It is well known that the Alamouti scheme is the only space-time code from orthogonal designs achieving the capacity of a multiple-input multiple-output (MIMO) wireless communication system with n T =2 transmit antennas and n R =1 receive antenna. In this paper, we propose the n-times stacked Alamouti scheme for n T =2n transmit antennas and show that this scheme achieves the capacity in the case of n R =1 receive antenna. This result may regarded as an extension of the Alamouti case. For the more general case of more than one receive antenna, we show that if the number of transmit antennas is higher than the number of receive antennas, we achieve a high portion of the capacity with this scheme. Further, we show that the MIMO capacity is at most twice the rate achieved with the proposed scheme for all signal-to-noise ratio (SNR). We derive lower and upper bounds for the rate achieved with this scheme and compare it with upper and lower bounds for the capacity. In addition to the capacity analysis based on the assumption of a coherent channel, we analyze the error rate performance of the stacked orthogonal space-time block code (OSTBC) with the optimal maximum-likelihood (ML) detector and with the suboptimal lattice-reduction (LR)-aided zero-forcing detector. We compare the error rate performance of the stacked OSTBC with spatial multiplexing (SM) and full-diversity achieving schemes. Finally, we illustrate the theoretical results by numerical simulations.",Stacked OSTBC: Error Performance and Rate Analysis
['Kurt Mehlhorn'],"We consider search trees under time-varying access probabilities. Let S=left{ B_{1},...,B_{n}right} and let p_{i}^{t} be the number of accesses to object B_{i} up to time t, W^{t}=sum p_{i}^{t}. We introduce D-trees with the following properties.#R##N#1) A search for X=B_{i} at time t takes time O(logW^{t}/p_{i}^{t}). This is nearly optimal.#R##N#2) Update time after a search is at most proportional to search time, i.e. the overhead for administration is small.",Dynamic Binary Search
"['Reng Zeng', 'Jianling Liu', 'Xudong He']","This paper presents a formal specification of Mondex, an electronic purse, using SAM. Mondex is the first pilot project for the 6th Grand Challenge to develop an integrated, automated toolset that developers can use to establish the correctness of software. Several research groups around the world have applied different formal methods in specifying and analyzing the Mondex smart card since 2006. Our specification is unique, which uses a software architecture model integrating high level Petri nets and temporal logic; thus contributes to the world wide effort in tackling one of the grand challenges in computer sciences.",A Formal Specification of Mondex Using SAM
"['Jingjun Zhang', 'Yanmin Shang', 'Ruizhen Gao', 'Yuzhen Dong']","In this paper an improved genetic algorithm is proposed to solve optimal problems applying triangulation theory of continuous self-mapping in Euclidean space. The algorithm operates on a simplicial subdivision of searching space and generates the integer labels at the vertices, and then crossover operators and increasing dimension operators relying on the integer labels are designed. In this case, whether each individual is a completely labeled simplex can be used as an objective convergence criterion and that determined whether the algorithm will be terminated. The algorithm combines genetic algorithms with subdivision theory, maintaining the proper diversity, stability and convergence of the population. Finally, several numerical examples are provided to be examined. Numerical results indicate that the proposed algorithm has higher global optimization capability, computing efficiency and stronger stability than traditional numerical optimization methods and standard genetic algorithms.",An Improved Genetic Algorithm Based on Subdivision Theory
"['Kun Yu', 'Yusuke Miyao', 'Takuya Matsuzaki', 'Xiangli Wang', ""Jun'ichi Tsujii""]","This paper discusses the difficulties in Chinese deep parsing, by comparing the accuracy of a Chinese HPSG parser to the accuracy of an English HPSG parser and the commonly used Chinese syntactic parsers. Analysis reveals that deep parsing for Chinese is more challenging than for English, due to the shortage of syntactic constraints of Chinese verbs, the widespread pro-drop, and the large distribution of ambiguous constructions. Moreover, the inherent ambiguities caused by verbal co-ordination and relative clauses make semantic analysis of Chinese more difficult than the syntactic analysis of Chinese.",Analysis of the Difficulties in Chinese Deep Parsing
"['Shun-ichi Amari', 'Tianping Chen', 'Andrzej Cichocki']","Independent component analysis or blind source separation extracts independent signals from their linear mixtures without assuming prior knowledge of their mixing coefficients. It is known that the independent signals in the observed mixtures can be successfully extracted except for their order and scales. In order to resolve the indeterminacy of scales, most learning algorithms impose some constraints on the magnitudes of the recovered signals. However, when the source signals are nonstationary and their average magnitudes change rapidly, the constraints force a rapid change in the magnitude of the separating matrix. This is the case with most applications (e.g., speech sounds, electroencephalogram signals). It is known that this causes numerical instability in some cases. In order to resolve this difficulty, this article introduces new nonholonomic constraints in the learning algorithm. This is motivated by the geometrical consideration that the directions of change in the separating matrix should be orthogonal to the equivalence class of separating matrices due to the scaling indeterminacy. These constraints are proved to be nonholonomic, so that the proposed algorithm is able to adapt to rapid or intermittent changes in the magnitudes of the source signals. The proposed algorithm works well even when the number of the sources is overestimated, whereas the existent algorithms do not (assuming the sensor noise is negligibly small), because they amplify the null components not included in the sources. Computer simulations confirm this desirable property.",Nonholonomic Orthogonal Learning Algorithms for Blind Source Separation
"['Zhengrong Ji', 'Boon-Ping Gan', 'Stephen John Turner', 'Wentong Cai']","Abstract: In High Level Architecture (HLA) based distributed simulation, a federation is a set of federates, where each federate is normally executed sequentially. Our recent work on HLA-based distributed supply chain simulation shows that a federate with well above-par workload will become the performance bottleneck of the entire federation, since faster federates have to wait for the slower one to advance its simulation time. In this paper, we propose a parallel federate architecture which exploits the internal parallelism of a single federate. The parallel federate architecture is an integration of a parallel simulation protocol and HLA-based distributed simulation. A parallel federate can be executed on a SMP system to improve the performance of the entire federation. Our approach gains the advantages of both the interoperability, reusability and scalability of the HLA and faster execution of the parallel simulation protocol.",Parallel federates - an architecture for hybrid distributed simulation
"['Alina Olteanu', 'Yang Xiao', 'Kui Wu', 'Xiaojiang Du']","Wireless sensor networks have been widely used in environment and habitat monitoring, as well as in military applications such as battlefield surveillance. In this paper, we focus on detecting intruders in such surveillance systems. Our goal is to optimize the network coverage when the network is deployed to detect an intrusion object with the shape of a disc or a rectangle. We study how the size and shape of the intrusion object influence the configuration of the sensor network. We prove many mathematical results related to detection probability and intrusion coverage intensity and study the asymptotic properties of these detection metrics. We also study the problem of maximizing network lifetime under some QoS constraints. We prove the existence of the solution and derive the explicit form of the solution under certain conditions.",An Optimal Sensor Network for Intrusion Detection
"['Robert L. Henderson', 'Benjamin G. Zorn']","SUMMARY Object-oriented programming has become a widely used, important programming paradigm that is supported in many different languages. C++ has become the most widely used object-oriented language and many C++ programmers are unfamiliar with the different approaches taken by other languages in the paradigm. This paper is intended as an introduction to a broad range of ideas in object-oriented programming. Specifically, we introduce four modern programming languages that support object-oriented programming (Oberon-2, Modula-3, Sather and Self), and show how a simple application is coded in these languages. While each of these programming languages provide support for inheritance, dynamic dispatch, code reuse, and information hiding, they do so in very different ways and with varying levels of efficiency and simplicity. The use of a simple example, based on a common programming problem, facilitates our comparison. We have coded the application in all of these languages, including C++, and we compare the compile times, object code sizes, and run times of the available implementations. Implementations of all the languages compared and all of the programs we measure are available on the Internet. Ultimately, our goal is to encourage and facilitate programmers in understanding and exploring a variety of object-oriented programming languages.",A comparison of object-oriented programming in four modern languages
['Ethelene Whitmire'],"During the fall 2001 semester 15 first-year undergraduates were interviewed about their information-seeking behavior. Undergraduates completed a short-answer questionnaire, the Measure of Epistemological Reflection, measuring their epistemological beliefs and searched the Web and an online public access catalog using tasks from the Reflective Judgment Interview that assessed their reflective judgment level. Undergraduates talked aloud while searching digital environments about the decisions they were making about the information they encountered while transaction analyses software (Lotus ScreenCam) recorded both their search moves and their decision-making through verbal protocol analysis. Analyses included examining the relationship between undergraduates' epistemological beliefs and reflective judgment and how they searched for information in these digital environments. Results indicated that there was a relationship between epistemological beliefs and reflective judgment and information-seeking behavior. Undergraduates' at higher stages of epistemological development exhibited the ability to handle conflicting information sources and to recognize authoritative information sources.","The relationship between undergraduates' epistemological beliefs, reflective judgment, and their information-seeking behavior"
"['Simone Calderara', 'Uri Heinemann', 'Andrea Prati', 'Rita Cucchiara', 'Naftali Tishby']","Video surveillance is becoming the technology of choice for monitoring crowded areas for security threats. While video provides ample information for human inspectors, there is a great need for robust automated techniques that can efficiently detect anomalous behavior in streaming video from single or multiple cameras. In this work we synergistically combine two state-of-the-art methodologies. The first is the ability to track and label single person trajectories in a crowded area using multiple video cameras, and the second is a new class of novelty detection algorithms based on spectral analysis of graphs. By representing the trajectories as sequences of transitions between nodes in a graph, shared individual trajectories capture only a small subspace of the possible trajectories on the graph. This subspace is characterized by large connected components of the graph, which are spanned by the eigenvectors with the low eigenvalues of the graph Laplacian matrix. Using this technique, we develop robust invariant distance measures for detecting anomalous trajectories, and demonstrate their application on real video data.",Detecting anomalies in people's trajectories using spectral graph analysis
"['Yan Shvartzshnaider', 'Maximilian Ott']","This paper presents a case for the adoption of an information-centric architecture for a global disaster management system. Drawing from a case study of the 2010/2011 Queensland floods, we describe the challenges in providing every participant with relevant and actionable information. We use various examples to argue for a more flexible information dissemination framework which is designed from the ground up to minimise the effort needed to fix the unexpected and unavoidable information acquisition, quality, and dissemination challenges posed by any real disaster.",Design for change: Information-centric architecture to support agile disaster response
"['Riccardo De Masellis', 'Domenico Lembo', 'Marco Montali', 'Dmitry Solomakhin']","We provide a comprehensive framework for semantic GSM artifacts, discuss in detail its properties, and present main software engineering architectures it is able to capture. The distinguishing aspect of our framework is that it allows for expressing both the data and the lifecycle schema of GSM artifacts in terms of an ontology, i.e., a shared and formalized conceptualization of the domain of interest. To guide the modeling of data and lifecycle we provide an upper ontology, which is specialized in each artifact with specific lifecycle elements, relations, and business objects. The framework thus obtained allows to achieve several advantages. On the one hand, it makes the specification of conditions on data and artifact status attribute fully declarative and enables semantic reasoning over them. On the other, it fosters the monitoring of artifacts and the interoperation and cooperation among different artifact systems. To fully achieve such an interoperation, we enrich our framework by enabling the linkage of the ontology to autonomous database systems through the use of mappings. We then discuss two scenarios of practical interest that show how mappings can be used in the presence of multiple systems. For one of these scenarios we also describe a concrete instantiation of the framework and its application to a real-world use case in the energy domain, investigated in the context of the EU project ACSI.",Semantic Enrichment of GSM-Based Artifact-Centric Models
"['Ho-Jin Ha', 'Changhoon Yim']","The scalable video coding extension of H.264/AVC is a current standardization project. This paper deals with unequal error protection (UEP) scheme for scalable video bitstream over packet-lossy networks using forward error correction (FEC). The proposed UEP scheme is developed by exploiting jointly the unequal importance existing both in temporal layers and quality layers of hierarchial scalable video bitstream. For efficient assignment of FEC codes, the proposed UEP scheme uses a simple and efficient performance metric, namely layer-weighted expected zone of error propagation (LW-EZEP). The LW-EZEP is adopted for quantifying the error propagation effect on video quality degradation from packet loss in temporal layers and in quality layers. Compared to other UEP schemes, the proposed UEP scheme demonstrates strong robustness and adaptation for variable channel status.",Layer-weighted unequal error protection for scalable video coding extension of H.264/AVC
"['Yongfu Li', 'Anrong Dang', 'Hongying Cao']","Zhouzhuang is a township of water. It is located within Kunshan county-level city, 30km southeast of Suzhou. In the long history, surrounded by water, it has survived due to the traffic inconvenience. However, since the discovery of the wonderland, that kind of tranquility has been broken, no longer exists. Currently, the phenomenon of over-commercialization as well as overload tourism in Zhouzhuang have provoked the denunciation and thinking of people of insight. It is found that traffic accessibility has a close relation with the unique characteristics of historical and cultural towns in our country. Therefore, we take Zhouzhuang as an example to elaborate the significance of traffic accessibility in the conservation process of historical and cultural towns. During the selection process of historical and cultural towns (villages), the historical vicissitude of traffic accessibility should also be considered as an important factor.",Conservation oriented accessibility assessment for historical and cultural towns: Taking Zhouzhuang town as an example
"['Daji Ergu', 'Gang Kou', 'Yi Peng', 'Yong Shi', 'Yu Shi']","Resource allocation is a complicated task in cloud computing environment because there are many alternative computers with varying capacities. The goal of this paper is to propose a model for task-oriented resource allocation in a cloud computing environment. Resource allocation task is ranked by the pairwise comparison matrix technique and the Analytic Hierarchy Process giving the available resources and user preferences. The computing resources can be allocated according to the rank of tasks. Furthermore, an induced bias matrix is further used to identify the inconsistent elements and improve the consistency ratio when conflicting weights in various tasks are assigned. Two illustrative examples are introduced to validate the proposed method.",The analytic hierarchy process: task scheduling and resource allocation in cloud computing environment
"['Michael J. Maher', 'Andrew Rock', 'Grigoris Antoniou', 'David P. Billington', 'Tristan Miller']","For many years, the non-montonic reasoning community has focussed on highly expressive logics. Such logics have turned out to be computationally expensive, and have given little support to the practical use of non-monotonic reasoning. In this work we discuss defeasible logic, a less-expressive but more efficient non-monotonic logic. We report on two new implemented systems for defeasible logic: a query answering system employing a backward-chaining approach, and a forward-chaining implementation that computes all conclusions. Our experimental evaluation demonstrates that the systems can deal with large theories (up to hundreds of thousands of rules). We show that defeasible logic has linear complexity, which contrasts markedly with most other non-monotonic logics and helps to explain the impressive experimental results. We believe that defeasible logic, with its efficiency and simplicity, is a good candidate to be used as a modeling language for practical applications, including modelling of regulations and business rules.",EFFICIENT DEFEASIBLE REASONING SYSTEMS
"['Stefano Ricci', 'R. Matera', 'A. Dallai']","In a typical echo-Doppler investigation the moving blood is periodically insonated by the transmitting bursts of ultrasound energy. The echoes, shifted in frequency according to the Doppler effect, are received, coherently demodulated and processed through a spectral estimator. The detected frequency shift can be exploited for blood velocity assessment. The spectral analysis is typically performed by the conventional Fast Fourier Transform (FFT), but, recently, the application of the Amplitude and Phase EStimator (APES) was proved to produce a good quality sonogram based on a reduced number of transmissions. Unfortunately, the much higher calculation effort needed by APES hampers its use in real-time applications. In this work, a fixed point DSP implementation of APES is presented. A spectral estimate É?? based on 32 transmissions É?? occurs in less than 120?¨s. Results obtained on echo-Doppler investigations on a volunteer are presented.",Amplitude and phase estimator for real-time biomedical spectral Doppler applications
"['Lianfa Li', 'Jinfeng Wang', 'Chengyi Wang']","In disaster insurance and reinsurance, GIS has been used to visualize and manage geospatial data and to help vulnerability and risk analysis for years. However, hazard insurance is a multidisciplinary issue that involves complex factors and uncertainty. GIS, if used alone, has limited functionality due to poor incorporation of intelligence and spatial statistics. The Spatial Decision Support System (SDSS) presented in this paper, addresses some of the deficiencies of traditional GIS, by providing powerful tools to support disaster insurance pricing that involves procedural and declarative knowledge. In the SDSS, the knowledge-based system shell, using the open-source CLIPS and supporting fuzziness and uncertainty, can be applied in at least three phases: hazard simulation, fuzzy comprehensive evaluation of risk, and query for insurance pricing. The libraries of statistics and spatial statistics provide a robust support for analysis of spatial factors, including spatial correlation between zones vulnerable to hazard and spatial variation of exposures. The GIS components provide sophisticated visualization and database management support for geospatial data, helping easily locate the insured points and risk zones as well as exploratory analysis of spatial data. Standard database management interfaces are used to manage other aspatial data. COM, an industry-wide interface protocol, tightly integrates these technologies (the expert shell, GIS, spatial statistics and DBM within an integral system), and can be used to develop mixed complex algorithms in support of other COM objects. An application of typhoon insurance pricing is demonstrated with a case study in Guangdong, China. Developed as a suite of generic tools with abilities to deal with the complex problem of disaster insurance involving spatial factors and field knowledge, this prototype SDSS can also be applied to other disaster insurance and fields that involve similar spatial decision making.",Typhoon insurance pricing with spatial decision support tools
"['Zhenyu Wu', 'Mengjun Xie', 'Haining Wang']","This paper presents Swift, a packet filter for high-performance packet capture on commercial off-the-shelf hardware. The key features of the Swift include: 1) extremely lowfilter update latency for dynamic packet filtering, and 2) gigabits-per-second high-speed packet processing. Based on complex instruction set computer (CISC) instruction set architecture (ISA), Swift achieves the former with an instruction set design that avoids the need for compilation and security checking, and the latter by mainly utilizing single instruction, multiple data (SIMD). We implement Swift in the Linux 2.6 kernel for both i386 and ??86-64 architectures and extensively evaluate its dynamic and static filtering performance on multiple machines with different hardware setups. We compare Swift to BPF (the BSD packet filter)--the de facto standard for packet filtering in modern operating systems--and hand-coded optimized C filters that are used for demonstrating possible performance gains. For dynamic filtering tasks, Swift is at least three orders of magnitude faster than BPF in terms of filter update latency. For static filtering tasks, Swift outperforms BPF up to three times in terms of packet processing speed and achieves much closer performance to the optimized C filters. We also show that Swift can harness the processing power of hardware SIMD instructions by virtue of its SIMD-capable instruction set.",Design and implementation of a fast dynamic packet filter
"['Usman Wajid', 'Abdallah Namoune', 'Nikolay Mehandjiev']","This paper compares user preferences for three alternative approaches to service composition, namely: control flow, data flow, and assisted composition approach. The end user perspective is gathered by organizing three focus groups that include discussions and subjective questionnaires, involving 35 non-technical participants. The comparison of alternative composition approaches yielded results confirming that users favour system-driven or assisted composition which deals with technological complexities such as service compatibility problems while allowing user control and involvement. The results also define the requirements for user-friendly design of a service composition tool, which is being developed in the EC-funded project SOA4 All.",A comparison of three service composition approaches for end users
"['Jianshu Weng', 'Ee-Peng Lim', 'Qi He', 'C. W. Leung']","When micro logging becomes a very popular social media, finding interesting posts from high volume stream of user posts is a challenging research problem. To organize large number of posts, users can assign tags to posts so that these posts can be navigated and searched by tag. In this paper, we focus on modeling the interestingness of hash tags in Twitter, the largest and most active micro logging site. We propose to first construct communities based on both follow links and tagged interactions. We then measure the dispersion and divergence of users and tweets using hash tags among the constructed communities. The interestingness of hash tags are then derived from these community-based dispersion and divergence features. We further introduce a supervised approach to rank hash tags by interestingness. Our experiments on a Twitter dataset show that the proposed approach achieves a fairly good performance.",What Do People Want in Microblogs? Measuring Interestingness of Hashtags in Twitter
"['Chong Xu', 'Lie-Liang Yang', 'Lajos Hanzo']","In this contribution, a novel Ant Colony Optimization (ACO) based Space-Time (ST) Multiuser Detection (MUD) algorithm was proposed in order to improve the Bit Error Ratio (BER) versus Signal-to-Noise Ratio (SNR) performance achieved by the Space-Time Block Code (STBC) assisted two transmit antenna aided fully loaded DS-CDMA system directly employing an ACO based MUD algorithm. Our simulation results demonstrate that the proposed ACO based MUD algorithm enables the fully loaded system to approach the single user performance associated with a second-order diversity gain. Furthermore, the complexity of the improved ACO based MUD algorithm is a factor of $10^{18}$ lower than that of the Maximum Likelihood (ML) MUD, when $K=32$ users are supported by the STBC $\mbox{\boldmath{$\mathcal{G}$}}_2$ asssited DS-CDMA systems employing Gold codes having a length of $31$ chips.",Ant-Colony Based Near-ML Space-Time Multiuser Detection for the STBC Assisted DS-CDMA Uplink
"['Yu Fu', 'Chintha Tellambura', 'Witold A. Krzymien']","The mitigation of intercarrier interference (ICI) in closed-loop single-input-single-output (SISO) and multiple-input-multiple-output (MIMO) orthogonal frequency-division multiplexing (OFDM) is considered. The authors show that the ICI coefficient matrix is approximately unitary and exploit this property to design a nonlinear Tomlinson-Harashima precoder for the reduction of ICI in closed-loop SISO OFDM and orthogonal space-time block-coded (OSTBC) MIMO OFDM. With the proposed design, the transmitter does not need to know the frequency offsets, and hence, their impact on the bit error rate (BER) is significantly reduced. Moreover, for spatially correlated MIMO channels, the precoder and OSTBC OFDM perform with a negligible BER-performance loss",Transmitter Precoding for ICI Reduction in Closed-Loop MIMO OFDM Systems
"['Youn-Hee Han', 'Seung-Hee Hwang', 'Hee-Jin Jang']","IETF mobile Internet protocol version 6 and its fast handover protocol are proposals for handling routing of IPv6 packets to mobile nodes that have moved away from their home network. To do this, each time a mobile node moves to a new location, iit configures and confirms it temporal IP address. In this paper, we study the impact of the address configuration and confirmation procedure on the their IP handover latency. We first argue that the current strategies for them are unnecessarily slow, hampering the use of current proposals for real-time traffic. We present a new scheme which can be a substitute of the current strategies. The underlying objective of our scheme is to completely eliminate the latency taken by the address configuration and confirmation from the whole handover latency. Further, a mathematical analysis is developed to show the benefits of our scheme. In this analysis, various parameters are discussed to analyze the existing proposals, while our approach focuses on the reduction of handover latency.",Design and evaluation of an address configuration and confirmation scheme for IPv6 mobility support
"['Javier Ruiz', 'Javier Jim??nez Gil', 'Jos?? Eugenio Naranjo', 'Jos?? Ignacio Su?≠rez', 'Blas M. Vinagre']","This research studies the overtaking maneuver in some representative situations. A simulator using Matlab Simulink connecting its outputs to a Virtual Reality module to show the complete overtaking performance from all points of view has been made. With this useful tool the parameters of the car controllers and decision making systems can be set up, and unforeseen new features of the overtaking maneuver can also be taken into account.",Cooperative maneuver study between autonomous cars: overtaking
['Rikard Berthilsson'],"We present a novel method for finding the optimal affine transformation for matching of images. The method requires no feature points, does not rely on normalization of images and can be tuned to highlight interesting parts in the images. Furthermore, the method does not need any derivatives for obtaining the affine transformation and it has a computational cost proportional to n/sup 2/logn for n/spl times/n images. The problem of finding the optimal affine transformation is solved by an iterative algorithm. In each step a global optimization is performed by the use of FFT. This global characteristic helps the algorithm from getting trapped in a local optimum. Novel theoretical results are presented that show under what restrictions the algorithm can be expected to work properly. Its intended primary use is for reconstruction problems in computer vision. These rely heavily on the establishment of point correspondences in the images. Since the method makes no assumptions on the images it can be used when feature points are difficult to detect. Experiments on real images are included and it is shown that the algorithm is robust and performs well even in difficult situations, with occlusions.",Finding correspondences of patches by means of affine transformations
"['Kimoon Kim', 'Gwangil Geon', 'Seongsoo Hong', 'Sunil Kim', 'Tae-Hyung Kim']","While recently emerging middleware technologies such as CORBA and DCOM address the complexity of distributed programming, they cannot be directly applied to distributed control system design due to their excessive resource demand and inadequate communication models. We propose a new CORBA design for CAN-based distributed embedded control systems. Our design goal is to minimize its resource need and make it support group communication without losing the IDL (interface definition language) level compliance to the OMG standards. To achieve this, we develop a transport protocol on the CAN and a group communication scheme based on the well-known publisher/subscriber model. The protocol effectively realizes subject-based addressing and supports anonymous publisher/subscriber communication. We also customize the method invocation and message passing protocol, referred to as the general inter-ORB protocol (GIOP), of CORBA so that CORBA method invocations are efficiently serviced on a low-bandwidth network such as the CAN. This customization includes packed data encoding and variable-length integer encoding for compact representation of IDL data types. The new CORBA design clearly demonstrates that it is feasible to use CORBA in developing distributed embedded systems on real-time networks possessing severe resource limitations.",Resource-conscious customization of CORBA for CAN-based distributed embedded systems
"['Shoji Kajita', 'Kazuya Takeda', 'Fumitada Itakura']","Subband-autocorrelation (SBCOR) analysis is a noise robust acoustic analysis based on filter bank and autocorrelation analysis, and aims to extract the periodicities associated with the inverse of the center frequency in a subband. In this paper, it is derived that SBCOR results in the lateral inhibitive weighting (LIW) processing of the power spectrum, and it is shown that the LIW is significantly effective for noise robust acoustic analysis using a DTW word recognizer. An interpretation of the LIW is also described. A flattening technique of the noise spectral envelope using an LPC inverse filter is applied to speech degraded with noise, and DTW word recognition is performed. The idea of this inverse filtering technique comes from weakening the strong periodic components included in noise. The experimental results using a 32th order LPC inverse filter show that the recognition performance of SBCOR (or LIW) is improved for computer room noise.",Spectral weighting of SBCOR for noise robust speech recognition
"['Lina Wu', 'Yaping Huang', 'Wei Sun', 'Jianyu Ke']","Image categorization is an important issue in computer vision. The bag-of-visual words(BOV) model which ignores spatial restriction of local features has gained state-of-the-art performance in recent years. The basic BOV model uses k-means to form codebook. As sparse codes can better represent local features, we use sparse codes of SIFT features instead of k-means to form codebook. Additional, as local features in most categories have spatial dependence in real world, this paper proposed to use visual word pairs to represent the spatial information between words. To reduce the complexity both in time and storage, we add word pairs dynamically. Our experiments show that our algorithm can improve the categorization performance.",Create visual word pairs dynamically based on sparse codes of SIFT features for image categorization
"['Shashi Phoha', 'Noah Jacobson', 'David Friedlander', 'Richard R. Brooks']","The severe power, time and processing constraints on ad hoc wireless sensor networks for area surveillance require in-situ adaptations to conserve resources and optimize performance. In particular, it may be necessary to make dynamic tradeoffs between centralized processing algorithms, like beamforming, and knowledge based distributed processing algorithms like dynamic space-time clustering (DSTC) that rely on local processing of raw sensor data. Beamforming methods can achieve high levels of accuracy in estimating direction of arrival with a sound wave even when the source is in the far field. Hence accurate localization can be achieved with a relatively sparse sensor network. However, beamforming has severe limitations when the number of nodes increases. It requires orders of magnitude higher energy for transporting the entire time series over the network. DSTC methods, on the other hand, work well when the number of nodes is large because clusters can be formed within a smaller space-time window. This work examines the operational domains of the two centralized and distributed algorithms by analyzing sources of error, dependence on sensor density, sensor geometries, energy usage, control logic for data processing and the effects of network topology on the two algorithms. Based on this analysis, we develop hybrid algorithms that take advantage of the operational characteristics of each one in designing a high performance sensor network.",Sensor network based localization and target tracking through hybridization in the operational domains of beamforming and dynamic space-time clustering
"['Maher Salah', 'Spiros Mancoridis']","Many modern software systems are often large, distributed,written in more than one programming language,and developed using pre-built components. This paperpresents the results of the first phase of a project to developan environment that supports the comprehension ofdistributed systems.The environment has a layered architecture consistingof three subsystems: data gathering, data repository, andmodeling/visualization. The first phase of the project focuseson the design and implementation of the data gatheringand data repository subsystems. The key requirementsof the environment are to support: (a) static and dynamicanalysis, (b) multiple languages, (c) distributed systems,and (d) component-based models.",Toward an environment for comprehending distributed systems
['Pierre Soille'],"Image mosaicking can be defined as the registration of two or more images that are then combined into a single image. Once the images have been registered to a common coordinate system, the problem amounts to the definition of a selection rule to output a unique value for all those pixels that are present in more than one image. This process is known as image compositing. In this paper, we propose a compositing procedure based on mathematical morphology and its marker-controlled segmentation paradigm. Its scope is to position seams along salient image structures so as to diminish their visibility in the output mosaic even in the absence of radiometric corrections or blending procedures. We also show that it is suited to the seamless minimization of undesirable transient objects occurring in the regions where two or more images overlap. The proposed methodology and algorithms are illustrated for the composition of satellite images minimizing cloud cover",Morphological image compositing
"['D. J. Eigen', 'Frederick R. Fromm', 'Richard A. Northouse']","A new clustering algorithm is presented that is based on dimensional information. The algorithm includes an inherent feature selection criterion, which is discussed. Further, a heuristic method for choosing the proper number of intervals for a frequency distribution histogram, a feature necessary for the algorithm, is presented. The algorithm, although usable as a stand-alone clustering technique, is then utilized as a global approximator. Local clustering techniques and configuration of a global-local scheme are discussed, and finally the complete global-local and feature selector configuration is shown in application to a real-time adaptive classification scheme for the analysis of remote sensed multispectral scanner data.",Cluster Analysis Based on Dimensional Information with Applications to Feature Selection and Classification
['Kenneth C. Spry'],"To improve quality of care, it is essential for hospital systems to find the most efficient, flexible, comprehensible, and robust means to organize and display patient information to physicians [1]. Due to the complexity of patient information and the restrictions on sharing real world problems that arise from real world information, creating and implementing information systems for clinical support is difficult. Lessons from human-computer interaction design can mitigate time and cost while ameliorating the functionality and understandability of the system.   This paper describes the design of the problem list currently in development at the Regenstrief Institute for Health Care Informatics. Specifically, we delineate the exploration of goals and requirements for the patient problem list, its design conceptualization, and the evolution of the final product, a rich, interactive infographic that displays far more digestible information than is possible with more conventional methods.",An infographical approach to designing the problem list
"['Thorsten Strufe', 'Jens Wildhagen', 'G?¨nter Sch??fer']","Even though overlay streaming is an inherently fault tolerant and stable system architecture, careful neighbor selection is a significant task. Inappropriate routing decisions can lead to an unstable topology with only a few very important nodes on which a large set of succeeding nodes depend. The presented algorithm selects streaming neighbors based on local information, passing knowledge to parent nodes only. Similar to SplitStream [Castro, M., P. Druschel, A. Kermarrec, A. Nandi, A. Rowstron and A. Singh, SplitStream: High-bandwidth content distribution in a cooperative environment, in: Proceedings of (IPTPS'03), 2003, pp. 298-313], it creates inner-node disjoint multicast trees. The created topologies are broad and have short paths, thus improving the resistance to node failure and intentional attacks. A malicious node can neither gain any knowledge about different regions of the topology other than its own successors nor deliberately move to a more important position in the hierarchy. The characteristics of the created topologies are revised in a static simulation study calculating the vertex connectivity and packet loss on node disconnections.",Towards the Construction of Attack Resistant and Efficient Overlay Streaming Topologies
['Fons Wijnhoven'],"The Internet reduces much of the costs of information sharing, but it does not solve information receiversÉ?? reading and interpretation limitations.#R##N#Search engines ease information retrieval but do not solve the problems of specifying information needs and evaluating retrieval results.#R##N#This article approaches these problems as information market problems with solutions consisting of information market service process models.#R##N#These models link information suppliers and information buyers and define activities, information resources, and information flows for the information#R##N#market services. The models identified may improve the quality, speed, design and realisation of information market services summary#R##N#goes here.","Models of Information Markets: Analysis of Markets, Identification of Services, and Design Models"
"['Maria Cecilia Mazzaro', 'Mario Sznaier']","This paper deals with the problem of model (in)validation of discrete time, causal, linear time-invariant (LTI) stable models subject to slowly linear time-varying structured uncertainty, using frequency domain data corrupted by additive noise. It is well known that in the case of structured LTI uncertainty the problem is NP hard in the number of uncertainty blocks. The main contribution of this paper shows that, on the other hand, if one considers arbitrarily slowly time varying uncertainty and noise in L/sub 2/, then tractable, convex necessary and sufficient conditions for (in)validation can be obtained. Additional results include a discussion of the case where the noise is characterized in terms of the L/sub /spl infin// norm.",Convex necessary and sufficient conditions for frequency domain model (in)validation under SLTV structured uncertainty
"['M. Kanat Camlibel', 'Jong-Shi Pang', 'Jinglai Shen']","Conewise linear systems are dynamical systems in which the state space is partitioned into a finite number of nonoverlapping polyhedral cones on each of which the dynamics of the system is described by a linear differential equation. This class of dynamical systems represents a large number of piecewise linear systems, most notably, linear complementarity systems with the P-property and their generalizations to affine variational systems, which have many applications in engineering systems and dynamic optimization. The challenges of dealing with this type of hybrid system are due to two major characteristics: mode switchings are triggered by state evolution, and states are constrained in each mode. In this paper, we first establish the absence of Zeno states in such a system. Based on this fundamental result, we then investigate and relate several state observability notions: short-time and $T$-time (or finite-time) local/global observability. For the short-time observability notions, constructive, finitely verifiable algebraic (both sufficient and necessary) conditions are derived. Due to their long-time mode-transitional behavior, which is very difficult to predict, only partial results are obtained for the $T$-time observable states. Nevertheless, we completely resolve the $T$-time local observability for the bimodal conewise linear system, for finite $T$, and provide numerical examples to illustrate the difficulty associated with the long-time observability.",Conewise Linear Systems: Non-Zenoness and Observability
"['J. Rafael Tena', 'Fernando De la Torre', 'Iain A. Matthews']","Linear models, particularly those based on principal component analysis (PCA), have been used successfully on a broad range of human face-related applications. Although PCA models achieve high compression, they have not been widely used for animation in a production environment because their bases lack a semantic interpretation. Their parameters are not an intuitive set for animators to work with. In this paper we present a linear face modelling approach that generalises to unseen data better than the traditional holistic approach while also allowing  click-and-drag  interaction for animation. Our model is composed of a collection of PCA sub-models that are independently trained but share boundaries. Boundary consistency and user-given constraints are enforced in a soft least mean squares sense to give flexibility to the model while maintaining coherence. Our results show that the region-based model generalises better than its holistic counterpart when describing previously unseen motion capture data from multiple subjects. The decomposition of the face into several regions, which we determine automatically from training data, gives the user localised manipulation control. This feature allows to use the model for face posing and animation in an intuitive style.",Interactive region-based linear 3D face models
"['Ryota Tomioka', 'Taiji Suzuki', 'Kohei Hayashi', 'Hisashi Kashima']","We analyze the statistical performance of a recently proposed convex tensor decomposition algorithm. Conventionally tensor decomposition has been formulated as non-convex optimization problems, which hindered the analysis of their performance. We show under some conditions that the mean squared error of the convex method scales linearly with the quantity we call the normalized rank of the true tensor. The current analysis naturally extends the analysis of convex low-rank matrix estimation to tensors. Furthermore, we show through numerical experiments that our theory can precisely predict the scaling behaviour in practice.",Statistical Performance of Convex Tensor Decomposition
['Vijendra K. Boken'],"The 16-day NDVI composites of MODIS data for the cropping season were examined for their relationship with the harvested area and the yield of soybean and cotton and the fertilizer ingredients (Ammonium nitrogen, Nitrate nitrogen, Phosphates, and Potassium) found in the groundwater within the Delta region of Mississippi, United States. The best relationship was found in the case of phosphates.",Linking landuse and groundwater quality in the Mississippi delta Using MODIS satellite data
"['Ikuko Hamamoto', 'Ben R. Mottelson']","The ground states of some nuclei are described by densities and mean fields that are spherical, while others are deformed. The existence of non-spherical shape in nuclei represents a spontaneous symmetry breaking.",Shape Deformations in Atomic Nuclei
"['Erjen Lefeber', 'Kristin Ytterstad Pettersen', 'H Henk Nijmeijer']","In this paper, we address the tracking problem for an underactuated ship using two controls, namely surge force and yaw moment. A simple state-feedback control law is developed and proved to render the tracking error dynamics globally K- exponentially stable. Experimental results are presented where the controller is implemented on a scale model of an offshore supply vessel.",Tracking control of an underactuated ship
"['David K. Lynch', 'Mark A. Chatelain', 'Theo K. Tessensohn', 'Paul M. Adams']","The authors report the remote identification of in situ airborne silicate (Si/sub X/O/sub Y/) and carbonate (CO/sub 3/) dust using passive infrared spectroscopy. The silicates are identified based on the 9.7 /spl mu/m emission feature, a resonance due to the fundamental asymmetric vibrational mode of the O-Si-O structure. The carbonate structure from calcite (CaCO/sub 3/) was identified based on the CO bending mode resonance at 11.4 /spl mu/m. Laboratory spectra of dust collected after the event confirm the high silica and carbonate content. Minimum detectable column densities are about 10/sup -6/ to 10/sup -7/ g/cm/sup -2/. These will be limited by the signal-to-noise ratio (SNR) of the detection system and the change in the refractive index across the spectral feature. The technique is useful for quick, first-order identifications of aerosol clouds when in-situ probes are unavailable or when the origin (hence probable composition) of the cloud is not known.",Remote identification of in situ atmospheric silicate and carbonate dust by passive infrared spectroscopy
"['Tobias Rick', 'Anette von Kapri', 'Torsten Kuhlen']","The knowledge of the propagation behavior of radio waves is a fundamental prerequisite for planning and optimizing mobile radio networks. Propagation effects are usually simulated numerically, since real-world measurement campaigns are time-consuming and expensive. Automatic planning algorithms can explore a vast amount of network configurations to find good deployment schemes. However, complex urban scenarios demand for a great emphasis on site-specific details in the propagation environment which are often not covered by automatic approaches. Therefore, we have combined the simulation of radio waves with an interactive exploration and modification of the propagation environment in a virtual reality prototype application. By coupling real-time simulation and manipulation tasks we can provide an uninterrupted user-centered workflow.",A virtual reality system for the simulation and manipulation of wireless communication networks
"['K??vin Huguenin', 'Anne-Marie Kermarrec', 'Konstantinos Kloudas', 'Fran??ois Ta??ani']","User Generated Content (UGC), such as YouTube videos, accounts for a substantial fraction of the Internet traffic. To optimize their performance, UGC services usually rely on both proactive and reactive approaches that exploit spatial and temporal locality in access patterns. Alternative types of locality are also relevant and hardly ever considered together. In this paper, we show on a large (more than 650,000 videos) YouTube dataset that  content locality  (induced by the related videos feature) and  geographic locality , are in fact correlated. More specifically, we show how the geographic view distribution of a video can be inferred to a large extent from that of its related videos. We leverage these findings to propose a UGC storage system that  proactively  places videos close to the  expected  requests. Compared to a caching-based solution, our system decreases by 16% the number of requests served from a different country than that of the requesting user, and even in this case, the distance between the user and the server is 29% shorter on average.",Content and geographical locality in user-generated content sharing systems
"['Will Thompson', 'Darren Gergle']","A  Situated Conversational Agent  (SCA) is an agent that engages in dialog about the context within which it is embedded. An SCA is distinguished from non-situated conversational agents by an intimate connection of the agent's dialog to its embedding context, and by intricate dependencies between its linguistic and physical actions. Constructing an SCA that can interact naturally with users while engaged in collaborative physical tasks requires the agent to interleave decision making under uncertainty, action execution, and observation while maximizing expected utility over a sequence of interactions. These requirements can be fulfilled by modeling an SCA as a  partially observable Markov decision process  (POMDP). We show how POMDPs can be used to formalize and implement psycholinguistic proposals on how situated dialog participants collaborate in order to make and ground dialog contributions.",Modeling situated conversational agents as partially observable Markov decision processes
"['Daniel Grosse', 'G. Fey', 'Rolf Drechsler']","The complexity of todays hardware systems steadily increases. Due to this fact new ways of efficiently describing systems are investigated. A very promising approach in this area is SystemC which is a C++-library. To take advantage of SystemC in the multi-valued domain, the concept of multi-valued logic has to be embedded in SystemC In this paper such a concept is introduced and details of the implementation are given. This creates a powerful development environment to model and efficiently simulate complex multi-valued circuits and systems. Due to C++-concepts, like operator overloading and templates, the task of modeling circuits becomes very convenient and handling of multi-valued signals is elegant. This gives the opportunity to design large circuits that can be mapped onto physically multi-valued gates. A scalable arithmetic logic unit is studied and experimental results are given.",Modeling multi-valued circuits in SystemC*
"['H. Ma', 'Ying Liu']","A design for diagnosable multiple-output digital systems is presented, in which not only error detection is implemented by the minimum number of checkers but also fault isolation is realized by minimal additional hardware instead of traditional software diagnostic procedures such that the computation time and memory space for fault isolation are eliminated. A partition algorithm is utilized for partitioning a digital system specially into subsystems with each subsystem containing only one primary output. With the aid of this partition algorithm, an algorithm for automatic fault isolation is presented. The algorithm is suitable for computer implementation so that faults in large size digital systems can be isolated automatically. The overhead of the additional hardware to isolate faults automatically is less than 2.1%. >",Design for diagnosable multiple-output digital systems
"['Tomoya Enokido', 'Ailixier Aikebaier', 'Makoto Takizawa']","It is critical to discuss how to realize not only energy-aware but also robust clusters of servers. A client usually issues a request to one server in a cluster and the server sends a reply to the client. Once the server stops by fault, the client does not receive a reply of the request and might be suspended to wait for a reply. Hence, each request is redundantly performed on multiple servers to be tolerant of server faults. In our previous studies, multiple servers are selected to redundantly and energy-efficiently perform a request process in the redundant power consumption laxity-based (RPCLB) algorithm. Here, since each application process is redundantly performed on more than one server, the larger amount of electric power is consumed. In this paper, we newly propose the improved RPCLB (IRPCLB) algorithm where once a process successfully terminates on one server, meaningless redundant processes are not performed on the other servers. We show the total power consumption of servers is reduced in the IRPCLB algorithm while the execution time of processes is almost the same as the RPCLB and round-robin (RR) algorithms.",An Energy-Efficient Redundant Execution Algorithm by Terminating Meaningless Redundant Processes
"['Man Guo', 'M.O. Ahmad', 'M.N.S. Swamy', 'Chunyan Wang']","In this paper, by modifying the well-known Viterbi algorithm, an adaptive Viterbi algorithm that is based on strongly connected trellis decoding is proposed. Using this algorithm, the design and a field-programmable gate array implementation of a low-power adaptive Viterbi decoder with a constraint length of 9 and a code rate of 1/2 is presented. In this design, a novel systolic array-based architecture with time multiplexing and arithmetic pipelining for implementing the proposed algorithm is used. It is shown that the proposed algorithm can reduce by up to 70% the average number of ACS computations over that by using the nonadaptive Viterbi algorithm, without degradation in the error performance. This results in lowering the switching activities of the logic cells, with a consequent reduction in the dynamic power. Further, it is shown that the total power consumption in the implementation of the proposed algorithm can be reduced by up to 43% compared to that in the implementation of the nonadaptive Viterbi algorithm, with a negligible increase in the hardware.",FPGA design and implementation of a low-power systolic array-based adaptive Viterbi decoder
"['Shunsuke Doi', 'Hiroo Ide', 'Shinji Ogawa', 'Katsuhiko Takabayashi', 'Shinsuke Fujita', 'Soichi Koike']","In this study, the authors developed a probabilistic model to analyze patient accessibility to hospitals by using a geographic information system (GIS). In the consideration of patient accessibility, they do not have correct patient addresses because of the laws related to protection of a patient's personal information. Thus, they use a representational place in the city or the ZIP code of the area. However, this may lead to a decline in the accuracy of the analysis. In this study, the authors used a 500-m mesh map and obtained the population gender and age group data from the national census data, in order to estimate the patient accessibility to medical facilities in small areas. They calculated the probability that a patient lives in each mesh on the basis of the population gender and five-year age group data. They selected the appropriate mesh on the basis of this probability and investigated the time distance from the estimated mesh to each hospital by using a GIS. As a result, they calculated the time distance and its distribution by using the proposed method from limited available information. Further, the authors found that in the target cities, the average time distance to hospital calculated using the proposed method was longer than that calculated by using a previous method; the percentage of patients who took more time than the city average to reach a hospital by using the proposed method. This method is very useful when planning the geographical resource allocation of medical services.",Probabilistic Model to Analyze Patient Accessibility to Medical Facilities Using Geographic Information Systems
"['Fran??ois Bergeron', 'Lin Gingras', 'Pierre Hadaya', 'Claude Caron']","Location-based and location-oriented applications will be amongst the most powerful drivers of organizational change in the coming years. However, the strategic use of these technologies and the benefits tied to them will vary from one organization to another based on the business needs to fill. The alignment between the use of location-based technologies and the firms' business strategic orientation is thus of primary importance. In an attempt to support firms in this alignment process, this paper presents the GEOGRID framework for identifying and analyzing the strategic opportunities offered to firms by the use of leading edge location technologies. This framework groups the main variables that need to be considered in order to analyze the competitive positioning of the business, to surface assumptions, to ascertain benefits, and to control costs tied to the strategic applications of location technologies in organizations.",A Framework for Evaluating Strategic Location-Based Applications in Businesses
"['James W. Davis', 'Alexander M. Morison', 'David D. Woods']","In current video surveillance systems, commercial pan/tilt/zoom (PTZ) cameras typically provide naive (or no) automatic scanning functionality to move a camera across its complete viewable field. However, the lack of scene-specific information inherently handicaps these scanning algorithms. We address this issue by automatically building an adaptive, focus-of-attention, scene-specific model using standard PTZ camera hardware. The adaptive model is constructed by first detecting local human activity (i.e., any translating object with a specific temporal signature) at discrete locations across a PTZ cameraÉ??s entire viewable field. The temporal signature of translating objects is extracted using motion history images (MHIs) and an original, efficient algorithm based on an iterative candidacy-classification-reduction process to separate the target motion from noise. The target motion at each location is then quantified and employed in the construction of a global activity map for the camera. We additionally present four new camera scanning algorithms which exploit this activity map to maximize a PTZ cameraÉ??s opportunity of observing human activity within the cameraÉ??s overall field of view. We expect that these efficient and effective algorithms are implementable within current commercial camera systems.",An adaptive focus-of-attention model for video surveillance and monitoring
"['Howard T. Welser', 'Eric Gleave', 'Vladimir Barash', 'Marc A. Smith', 'Jessica Meckes']","Community based Question and Answer systems have been promoted as web 2.0 solutions to the problem of finding expert knowledge. This promise depends on systemsÉ?? capacity to attract and sustain experts capable of offering high quality, factual answers. Content analysis of dedicated contributorsÉ?? messages in the Live QnA system found: (1) few contributors who focused on providing technical answers (2) a preponderance of attention paid to opinion and discussion, especially in non-technical threads. This paucity of experts raises an important general question: how do the social affordances of a site alter the ecology of roles found there? Using insights from recent research in online community, we generate a series of expectations about how social affordances are likely to alter the role ecology of online systems.",Whither the Experts? Social Affordances and the Cultivation of Experts in Community Q&A Systems
['Feng-Tse Lin'],"This paper investigates solving the imprecise weight coefficients knapsack problem by genetic algorithms. We investigate the possibility of using genetic algorithms solving the fuzzy knapsack problem without defining membership functions for each imprecise weight coefficient. The proposed approach simulates a fuzzy number by distributing it into some partition points. We use genetic algorithms to evolve the values in each partition point so that the final values represent the membership grade of a fuzzy number. The fuzzy concept of the genetic algorithms approach is different, but gives better results than the traditional fuzzy approach.",Solving The Imprecise Weight Coefficients Knapsack Problem by Genetic Algorithms
"['Hatem Ben-Ameur', ""Pierre L'Ecuyer"", 'Christiane Lemieux']","Several methods for reducing the variance in the context of Monte Carlo simulation are based on correlation induction. This includes antithetic variates, Latin hypercube sampling, and randomized version of quasi-Monte Carlo methods such as lattice rules and digital nets, where the resulting estimators are usually weighted averages of several dependent random variables that can be seen as function evaluations at a finite set of random points in the unit hypercube. In this paper, we consider a setting where these methods can be combined with the use of control variates and we provide conditions under which we can formally prove that the variance is minimized by choosing equal weights and equal control variate coefficients across the different points of evaluation, regardless of the function (integrand) that is evaluated.",Combination of General Antithetic Transformations and Control Variables
"['Johan Fredriksson', 'Thomas Nolte', 'Andreas Ermedahl', 'Mikael Nolin']","For component-based systems, classical techniques for WCET-estimation produce unacceptable overestimations of the WCET. This is because software components have more general behavior in order to su ...",Clustering Worst-Case Execution Times for Software Components
"['Yijun Yu', 'Yiqiao Wang', 'John Mylopoulos', 'Sotirios Liaskos', 'Alexei Lapouchnian', 'J.C.S. do Prado Leite']","A reverse engineering process aims at reconstructing high-level abstractions from source code. This paper presents a novel reverse engineering methodology for recovering stakeholder goal models from both structured and unstructured legacy code. The methodology consists of the following major steps: 1) Refactoring source code by extracting methods based on comments; 2) Converting the refactored code into an abstract structured program through statechart refactoring and hammock graph construction; 3) Extracting a goal model from the structured program's abstract syntax tree; 4) Identifying nonfunctional requirements and derive soft goals based on the traceability between the code and the goal model. To illustrate this requirements recovery process, we refactor stakeholder goal models from two legacy software code bases: an unstructured Web-based email in PHP (SquirrelMail) and a structured email client system in Java (Columba).",Reverse engineering goal models from legacy code
"['Norman Yuen', 'Benjamin Friedlander']","In this paper, we present an asymptotic performance analysis of three subspace-based methods for direction of arrival (DOA) estimation-the ESPRIT algorithm using second order statistics, the higher order ESPRIT algorithm using fourth-order cumulants, and the virtual ESPRIT (VESPA) algorithm using fourth-order cumulants. We examine the least-squares version of these algorithms, derive the expressions for the asymptotic variance of the estimated DOAs, and use specific examples to compare the relative performance of the algorithms. Finally, we present Monte Carlo simulations to validate the theoretical analysis.","Asymptotic performance analysis of ESPRIT, higher order ESPRIT, and virtual ESPRIT algorithms"
"['Guoliang Luo', 'Frederic Cordier', 'Hyewon Seo']","We describe a compression method for three-dimensional animation sequences that has notable advantages over existing techniques. We first aggregate the frame data by similarity and reorganize them into clusters, which results in the sequence split into several motion fragments of varying lengths. To minimize the number of clusters and obtain optimal clustering, we perform frame alignment, which eliminates the É??globalÉ?ù rigid transformation from each frame data and use only É??poseÉ?ù when evaluating the similarity between frames. We then apply principal component analysis for each cluster, from which we get coordinates of corresponding frames in a reduced dimension. Because similar frames are considered, the number of coefficients required for each frame becomes smaller; thus, we obtain better dimension reduction for a given reconstruction error. Further, we perform intracluster compression based on linear coding. Because every motion fragment presents similar frames, conventional linear predictive coding can be replaced by key frame-based linear coding to achieve minimal reconstruction error. Results show that our method can obtain a high compression ratio, with a limited reconstruction error. Copyright ?? 2013 John Wiley & Sons, Ltd.",Compression of 3D mesh sequences by temporal segmentation
"['Xin Sun', 'Sanjay G. Rao', 'Geoffrey G. Xie']","Enterprise networks often have complex routing designs given the need to meet a wide set of resiliency, security and routing policies. In this paper, we take the position that minimizing design complexity must be an explicit objective of routing design. We take a first step to this end by presenting a systematic approach for modeling and reasoning about complexity in enterprise routing design. We make three contributions. First, we present a framework for precisely defining objectives of routing design, and for reasoning about how a combination of routing design primitives (e.g. routing instances, static routes, and route filters etc.) will meet the objectives. Second, we show that it is feasible to quantitatively measure the complexity of a routing design by modeling individual routing design primitives, and leveraging configuration complexity metrics [5]. Our approach helps understand how individual design choices made by operators impact configuration complexity, and can enable quantifying design complexity in the absence of configuration files. Third, we validate our model and demonstrate its utility through a longitudinal analysis of the evolution of the routing design of a large campus network over the last three years. We show how our models can enable comparison of the complexity of multiple routing designs that meet the same objective, guide operators in making design choices that can lower complexity, and enable what-if analysis to assess the potential impact of a configuration change on routing design complexity.",Modeling complexity of enterprise routing design
"['Kouji Murakami', 'Tsutomu Hasegawa']",This paper describes a sensing function of a multi-jointed multi-fingered hand with soft fingertips. We propose a new method to detect change of contact types with an object while manipulating it: the fingertip contact with a flat surface and with a convex sharp edge. Proposed method can apply to the manipulation with rolling contact. Experimental results are demonstrated.,A new method of tactile sensing using fingertip with soft skin
"['Dongping Huang', 'Hessam S. Sarjoughian']","Successful development of large-scale complex and distributed real-time systems commonly relies on models developed separately for simulation studies and software implementation. Systems theory provides sound modeling principles to characterize structural and behavioral aspects of systems across time and space. The behavior of these models can be observed using simulation protocols that can correctly interpret time-based logical dynamics. Similarly, object-orientation theories and software architecture principles enable modeling static and dynamic behavior of systems. While models described either in system-theoretic or object-orientated languages may be used for both software design and simulation modeling, each has its own strengths and weaknesses. For example, a class of system-theoretic modeling approach called Discrete-event System Specification (DEVS) provides an appropriate basis to develop simulation models exhibiting concurrent and distributed behavior. Similarly, the Unified Modeling Language with real-time (UML-RT) constructs can be used to develop software design models that can be implemented and executed. Since software models are not suitable to be used as simulation models and simulation models may not adequately lend themselves to serve as software design blueprints, it is important to examine these approaches. We show some of the key shortcomings of these simulation and software design modeling approaches by developing some detailed specifications and implementation of a coffee machine with a focus on their treatment of logical and physical time.",Software and Simulation Modeling for Real-Time Software-Intensive Systems
"['Panos Kouvelis', 'Yixuan Xiao', 'Nan Yang']","We model the competition among multiple pharmacy benefit managers (PBMs) for the patronage of a client organization. Each PBM selects a list of prices to be charged to the client organization for each of the branded and generic drugs within a therapeutic class (price decision) and a formulary list that assigns branded drugs to preferred or nonpreferred tiers (formulary decision). Drug manufacturers offer rebates to PBMs for drugs on preferred tier of formularies. The individuals participating in the clientÉ??s pharmacy benefit plan are the ones consuming the drugs and making purchasing decisions, whereas the client organization is paying the majority of drug cost. The choices of the individuals and the client organization are governed by different utility measures. For this complex drug distribution setting and for competing PBMs, we show the existence and uniqueness of a pure Nash equilibrium on aggregate formulary and price decisions, which represent the welfare-adjusted cost and welfare-adjusted price of...",PBM Competition in Pharmaceutical Supply Chain: Formulary Design and Drug Pricing
"['Su Zhao', 'Jeremy Ovadia', 'Xinfeng Liu', 'Yong-Tao Zhang', 'Qing Nie']","For reaction-diffusion-advection equations, the stiffness from the reaction and diffusion terms often requires very restricted time step size, while the nonlinear advection term may lead to a sharp gradient in localized spatial regions. It is challenging to design numerical methods that can efficiently handle both difficulties. For reaction-diffusion systems with both stiff reaction and diffusion terms, implicit integration factor (IIF) method and its higher dimensional analog compact IIF (cIIF) serve as an efficient class of time-stepping methods, and their second order version is linearly unconditionally stable. For nonlinear hyperbolic equations, weighted essentially non-oscillatory (WENO) methods are a class of schemes with a uniformly high order of accuracy in smooth regions of the solution, which can also resolve the sharp gradient in an accurate and essentially non-oscillatory fashion. In this paper, we couple IIF/cIIF with WENO methods using the operator splitting approach to solve reaction-diffusion-advection equations. In particular, we apply the IIF/cIIF method to the stiff reaction and diffusion terms and the WENO method to the advection term in two different splitting sequences. Calculation of local truncation error and direct numerical simulations for both splitting approaches show the second order accuracy of the splitting method, and linear stability analysis and direct comparison with other approaches reveals excellent efficiency and stability properties. Applications of the splitting approach to two biological systems demonstrate that the overall method is accurate and efficient, and the splitting sequence consisting of two reaction-diffusion steps is more desirable than the one consisting of two advection steps, because CWC exhibits better accuracy and stability.",Operator splitting implicit integration factor methods for stiff reaction-diffusion-advection systems
"['Takahiro Saito', 'Yuki Ishii', 'Haruya Aizawa', 'Daisuke Yamada', 'Takashi Komatsu']","This paper presents a demosaicing approach via nonlinear image decomposition for a digital color camera. This approach is composed of four stages. At the first stage, with the multiplicative BV-G image-decomposition method, each primary color channel of observed raw color data mosaicked with the Bayer color filter array is represented as a product of two components so that its structural component may correspond to a cartoon signal-approximation and its texture component may collect almost all oscillatory variations representing textures. At the second stage, each separated component is demosaicked with an interpolation method suitable to it. For demosaicing of the structural component, this paper employs a TV-regularization super-resolution deblurring-demosaicing method that can remove image blurs without producing ringing artifacts near edges. For interpolation of the texture component, this paper employs a basic averaging-type demosaicing method that is robust against noise, because the texture component gathers observation noise. At the third stage, white balancing, color enhancement and inverse gamma correction are applied to only the demosaicked structural component. At the final stage, the two interpolated components are combined. Our decomposition-and-demosaicing approach successfully performs denoising and deblurring as well as color interpolation.",Nonlinear decomposition-and-demosaicing approach for a digital color camera
"['Vladimir Makarenkov', 'Dmytro Kevorkov', 'Pablo Zentilli', 'Andrei Gagarin', 'Nathalie Malo', 'Robert Nadon']","Motivation: High-throughput screening (HTS) plays a central role in modern drug discovery, allowing for testing of >100 000 compounds per screen. The aim of our work was to develop and implement methods for minimizing the impact of systematic error in the analysis of HTS data. To the best of our knowledge, two new data correction methods included in HTS-Corrector are not available in any existing commercial software or freeware.#R##N##R##N#Results: This paper describes HTS-Corrector, a software application for the analysis of HTS data, detection and visualization of systematic error, and corresponding correction of HTS signals. Three new methods for the statistical analysis and correction of raw HTS data are included in HTS-Corrector: background evaluation, well correction and hit-sigma distribution procedures intended to minimize the impact of systematic errors. We discuss the main features of HTS-Corrector and demonstrate the benefits of the algorithms.#R##N##R##N#Availability: The Microsoft Windows version and a detailed description of the software are freely available at the following URL: http://www.labunix.uqam.ca/~makarenv/hts.html#R##N##R##N#Contact: makarenkov.vladimir@uqam.ca",HTS-Corrector: software for the statistical analysis and correction of experimental high-throughput screening data
"['Rong Zhang', 'Lajos Hanzo']","A novel Multiuser Detection (MUD) scheme is proposed for DS-CDMA systems employing the so-called Harmony Search (HS) algorithm, which is a novel meta-heuristic optimisation method. We specifically design the HS aided MUD for the communications problem considered and apply it in an iterative joint Channel Estimation (CE), MUD and channel decoding framework. The simulation results demonstrate that a near-single-user performance can be achieved by the proposed algorithm while avoiding the excessive-complexity full-search-based optimum detection even in overloaded DS-CDMA systems. Moreover, the HS algorithm can be efficiently applied in the Expectation Maximisation (EM) based CE framework.","Harmony Search Aided Iterative Channel Estimation, Multiuser Detection and Channel Decoding for DS-CDMA"
"['Prem Gopalan', 'Chong Wang', 'David M. Blei']","We develop a probabilistic approach for accurate network modeling using node popularities within the framework of the mixed-membership stochastic block-model (MMSB). Our model integrates two basic properties of nodes in social networks: homophily and preferential connection to popular nodes. We develop a scalable algorithm for posterior inference, based on a novel nonconjugate variant of stochastic variational inference. We evaluate the link prediction accuracy of our algorithm on nine real-world networks with up to 60,000 nodes, and on simulated networks with degree distributions that follow a power law. We demonstrate that the AMP predicts significantly better than the MMSB.",Modeling Overlapping Communities with Node Popularities
"['Milan Milenkovic', 'S.H. Robinson', 'Rob C. Knauerhase', 'David Barkai', 'Sharad Garg', 'Anu Tewari', 'Todd A. Anderson', 'Mic Bowman']","Emerging Internet uses-including peer-to-peer and grid computing-provide both a glimpse of and the impetus for evolving the Internet into a distributed computing, platform of unprecedented scale. Taking a longer view, the authors consider what would be needed to make the Internet an application-hosting platform: a networked, distributed counterpart of the hosting environment traditional operating systems provide to applications within a single node. The foundation of their proposed approach is to disaggregate and virtualize individual system resources as services that can be described, discovered, and dynamically configured at runtime to execute an application.",Toward Internet distributed computing
"['Doug Kimelman', 'Harold Ossher', 'Andr?? van der Hoek', 'Margaret-Anne D. Storey']","""Flexible modeling tools""  hold the promise of bridging the gap between formal modeling and free-form authoring. This workshop will bring together researchers and practitioners to explore ideas and showcase early results in this emerging field.   Both formal modeling and free-form authoring offer important benefits for software architects and designers, as well as others. Unfortunately, contemporary tools often force users to choose one style of work over the other. During the exploratory phases of design, it is more common to use white boards than modeling tools. During the early stages of architectural analysis, it is more common to use office tools like PowerPoint and Excel. These tools offer ease of use, freedom from strict representation rules, and the ability to readily prepare attractive presentations for a variety of stakeholders. However, users miss out on the clarity, consistency, and completeness that can accrue from using modeling tools, as well as the powerful visualization, navigation, manipulation, and guidance that semantics-driven tools can provide.   At this workshop, people who build tools and people who use tools for software development will discuss the reasons for the current state of the practice, and will focus on tool users' needs and tool capabilities to address those needs. Papers and live demonstrations will present work on free-form authoring tools, formal modeling tools, and hybrid tools that aim to achieve the benefits of both",SPLASH 2010 workshop on flexible modeling tools
"['Raul Carneiro Martins', 'Ant??nio Manuel da Cruz Serra']","In this paper, we will present a simple and cost effective, yet accurate, setup for measuring the amplitude distribution of a signal and its probability density function. From this, it will be shown how to measure linear and nonlinear errors through a third-order Taylor series representation of the distorted signal. We will also show how this representation may be used to subtract the nonlinearity of the stimulus signal from a histogram characterization and how it could be employed as a horizontal metric for adequacy of the amplitude distribution, also in the histogram method. Finally, experimental results concerning the characterization of two sine wave generators will be presented and compared with measurements made with a spectrum analyzer.",Representation and measurement of nonlinearities in stimulus signals
"['C. De Vleeschouwer', 'J.-F. Delaigle', 'Beno??t Macq']","The need for reversible or lossless watermarking methods has been highlighted in the literature to associate subliminal management information with losslessly processed media and to enable their authentication. The paper first analyzes the specificity and the application scope of lossless watermarking methods. It explains why early attempts to achieve reversibility are not satisfactory. They are restricted to well-chosen images, strictly lossless context and/or suffer from annoying visual artifacts. Circular interpretation of bijective transformations is proposed to implement a method that fulfills all quality and functionality requirements of lossless watermarking. Results of several bench tests demonstrate the validity of the approach.",Circular interpretation of bijective transformations in lossless watermarking for media asset management
"['Oscar D??niz', 'Mario Hern?≠ndez', 'Javier Lorenzo', 'Modesto Castrill??n']","Robotics researchers and cognitive scientists are becoming more and more interested in so-called sociable robots. These machines normally have expressive power (facial features, voice,...) as well as abilities for locating, paying attention to, and addressing people. The design objective is to make robots which are able to sustain natural interactions with people. This capacity falls within the range classed as social intelligence in humans. This position paper argues that the reproduction of social intelligence, as opposed to other types of human ability, may lead to fragile performance, in the sense that tested cases may produce rather different performances to future (untested) cases and situations. This limitation stems from the fact that our social abilities, which appear early in life, are mainly unconscious in origin. This is in contrast with other human abilities that we carry out using conscious effort, and for which we can easily conceive algorithms and representations. This novel perspective is deemed useful for defining the obstacles and limitations of a field that is generating increasing interest. Taking into account the mentioned issues, a development approach suited to the problem is proposed. The use of this approach is demonstrated in the development of CASIMIRO, a robotic head with basic interaction abilities.",An engineering approach to sociable robots
"['Rafael Martins de Souza', 'Adriano C. M. Pereira']","In the first quarter of year 2008, electronic reverseauctions (e-RAs) allowed the Brazilian Government tosave up to US$ 270 million, which account for 87% ofits acquisitions in the period, against 1% in the sameperiod 6 years earlier É?? according to the Ministry ofPlanning, Budget and Management. A tool of suchimportance is subjected to fraud or even anomalousbehaviors, which are difficult to detect with simpleanalysis over the current system. In this paper we lookforward into identifying suspicious behaviors in egovernmentprocurement systems, through the use ofbusiness intelligence techniques. The results confirmthat our methodology can help discovering interestingaspects that can be used to help market players indecision support and auctioneerÉ??s management.",A Business Intelligence Methodology for E-government Reverse Auctions
"['Bianca Falcidieno', 'Michela Spagnuolo']",Shape is a very important way of perceiving and reasoning about the world. The authors introduce some considerations about shape representation and abstraction tools and their interaction with analysis and synthesis processes. A generic architecture for shape-based modelling is also outlined. The concepts discussed in the first part of the article are also exemplified through the description of some significant applications developed at the Istituto per la Matematica Applicata.,Shape abstraction tools for modeling complex objects
"['Jin Seo Park', 'Yong-Wook Jung', 'Jun Won Lee', 'Dong Sun Shin', 'Min Suk Chung', 'Martin Riemer', 'Heinz Handels']","For the Visible Korean Human (VKH), a male cadaver was serially ground off to acquire the serially sectioned images (SSIs) of a whole human body. Thereafter, more than 700 structures in the SSIs were outlined to produce detailed segmented images; the SSIs and segmented images were volume- and surface-reconstructed to create three-dimensional models. For outlining and reconstruction, popular software (Photoshop, MRIcro, Maya, AutoCAD, 3ds max, and Rhino) was mainly used; the technique can be reproduced by other investigators for creating their own images. For refining the segmentation and volume reconstruction, the VOXEL-MAN system was used. The continuously upgraded technique was applied to a female cadaverÉ??s pelvis to produce the SSIs with 0.1 mm sized intervals and 0.1 mm ?? 0.1 mm sized pixels. The VKH data, distributed worldwide, encouraged researchers to develop virtual dissection, virtual endoscopy, and virtual lumbar puncture contributing to medical education and clinical practice. In the future, a virtual image library including all the Visible Human Project data, Chinese Visible Human data, and VKH data will hopefully be established where",Generating useful images for medical applications from the Visible Korean Human
"['Mark Gamadia', 'Nasser Kehtarnavaz', 'Katie Roberts-Hoffman']","Images captured by a digital or cell-phone camera in low-light environments usually suffer from a lack of sharpness due to the failure of the camera's passive auto-focus (AF) system to locate the peak in-focus position of a sharpness function that is extracted from the image. In low-light, the sharpness function becomes flat, making it quite difficult to locate the peak.In this paper, a systematic approach is introduced to address the problem of low-light AF by performing computationally simple image enhancement preprocessing steps as part of the image pipeline. These enhancement steps elevate the sharpness function peak, leading to auto-focusing in low-light conditions. A sharpness junction quality measure along with experimental guidelines are presented for determining the most prominent enhancement steps for low-light AF. The implementation results on an actual digital camera platform are also shown to demonstrate the effectiveness of our solution.",Low-Light Auto-Focus Enhancement for Digital and Cell-Phone Camera Image Pipelines
"['H?¨seyin Acan', 'Pawel Hitczenko']","In 2005 Janson, extending earlier work of Mahmoud, Smythe, and Szyma\'nski, established the joint asymptotic normality of the outdegrees of a random plane recursive tree. In particular, he gave an explicit description of the limiting covariance matrix. Our aim here is to provide substantially simplified expression for the limiting covariance matrix.",On the covariances of outdegrees in random plane recursive trees
"['Mark Wittkamp', 'Luigi Barone', 'R. Lyndon While']","Many games require opponent modeling for optimal performance. The implicit learning and adaptive nature of evolutionary computation techniques offer a natural way to develop and explore models of an opponent's strategy without significant overhead. In this paper, we compare two learning techniques for strategy development in the game of Spoof, a simple guessing game of imperfect information. We compare a genetic programming approach with a look-up table based approach, contrasting the performance of each in different scenarios of the game. Results show both approaches have their advantages, but that the genetic programming approach achieves better performance in scenarios with little public information. We also trial both approaches against opponents who vary their strategy; results showing that the genetic programming approach is better able to respond to strategy changes than the look-up table based approach",A Comparison of Genetic Programming and Look-up Table Learning for the Game of Spoof
"['Sebastian Bosse', 'Heiko Schwarz', 'Tobias Hinz', 'Thomas Wiegand']","This paper describes a new encoder control method for multiview video plus depth coding. Since large parts of a multiview scenery are present in more than one of the captured video sequences, a depth-aware encoder control is introduced, which identifies those regions based on given depth maps and omits the coding of the residual signal for those regions. Experimental results indicate that bit rate reductions of about 5É??9 %, depending on the bit rate, can be achieved for the 2-view case at a constant subjective quality.",Encoder control for renderable regions in high efficiency multiview video plus depth coding
"['Aaron Gage', 'Robin R. Murphy']","This paper presents an algorithm for allocating sensing resources for an autonomous mobile robot with logically redundant sensing capabilities. The algorithm creates a partial plan based on the set of requests by behaviors. If two or more behaviors place conflicting requests, a variant of the MIN-CONFLICT algorithm is used to find a replacement logical sensor. Unlike traditional MIN-CONFLICT, our variant maximizes each behavior's preference for a particular sensor (""happiness""). Simulations compared MIN-CONFLICT with Happiness to other methods (random and greedy assignment) for 10 sequences of 20 random requests for 8 sensors from up to 11 concurrent behaviors. Results showed that it is able to generate more schedules (on the order of 71% to 155% more) and that a further variant could maximize happiness better (7% to 30%).",Allocating sensor resources to multiple behaviors
"['Jekuk Yun', 'Beomseok Hong', 'Yanggon Kim']","The Border Gateway Protocol (BGP) is the routing protocol that enables large IP networks to form a single Internet. The main objective of BGP is to exchange Network Layer Reachability Information (NLRI) between Autonomous Systems (ASes) so that a BGP speaker can announce their IP prefix and find a path to the destination of packets. However, a BGP hijacker can pretend to be any third BGP speaker because BGP itself doesn't have the functionality of validating BGP messages. In order to solve this problem, BGP speaker needs to validate messages coming from other BGP speakers. In this paper, we propose the BGP Monitoring and Alarm System (BGPMAS) which monitors incoming announcements and starts to make sounds of the alarm if the BGPMAS detects an invalid announcement. In addition, the BGPMAS provides AS administrators with web service to show where the invalid message is coming from so that the administrators can rapidly deal with the IP prefix hijacking by ignoring the malicious BGP router's prefix. In order to set this environment, the BGPMAS needs to be connected to the BGP router and the AS administrator needs the Alarm Application (AA) which will make sounds of the alarm and the AA receives a signal from the BGPMAS when the BGPMAS detect an invalid announcement. As a result, the BGP routers can easily have the RPKI-based origin validation function with the BGPMAS.",The BGP monitoring and alarming system to detect and prevent anomaly IP prefix advertisement
"['Desislava C. Dimitrova', 'J.L. van den Berg', 'Geert J. Heijenk']","Deployment of intermediate relay nodes in cellular networks, e,g, UMTS/ HSPA, has been proposed for service enhancement, which is of particular importance for uplink users at the cell edge suffering from low power capacity and relatively poor channel conditions. In this paper, we propose and investigate a number of uplink packet scheduling schemes deploying the relay functionality in different ways. Using a combined packet and flow level analysis capturing the specifics of the scheduling schemes and the random behavior of the users (initiation and completion of flow transfers), the performance of the various schemes is evaluated and compared to a reference scenario where relaying is not used. The main performance measures considered in our study are the instantaneous data rate, the energy consumption and the mean flow transfer time. Interestingly, considering flow transfer times, it is found that the use of relay nodes is not only particularly beneficial for users at the cell edge but also has a strongly positive effect on the performance of users at locations close to the base station.",Performance Analysis of Uplink Packet Schedulers in Cellular Networks with Relaying
"[""Robert J. O'Callaghan"", 'David Bull']","Colour object recognition is heavily influenced by variation in the scene illumination conditions. This paper proposes a set of illumination-invariant descriptors of image content. The descriptors are based on a moment-based approach to histogram comparison and, in the case of an object imaged under two different lighting conditions, permit straightforward recovery of the illumination change involved. The efficacy of the descriptors is compared experimentally with a variety of existing techniques, using an established methodology and an existing purpose-built dataset. The evidence suggests that the new descriptors outperform existing techniques in the area of colour object recognition.",Improved illumination-invariant descriptors for robust colour object recognition
"['Allan Collins', 'Richard Halverson']","Abstract#R##N##R##N#This paper drew upon a recent book (Rethinking Education in the Age of Technology) to summarize a number of prospects and challenges arising from the appropriation of digital technology into learning and educational practice. Tensions between traditional models of schooling and the affordances of digital media were noted, while the promise of these technologies for shaping a new system of education was reviewed. It was argued that new technology brings radical opportunities but also significant challenges. The urgency of seeking a coherent model for the future of education in a technological age was stressed.",The second educational revolution: rethinking education in the age of technology
"['Tom Waayers', 'Richard Morren', 'Xijiang Lin', 'Mark Kassab']","This paper presents a clock control architecture for designs with multiple clock domains, and a novel mix of existing ATPG techniques as well as novel ATPG enhancements. The combination of the ATPG techniques and the clock control hardware lowers the number of test patterns in a fully automated flow, while maintaining the high coverage that is required nowadays by production test. Experimental results are shown for two industrial designs.",Clock control architecture and ATPG for reducing pattern count in SoC designs with multiple clock domains
"['Sudipta N. Sinha', 'Marc Pollefeys']","In this paper, we discuss the problem of estimating parameters of a calibration model for active pan-tilt-zoom cameras. The variation of the intrinsic parameters of each camera over its full range of zoom settings is estimated through a two step procedure. We first determine the intrinsic parameters at the camera's lowest zoom setting very accurately by capturing an extended panorama. The camera intrinsics and radial distortion parameters are then determined at discrete steps in a monotonically increasing zoom sequence that spans the full zoom range of the camera. Our model incorporates the variation of radial distortion with camera zoom. Both calibration phases are fully automatic and do not assume any knowledge of the scene structure. High-resolution calibrated panoramic mosaics are also computed during this process. These fully calibrated panoramas are represented as multi-resolution pyramids of cube-maps. We describe a hierarchical approach for building multiple levels of detail in panoramas, by aligning hundreds of images captured within a 1-12?? zoom range. Results are shown from datasets captured from two types of pan-tilt-zoom cameras placed in an uncontrolled outdoor environment. The estimated camera intrinsics model along with the cube-maps provides a calibration reference for images captured on the fly by the active pan-tilt-zoom camera under operation making our approach promising for active camera network calibration.",Pan-tilt-zoom camera calibration and high-resolution mosaic generation
"['Yi-Yu Liu', 'Ting Ting Hwang']","We propose a logic synthesis flow which utilizes the functionality of circuit to synthesize a domino-cell network which will have more wires crosstalk-immune to each other For that purpose, techniques of output phase flipping and crosstalk-aware technology mapping are used. Meanwhile, metric to measure the crosstalk sensitivity of domino cells in synthesis level is proposed. Experimental results demonstrate that the crosstalk sensitivity of the synthesized domino-cell network is greatly reduced by 51% using our synthesis flow as compared with conventional methodology. Furthermore, after placement and routing are performed, the ratio of the number of crosstalk-immune wire pairs to the number of total wire pairs is about 25% using our methodology as compared to 9% using conventional techniques.",Crosstalk-aware Domino Logic Synthesis
"['Joseph A. Cottam', 'Andrew Lumsdaine']","The data-state and data-flow models of information visualization are known to be expressively equivalent. Each model is most effective for different combinations of analysis processes and data characteristics. Visualization frameworks tend to either (1) work within a single model or (2) permit either model in separate sub-frameworks. In either case, converting between the two models falls entirely to the programmer. The theoretical basis for automatic translation between the two models was established by Chi. However, that process is insufficiently specified to be directly implemented. This paper characterizes the practical advantages of the data-state model. This is used to identify when such a transformation is beneficial. It then expands on Chi's theoretical framework to provide the tools for translating visualization program fragments from the data-flow to the data-state model. A partial implementation of the expanded theory is described for the Stencil visualization environment.",Automatic Application of the Data-State Model in Data-Flow Contexts
"['G. Andrew Woolley', 'En-Shiun Lee', 'Fuzhong Zhang']","sGAL is a computer program designed to find pairs of sites suitable for introducing chemical cross-links into proteins. sGAL takes a protein structure file in PDB format as input, truncates each residue sequentially to its gamma side chain atom to mimic mutation to Cys, and calculates the exposed surface area of the gamma atom. The user then inputs the minimum and maximum lengths of the cross-linker. sGAL provides as output pairs of residues that would have exposed gamma atom separations that fall within this range. Furthermore, if a line joining the pair of gamma atoms contacts more than a given number of buried atoms, that pair is discarded. In this way, sites for which the protein would sterically interfere with cross-linking are avoided.#R##N##R##N#Availability:http://www.chem.utoronto.ca/staff/GAW/links.html; (Surface Racer is also required see: http://monte.biochem.wisc.edu/~tsodikov/surface.html).#R##N##R##N#Contact: awoolley@chem.utoronto.ca",sGAL: a computational method for finding surface exposed sites in proteins suitable for Cys-mediated cross-linking
"['Amine Kchiche', 'Farouk Kamoun']","Mobile ad hoc networks such as Mesh networks or Vehicular ad hoc networks (VANETs) are in most cases relying on infrastructure deployment to provide access to internet services or any kind of shared resources. Developing dedicated deployment strategies becomes hence essential. In fact, research proved that the backbone topology has noticeable impact on the network performance.#R##N##R##N#The main contribution of this paper is a thorough study of the impact of roadside units (RSUs) deployment strategies in vehicular ad hoc networks on the performances. We point out in a first part the main characteristics of such an environment and extract the most significant deployment criteria. We show in a second part that centrality is the key point for achieving best performances. Our analysis reveals that group-based deployment strategies could even provide a certain quality of service while communicating through RSUs. Simulation results clearly illustrate a general improvement of performances in terms of end-toend delay while adopting a group-centrality based deployment approach.",Access-Points Deployment for Vehicular Networks Based on Group Centrality
"['Sharman Lichtenstein', 'Paula M. C. Swatman']","The use of the Internet in organisations and companies for carrying out various business activities is becoming an increasingly major component of e-business. Accidental and deliberate misuse and abuse of the Internet by internal employees and external parties, combined with the increasingly vulnerable global Internet infrastructure and the paucity of Internet regulation, has led to an Internet security problem for organisations. This paper reports the major findings from a four year study (1996 É?? 2000) which included substantial exploration of e-business security issues via six case studies at five medium-to-large organisations, as well as a focus group of industry leaders. The research results include an holistic framework for ebusiness security policy. The research also highlights the importance of human issues and the need for changes, in current practices in e-business security management and policy.",Effective Management and Policy in e-Business Security
"['Peng He', 'Abbas Edalat']","We present a framework to compute the visual hull of a polyhedral scene, in which the vertices of the polyhedra are given with some imprecision. Two kinds of visual event surfaces, namely VE and EEE surfaces are modelled under the geometric framework to derive their counterpart object, namely partial VE and partial EEE surfaces, which contain the exact information of all possible visual event surfaces given the imprecision in the input. Correspondingly, a new definition of visual number is proposed to label the cells of Euclidean space partitioned by partial VE and partial EEE surfaces. The overall algorithm maintains the same computational complexity as the classical method and generates a partial visual hull which converges to the classical visual hull as the input converges to an exact value.",Visual Hull from Imprecise Polyhedral Scene
"['B. Brunner', 'Gerd Hirzinger', 'K. Landzettel', 'J. Heindl']","Outlines key technologies in the approach of the author's research establishment to space robotics. Based on multisensory gripper technology, local on-board sensory feedback, and predictive graphic simulation (with emphasis on sensory simulation) a tele-sensor programming concept is introduced that allows sensor-based teleoperation in spite of large signal delays as well as sensor-based off-line programming following a ""learning by showing"" concept. A small multisensory robot based on these concepts has flown in space with a ten-day Space Shuttle mission. This robot technology experiment ROTEX was very successful and showed that, with these sensor-based concepts, even present-day space robots can perform different prototype tasks in a variety of operational modes, including automatic (reprogrammable) operation, and on-board teleoperation using human and/or machine intelligence.",Multisensory shared autonomy and tele-sensor-programming-Key issues in the space robot technology experiment ROTEX
"['Jasper R. R. Uijlings', 'Arnold W. M. Smeulders', 'Remko Scha']","As datasets grow increasingly large in content-based image and video retrieval, computational efficiency of concept classification is important. This paper reviews techniques to accelerate concept classification, where we show the trade-off between computational efficiency and accuracy. As a basis, we use the Bag-of-Words algorithm that in the 2008 benchmarks of TRECVID and PASCAL lead to the best performance scores. We divide the evaluation in three steps: 1) Descriptor Extraction, where we evaluate SIFT, SURF, DAISY, and Semantic Textons. 2) Visual Word Assignment, where we compare a k-means visual vocabulary with a Random Forest and evaluate subsampling, dimension reduction with PCA, and division strategies of the Spatial Pyramid. 3) Classification, where we evaluate the ?? 2 , RBF, and Fast Histogram Intersection kernel for the SVM. Apart from the evaluation, we accelerate the calculation of densely sampled SIFT and SURF, accelerate nearest neighbor assignment, and improve accuracy of the Histogram Intersection kernel. We conclude by discussing whether further acceleration of the Bag-of-Words pipeline is possible. Our results lead to a 7-fold speed increase without accuracy loss, and a 70-fold speed increase with 3% accuracy loss. The latter system does classification in real-time, which opens up new applications for automatic concept classification. For example, this system permits five standard desktop PCs to automatically tag for 20 classes all images that are currently uploaded to Flickr.",Real-Time Visual Concept Classification
"['John G. Waclawsky', 'Mahendran Velauthapillai']","We present a new approach for analyzing token ring LANs. The token ring model presented supports the behavior obtained from actual ring measurements. Using this model, employing deterministic bounds analysis techniques, we prove several theorems which lead to some interesting conclusions that support observed ring performance. The results obtained give insight into practical behavior of a token ring environments. >",Dynamics of token ring protocols
"['Xiaosheng Liu', 'Lei Hao', 'Yuxuan Liu']","In order to be able to reduce the casualties and property losses which is caused by floods in Poyang Lake area, and to provide assists and supports to the flood control department concerned of Poyang Lake region, authors firstly determined the system objectives and structure, then designed three databases such as history regimen database, real-time regimen database, and engineering database, implemented the functions such as data acquisition and updating, information query retrieval, statistical queries, spatial analysis, data output, system instructions and help, and finally completed the development with flood prevention rainfall and regiment information system of Poyang Lake areas.",The Research on Flood Control and Rainfall Regimen Information System of Poyang Lake Areas
"['Jan Ciesko', 'Javier Bueno', 'Nikola Puzovic', 'Alex Ramirez', 'Rosa M. Badia', 'Jes?ßs Labarta']","Reductions matter and they are here to stay. Wide adoption of parallel processing hardware in a broad range of computer applications has encouraged recent research efforts on their efficient parallelization. Furthermore, trends towards high productivity languages in mainstream computing increases the demand for efficient programming support. In this paper we present a new approach on parallel reductions for distributed memory systems that provides both scalability and programmability. Using OmpSs, a task-based parallel programming model, the developer has the ability to express scalable reductions through a single pragma annotation. This pragma annotation is applicable for tasks as well as for work-sharing constructs (with implicit tasking) and instructs the compiler to generate the required runtime calls. The supporting runtime handles data and task distribution, parallel execution and data reduction. Scalability is achieved through a software cache that maximizes local and temporal data reuse and allows overlapped computation and communication. Results confirm scalability for up to 32 12-core cluster nodes.",Programmable and Scalable Reductions on Clusters
"[""Martin J. O'Connor"", 'Ravi D. Shankar', 'David B. Parrish', 'Amar K. Das']","Managing time-stamped data is essential to clinical research activities and often requires the use of considerable domain knowledge. Adequately representing and integrating temporal data and domain knowledge is difficult with the database technologies used in most clinical research systems. There is often a disconnect between the database representation of research data and corresponding domain knowledge of clinical research concepts. In this paper, we present a set of methodologies for undertaking ontology-based specification of temporal information, and discuss their application to the verification of protocol-specific temporal constraints among clinical trial activities. Our approach allows knowledge-level temporal constraints to be evaluated against operational trial data stored in relational databases. We show how the Semantic Web ontology and rule languages OWL and SWRL, respectively, can support tools for research data management that automatically integrate low-level representations of relational data with high-level domain concepts used in study design.",Knowledge-data integration for temporal reasoning in a clinical trial system
"['Laura Bernado', 'Anna Roma', 'Alexander Paier', 'Thomas Zemen', 'Nicolai Czink', 'Johan Karedal', 'Andreas Thiel', 'Fredrik Tufvesson', 'Andreas F. Molisch', 'Christoph F. Mecklenbrauker']","Inside a tunnel, electromagnetic wave propagation differs strongly from the well understood ""open-air"" situation. The characterization of the tunnel environment is crucial for deploying vehicular communication systems. In this paper we evaluate vehicle-to-vehicle (V2V) radio channel measurements inside a tunnel. We estimate the time-varying root mean square (rms) delay and Doppler spreads, as well as the excess delay and the maximum Doppler dispersion. The fading process in V2V communications is inherently non-stationary. Hence, we characterize the stationarity time, for which we can consider the fading process to be wide sense stationary. We show that the spreads, excess delay, and maximum Doppler dispersion are larger on average when both vehicles are inside the tunnel compared to the ""open-air"" situation. The temporal evolution of the stationarity time is highly influenced by the strength of time-varying multipath components and the distance between vehicles. Furthermore, we show the good fit of the rms delay and Doppler spreads to a lognormal distribution, as well as for the stationarity time. From our analysis we can conclude that the IEEE 802.11p standard will be robust towards inter-symbol and inter-carrier interference inside a tunnel.",In-Tunnel Vehicular Radio Channel Characterization
"['Renjie Chen', 'Yin Xu', 'Craig Gotsman', 'Ligang Liu']","The Delaunay triangulation of a planar point set is a fundamental construct in computational geometry. A simple algorithm to generate it is based on flips of diagonal edges in convex quads. We characterize the effect of a single edge flip in a triangulation on the geometric Laplacian of the triangulation, which leads to a simpler and shorter proof of a theorem of Rippa that the Dirichlet energy of any piecewise-linear scalar function on a triangulation obtains its minimum on the Delaunay triangulation. Using Rippa's theorem, we provide a spectral characterization of the Delaunay triangulation, namely that the spectrum of the geometric Laplacian is minimized on this triangulation. This spectral theorem then leads to a simpler proof of a theorem of Musin that the harmonic index also obtains its minimum on the Delaunay triangulation.",A spectral characterization of the Delaunay triangulation
"['Damir Malnar', 'Victor Sucic', 'Boualem Boashash']","A novel method for the signal components instantaneous frequency (IF) estimation based on the CrossWigner-Ville distribution (XWVD) is presented. The cross-terms in the XWVD are deliberately formed between the analyzed signal and a reference signal. The proposed method yields a scaled and time shifted image that closely resembles the instantaneous frequency laws of the components present in the signal. As the interferences location follow geometrical rules, and by using a reference signal well localized in time and frequency, the time-frequency coordinates of the analyzed signal components IF can be calculated by an automatic procedure described below. The performance of the method is tested on both synthetic and real-life signals, showing improvements over another recently proposed components extraction method.",A cross-terms geometry based method for components instantaneous frequency estimation using the Cross Wigner-Ville distribution
"['Xiao-Fei Zhang', 'Le Ou-Yang', 'Yuan Zhu', 'Meng-Yun Wu', 'Dao-Qing Dai']","Background#R##N#Recently, several studies have drawn attention to the determination of a minimum set of driver proteins that are important for the control of the underlying protein-protein interaction (PPI) networks. In general, the minimum dominating set (MDS) model is widely adopted. However, because the MDS model does not generate a unique MDS configuration, multiple different MDSs would be generated when using different optimization algorithms. Therefore, among these MDSs, it is difficult to find out the one that represents the true driver set of proteins.",Determining minimum set of driver nodes in protein-protein interaction networks
"['Frances A. Reed', 'Paul L. Feintuch', 'Neil J. Bershad']","This paper considers the use of the frequency domain LMS adaptive filter in a split array sonar to estimate the bearing of a plane wave acoustic source radiating a sinusoidal signal. For such a signal, the bearing can be determined from the relative signal phase between split array outputs at the signal frequency. In this paper, the split array outputs are fast Fourier transformed and used as the primary and reference inputs to the frequency domain LMS adaptive filter configured as a canceller. The signal frequency is estimated based upon the magnitude of the weights, and the relative signal phase between split array outputs is estimated as the phase of the frequency domain weight with the largest magnitude. The statistics of the magnitude and phase of the frequency domain weights are determined and then mapped to the variance of the bearing estimate. Computer simulations of the adaptive tracker which verify the analytical results are included.",The application of the frequency domain LMS adaptive filter to split array bearing estimation with a sinusoidal signal
"['George Lampropoulos', 'Alexandros Kaloxylos', 'Nikos I. Passas', 'Lazaros F. Merakos']","The proliferation of WLANs and the ubiquitous coverage of cellular networks have resulted in several integration proposals towards 4G networks. Among them, the tight- coupled WLAN/UMTS architectures promise seamless service continuity to users and enhanced network performance. Most of these solutions assume that only one interface is active at a time, while much fewer consider the concurrent use of both WLAN and UMTS interfaces as this is expected to consume more energy. This paper presents a detailed description of power consumption for the two different tight-coupled WLAN/UMTS approaches based on the states of the wireless devices. A simple analytical model is provided for estimating the power consumption in each approach, while a simulation model measures the power needs for more complicated cases. Moreover, the enhancement due to a power-saving mechanism in WLAN is also assumed in the system and useful deductions are provided about the average power consumption per mobile terminal.",A Power Consumption Analysis of Tight-Coupled WLAN/UMTS Networks
"['Fr??d??ric Guyon', 'Pierre Tuff??ry']","Motivation: Meaningful scores to assess protein structure similarity are essential to decipher protein structure and sequence evolution. The mining of the increasing number of protein structures requires fast and accurate similarity measures with statistical significance. Whereas numerous approaches have been proposed for protein domains as a whole, the focus is progressively moving to a more local level of structure analysis for which similarity measurement still remains without any satisfactory answer. Results: We introduce a new score based on BinetÉ??Cauchy kernel. It is normalized and bounded between 1É??maximal similarity that implies exactly the same conformations for protein fragmentsÉ??and ã®´ 1É?? mirror image conformations, the unrelated conformations having a null mean score. This allows for the search of both similar and mirror conformations. In addition, such score addresses two major issue of the widely used root mean square deviation (RMSD). First, it achieves length independent statistics even for short fragments. Second, it shows better performance in the discrimination of medium range RMSD values. Being simpler and faster to compute than the RMSD, it also provides the means for large-scale mining of protein structures. Availability and implementation: The computer software implementing the score is available at http://bioserv.rpbs.univ-paris",Fast protein fragment similarity scoring using a BinetÉ??Cauchy kernel
"['Junichi Maruyama', 'Go Hasegawa', 'Masayuki Murata']","In this paper, we propose a new mechanism which detects tampered-TCP connections at edge routers and protects well-behaved TCP connections from the tampered-TCP connections, resulting in maintaining the fairness amongst TCP connections. The proposed mechanism monitors the TCP packets at an edge router and estimates the window size or the throughput for each TCP connection. By using estimation results, the proposed mechanism assesses whether each TCP connection is tampered or not and drops packets intentionally if necessary to improve the fairness amongst TCP connections. From the results of simulation experiments, we exhibit that the proposed mechanism can accurately identify tampered-TCP connections. We also show that the proposed mechanism can regulate throughput ratio between tampered-TCP connections and competing TCP Reno connections to about 1.",Protection Mechanisms for Well-behaved TCP Flows from Tampered-TCP at Edge Routers
"['Lijun Li', 'Carl Tropper']","In optimistic simulations, checkpointing techniques are often used to reduce the overhead caused by state saving. In this paper, we propose event reconstruction as a technique with which to reduce the overhead caused by event saving, and compare its memory consumption and execution time to the results obtained by dynamic checkpointing. As the name implies, event reconstruction reconstructs input events and anti-events from the differences between adjacent states, and does not save input events in the event queue. For simulations with fine event granularity and small state size, such as the logic simulation of VLSI circuitry, event reconstruction can yield an improvement in execution time as well as a significant reduction in memory utilization when compared to dynamic checkpointing. Moreover, this technique facilitates load migration because only the state queue needs to be moved from one processor to another.",Event reconstruction in time warp
"['Uwe Hentschel', 'Fahad Khalid', 'Andreas Polze']","Within the Fontane project medical data has to be transmitted using public cellular networks. The most frequently transmitted data has only weak timing requirements. But in particular cases we also transmit streaming data that typically has soft real-time requirements. Regardless of the real-time nature of transmission though, the data being transmitted has associated priorities. However, public cellular networks have varying transmission characteristics and do not consider any kind of data priority on application level. We suggest using network specific information and priority aware mechanisms on client and server side to improve the behavior of our application. In public cellular networks each cell has a unique identifier that may be used to locate the sender within the network. We use the location area identity - a part of the cell identity - and the priority of the medical data to control the data transmission of our application. In addition to catering for data priority during transmission, we avoid bur sty traffic, which reduces adverse effects on other traffic flows that coexist in the same network. In this paper we present our traffic control protocol and the algorithm which is used to calculate the minimum priority level for each location area. The minimum priority is the least priority that the user data must have in order to be sent. Furthermore, we show how this protocol will be integrated into our middleware.",An Approach to Control Transmission of Medical Data over Cellular Networks Using Location Information
['Rajiv Gupta'],"The author presents strategies for static loop decomposition and scheduling as well as computer-assisted run-time scheduling that take into account, in addition to the cost of performing operations, the overhead costs associated with a decomposition and schedule. An algorithm for static decomposition of multidimensional loops based on the operation execution costs, communication costs, and synchronization costs is discussed. Synchronization instructions are introduced to ensure correct program execution following program decomposition. An algorithm for determining the explicit synchronization instruction that should be introduced to ensure correct execution of a program with arbitrarily nested loops is presented. Techniques for reducing run-time scheduling and communication and synchronization costs due to self-scheduling of multidimensional loops are also presented. Experiments performed on the Encore multiprocessor system demonstrate that the techniques developed can reduce overhead costs. >",Synchronization and communication costs of loop partitioning on shared-memory multiprocessor systems
"['Son K. Dao', 'Brad Perry']","An application of data mining techniques to heterogeneous database schema integration is introduced. We use attribute-oriented induction to mine for characteristic and classification rules about individual attributes from heterogeneous databases. Each mining request is conditioned on a subset of attributes identified as ""common"" between the multiple databases. We develop a method to compare the rules for two or more attributes (from different databases) and use the similarity between the rules as a basis to suggest similarity between attributes. As a result, we use relationships between and among entire sets of attributes from multiple databases to drive the schema integration process. Our initial efforts and prototypes applying data mining to assist schema integration prove promising and, we feel, identify a fruitful application area for data mining research.",Applying a data miner to heterogeneous schema integration
"['Carsten S. ??sterlund', 'Pernille Bj??rn', 'Paul Dourish', 'Richard Harper', 'Daniela K. Rosner']","Design research and the literature on sociomateriality emerge out of different academic traditions but share a common interest in the material. A sociomaterial perspective allows us to account for the complex ways people mingle and mangle information systems of all sorts into their social endeavors to accomplish organizational tasks. But, how do we account for these sociomaterial phenomena in all their complexity when faced with the task of designing information systems? The panel brings together prominent researchers bridging the gap between design research and the current debate on sociomateriality. Each presenter addresses the challenges associated with informing grounded design work with insights from a highly abstract intellectual debate.",Sociomateriality and Design
['Claudio Andreatta'],"In this paper we present COMPASS/web, a distributed image retrieval system that provides novel solutions to the problem of efficient and effective content-based browsing and retrieval over large multimedia databases. The system aims to exploit the synergy between visual and textual information introducing a viso-semantic distance based on a visual and a semantic similarity measure. The visual similarity is computed using low level features describing the image content while the semantic similarity is computed over graphs representing ontologies. Interactivity is achieved using relevance feedback mechanisms to adapt the similarity measures to the current task exploiting successive hints from the user. Preliminary results and future work directions are presented.",Ontology based viso-semantic similarity for image retrieval
"['Jianneng Cao', 'Barbara Carminati', 'Elena Ferrari', 'Kian-Lee Tan']","Most of the existing privacy-preserving techniques, such as k-anonymity methods, are designed for static data sets. As such, they cannot be applied to streaming data which are continuous, transient, and usually unbounded. Moreover, in streaming applications, there is a need to offer strong guarantees on the maximum allowed delay between incoming data and the corresponding anonymized output. To cope with these requirements, in this paper, we present Continuously Anonymizing STreaming data via adaptive cLustEring (CASTLE), a cluster-based scheme that anonymizes data streams on-the-fly and, at the same time, ensures the freshness of the anonymized data by satisfying specified delay constraints. We further show how CASTLE can be easily extended to handle l-diversity. Our extensive performance study shows that CASTLE is efficient and effective w.r.t. the quality of the output data.",CASTLE: Continuously Anonymizing Data Streams
"['Li-Wen Lin', 'Suzanne M. Embury', 'Brian Warboys']","In many cases, the programmer may require to encode business rules into the database applications. To do this, a large number of program elements may need to be examined by the programmer, to determine which have the capacity to violate a new rule and if so what minimal changes are required to prevent such violations. This process can be time-consuming, and even seasoned programmers can miss difficult and obscure cases in the mass of code. In this paper, we describe a static source code analysis technique to assist the programmer in enforcing business rules in a way that cuts down the amount of irrelevant code to be examined. Our technique derives all the possible ways in which a new business rule can be violated by the programs in the system being modified, and the specific program elements responsible.",Tool Support to Implementing Business Rules in Database Applications
"['Michel Sarkis', 'Christian T. Senft', 'Klaus Diepold']","The application of zoom camera lenses in machine vision has gained a lot of attention lately. The main difficulty in their employment lies in the accurate estimation of their intrinsic parameters. In this paper, we propose novel approaches to determine these parameters by estimating continuous models of their variations as the focus and the zoom change. The first method is based on the moving least squares (MLS) multiple regression scheme which determines from a predefined number of samples, the complete model of the intrinsic parameters. MLS fits a polynomial function at each focus and zoom setting by using the measured neighboring points. In order to reduce the computational complexity of MLS, we propose another algorithm in which the MLS generated curves are clustered. Then, each cluster is modeled with a single polynomial function. This decreases the complexity of computations for the applications where delay is critical, e.g., telepresence, to the evaluation of simple polynomials. Compared to previous techniques, the proposed algorithms lead to a noticeable increase in the estimation accuracy of the intrinsic parameters. In addition, they are able to generate accurate models of these parameters with only a few measurement points.",Calibrating an Automatic Zoom Camera With Moving Least Squares
"['J.P. Grant', 'Kauzar Saleh-Contell', 'J.-P. Wigneron', 'Massimo Guglielmetti', 'Yann H. Kerr', 'Mike Schwank', 'Niels Skou', 'A.A. Van de Griend']","In this paper, the L-band Microwave Emission of the Biosphere (L-MEB) model used in the Soil Moisture and Ocean Salinity (SMOS) Level 2 Soil Moisture algorithm is calibrated using L-band (1.4 GHz) microwave measurements over a coniferous (pine) and a deciduous (mixed/beech) forest. This resulted in working values of the main canopy parameters optical depth (tau), single scattering albedo (omega), and structural parameters  tt (H) and  tt (V), besides the soil roughness parameters  H   R  and  N   R . Using these calibrated values in the forward model resulted in a root mean-square error in brightness temperatures from 2.8 to 3.8 K, depending on data set and polarization. Furthermore, the relationship between canopy optical depth and leaf area index is investigated for the deciduous site. Finally, a sensitivity study is conducted for the focus parameters, temperature, soil moisture, and precipitation. The results found in this paper will be integrated in the operational SMOS Level 2 Soil Moisture algorithm and used in future inversions of the L-MEB model, for soil moisture retrievals over heterogeneous, partly forested areas.",Calibration of the L-MEB Model Over a Coniferous and a Deciduous Forest
"['Kwee-Bo Sim', 'Kwang-Sub Byun', 'Dong-Wook Lee']","Researches on the design of the optimal fuzzy controller have been carried out for many years. Various approaches to fuzzy modeling have been proposed. In this paper, we introduce a novel algorithm, schema coevolutionary algorithm, for fuzzy modeling. We demonstrate the schema coevolutionary algorithm and compare it with other similar coevolutionary algorithms: virus-evolutionary genetic algorithm and coevolution of Handa. Then, we apply it to design the optimal fuzzy controller. The fuzzy controller is used to control the mobile robot and optimized by the schema coevolutionary algorithm. We verify the efficacy of this algorithm through the experiment and comparison with other algorithms.",Design of fuzzy controller using schema coevolutionary algorithm
"['Yinzhi Luan', 'Jiandong Li']","A new method to correct the block timing offset in MC-CDMA system with frequency offset is proposed. Consecutive training sequences an used to acquire the initial time synchronization. The effect of the frequency offset on this estimator is analyzed. We compared this new method with Schmidl and Cox's (1997) method (for short, SC method) through simulation in terms of estimator mean and variance, both in AWGN channel and time-varying frequency channel. Results show that the new method is better than SC's method.",New timing acquisition method for MC-CDMA system with frequency offset
"['Keyan Zhou', 'Chengqing Zong', 'Hua Wu', 'Haifeng Wang']","Dialog-act tagging is one of the hot topics in processing human-human conversation. In this paper, we introduce a novel model to predict and tag the dialog-act, in which Markov decision process (MDP) is utilized to predict the dialog-act sequence instead of using traditional dialog-act based n-gram, and Support Vector Machine (SVM) is employed to classify the dialog-act for each utterance. The predicting result of MDP and the classifying result of SVM are integrated as the final tagging. The experimental results have shown that our approach outperforms the traditional method.",Predicting and Tagging Dialog-Act Using MDP and SVM
"['Jon Whittle', 'John Edward Hutchinson']","EAMDE was a 12 month research project, investigating how industry uses model-driven software development (MDSD). Using quantitative and qualitative research techniques, experiences were collected on the adoption and application of MDSD in 17 companies. The study highlighted examples of good and bad practice that lead to success or failure with MDSD. Some of these practices appear to have ramifications on the way that MDSD, and software modeling more generally, is taught within universities. This paper presents three of the key findings relevant to education: (1) A significant number of successful MDSD companies build their own modeling languages and generators, suggesting a re-orientation of education away from UML notation to fundamental modeling principles; (2) MDSD is generally taught top-down, whereas industry success is more likely when MDSD is applied bottom-up; (3) successful application of MDSD requires skills both in abstract modeling and compilers/optimization; however, these skills tend to be separated in standard CS curricula.",Mismatches between industry practice and teaching of model-driven software development
"['Chunhua Chen', 'Yun Q. Shi', 'Wei Su']","Double JPEG compression detection is of significance in digital forensics. We propose an effective machine learning based scheme to distinguish between double and single JPEG compressed images. Firstly, difference JPEG 2D arrays, i.e., the difference between the magnitude of JPEG coefficient 2D array of a given JPEG image and its shifted versions along various directions, are used to enhance double JPEG compression artifacts. Markov random process is then applied to modeling difference 2-D arrays so as to utilize the second-order statistics. In addition, a thresholding technique is used to reduce the size of the transition probability matrices, which characterize the Markov random processes. All elements of these matrices are collected as features for double JPEG compression detection. The support vector machine is employed as the classifier. Experiments have demonstrated that our proposed scheme has outperformed the prior arts.",A machine learning based scheme for double JPEG compression detection
"['Pietro Baroni', 'Massimiliano Giacomin']","The increasing variety of semantics proposed in the context of Dung's theory of argumentation makes more and more inadequate the example-based approach commonly adopted for evaluating and comparing different semantics. To fill this gap, this paper provides two main contributions. First, a set of general criteria for semantics evaluation is introduced by proposing a formal counterpart to several intuitive notions related to the concepts of maximality, defense, directionality, and skepticism. Then, the proposed criteria are applied in a systematic way to a representative set of argumentation semantics available in the literature, namely grounded, complete, preferred, stable, semi-stable, ideal, prudent, and CF2 semantics.",On principle-based evaluation of extension-based argumentation semantics
"['Zheng Chen', 'Suzanne Tamang', 'Adam Lee', 'Xiang Li', 'Marissa Passantino', 'Heng Ji']","The Slot Filling task requires a system to automatically distill information from a large document collection and return answers for a query entity with speci- fied attributes ('slots'), and use them to expand the Wikipedia infoboxes. We de- scribe two bottom-up Information Extraction style pipelines and a top-down Question Answering style pipeline to address this task. We propose several novel approaches to enhance these pipelines, including statistical answer re-ranking and Markov Logic Networks based cross-slot reasoning. We demonstrate that our system achieves state-of-the-art performance, with 3.1% higher precision and 2.6% higher recall compared with the best system in the KBP2009 evaluation.",Top-down and Bottom-up: A Combined Approach to Slot Filling
"['Will Y. Zou', 'Richard Socher', 'Daniel M. Cer', 'Christopher D. Manning']","We introduce bilingual word embeddings: semantic embeddings associated across two languages in the context of neural language models. We propose a method to learn bilingual embeddings from a large unlabeled corpus, while utilizing MT word alignments to constrain translational equivalence. The new embeddings significantly out-perform baselines in word semantic similarity. A single semantic similarity feature induced with bilingual embeddings adds near half a BLEU point to the results of NIST08 Chinese-English machine translation task.",Bilingual Word Embeddings for Phrase-Based Machine Translation
"['M. Estrada', 'A. Cerdeira', 'L. Resendiz', 'B. Iniguez', 'L. F. Marzal', 'J. Pallares']","It has been reported that nanocrystalline and microcrystalline devices show an anomalous behavior in the transconductance where several rates of increase of the transconductance with applied gate voltage, not present in amorphous TFTs are observed. In this paper we show that the anomalous effect of the transconductance is observed for an acceptor tail states activation energy similar to the normal values for hydrogenated silicon amorphous devices, (a-Si:H), provided that some conditions are met regarding the density of trapped charge in tail and deep states and the density of free charge in the material, which does not necessarily suggest a behavior in between amorphous and polycrystalline. The effect appears if the density of deep tail states, is smaller (higher) than the typical values in a-Si:H. The localized state distribution present in a nanocrystalline TFT prepared by hot wire deposition technique is estimated by comparison of experimental and simulated transconductance curves. In our case a lower density of deep states is obtained, which corresponds with their better light and bias stability.",Effect of localized traps on the anomalous behavior of the transconductance in nanocrystalline TFTs
"['Arvid O. I. Hoffmann', 'Wander Jager', 'J.H. von Eije']","This paper studies the use of social simulation in linking micro level investor behaviour and macro level stock market dynamics. Empirical data from a survey on individual investors' decision-making and social interaction was used to formalize the trading and interaction rules of the agents of the artificial stock market SimStockExchange. Multiple simulation runs were performed with this artificial stock market, which generated macro level results, like stock market prices and returns over time. These outcomes were subsequently compared to empirical macro level data from real stock markets. Partial qualitative as well as quantitative agreement between the simulated asset returns distributions and the asset returns distributions of the real stock markets was found.",Social simulation of stock markets: taking it to the next level
['Malcolm C. Pike'],,Remark on algorithm 145 [D1]: adaptive numerical integration by Simpson's rule
"['Yandong Yu', 'Peng Jiang', 'Yuan Zhuang', 'Huiyan Ning']","The process of superplastic blow forming was simulated and analyzed by the finite element software. The effect of the strain rate sensitivity index and forming pressure on the wall thickness distribution of forming parts were studied. Meanwhile the changes of strain rate during the forming process were simulated. It is shown that strain rate sensitivity index and forming pressure have a great impact on the thickness uniformity. When strain rate sensitivity index of 0.5 and forming pressure of 0.8 MPa, the thickness uniformity of forming parts is good and the highest thinning rate is 39%. The thickness gradients appear during blow forming due to the differences in the local stresses.",Finite element analysis of superplastic blow forming of fine-grained magnesium alloy sheet
"['Ke Xiong', 'Pingyi Fan', 'Yunquan Dong', 'Zhengding Qiu', 'Khaled Ben Letaief']","Signal-Time Coding (STC), a novel transmission mechanism, was proposed recently. It combines the traditional encoding/modulation mode in the signal domain with the signal pulse phase modulation in the time domain and can achieve higher information flow rate in some cases for relay networks. However, there are still many fundamental problems to be investigated. This paper considers the implementing issue of STC in AWGN relay networks. Firstly, an energy detection based STC (ED-STC) scheme is proposed and the error probabilities of ED-STC in both the signal domain and the time domain are given. Secondly, a performance evaluation criterion, the reliable information per symbol (RIPS), is proposed to characterize the performance of STC in noisy wireless networks. Moreover, the performance bounds of the RIPS of ED-STC are derived. Numerical analysis show that ED-STC outperforms traditional transmission method in terms of effective information rate within some practical conditions.",Energy Detection Based Signal-Time Coding for AWGN Relay Networks
"['Ou Ma', 'Jorge Angeles']","The optimum design of manipulator architectures under dynamic isotropy conditions is addressed. The goal of the design is to choose the kinematic and inertial parameters of manipulators such that their generalized inertia matrices can attain values as close to isotropy as possible. Two design examples are given. One pertains to a three-degree-of-freedom serial manipulator, and the other to a platform parallel manipulator. >",Optimum design of manipulators under dynamic isotropy conditions
"['Wen-tau Yih', 'Ming-Wei Chang', 'Christopher A. Meek', 'Andrzej Pastusiak']","In this paper, we study the answer sentence selection problem for question answering. Unlike previous work, which primarily leverages syntactic analysis through dependency tree matching, we focus on improving the performance using models of lexical semantic resources. Experiments show that our systems can be consistently and significantly improved with rich lexical semantic information, regardless of the choice of learning algorithms. When evaluated on a benchmark dataset, the MAP and MRR scores are increased by 8 to 10 points, compared to one of our baseline systems using only surface-form matching. Moreover, our best system also outperforms pervious work that makes use of the dependency tree structure by a wide margin.",Question Answering Using Enhanced Lexical Semantic Models
"['Krzysztof Krejtz', 'Tomasz Szmidt', 'Andrew T. Duchowski', 'Izabela Krejtz']","The paper introduces a two-step method of quantifying eye movement transitions between Areas of Interests (AOIs). First, individuals' gaze switching patterns, represented by fixated AOI sequences, are modeled as Markov chains. Second, Shannon's entropy coefficient of the fit Markov model is computed to quantify the complexity of individual switching patterns. To determine the overall distribution of attention over AOIs, the entropy coefficient of individuals' stationary distribution of fixations is calculated.   The novelty of the method is that it captures the variability of individual differences in eye movement characteristics, which are then summarized statistically. The method is demonstrated on gaze data collected during free viewing of classical art paintings. Shannon's coefficient derived from individual transition matrices is significantly related to participants' individual differences as well as to their aesthetic experience of art pieces.",Entropy-based statistical analysis of eye movement transitions
"['Juntao Liu', 'Yi Xiong', 'Wu Cc', 'Zhijun Yao', 'Wenyu Liu']","The problem of learning conditional preference networks (CP-nets) from a set of examples has received great attention recently. However, because of the randomicity of the users' behaviors and the observation errors, there is always some noise making the examples inconsistent, namely, there exists at least one outcome preferred over itself (by transferring) in examples. Existing CP-nets learning methods cannot handle inconsistent examples. In this work, we introduce the model of learning consistent CP-nets from inconsistent examples and present a method to solve this model. We do not learn the CP-nets directly. Instead, we first learn a preference graph from the inconsistent examples, because dominance testing and consistency testing in preference graphs are easier than those in CP-nets. The problem of learning preference graphs is translated into a 0-1 programming and is solved by the branch-and-bound search. Then, the obtained preference graph is transformed into a CP-net equivalently, which can entail a subset of examples with maximal sum of weight. Examples are given to show that our method can obtain consistent CP-nets over both binary and multivalued variables from inconsistent examples. The proposed method is verified on both simulated data and real data, and it is also compared with existing methods.",Learning Conditional Preference Networks from Inconsistent Examples
"['Patrick C. K. Hung', 'Kamalakar Karlapalem']","These days smart cards are replacing traditional magnetic cards for payment transactions. One of the main reasons is the enhanced security capabilities built into a smart card. Most of the related works in smart card only concentrates on the single application such as network access control, prepaid phone card or debit card. With popularity in web technologies, there is a trend towards smart cards being used for different electronic commerce applications such as electronic purse for payment transaction over Internet. But the payment protocols proposed so far do not support negotiation, bargaining or privacy issues between the parties. Based on the framework of CapBasED-AMS (a web based secure workflow management system), we developed a prototype system called SmartFlow to demonstrate multi-applications on the Internet using a smart card. The main focus of this paper is to present the framework of SmartFlow and demonstrate a negotiation and bargaining protocol for electronic commerce activities in both static and dynamic environment. We have already implemented the prototype system with these functionalities.",Prototyping Web-Based Smart Flow Multi-Application System using Smart Card Technology
"['Michael Georgiades', 'Nadeem Akhtar', 'Christos Politis', 'Rahim Tafazolli']","In an all-IP-based network architecture that provides multimedia services for 3G and WLAN users, security provisioning is a paramount requirement. However, a security infrastructure (e.g., AAA) introduces additional impairments, in the form of delay components, to the total handoff performance crucially affecting the prime objective towards seamless and secure mobility. In this work, a Context Transfer solution is proposed, in order to support seamless and secure multimedia services over all-IP infrastructures. The solution acts as an adhesive between the AAA and mobility management entities by using the latter to forward AAA state information locally. In this solution, the mobility management protocol is used to trigger Context Transfer and additional messages have been introduced to carry the desired AAA context information. The results presented here show that the overall handoff delay is reduced significantly and a considerable improvement is seen in the performance of real-time and non-real-time applications.",Enhancing mobility management protocols to minimise AAA impact on handoff performance
"['Edgar McGuinness', 'Ingrid Bouwer Utne']","The fishing industry is plagued by a long history of fatality and injury occurrence. Commercial fishing is hence recognized as the most dangerous and difficult of professional callings, in all jurisdictions. Fishing vessels have their own unique set of hazards, a myriad collection of complex occupational accident potentials, barely controlled, co-existing in a perilous work environment. The work in this article is directed by the Norwegian Systematic Health, Environmental and Safety Activities in Enterprises (1997) (Internal Control Regulations [1]), the ISM Code [2] for vessels and their recent applicability to the fishing fleet of Norway. Both safety management works place requirements on the vessel operators and crew to actively manage safety as an on-going concern. The application of these safety management system (SMS) control documents to fishing vessels is just the latest instalment in a continual drive to improve safety in this sector. The difficulty is that there has been no previous systematic approach to safety within the fishing fleet. This article uses the tenants of systems engineering to determine the requirements for such a SMS, detailing the limiting factors and restrictive issues of this complex operating environment.",A systems engineering approach to implementation of safety management systems in the Norwegian fishing fleet
"['Du Zhang', 'Quoc Luan Ha', 'Meiliu Lu']","Vital statistics data offer a fertile ground for data mining. In this paper, we discuss the results of a data-mining project on the causes of death aspect of the vital statistics data in the state of California. A data-mining tool called Cubist is used to build predictive models out of two million cases over a nine-year period. The objective of our study is to discover knowledge (trends, correlations or patterns) that may not be gleaned through standard techniques. The generated predictive models allow pertinent state agencies to gain insight into various aspects of the death rates in the state of California, to predict health issues related to the causes of death, to offer an aid to decision or policy-making process and to provide useful information services to the customers. The results obtained in our study contain valuable new information.",Mining California vital statistical data
"['Kaizhi Zhang', 'Haidong Yu', 'Zhongpo Liu', 'Xinmin Lai']","In this paper, the load transmission model of the shield thrust system is established taking into account the variable boundary constraints between the shield skin and the surrounding strata, which is obtained based on the finite element methods. The results show that the resisting moment on the shield skin from the geologic layer has a nonlinear relationship with the bending moment loads on the cutterhead when the material behavior of the strata is modeled by an elasto-plastic Mohr-Coulomb model. The dynamic load behavior of the shield thrust system is significantly influenced by the variable shield-strata boundary constraints, which may cause incorrect load predictions of the hydraulic thrust system and consequent snake-like motions of the shield machine.",Analysis for Dynamic Load Behavior of Shield Thrust System Considering Variable Boundary Constraints
['Oliver Niebuhr'],"Based on the phonology of the Kiel Intonation Model (KIM), a tripartite opposition#R##N#of German intonation is investigated: early, medial, and late peaks. These#R##N#intonation categories, which can be proj","The Signalling of German Rising-Falling Intonation Categories É?? The Interplay of Synchronization, Shape, and Height"
"['Christopher E. Peters', ""Carol O'Sullivan""]","We present a system for the automatic generation of bottom-up visual attention behaviours in virtual humans. Bottom-up attention refers to the way in which the environment solicits one's attention without regard to task-level goals. Our framework is based on the interactions of multiple components: a synthetic vision system for perceiving the virtual world, a model of bottom-up attention for early visual processing of perceived stimuli, a memory system for the storage of previously sensed data and a gaze controller for the generation of resultant behaviours. Our aim is to provide a feeling of presence in inhabited virtual environments by endowing agents with the ability to pay attention to their surroundings.",Bottom-up visual attention for virtual human animation
"['Ra?ßl A. Santelices', 'Mary Jean Harrold']","Structural testing of software requires monitoring the software's execution to determine which program entities are executed by a test suite. Such monitoring can add considerable overhead to the execution of the program, adversely affecting the cost of running a test suite. Thus, minimizing the necessary monitoring activity lets testers reduce testing time or execute more test cases. A basic testing strategy is to cover all statements or branches but a more effective strategy is to cover all definition-use associations (DUAs).   In this paper, we present a novel technique to efficiently monitor DUAs, based on branch monitoring. We show how to infer from branch coverage the coverage of many DUAs, while remaining DUAs are predicted with high accuracy by the same information. Based on this analysis, testers can choose branch monitoring to approximate DUA coverage or instrument directly for DUA monitoring, which is precise but more expensive. In this paper, we also present a tool, called DUA-Forensics, that we implemented for this technique along with a set of empirical studies that we performed using the tool",Efficiently monitoring data-flow test coverage
"['Valentina Cecchi', 'Xiaoguang Yang', 'Karen Miu', 'Chika O. Nwankpa']",The monitoring and automation of power distribution systems has significantly improved with advancements in digital signal processing (DSP) and in computer and Web-based technologies. Power distribution systems could realize benefits from an improved instrumentation and measurement (IM it has a power distribution system in which various meter placements and network reconfiguration techniques can be implemented and studied. The authors designed a unique and flexible IM it can adapt to power system planning and operating scenarios. This system in the RDAC laboratory is tested and present the results in this paper.,Instrumentation and Measurement of a Power Distribution System Laboratory for Meter Placement and Network Reconfiguration Studies
"['Sung Hee Park', 'Andrew Adams', 'Eino-Ville Talvala']","The FCam API is an open-source camera control library, enabling precise control over a camera's imaging pipeline. Intended for researchers and students in the field of computational photography, it allows easy implementation of novel algorithms and applications. Currently implemented on the Nokia N900 smartphone, and a custom-built ""Frankencamera"", it has been used in teaching at universities around the world, and is freely available for download for the N900. This paper describes the architecture underlying the API, the design of the API itself, several applications built on top of it, and some examples of its use in education.",The FCam API for programmable cameras
"['Aaron Levisohn', 'Jayme Cochrane', 'Diane Gromala', 'Jinsil Seo']","The Meatbook, an interactive art installation, explores the use of a novel tangible interface to provoke a visceral response in the viewer. The Meatbook presents the symbiosis of the mechanical and the organic as it simultaneously juxtaposes the conflicting materiality of these media. Sensors, motors and other mechanics are used to animate the meat, generating movements specifically designed to produce visceral, even cathartic responses from the user. By simultaneously generating revulsion and fascination, the user undergoes an embodied experience in which the alien and the familiar come together in the form of a book.",The Meatbook: tangible and visceral interaction
"['Chih-Jen Lee', 'I-Horng Jeng', 'Tai-Ning Yang', 'Chun-Jung Chen', 'Peng Su']","Singular point detection is a critical process for both fingerprint matching and fingerprint classification. The process of singular points detection must be fast and robust; otherwise, the performance of the whole fingerprint recognition system would be influenced heavily. In this paper, we will combine the advantages of Poincare index and fast Fourier transform to develop a fast and robust approach for singular point detection and to avoid the time-consuming problem of Gabor transform-based approach.",Singular Point Detection in Fingerprint Images by a Bank of Discrete Fourier Filters
['Juan Zhang'],This paper applies the variational iteration method to solve fifth-order boundary value problems; just one iteration results in highly accurate solutions. A comparison of the results with exact ones is made to confirm the validity and efficiency.,The numerical solution of fifth-order boundary value problems by the variational iteration method
"['Jack Xiao-Dong Yang', 'Roderick Melnik']","The analysis of dynamics of semi-flexible polymers, such as DNA molecules, is an important multiscale problem with a wide range of applications in science and bioengineering. In this contribution, we show how accounting for internal viscosity in dumbbell-type models may render physically plausible results with minimal computational cost. We focus our attention on the cases of steady shear and extensional flows of polymeric solutions. First, the tensors with moments other than the second order moment are approximated. Then, the nonlinear algebraic equation for the second moment conformation tensor is solved. Finally, substituting the resulting conformation tensor into the Kramers equation of Hookean spring force, the constitutive equations for the model are obtained. The shear material properties are discussed in the context of different internal viscosities and our computational results are compared with the results of other methods applicable for high shear or extensional rates.",A New Constitutive Model for the Analysis of Semi-flexible Polymers with Internal Viscosity
"['Christopher G. Chute', 'Scott A. Beck', 'Thomas B. Fisk', 'David N. Mohr']","Mayo ClinicÉ??s Enterprise Data Trust is a collection of data from patient care, education, research, and administrative transactional systems, organized to support information retrieval, business intelligence, and high-level decision making. Structurally it is a top-down, subject-oriented, integrated, time-variant, and non-volatile collection of data in support of Mayo ClinicÉ??s analytic and decisionmaking processes. It is an interconnected piece of Mayo ClinicÉ??s Enterprise Information Management initiative, which also includes Data Governance, Enterprise Data Modeling, the Enterprise Vocabulary System, and Metadata Management. These resources enable unprecedented organization of enterprise information about patient, genomic, and research data. While facile access for cohort definition or aggregate retrieval is supported, a high level of security, retrieval audit, and user authentication ensures privacy, confidentiality, and respect for the trust imparted by our patients for the respectful use of information about their conditions.",The Enterprise Data Trust at Mayo Clinic: A semantically integrated warehouse of biomedical data
"['Nadine Gobron', 'Bernard Pinty', 'Michel M. Verstraete', 'Jean-Luc Widlowski', 'David J. Diner']","For pt.I see ibid., vol.40, no.7, p.1560-73 (2002). The Multi-angle Imaging SpectroRadiometer (MISR) instrument on board the Terra platform offers the capability of acquiring reflectance data on any Earth target in four spectral bands, from nine different directions, in at most seven minutes, at a spatial resolution adequate for the monitoring of the status of terrestrial surfaces. This paper describes the implementation of a physical and mathematical approach to design a simple two-dimensional algorithm dedicated to the interpretation of data collected by this instrument. One dimension fully exploits the spectral information in the blue, red and near-infrared bands while the other dimension capitalizes on the multiangular capability of MISR to assess the anisotropic behavior of terrestrial surfaces with respect to solar radiation. The spectral information is derived following an approach proposed for single angle instruments, such as the MEdium Resolution Imaging Spectrometer (MERIS), the Global Imager (GLI), the Sea-viewing Wide Field-of-view Sensor (SeaWIFS) and VEGETATION. The access to simultaneous multiangular observations from MISR allows extending this approach. This strategy delivers an estimate of the Fraction of Absorbed Photosynthetically Active Radiation (FAPAR), which pertains to vegetation photosynthetic activity and is a measure of the presence and density of vegetation.",Uniqueness of multiangular measurements. II. Joint retrieval of vegetation structure and photosynthetic activity from MISR
"['Z. Bao', 'Jeong-Mo Hong', 'Joseph Teran', 'Ronald Fedkiw']","We propose a novel approach to fracturing (and denting) brittle materials. To avoid the computational burden imposed by the stringent time step restrictions of explicit methods or with solving nonlinear systems of equations for implicit methods, we treat the material as a fully rigid body in the limit of infinite stiffness. In addition to a triangulated surface mesh and level set volume for collisions, each rigid body is outfitted with a tetrahedral mesh upon which finite element analysis can be carried out to provide a stress map for fracture criteria. We demonstrate that the commonly used stress criteria can lead to arbitrary fracture (especially for stiff materials) and instead propose the notion of a time averaged stress directly into the FEM analysis. When objects fracture, the virtual node algorithm provides new triangle and tetrahedral meshes in a straightforward and robust fashion. Although each new rigid body can be rasterized to obtain a new level set, small shards can be difficult to accurately resolve. Therefore, we propose a novel collision handling technique for treating both rigid bodies and rigid body thin shells represented by only a triangle mesh",Fracturing Rigid Materials
"['Irfan Ghauri', 'Thiede Dirk']","Cet article traite du probleme de lÉ??annulation de brouillage dans un systeme a acces multiple par repartition en code (AMRC) a sequence directe pour des canaux selectifs en frequence et des conditions asynchrones. Le signal etale cyclostationnaire est recu par des antennes multiples ou est sur echantillonne par rapport a la periode des bribes. Il est donc converti en un signal vectoriel sta-tionnaire. Le sur-echantillonnage ou les antennes multiples donnent naissance a un systeme a entrees et sorties multiples (MIMO) disposant de nombreuses connaissances a priori relatives a la forme des sequences dÉ??etalement distinctes pour les signaux dÉ??entree (utilisateurs), En fonction de la longueur de la reponse impulsionnelle finie (RIF) du canal de propagation et du facteur dÉ??etalement, le canal dÉ??un utilisateur determine sÉ??etale sur un certain nombre de symboles, engendrant une interference intersymbole (ISI) qui sÉ??ajoute au brouillage dÉ??acces multiple (MAI) du aux utilisateurs concurrents. LÉ??estimation du canal de lÉ??utilisateur voulu est obtenue par une nouvelle technique aveugle qui utilise la sequence dÉ??etalement de lÉ??utilisateur et la statistique dÉ??ordre deux du signal recu. Le recepteur a forcage de zero minimisant lÉ??erreur quadratique moyenne (MMSE-ZF) sÉ??en deduit. Ce recepteur generalise au cas asynchrone et pour les canaux a trajets multiples le recepteur ancre qui minimise lÉ??energie a la sortie (MDE) [1]. Differents recepteurs lineaires sont obtenus par criteres divers. On montre que le recepteur MMSE-ZF peut etre realise dÉ??une maniere decentralisee en appliquant le critere MOE non biaise, conduisant au recepteur a reponse sans distorsion et a variance minimale (MVDR) pour le signal de lÉ??utilisateur donne. Le recepteur MVDR est ensuite adapte en aveugle en appliquant le principe de Capon. Cela donne, par ailleurs, lÉ??estimation de la reponse impulsionnelle du canal. En etablissant des bornes inferieures sur la longueur du filtre de reception, on obtient une estimation de lÉ??ISI et du MAI tolerable par le recepteur, ainsi que des conditions dÉ??identificabilite de celui-ci.",Blind decentralized projection receiver for asynchronous CDMA in multipath channels
['Jonathan S. Turner'],"A method for analyzing the queueing behavior of switching networks constructed from switches that employ shared buffering or parallel bypass input buffering is presented. The queueing models introduced by Y.C. Jenq (1983) and generalized by T. Szymanski and S. Shaikh (1989) are extended to handle these classes of networks. The analysis explicitly models the state of an entire switch and infers information about the distribution of packets associated with particular inputs or outputs when needed. It is shown that the method can be extended to switching systems with input buffering, including systems supporting bypass queueing. Numerical comparisons of the different buffering techniques are included. >",Queueing analysis of buffered switching networks
"['Sergi Vives', 'Bradford D. Loucas', 'Mariel Vazquez', 'David J. Brenner', 'Rainer K. Sachs', 'Lynn Hlatky', 'Michael N. Cornforth', 'Javier Arsuaga']","Motivation: The position of chromosomes in the interphase nucleus is believed to be associated with a number of biological processes. Here, we present a web-based application that helps analyze the relative position of chromosomes during interphase in human cells, based on observed radiogenic chromosome aberrations. The inputs of the program are a table of yields of pairwise chromosome interchanges and a proposed chromosome geometric cluster. Each can either be uploaded or selected from provided datasets. The main outputs are P-values for the proposed chromosome clusters. SCHIP is designed to be used by a number of scientific communities interested in nuclear architecture, including cancer and cell biologists, radiation biologists and mathematical/computational biologists.#R##N##R##N#Availability: http://cramer.stat.ub.es/schip#R##N##R##N#Contact: jarsuaga@cc.ucsf.edu#R##N##R##N#Supplementary information: http://cramer.stat.ub.es/schip/help.htm",SCHIP: statistics for chromosome interphase positioning based on interchange data
"['Todd A. King', 'Steven Joy', 'R. J. Walker']","The design, development and operations of a distributed data inventory system is a concern of many government agencies and commercial enterprises. All are searching for ways to harness the information and data resources that are available, as well as provide a framework for bringing future data into an unifying structure. We provide a discussion of the design, development and operation of a system which can be used to build, manage and access distributed data inventories. We discuss the major decisions we made, why we made those decisions and what the end product of our efforts has been. Currently the system we describe is being used as the data access system for the Plasma Interactions Node (PDS/PPI) of NASA's Planetary Data System in order to provide access to fields and particles data returned from NASA's planetary missions. >","The design, development and operation of a distributed data inventory system"
"['Jiangxin Chen', 'Urbashi Mitra']","Optimum near-far resistance is studied for synchronous dual-rate DS/CDMA systems. Three multirate access schemes are considered: multicode (MC) access where high-rate users multiplex their data bits onto multiple codes and form a single-rate system; variable spreading length (VSL) access where the spreading lengths of signature sequences are inversely proportional to users' data rates; and variable chipping rate (VCR) access where the chipping rates of the signature sequences are proportional to users' data rates. In order to remove the influence of signature sequences in the comparison of the three schemes, random signature sequences are assumed. Optimum mar-far resistance is then averaged over all possible realizations. Two types of code sets are considered for the VSL system: general random codes and random repetition codes. Bounds and approximations are provided for the average optimum near-far resistance. Analytical results show that the performance depends on the access schemes and the data rate of the users. The results for the VSL scheme with general random codes are extended for performance evaluation of systems with signature sequences which span many symbol intervals.",Optimum near-far resistance for dual-rate DS/CDMA signals: random signature sequence analysis
"['Jose M. Pe?˝a', 'Johan Bj??rkegren', 'Jesper Tegn??r']","Motivation: For the last few years, Bayesian networks (BNs) have received increasing attention from the computational biology community as models of gene networks, though learning them from gene-expression data is problematic. Most gene-expression databases contain measurements for thousands of genes, but the existing algorithms for learning BNs from data do not scale to such high-dimensional databases. This means that the user has to decide in advance which genes are included in the learning process, typically no more than a few hundreds, and which genes are excluded from it. This is not a trivial decision. We propose an alternative approach to overcome this problem.#R##N##R##N#Results: We propose a new algorithm for learning BN models of gene networks from gene-expression data. Our algorithm receives a seed gene S and a positive integer R from the user, and returns a BN for the genes that depend on S such that less than R other genes mediate the dependency. Our algorithm grows the BN, which initially only contains S, by repeating the following step R + 1 times and, then, pruning some genes; find the parents and children of all the genes in the BN and add them to it. Intuitively, our algorithm provides the user with a window of radius R around S to look at the BN model of a gene network without having to exclude any gene in advance. We prove that our algorithm is correct under the faithfulness assumption. We evaluate our algorithm on simulated and biological data (Rosetta compendium) with satisfactory results.#R##N##R##N#Contact: jmp@ifm.liu.se",Growing Bayesian network models of gene networks from seed genes
"['Injoo Jang', 'Hyeong Seon Yoo']","This paper proposes an efficient inversion algorithm for Galois field GF(2n) by using a modified multi-bit shifting method. It is well known that the efficiency of arithmetic algorithms depends on the basis and many foregoing papers use either polynomial or optimal normal basis. An inversion algorithm, which modifies a multi-bit shifting based on the Montgomery algorithm, is studied. Trinomials and AOPs (all-one polynomials) are tested to calculate the inverse. It is shown that the suggested inversion algorithm reduces the computation time 1 ~ 26% of the forgoing multi-bit shifting algorithm. The modified algorithm can be applied in various applications and is easy to implement.",Efficient multi-bit shifting algorithm in multiplicative inversion problems
"['Eric R. Buhrke', 'Chen Liu']",Prior information about the operating environment of a speech recognizer is often general and abstract. Frequently information such as the number of speakers with foreign accents or the number of callers using cellular phones is readily available. Incorporating this information during model training is difficult. This paper generalizes the popular MAP training algorithm derived by Gauvain and Lee (1994) so that more prior information can be utilized during training. The priors are in the form of mixture distributions with each mixture component representing a unique property of the data and the mixing weights defined by the a priori constraints. Using the training algorithms derived here it is shown that significant performance improvements can be obtained.,A generalization of the maximum a posteriori training algorithm for mixture priors
"['Hee-Byoung Choi', 'Fran??ois Pierrot', 'Atsushi Konno', 'Tetsuro Shibukawa', 'Masaru Uchiyama']","This paper deals with the design and dynamic control simulation of a new type and dynamic control simulation of a new type of 4-DOFs parallel mechanism providing 3 translations and 1 rotation for high-speed handling and machining. This parallel mechanism is named as H4. A model-based dynamic control scheme is developed to improve the accuracy of the trajectory tracking. A simplified dynamic model is used for the H4 robot to decrease the cost of computation. A dynamic simulation is performed using ADAMS/sup TM/. In addition, the adept motion is used as a benchmark test to evaluate the effect of the dynamic control. The simulation results show that the dynamic control dramatically improves the trajectory tracking accuracy.",Design and control of a novel 4-DOFs parallel robot H4
"['Xiaodong Jiang', 'Jason I. Hong', 'Leila Takayama', 'James A. Landay']","In this paper, we demonstrate how field studies, interviews, and low-fidelity prototypes can be used to inform the design of ubiquitous computing systems for firefighters. We describe the artifacts and processes used by firefighters to assess, plan, and communicate during emergency situations, showing how accountability affects these decisions, how their current Incident Command System supports these tasks, and some drawbacks of existing solutions. These factors informed the design of a large electronic display for supporting the incident commander, the person who coordinates the overall response strategy in an emergency. Although our focus was on firefighters, our results are applicable for other aspects of emergency response as well, due to common procedures and training.",Ubiquitous computing for firefighters: field studies and prototypes of large displays for incident command
"['Philippe Morignot', 'Joshu?? P??rez Rastelli', 'Fawzi Nashashibi']","É?? Automated functions for real scenarios have been increasing in last years in the automotive industry. Many research contributions have been done in this field. However, other problems have come to the drivers: When should they (the drivers or the new automated systems) be able to take control of the vehicle? This question has not a simple answer; it de-pends on different conditions, such as: the environment, driver condition, vehicle capabilities, fault tolerance, among others. For this reason, in this work we will analyze the acceptability to the ADAS functions available in the market, and its relation with the different control actions. In this paper a survey on arbitration and control solutions in ADAS is presented. It will allow to create the basis for future development of a generic ADAS control (the lateral and longitudinal behavior), based on the integration of the application request, the driver behavior and driving conditions in the framework of the DESERVE project (DEvelopment platform for Safe and Efficient dRiVE 1 , a ARTEMIS project 2012-2105). The main aim of this work is to allow the development of a new generation of ADAS solutions where the control could be effectively shared between the vehicle and the driver. Different solutions of shared control have been analyzed. A first approach is proposed, based on the presented solutions.",Arbitration for balancing control between the driver and ADAS systems in an automated vehicle: Survey and approach
"['Tzu-Chi Huang', 'Ce-Kuen Shieh', 'Wen-Huang Lai', 'Yu-Ben Miao']","Playing multimedia data with mobile devices is the trend in mobile computing. However, roaming around networks has the session continuity problem because of Internet routing fundamentals. We propose the session splice on multimedia communication for session continuity. It has following advantages: 1) operating without Mobile IP nor the triangular route problem, 2) leaving applications unchanged to keep the compatibility, 3) having no need to deploy external proxies, and 4) hiding movements from applications so applications or users do not need to rejoin sessions at the communication handoff. We implement the session splice in Windows 2000/XP and have experiments to show that the session splice with the negligible overhead can conserve home network bandwidth and maintain stable variation of packet round trip time.",Session Splice on Multimedia Communication for Mobile Computing
"['Jun Yan', 'Kegen Yu', 'Lenan Wu']","To mitigate the non-line-of-sight (NLOS) effect, a three-step positioning approach is proposed in this article for target tracking. The possibility of each distance measurement under line-of-sight condition is first obtained by applying the truncated triangular probability-possibility transformation associated with fuzzy modeling. Based on the calculated possibilities, the measurements are utilized to obtain intermediate position estimates using the maximum likelihood estimation (MLE), according to identified measurement condition. These intermediate position estimates are then filtered using a linear Kalman filter (KF) to produce the final target position estimates. The target motion information and statistical characteristics of the MLE results are employed in updating the KF parameters. The KF position prediction is exploited for MLE parameter initialization and distance measurement selection. Simulation results demonstrate that the proposed approach outperforms the existing algorithms in the presence of unknown NLOS propagation conditions and achieves a performance close to that when propagation conditions are perfectly known.","Fuzzy modeling, maximum likelihood estimation, and Kalman filtering for target tracking in NLOS scenarios"
['Vladimir J. Lumelsky'],"Experimental attempts to build teleoperated master-slave robot arm manipulators revealed that a human operator has difficulty in interpreting input information (coming, e.g. directly via visual tract or from fixed or moving TV monitors at the scene), and consequently in teleoperation decision making. The problem becomes more pronounced when the slave arm has to operate in a complex environment where every point of the arm body is subject to potential collision. Results are presented of experimental tests with human operators that trace the source of the difficulty to the limitations in human abilities for space orientation and interpretation of geometrical data, and a solution that capitalizes on recent developments in sensor-based motion planning for whole-sensitive robot arms is proposed. The result would be a hybrid system in which global planning is done by a human operator, whereas local collision-free motion is controlled by an assisting autopilot. >",On human performance in telerobotics
"['Steffen Fritz', 'Ian McCallum', 'C. Schill', 'C. Perger', 'Roland Grillmayer', 'Fr??d??ric Achard', 'F. Kraxner', 'Michael Obersteiner']","Global land cover is one of the essential terrestrial baseline datasets available for ecosystem modeling, however uncertainty remains an issue. Tools such as Google Earth offer enormous potential for land cover validation. With an ever increasing amount of very fine spatial resolution images (up to 50 cm ?? 50 cm) available on Google Earth, it is becoming possible for every Internet user (including non remote sensing experts) to distinguish land cover features with a high degree of reliability. Such an approach is inexpensive and allows Internet users from any region of the world to get involved in this global validation exercise. The Geo-Wiki Project is a global network of volunteers who wish to help improve the quality of global land cover maps. Since large differences occur between existing global land cover maps, current ecosystem and land-use science lacks crucial accurate data (e.g., to determine the potential of additional agricultural land available to grow crops in Africa), volunteers are asked to review hotspot maps of global land cover disagreement and determine, based on what they actually see in Google Earth and their local knowledge, if the land cover maps are correct or incorrect. Their input is recorded in a database, along with uploaded photos, to be used in the future for the creation of a new and improved hybrid global land cover map.",Geo-Wiki.Org: The use of crowdsourcing to improve global land cover
"['Qinghua Huang', 'Tong Wang']","Spherical microphone arrays have been used for source localization in three-dimensional space recently. In this paper, a two-stage algorithm is developed to localize mixed far-field and near-field acoustic sources in free-field environment. In the first stage, an array signal model is constructed in the spherical harmonics domain. The recurrent relation of spherical harmonics is independent of far-field and near-field mode strengths. Therefore, it is used to develop spherical estimating signal parameter via rotational invariance technique (ESPRIT)-like approach to estimate directions of arrival (DOAs) for both far-field and near-field sources. In the second stage, based on the estimated DOAs, simple one-dimensional MUSIC spectrum is exploited to distinguish far-field and near-field sources and estimate the ranges of near-field sources. The proposed algorithm can avoid multidimensional search and parameter pairing. Simulation results demonstrate the good performance for localizing far-field sources, or near-field ones, or mixed field sources.",Acoustic source localization in mixed field using spherical microphone arrays
"['Darren McNamara', 'Mark A Beach', 'Pn Fletcher']","The measurement and characterisation of multiple-input multiple-output (MIMO) channels has gained increasing attention over recent years. Previous analysis of MIMO measurements has generally focussed on the evaluation of the capacity of such systems in differing locations and configurations. We present measurements made in an indoor environment at 5.2 GHz, specifically to investigate temporal channel variation. Line-of-sight and non-line-of-sight situations are compared by means of an analysis of the rate of variation in the powers of the 'orthogonal spatial channels' comprising the overall MIMO channel. The conclusions are extended to encompass the rate of variation in performance of MIMO channels under different environmental conditions.",Experimental investigation of the temporal variation of MIMO channels
"['Matthew Mayhew', 'Radu Muresan']","This paper presents a capacitor bank and a switch box as an on-chip Power Analysis Attack (PAA) countermeasure for embedded systems. This approach allows for the decoupling and isolation of individual sensitive and non-sensitive modules from the power supply. The random connections made by the switch box between functional modules and charged capacitors also contribute to the decorrelation between collected traces and the operations being performed by sensitive modules. A DC-DC converter is also present to generate a stable and suitable supply voltage. The design was simulated using 65 nm CMOS technology in Cadence, with an implementation of the Advanced Encryption Standard (AES) Sbox as a test module. Initial results showed a reduction in the effectiveness of the Correlation Power Analysis (CPA) attack on the Sbox module.",Integrated capacitor switchbox for security protection
"['Etienne Marcheret', 'Gerasimos Potamianos', 'Karthik Visweswariah', 'Jing Huang']","In this paper, we describe the IBM system submitted to the NIST Rich Transcription Spring 2006 (RT06s) evaluation campaign for automatic speech activity detection (SAD). This SAD system has been developed and evaluated on CHIL lecture meeting data using far-field microphone sensors, namely a single distant microphone (SDM) configuration and a multiple distant microphone (MDM) condition. The IBM SAD system employs a three-class statistical classifier, trained on features that augment traditional signal energy ones with features that are based on acoustic phonetic likelihoods. The latter are obtained using a large speaker-independent acoustic model trained on meeting data. In the detection stage, after feature extraction and classification, the resulting sequence of classified states is further collapsed into segments belonging to only two classes, speech or silence, following two levels of smoothing. In the MDM condition, the process is repeated for every available microphone channel, and the outputs are combined based on a simple majority voting rule, biased towards speech. The system performed well at the RT06s evaluation campaign, resulting to 8.62% and 5.01% É??speaker diarization errorÉ?ù in the SDM and MDM conditions respectively.",The IBM RT06s evaluation system for speech activity detection in CHIL seminars
"['Carsten Franke', 'Joachim Lepping', 'Uwe Schwiegelshohn']","This paper presents a comparison of three different design concepts for genetic fuzzy systems. We apply a symbiotic evolution that uses the Michigan approach and two approaches that are based on the Pittsburgh approach: a complete optimization of the problem and a cooperative coevolutionary algorithm. The three different genetic fuzzy systems are applied to a real-world online problem, the generation of scheduling strategies for massively parallel processing systems. The genetic fuzzy systems must classify different scheduling states and decide about a corresponding scheduling strategy within each scheduling state. The main challenge arise in the delayed reward given by a critic. Therefore, it is impossible to directly evaluate the assignment of scheduling strategies to scheduling states. In our paper, the three design concepts are evaluated with real workload traces considering result quality, computational effort, convergence behavior, and robustness.",Genetic Fuzzy Systems applied to Online Job Scheduling
"['Francois-Xavier Socheleau', 'Christophe Laot', 'Jean-Michel Passerieux']","In order to provide a concise time-varying SISO channel model, the principle of maximum entropy is applied to scattering function derivation. The resulting model is driven by few parameters that are expressed as moments such as the channel average power or the Doppler spread. Physical interpretations of the model outputs are discussed. In particular, it is shown that common Doppler spectra such as the flat or the Jakes spectrum fit well into the maximum entropy framework. The Matlab code corresponding to the proposed model is available at http://perso.telecom-bretagne.eu/fxsocheleau/software.",Concise Derivation of Scattering Function from Channel Entropy Maximization
"['Leen Stougie', 'Apa Arjen Vestjens']","We prove lower bounds on the competitive ratio of randomized algorithms for several on-line scheduling problems. The main result is a bound of e/(e-1) for the on-line problem with objective minimizing the sum of completion times of jobs that arrive over time at their release times and are to be processed on a single machine. This lower bound shows that a randomized algorithm designed in Chekuri et al. (Proceedings of the Eighth ACM-SIAM Symposium on Discrete Algorithms, 1997, 609-618) is a best possible randomized algorithm for this problem.",Randomized algorithms for on-line scheduling problems: how low can't you go?
"['Anton A. Stoorvogel', 'Ali Saberi', 'P. Sannuti']","We study the problem where we have a regulation (asymptotic tracking) requirement together with a performance requirement. Typically we measure performance by the H/sub 2/ or H/sub /spl infin// norm of a chosen transfer function matrix, although any other norm such as the L/sub 1/ norm could also be used. In the case when the performance is measured by the H/sub 2/ norm, there is no loss in performance due to the regulation constraint. On the other hand, when the performance is measured by the H/sub /spl infin// norm, there exists in general certain loss of performance due to the regulation constraint, and we explicitly characterize such a loss in terms of a static optimization problem.",Performance with regulation constraints
"['Sudhan Majhi', 'A. S. Madhukumar', 'A. B. Premkumar', 'Francois P. S. Chin']","This paper describes a combined modulation scheme for time hopping ultra wideband (TH-UWB) radio systems based on biphase shift keying (BPSK), orthogonal pulse position modulation (OPPM) and pulse shape modulation (BPSM). A set of M = 2 k  symbols are constructed by using L = 22 l  orthogonal pulse positions and N = 22 k-l-1  biorthogonal pulses, where k and l are nonnegative integers such that 0 É?? l É?? k - 1. The selection of number of pulse positions and pulses depend on the system performance and the availability of orthogonal pulses with estimable auto-correlation properties. The proposed scheme achieves higher data rate by introducing more number of orthogonal pulses in the same pulse repetition interval. It also reduces the system complexity by half by introducing antipodal version of orthogonal pulses. The proposed transmission scheme is analyzed through computer simulations in the presence of multipath channel.",A Hybrid M-Ary Modulation Scheme for Time Hopping UWB Communication Systems
"['M. Zhang', 'Markus Olbrich', 'D. Seider', 'Michael Frerichs', 'Harald Kinzelbach', 'Erich Barke']","As technology rapidly scales, performance variations (delay, power etc.) arising from process variation are becoming a significant problem. The use of linear models has been proven to be very critical in many today's applications. Even for well-behaved performance functions, linearising approaches as well as quadratic model provide serious errors in calculating expected value, variance and higher central moments. In this paper, we present a novel approach to analyse the impacts of process variations with low efforts and minimum assumption. We formulate circuit performance as a function of the random parameters and approximate it by Taylor Expansion up to 4th order. Taking advantage of the knowledge about higher moments, we convert the Taylor series to characteristics of performance distribution. Our experiments show that this approach provides extremely exact results even in strongly non-linear problems with large process variations. Its simpleness, efficiency and accuracy make this approach a promising alternative to the Monte Carlo Method in most practical applications.",CMCal: an accurate analytical approach for the analysis of process variations with non-gaussian parameters and nonlinear functions
"['Joey Wilson', 'Neal Patwari']","Device-free localization (DFL) is the estimation of the position of a person or object that does not carry any electronic device or tag. Existing model-based methods for DFL from RSS measurements are unable to locate stationary people in heavily obstructed environments. This paper introduces measurement-based statistical models that can be used to estimate the locations of both moving and stationary people using received signal strength (RSS) measurements in wireless networks. A key observation is that the statistics of RSS during human motion are strongly dependent on the RSS ""fade levelÉ?ù during no motion. We define fade level and demonstrate, using extensive experimental data, that changes in signal strength measurements due to human motion can be modeled by the skew-Laplace distribution, with parameters dependent on the position of person and the fade level. Using the fade-level skew-Laplace model, we apply a particle filter to experimentally estimate the location of moving and stationary people in very different environments without changing the model parameters. We also show the ability to track more than one person with the model.",A Fade-Level Skew-Laplace Signal Strength Model for Device-Free Localization with Wireless Networks
"['Silvio Ghilardi', 'Giancarlo Meloni']",,Relational and Partial Variable Sets and Basic Predicate Logic
"['Cynthia A. Thompson', 'Raymond J. Mooney']","This paper describes a system, WOLFIE (WOrd Learning From Interpreted Examples), that acquires a semantic lexicon from a corpus of sentences paired with semantic representations. The lexicon learned consists of words paired with meaning representations. WOLFIE is part of an integrated system that learns to parse novel sentences into semantic representations, such as logical database queries. Experimental results are presented demonstrating WOLFIE's ability to learn useful lexicons for a database interface in four different natural languages. The lexicons learned by WOLFIE are compared to those acquired by a similar system developed by Siskind (1996).",Automatic construction of semantic lexicons for learning natural language interfaces
['Peter Hellekalek'],"Parallel Monte Carlo simulation requires reliable RNGs. For sequential machines, good generators exist. It is not at all trivial to find high quality RNGs for parallel machines. We present a review of the main concepts to produce random numbers on parallel processors and further, we illustrate some phenomena that occur with parallelization.",Don't trust parallel Monte Carlo!
"['Carolina Salto', 'Enrique Alba', 'Juan M. Molina', 'Guillermo Leguizam??n']","In this paper, the two-dimensional strip packing problem with 3-stage level patterns is tackled using genetic algorithms (GAs). We evaluate the usefulness of a knowledge-based greedy seeding procedure used for creating the initial population. This is motivated by the expectation that the seeding will speed up the GA by starting the search in promising regions of the search space. An analysis of the impact of the seeded initial population is offered, together with a complete study of the influence of these modifications#N#on the genetic search. The results show that the use of an appropriate seeding of the initial population outperforms existing GA approaches on all the used problem instances, for all the metrics used, and in fact it represents the new state of the art for this problem.",Greedy Seeding Procedure for GAs Solving a Strip Packing Problem
"['Rachel Harrison', 'Sol J. Greenspan', 'Tim Menzies', 'Marjan Mernik', 'Pedro Rangel Henriques', 'Daniela Carneiro da Cruz', 'Daniel Rodr??guez']","The RAISE'13 workshop brought together researchers from the AI and software engineering disciplines to build on the interdisciplinary synergies which exist and to stimulate research across these disciplines. The first part of the workshop was devoted to current results and consisted of presentations and discussion of the state of the art. This was followed by a second part which looked over the horizon to seek future directions, inspired by a number of selected vision statements concerning the AI-and-SE crossover. The goal of the RAISE workshop was to strengthen the AI-and-SE community and also develop a roadmap of strategic research directions for AI and software engineering.",2nd international workshop on realizing artificial intelligence synergies in software engineering (RAISE 2013)
"['N. Wang', 'Dit-Yan Yeung']","In this paper, we study the challenging problem of tracking the trajectory of a moving object in a video with possibly very complex background. In contrast to most existing trackers which only learn the appearance of the tracked object online, we take a different approach, inspired by recent advances in deep learning architectures, by putting more emphasis on the (unsupervised) feature learning problem. Specifically, by using auxiliary natural images, we train a stacked de-noising autoencoder offline to learn generic image features that are more robust against variations. This is then followed by knowledge transfer from offline training to the online tracking process. Online tracking involves a classification neural network which is constructed from the encoder part of the trained autoencoder as a feature extractor and an additional classification layer. Both the feature extractor and the classifier can be further tuned to adapt to appearance changes of the moving object. Comparison with the state-of-the-art trackers on some challenging benchmark video sequences shows that our deep learning tracker is more accurate while maintaining low computational cost with real-time performance when our MATLAB implementation of the tracker is used with a modest graphics processing unit (GPU).",Learning a Deep Compact Image Representation for Visual Tracking
"['Yo-Ping Huang', 'Te-Wei Chiang', 'Mann-Jung Hsiao', 'Tienwei Tsai']","In this paper, an efficient two-stage approach is proposed for content-based image retrieval (CBIR). In establishing the database, the features of an image are extracted from its color histograms and discrete cosine transform (DCT) coefficients. To improve the retrieval performance, the quantization technique is applied to quantize the vector of color histograms such that the feature space is partitioned into a finite number of grids, each of which corresponds to a grid code (GC). At the first stage, a reduced set of candidate images which have the same GC (or adjacent GCs) as that of the query image is obtained. At the second stage, the remaining candidates are examined by using grey relational analysis on the significant DCT coefficients. The experimental results show that the proposed approach leads to a fast retrieval with good accuracy.",Content-based image retrieval using grid-based indexing and grey relational analysis
"['G??rschwin Fey', 'Rolf Drechsler']","Today up to 80% of the design costs for integrated circuits are due to verification. Verification tools guarantee completeness if equivalence of two designs or a property for a design is proven. In the other case, usually only one counter-example is produced. Then debugging has to be carried out to locate the design error. This paper investigates, how debugging can benefit from using more than one counter-example generated by the verification tool. The problem of finding useful counter-examples is theoretically analyzed and proven to be difficult. Heuristics are introduced and their quality is underlined by experimental results. Guidelines how to generate counter-examples are extracted from one of these heuristics.",Finding good counter-examples to aid design verification
"['Pei Hsia', 'Alan Tsu-I Yaung']","Requirements clustering provides a new approach to system decomposition. It makes it possible to subdivide a large system into user-recognizable components where each component can be used, almost independently, to satisfy part of the user's needs. The underlying concept of requirements clustering is presented. A requirements-clustering algorithm, which is based on the concept of scenario-based prototyping and the notion of covering in graph theory, is described. Its implications to software incremental delivery are discussed. A case study conducted to demonstrate the application of requirements clustering to facilitate software development is described. >",Another approach to system decomposition: requirements clustering
"['Changyong Yu', 'Guoren Wang', 'Junjie Wu', 'Keming Mao']","In computational proteomics, the peptide identification via interpreting its tandem mass spectrum is an important issue. The classification of b and y ions in the spectrum plays a vital role for improving the accuracy of most existing algorithms. To solve this problem, a classification method based on frequent pattern mining and decision tree is proposed in this paper. First a dataset is established by use of the identified spectrum in which each datum records the ion positions around an ion with b or y type. The discriminative ion frequent patterns (DIFP) of b and y ions are mined with the dataset. And then a decision tree model organizing these DIFPs is proposed for classifying the b and y ions. Finally, we develop an algorithm for the b and y ions classification called B/Y-Classifier. The experimental results demonstrate that an accuracy level of 92\% is achieved.",Classifying b and y Ions in Peptide Tandem Mass Spectra
"['Ulrike Geissler', 'Markus Will']","Entrepreneurs in electronic markets move in industries where a strong corporate brand can deliver competitive advantage through differentiation. It facilitates increased attention. Despite ever-increasing followers and me-too products, it enables a company to build a positive image on behalf of the relevant target groups be they consumers/customers, shareholders, employees, or industry partners. This facilitates access to resources most scarce in early company life stages: capital, human resources, customers, industry partnerships, and management knowledge. Contemporary management literature on entrepreneurship neglects corporate communication to a great extent. This pioneering article takes a first step to outlining a framework for brand building by entrepreneurs in electronic markets. After an introduction of basic concepts relevant to this topic, we outline today's key challenges for e-business ventures. We then introduce a generic framework for corporate branding, which in a next step is being applied to e-business ventures.",Corporate branding of e-business ventures
"['Peng Chen', 'Chunhua Zhao', 'Jian Li', 'Zhiming Liu']","To solve the economic dispatch problem (ED) in power system, this paper introduced the floating point representation to the genetic particle swarm optimization (GPSO). GPSO) was derived from the standard particle swarm optimization (SPSO) and incorporated with the genetic reproduction mechanisms, namely crossover and mutation. A modified heuristic crossover was introduced, which was derived from the differential evolution and genetic algorithm along with the mechanism of GPSO. The proposed approach was implemented to four well-known benchmark functions, and typical parameter sets were given based on the simulation results. Moreover, MGPSO was employed to a practical system, and by comparison with the other PSO methods, MGPSO has provided better results.",Solving the Economic Dispatch in Power System via a Modified Genetic Particle Swarm Optimization
"['Diogo R. Ferreira', 'J. J. Pinto Ferreira']","Existing e-marketplaces, built on traditional clientÉ??server architectures, severely restrict the scope and dynamics of Business-to-Business (B2B) interactions. Peer-to-peer (P2P) architectures will provide far more decentralized infrastructures, while allowing a much wider range of business patterns to take place. On one hand, the interaction over a P2P network resembles the way real-world enterprises perform business with each other. On the other hand, a small set of simple services is enough to support complex business processes over a P2P infrastructure. Incidentally, most of the required technology is readily available, although it may be necessary to bring in an appropriate integration of different concepts. The paper discusses the implementation of essential services for P2P e-marketplaces, based on one of the leading P2P platforms, and illustrates its benefits by applying the P2P approach to a vendor of industrial equipment.",Building an e-marketplace on a peer-to-peer infrastructure
"['Wim Vancroonenburg', 'Patrick De Causmaecker', 'Greet Vanden Berghe']","The present paper studies patient-to-room assignment planning in a dynamic context. To this end, an extension of the patient assignment (PA) problem formulation is proposed, for which two online ILP-models are developed. The first model targets the optimal assignment for newly arrived patients, whereas the second also considers future, but planned, arrivals. Both models are compared on an existing set of benchmark instances from the PA planning problem, which serves as the basic problem setting. These instances are then extended with additional parameters to study the effect of uncertainty on the patientsÉ?? length of stay, as well as the effect of the percentage of emergency patients. The results show that the second model provides better results under all conditions, while still being computationally tractable. Moreover, the results show that pro-actively transferring patients from one room to another is not necessarily beneficial.",A study of decision support models for online patient-to-room assignment planning
"['Dzevdan Kapetanovic', 'Fredrik Rusek']","In this paper we consider the least time-frequency product necessary to transmit a small finite symbol packet such that the symbols can be independently detected. The system model assumed is offset QAM-OFDM, based on a finite duration pulse shape. The outcome is that the optimal pulse shape is of very short duration and that the optimal symbol allocation strategy is often to use as many subcarriers as there are symbols to transmit. Symbol packets up to 150 symbols are considered.",Optimal Time-Frequency Occupancy of Finite Packet OFDM
"['Jong C. Park', 'Sabine Dietmann', 'Andreas Heger', 'Liisa Holm']","Motivation: How critical is the sequence order information in predicting protein secondary structure segments? We tried to get a rough insight on it from a theoretical approach using both a prediction algorithm and structural fragments from Protein Databank (PDB). Results: Using reverse protein sequences and PDB structural fragments, we theoretically estimated the significance of the order for protein secondary structure and prediction. On average: (1) 79% of protein sequence segments resulted in the same prediction in both normal and reverse directions, which indicated a relatively high conservation of secondary structure propensity in the reverse direction; (2) the reversed sequence prediction alone performed less accurately than the normal forward sequence prediction, but comparably high (2% difference); (3) the commonly predicted regions showed a slightly higher prediction accuracy (4%) than the normal sequences prediction; and (4) structural fragments which have counterparts in reverse direction in the same protein showed a comparable degree of secondary structure conservation (73% identity with reversed structures on average for pentamers). Availability:",Estimating the significance of sequence order in protein secondary structure and prediction
"['Gholamreza Haffari', 'Yang Wang', 'Shaojun Wang', 'Greg Mori', 'Feng Jiao']","In real-world machine learning problems, it is very common that part of the input feature vector is incomplete: either not available, missing, or corrupted. In this paper, we present a boosting approach that integrates features with incomplete information and those with complete information to form a strong classifier. By introducing hidden variables to model missing information, we form loss functions that combine fully labeled data with partially labeled data to effectively learn normalized and unnormalized models. The primal problems of the proposed optimization problems with these loss functions are provided to show their close relationship and the motivations behind them. We use auxiliary functions to bound the change of the loss functions and derive explicit parameter update rules for the learning algorithms. We demonstrate encouraging results on two real-world problems --- visual object recognition in computer vision and named entity recognition in natural language processing --- to show the effectiveness of the proposed boosting approach.",Boosting with incomplete information
"['Jiayu Zhou', 'Jianhui Chen', 'Jieping Ye']","Multi-task learning (MTL) learns multiple related tasks simultaneously to improve generalization performance. Alternating structure optimization (ASO) is a popular MTL method that learns a shared low-dimensional predictive structure on hypothesis spaces from multiple related tasks. It has been applied successfully in many real world applications. As an alternative MTL approach, clustered multi-task learning (CMTL) assumes that multiple tasks follow a clustered structure, i.e., tasks are partitioned into a set of groups where tasks in the same group are similar to each other, and that such a clustered structure is unknown a priori. The objectives in ASO and CMTL differ in how multiple tasks are related. Interestingly, we show in this paper the equivalence relationship between ASO and CMTL, providing significant new insights into ASO and CMTL as well as their inherent relationship. The CMTL formulation is non-convex, and we adopt a convex relaxation to the CMTL formulation. We further establish the equivalence relationship between the proposed convex relaxation of CMTL and an existing convex relaxation of ASO, and show that the proposed convex CMTL formulation is significantly more efficient especially for high-dimensional data. In addition, we present three algorithms for solving the convex CMTL formulation. We report experimental results on benchmark datasets to demonstrate the efficiency of the proposed algorithms.",Clustered Multi-Task Learning Via Alternating Structure Optimization
"['Mingjie Feng', 'Tao Jiang', 'Da Chen', 'Shiwen Mao']","Due to the high potential to enhance spectral efficiency and spatial reuse, small cell networks, SCNs, have emerged as a promising solution to improve the capacity of mobile communication systems so as to satisfy the ever growing demand for high data rate services. However, without proper planning, the dense deployment of SCNs may cause severe interference, resulting in limited capacity. In hotspots with a large number of users, the small cell network is challenged by the extremely high aggregated capacity requirement and may fail to guarantee the quality of service of all users. To leverage the benefits of SCNs and overcome the drawbacks, we propose a cooperative small cell network, CSCN, architecture that jointly utilizes several advanced techniques to enhance the capacity of hotspots. In this article, we first examine the existing solutions for capacity enhancement and hotspots. We then present the basic concept of the proposed CSCN architecture, and discuss the related technical aspects. The high potential of a CSCN in terms of capacity improvement and interference mitigation is demonstrated by a simulation study. Finally, we present several open problems for future research based on the CSCN architecture.",Cooperative small cell networks: high capacity for hotspots with interference mitigation
"['Kamol Kaemarungsi', 'Prashant Krishnamurthy']","Indoor positioning systems that make use of received signal strength based location fingerprints and existing wireless local area network infrastructure have recently been the focus for supporting location-based services in indoor and campus areas. A knowledge and understanding of the properties of the location fingerprint can assist in improving design of algorithms and deployment of position location systems. However, most existing research work ignores the radio signal properties. This paper investigates the properties of the received signal strength reported by IEEE 802.11b wireless network interface cards. Analyses of the data are performed to understand the underlying features of location fingerprints. The performance of an indoor positioning system in terms of its precision is compared using measured data and a Gaussian model to see how closely a Gaussian model may fit the measured data.",Properties of indoor received signal strength for WLAN location fingerprinting
"['Richard Torkar', 'Stefan Mankefors', 'Krister Hansson', 'Andreas Jonsson']","Using basic unit testing techniques we found 25 faults in a core component within a larger component oriented framework after the component had already started to he reused. We found that, even though this particular component had been subject to subsystem and system testing and used for some time, several faults were discovered which seriously would have affected applications using it, especially in terms of reliability. This study clearly indicates the need of a new approach to testing and verification within component-based development and reuse.",An exploratory study of component reliability using unit testing
"['Konstantinos Spyridis', 'Clark Robertson']","All modern communication systems use some form of forward error correction coding. Generally, coding gain is improved when soft decision decoding is used instead of hard decision decoding. While soft decision decoding is a mature technology for convolutional codes, practical soft decision decoding for the commonly used Reed-Solomon (RS) non-binary block code has only recently been developed. Most of the developed SD decoding schemes are either applicable for low to medium code rates with high complexity or for channels that are far from q-ary symmetric. A hybrid hard decision-soft decision (HD/SD) decoding scheme was recently developed for use with bandwidth efficient modulation schemes such as M-ary phase-shift keying (MPSK). Since many communication systems use orthogonal modulation schemes such as M-ary frequency-shift keying (MFSK) for more robust communication links, this paper extends the HD/SD decoding scheme developed for bandwidth efficient modulation to orthogonal modulation. The performance simulation and analysis of MFSK with RS encoding, coherent demodulation, and hybrid HD/SD decoding are presented. The effect of noise other than additive white Gaussian noise, such as pulse-noise interference, is also considered. Furthermore, in our simulations we extend the size of the soft decision reliability information matrix from 2 9 to 2 10 in order to increase the possible decoding lists for hybrid HD/SD decoding.","Performance simulation and analysis of M-ary frequency-shift keying with Reed Solomon encoding, noncoherent demodulation, and hybrid soft decision-hard decision decoding"
['Patrick Maier'],"We develop an abstract lattice-theoretic framework within which we study soundness and other properties of circular assume-guarantee (A-G) rules constrained by side conditions. We identify a particular side condition, non-blockingness, which admits an intelligible inductive proof of the soundness of circular A-G reasoning. Besides, conditional circular rules based on non-blockingness turn out to be complete in various senses and stronger than a large class of sound conditional A-G rules. In this respect, our framework enlightens the foundations of circular A-G reasoning. Due to its abstractness, the framework can be instantiated to many concrete settings. We show several known circular A-G rules for compositional verification to be instances of our generic rules. Thus, we do the circularity-breaking inductive argument once to establish soundness of our generic rules, which then implies soundness of all the instances without resorting to technically complicated circularity-breaking arguments for each single rule. In this respect, our framework unifies many approaches to circular A-G reasoning and provides a starting point for the systematic development of new circular A-G rules. #N#Wir entwickeln einen abstrakten verbandstheoretischen Rahmen in dem wir die#R##N#Korrektheit und andere Eigenschaften bedingter zirkulaerer Assume-Guarantee-#R##N#Regeln (A-G-Regeln) untersuchen. Wir isolieren eine besondere Nebenbedingung,#R##N#non-blockingness, die zu einem verstaendlichen induktiven Beweis der Korrektheit#R##N#zirkulaerer A-G-Regeln fuehrt. Ausserdem sind durch non-blockingness eingeschr#R##N#aenkte zirkulaere Regeln vollstaendig und staerker als eine grosse Klasse von#R##N#korrekten bedingten A-G-Regeln. So gesehen erhellt unsere Arbeit die Grundlagen#R##N#des zirkulaeren A-G-Paradigmas.Aufgrund seiner Abstraktheit kann unser Rahmen zu vielen konkreten Formalismen instanziiert werden. Wir zeigen, dass mehrere bekannte A-G-Regeln zur kompositionalen Verifikation Instanzen unserer generischen Regeln sind. So ist der zirkularitaetsaufloesende Beweis der Korrektheit nur einmal fuer unsere generische Regeln zu fuehren, dann erben alle Instanzen Korrektheit, ohne dass noch einmal ein zirkularitaets-aufloesender Beweis noetig ist. In dieser Hinsicht stellt unser Rahmen eine einheitliche Plattform dar, die verschiedene Ausformungen des#R##N#zirkulaeren A-G-Paradigmas umfasst und von der ausgehend systematisch neue#R##N#zirkulaere A-G-Regeln entwickelt werden koennen.",A Lattice-Theoretic Framework For Circular Assume-Guarantee Reasoning
"['Ahmet Kuzu', 'Eray A. Baran', 'Seta Bogosyan', 'Metin Gokasan', 'Asif Sabanovic']","This paper introduces a codec scheme for compressing the control and feedback signals in networked control and teleoperation systems. The method makes use of Wavelet Packet Transform (WPT) and Inverse Wavelet Packet Transform (IWPT) for coding and decoding operations, respectively. Data compression is carried out in low-pass filter output by reducing the sampling rate, and in high-pass filter output by truncating the wavelet coefficients. The proposed codec works on both directions of signal transmission between a master robot and a slave robot over a networked motion control architecture. Following the formulation of the compression/decompression methodology, experimental validation is conducted on a single-degree-of-freedom motion control system. In the experiments, responses from different Wavelet structures are analyzed and a comparative study is carried out considering the factors of compression rate, reconstruction power error and real-time computational complexity. It is confirmed that the controller using the proposed compression algorithm performs very close to the uncompressed one while enabling transmission of much less data over the network.",Wavelet packet transform-based compression for teleoperation
"['Nabhendra Bisnik', 'Alhussein A. Abouzeid']","MAC protocols for wireless sensor networks employ periodic switching to low energy sleep state in order to enhance network lifetime. During the sleep state, the sensors do not perform energy consuming operations such as receiving and transmitting packets. During the normal state, CSMA based multi-access mechanism is the MAC protocol of choice in distributed, unsynchronized sensor networks. The energy conserving mechanism has a two-fold effect on delay in the network. On one hand it increases delay since many a times the intended receiver may be in sleep state and the transmitter has to delay the transmission to allow the receiver to wake up. On the other hand, since the sensors do not transmit in sleep state, the contention for channel is reduced which tends to improve delay. In this paper we present a queuing theoretic analysis of delay and capacity in sensor networks with uncoordinated sleep mechanism and characterize the energy-delay-capacity tradeoffs. We consider several sleep states which consume different levels of energy. We model sensor networks as queuing networks and evaluate closed form expressions for average packet delay and maximum achievable per-node throughput in terms of network parameters and sleep schedule. Comparisons with the performance of networks that do not employ any energy conserving mechanisms show that any of the energy conserving sleep states in the networks considered in this paper leads to considerable degradation in delay and capacity of the network.",Delay and capacity in energy efficient sensor networks
"['Ernesto Jim??nez-Ruiz', 'Bernardo Cuenca Grau']","In this paper, we present LogMap--a highly scalable ontology matching system with 'built-in' reasoning and diagnosis capabilities. To the best of our knowledge, LogMap is the only matching system that can deal with semantically rich ontologies containing tens (and even hundreds) of thousands of classes. In contrast to most existing tools, LogMap also implements algorithms for 'on the fly' unsatisfiability detection and repair. Our experiments with the ontologies NCI, FMA and SNOMED CT confirm that our system can efficiently match even the largest existing bio-medical ontologies. Furthermore, LogMap is able to produce a 'clean' set of output mappings in many cases, in the sense that the ontology obtained by integrating LogMap's output mappings with the input ontologies is consistent and does not contain unsatisfiable classes.",LogMap: logic-based and scalable ontology matching
"['George J. Pappas', 'Slobodan N. Simic']","In this paper, we consider the problem of constructing abstractions of affine control systems that preserve reachability properties, and, in particular, local accessibility. In this framework, showing local accessibility of the higher level, abstracted model is equivalent to showing local accessibility of the, more detailed, lower level model. Given an affine control system and a smooth surjective map, we present a canonical construction for extracting an affine control system describing the trajectories of the abstracted variables. We then obtain conditions on the abstraction maps that render the original and abstracted system equivalent from a local accessibility point of view. Such consistent hierarchies of accessibility preserving abstractions of nonlinear control systems are then considered for various classes of affine control systems including linear, bilinear, drift free, and strict feedback systems.",Consistent abstractions of affine control systems
"['Silvia De Nadai', 'Francesco Parodi', 'D. Pizzorni']","The É??near miss accidentsÉ?ù system arises from a wider based project by DELAB, a joint laboratory, between ENI, one of the most important Italian petrolchemical companies, and the University of Genoa. The objective is to identify the causes leading to truck accidents enhancing the knowledge base through the data collection of near miss accidents. A system of systems engineering approach was developed where the main system components relate to the driver, the truck and the external environment conditions. The methodology which was adopted to monitor the whole system and preliminary results are illustrated.",A system of systems approach to near miss accidents in dangerous goods road transportation
"['Stefan Wappler', 'Frank Lammermann']","As the paradigm of object orientation becomes more and more important for modern IT development projects, the demand for an automated test case generation to dynamically test object-oriented software increases. While search-based test case generation strategies, such as evolutionary testing, are well researched for procedural software, relatively little research has been done in the area of evolutionary object-oriented software testing.This paper presents an approach with which to apply evolutionary algorithms for the automatic generation of test cases for the white-box testing of object-oriented software. Test cases for testing object-oriented software include test programs which create and manipulate objects in order to achieve a certain test goal. Strategies for the encoding of test cases to evolvable data structures as well as ideas about how the objective functions could allow for a sophisticated evaluation are proposed. It is expected that the ideas herein can be adapted for other unit testing methods as well.The approach has been implemented by a prototype for empirical validation. In experiments with this prototype, evolutionary testing outperformed random testing. Evolutionary algorithms could be successfully applied for the white-box testing of object-oriented software.",Using evolutionary algorithms for the unit testing of object-oriented software
"['Xiaojia Lu', 'Antti T??lli', 'Lauri Anttila', 'Markku J. Juntti', 'Mikko Valkama']",We consider the multiuser frequency allocation problem in singleinput single-out put (SISO) LTE-A type uplink with carrier aggregation (CA). The increased bandwidth in LTE-A system allows orthogonal allocation of subcarriers among users. We consider both consecutive and distributed frequency allocation strategies with per user transmit power constraints and more realistic power amplifier models accounting the dependence of the amplifier efficiency on the frequency allocation. A novel binary integer programming with water-filling power allocation is proposed for consecutive frequency allocation. The system level performance is evaluated via computer simulations. The results shed light to the problem of frequency allocation with real user devices both from theoretical and practical points of view.,Multiuser frequency allocation with wideband power amplifier models
"['Dimitrios Christopoulos', 'Athanasios Gaitatzes']","Educational applications often are slow to leverage and use new interaction devices in order to bring new value and allow new forms of gameplay. Following decades of research on how to use 3D simulation and Virtual Environments in education, attention has recently turned to exploring Multi-User-Virtual-Environments for the educational community. In the following paper we present the results of a pilot simulation battle, created for educational purposes combining the positive aspects of multi-user virtual environments, edutainment VR applications and new Human Computer Interaction (HCI) interfaces. We present the technology used, as well as an evaluation case study of the human-computer interaction results.",Multimodal Interfaces for Educational Virtual Environments
"['Lutz Bornmann', 'Hermann Schier', 'Werner Marx', 'Hans-Dieter Daniel']","Schubert (Scientometrics, 78:559---565, 2009) showed that ""a Hirsch-type index can be used for assessing single highly cited publications by calculating the h index of the set of papers citing the work in question"" (p. 559). To demonstrate that this single publication h index is a useful yardstick to compare the quality of different publications; the index should be strongly related to the assessment by peers. In a comprehensive research project we investigated the peer review process of the Angewandte Chemie International Edition. The data set contains manuscripts reviewed in the year 2000 and accepted by the journal or rejected but published elsewhere. Single publication h index values were calculated for a total of 1,814 manuscripts. The results show a correlation in the expected direction between peer assessments and single publication h index values: After publication, manuscripts with positive ratings by the journal's reviewers show on average higher h index values than manuscripts with negative ratings by reviewers (and later published elsewhere). However, our findings do not support Schubert's (2009) assumption that the additional dimension of indirect citation influence contributes to a more refined picture of the most cited papers.",Does the h index for assessing single publications really work? A case study on papers published in chemistry
"['Roy Tenny', 'Lev S. Tsimring']","We analyze the security of encryption schemes based on chaos synchronization and active/passive decomposition. The security is quantied by the number of transmitted samples that has to be acquired in order to reconstruct the transmitted message with an accuracy that may compromise the transmitted information. The dynamics is estimated as the average of dynamics of the observed data within a small neighborhood of the time delay embedding phase space. We examine the factors that aect the choice of embedding dimension and neighborhood size by the unauthorized receiver. We show that the security can be enhanced by mixing a large randomly modulated message component with a smaller chaotic component while keeping the message modulation ne grained. This result is in contrast to the common approach to ensure security by adding a small message component to a larger chaotic component. Further, we show that even when a low dimensional chaotic map is used, then the unauthorized receiver is required to use a reconstruction embedding dimension that can be made large by using chaotic dynamics with large conditional negative Lyapunov exponent. This result allows one to avoid the common restriction to use only high dimensional chaotic dynamics to maintain security. We also suggest guidelines for the design of ecien t active passive/passive decomposition schemes in order to maintain low transmission power, fast synchronization, and yet preserve security. We demonstrate our analysis using a relatively simple encryption scheme based on a one-dimensional chaotic tent map.",STEPS TOWARDS CRYPTANALYSIS OF CHAOTIC ACTIVE/PASSIVE DECOMPOSITION ENCRYPTION SCHEMES USING AVERAGE DYNAMICS ESTIMATION
"['Solomon Negash', 'Terry Ryan', 'Magid Igbaria']","The quality of a Web-based customer support system involves the information it supplies, the service it provides, and characteristics of the system itself; its effectiveness is reflected by the satisfaction of its users. This paper presents the results of a study of quality and effectiveness in Web-based customer support systems. Data from a survey of 726 Internet users were used to test theoretically expected relationships. The results of this study indicate that information and system quality determine effectiveness while service quality has no impact. Practical implications for managers and designers are offered.",Quality and effectiveness in web-based customer support systems
"['Efstathios Stavrakis', 'Michael Bleyer', 'Danijela Markovic', 'Margrit Gelautz']","We present a method to generate stylized stereo imagery that effectively communicates shape and distance of the depicted scene objects. We use computer vision techniques to analyze real stereo image pairs. In particular, a region based stereo matching algorithm with symmetrical treatment of occlusions is used to extract a disparity map and successively the depth information of the scene. The reference image is color segmented for the purpose of color stylization and an algorithm combining intensity image edges and depth discontinuities is applied to depict dominant object contours in the image. We use disparity information to propagate stylized color segments to the second view together with the object outlining contours. The stylized image pairs are consistent across the two views and can be easily fused for stereoscopic viewing. The stereoscopic image fusion provides an extra dimension of depth that is absent on the individual images.",Image-based stereoscopic stylization
"['Yong Wang', 'Xiaochun Yun', 'Yifei Li']","Mapping and analyzing the topological properties of P2P overlay network will benefit the further design and development of the P2P networks. In this paper, the measured Gnutella network topology is basically taken as an example. The properties of degree-rank distribution and frequency-degree distributions of the measured topology graphs are analyzed in detail. The small world characteristics for Gnutella network are discussed. The results indicate that each tier of Gnutella network shows individual characters, namely, the top level graph fits the power law in degree-rank distribution, but follows the Gaussian function in frequency-degree distribution. The bottom level graph shows power law both in its degree-rank distribution and in its frequency-degree distribution. Fitting results indicate that power law could fit better for the degree-rank distribution and frequency-degree distribution of bottom level graphs, while Gaussian could describe the frequency-degree distribution of the top level graphs. Gnutella overlay network has the small world characters, but it is not a scale-free network, which has developed over time following a different set of growth processes from those of the BA (Barabdsi-Albert) model. The measured results show that Gnutella network has pretty well scalability as well as the abilities to tolerating failures and attacks against peers, but with low routing efficiencies",Analyzing the Characteristics of Gnutella Overlays
"['Mohammad Ashiqur Rahaman', 'Henrik Plate', 'Yves Roudier', 'Andreas Schaad']","Collaborating on complex XML data structures is a non-trivial task in domains such as the public sector,healthcare or engineering. Specifically, providing scalable XML content dissemination services in a selective and secure fashion is a challenging task. This paper describes a publish/subscribe middleware infrastructure to achieve a content-based dissemination of XML documents. Our approach relies on the dissemination of XML documents based on their semantics, as described by concepts that form an interoperable description of documents. This infrastructure leverages our earlier scheme [1] for protecting the integrity and confidentiality of XML content during dissemination.",Towards Secure Content Based Dissemination of XML Documents
"['Salmiah Ahmad', 'M. O. Tokhi', 'Siti Fauziah Toha']","In this paper, an optimisation technique is adopted to manipulate the input and output scaling of a fuzzy logic controller for lifting the front wheels of a wheelchair and stabilizing the wheelchair in two-wheeled mode. A virtual wheelchair (WC) model is developed within Visual Nastran (VN) software environment where the model is further linked with Matlab/Simulink for control purposes. The lifting of the chair is done by transforming the first link, attached to the front wheels (casters) to the upright position while maintaining stability of the second link where the payload is attached. General rules of thumb allow heuristic tuning of the parameters but a proper optimisation mechanism will perform better. Genetic Algorithm is used to control the two-wheeled wheelchair and results show that the optimised parameters give better system performance.",Genetic Algorithm Optimisation for Fuzzy Control of Wheelchair Lifting and Balancing
['J??rg Peters'],"Abstract    C  1 -surface splines define tangent continuous surfaces from control points in the manner of tensor-product (B-)splines, but allow a wider class of control meshes capable of outlining arbitrary free-form surfaces with or without boundary. In particular, irregular meshes with non-quadrilateral cells and more or fewer than four cells meeting at a point can be input and are treated in the same conceptual frame work as tensor-product B-splines; that is, the mesh points serve as control points of a smooth piecewise polynomial surface representation that is local and evaluates by averaging. Biquartic surface splines extend and complement the definition of  C  1 -surface splines in a previous paper (Peters, J  SLAM J. Numer. Anal.  Vol 32 No 2 (1993) 645É??666) improving continuity and shape properties in the case where the user chooses to model entirely with four-sided patches. While tangent continuity is guaranteed, it is shown that no polynomial, symmetry-preserving construction with adjustable blends can guarantee its surfaces to lie in the local convex hull of the control mesh for very sharp blends where three patches join. Biquartic  C  1 -surface splines do as well as possible by guaranteeing the property whenever more than three patches join and whenever the blend exceeds a certain small threshold.",Biquartic C1-surface splines over irregular meshes
"['Hongliang Li', 'Guizhong Liu', 'Zhongwei Zhang', 'Yongli Li']","Several scene-detection algorithms, which are only based on bit rate fluctuations, have been proposed. All of them are presented on the fixed thresholds, which are obtained by the empirical records of the video characteristics. Due to the sensitivity of these methods to the accuracy of the records, which are generally obtained by testing several values repeatedly, bad performance evaluation might be observed for the actual scene detection, especially for real-time video traffic. In this paper, we review the previous works in this area, and study the correlation between the scene duration and the scene change at the frame level, and simultaneously investigate the local statistical characteristics of scenes such as variance and peak bit rate etc. Based on this analysis, an effective decision function is first constructed for the scene segmentation. Then, we propose a scene-detection algorithm using the defined dynamic threshold model, which can capture the statistical properties of the scene changes. Experimental results using 15 variable bit rate MPEG video traces indicate good performances of the proposed algorithm with significantly improved scene-detection accuracy.",Adaptive scene-detection algorithm for VBR video stream
"['Sungeun Eom', 'Ryoma Bise', 'Takeo Kanade']","We present a method for robustly detecting hematopoietic stem cells (HSCs) in phase contrast microscopy images. HSCs appear to be easy to detect since they typically appear as round objects. However, when HSCs are touching and overlapping, showing the variations in shape and appearance, standard pattern detection methods, such as Hough transform and correlation, do not perform well. The proposed method exploits the output pattern of a ring filter bank applied to the input image, which consists of a series of matched filters with multiple-radius ring-shaped templates. By modeling the profile of each filter response as a quadratic surface, we explore the variations of peak curvatures and peak values of the filter responses when the ring radius varies. The method is validated on thousands of phase contrast microscopy images with different acquisition settings, achieving 96.5% precision and 94.4% recall.",Detection of hematopoietic stem cells in microscopy images using a bank of ring filters
"[""Jamay'ah Zakaria"", 'Fytton Rowland']","Most of the studies conducted on future business models for electronic scholarly publishing have concentrated on the major publishing areas of North America and Western Europe, and on large publishers (both for-profit and not-for-profit). This paper considers the prospects for electronic scholarly publishing in a smaller country away from these two parts of the world.  Malaysia is a medium-sized developing country in Southeast Asia, with the ambition to become fully developed by 2020.  The government has invested heavily both in ICT infrastructure and in educating the population in its use, and as a result Internet usage is quite high, and most academics and research workers can access the Internet both at work and at home.  A number of journals are published by not-for-profit organisations in Malaysia, but their sales are small and their financial positions precarious, and few of them are available electronically yet. This work is based on a large-scale questionnaire survey of Malaysian scientists, and interviews with managers of university presses and other not-for-profit publishers in Malaysia, designed to reveal attitudes to online electronic journals among their potential authors and potential publishers. Those academics who published frequently in printed scholarly journals were significantly more positive in their attitude towards online journals, and this effect was particularly strong if they published in international journals and in English.  Interviews with representatives of the presses of universities and research institutes revealed that most Malaysian journals are small, publishing only 20-30 papers per year, and have low print runs, typically about 300.  They gain relatively little income from subscriptions, many copies being distributed though exchanges, and are subsidised by their host institutions. Only one journal among this sample of presses had a parallel electronic version; the remainder were print-only. An important feature of Malaysia is its very high value on Hofstede's Power Distance Index (PDI), which implies that a high level of deference to one's superiors is usual in Malaysia.  The results from the interviews suggested strongly that the effect seen in scholarly publishing is a consequence of senior managerial figures in universities not supporting online publishing.  Those researchers with a more international orientation are the ones most likely to go against the cultural trend.",What are the Prospects for Publishing Online Scholarly Journals in Malaysia? The Cultural Constraint
"['Debasis Mazumdar', 'Soma Mitra', 'Sonali Dhali', 'Sankar K. Pal']",A chosen plaintext steganalysis algorithm is described to isolate the corrupted bits in an image tampered with Hide4PGP V 2.0. The method is developed from the notion of representation of two dimensional image data in terms of a linear bit stream consisting of a set of basic building blocks. Its performance for message extraction is demonstrated on different 24 bit BMP images.,A chosen plaintext steganalysis of Hide4PGP v 2.0
"['Hsiao-feng Lu', 'P.V. Kumar', 'Habong Chung']","Orthogonal-design-based space-time (ODST) codes of size (n/spl times/n) offer maximum diversity gain advantage and a simple yet optimal decoding algorithm under an arbitrary signal alphabet or constellation A. However, these designs only exist for n=2, 4, 8 when A is real and for n=2 when A is complex. In this letter, we address the question of the existence of ODST codes of other sizes when A is restricted to be a proper subset of either real or complex numbers. We refer to these as restricted-alphabet ODST (RA-ODST) codes. We show that real RA-ODST codes of size greater than 8 that also guarantee maximum diversity advantage do not exist. Without the diversity requirement, RA-ODST codes exist only when A={a,-a}, 0<a/spl isin/R. Examples of such codes are provided. In the complex case, under the added requirement of maximum diversity advantage, we prove the nonexistence of complex RA-ODST codes under fairly simple assumptions regarding the signal alphabet.",On orthogonal designs and space-time codes
"['Mukun Cao', 'Yuqiang Feng', 'Yan Li', 'Chunyan Wang']","Traditional research in automated negotiation focuses on negotiation protocol and strategy. This paper studies automated negotiation from a new point, proposes a novel concept, namely negotiating agent, argues its significance in construction of automated negotiation system; designs its architecture, which can support both goal-directed reasoning and reactive response. In order to construct an interaction mechanism among negotiating agents, a communication model is proposed, in which the negotiation language used by agents is defined. Design of the communication model and the language has been attempted in such a way so as to provide general support for a wide variety of commercial negotiation circumstances, and therefore to be particularly suitable for electronic commerce. Finally, the design and expression of the negotiation ontology are discussed.","Negotiating agent: concept, architecture and communication model"
"['Guoxin Li', 'Jin Chen', 'Yuzhen Huang', 'Guochun Ren']","In this study, a dual-hop multiple-inputÉ??multiple-output relay network with the Nth-best relay selection scheme in the presence of co-channel interference is studied. Specifically, the Nth-best relay is selected based on the channel state information (CSI) of the first hop. The authors first consider the CSI is perfect feedback and derive exact as well as asymptotic closed-form expressions for the outage probability. Results reveal that the diversity order of N R ?? min{N S(K É?? N + 1), N D} is achieved when there is no feedback delay, where N S, N R and N D represent the number of antennas at the source, the relays and the destination, respectively, K is the number of the relays and N (1 É?? N É?? K) represents the rank of relay chosen. Then, they investigate the outage performance of the system with feedback delay, and exact and asymptotic outage probability expressions are also obtained. Results illustrate that outdated CSI degrades the diversity order of the system to min{N R, N D}, which is independent of the number of antennas at the source, the number of relays and the rank of the relay chosen. The findings of the study provide valuable insights into the practical system design.",Outage performance of multiple-input?®multiple-output decode-and-forward relay networks with the Nth-best relay selection scheme in the presence of co-channel interference
"['Troy A. Johnson', 'Patrick Seeling']","In this work, we present a scheme based on Bluetooth friendly device names to enable power-optimized ad-hoc localization of mobile devices. Eliminating the service discovery and connection (including potential pairing) phases in Bluetooth allows for speedier and more power-efficient conveying of location information using friendly device names. Furthermore, we observe that using the signal strength commonly provided in reference APIs of mobile OSs, client distances can be calculated with high accuracy and without additional power penalties.",Localization using bluetooth device names
"['Kostas Bousias', 'Liang Guang', 'Chris R. Jesshope', 'Mike Lankamp']","Future many-core processor systems require scalable solutions that conventional architectures currently do not provide. This paper presents a novel architecture that demonstrates the required scalability. It is based on a model of computation developed in the AETHER project to provide a safe and composable approach to concurrent programming. The model supports a dynamic approach to concurrency that enables self-adaptivity in any environment so the model is quite general. It is implemented here in the instruction set of a dynamically scheduled RISC processor and many such processors form a microgrid. Binary compatibility over arbitrary clusters of such processors and an inherent scalability in both area and performance with concurrency exploited make this a very promising development for the era of many-core chips. This paper introduces the model, the processor and chip architecture and its emulation on a range of computational kernels. It also estimates the area of the structures required to support this model in silicon.",Implementation and evaluation of a microthread architecture
"['S??bastien Limet', 'Pierre R??ty', 'Helmut Seidl']","A new class of tree-tuple languages is introduced: the weakly regular relations. It is an extension of the regular case (regular relations) and a restriction of tree-tuple synchronized languages, that has all usual nice properties, except closure under complement. Two applications are presented: to unification modulo a rewrite system, and to one-step rewriting.",Weakly Regular Relations and Applications
"['Georgia Koutsandria', 'Reinhard Gentz', 'Mahdi Jamei', 'Anna Scaglione', 'Sean Peisert', 'C. McParland']","The trustworthiness and security of cyber-physical systems (CPSs), such as the power grid, are of paramount importance to ensure their safe operation, performance, and economic efficiency. The aim of many cyber-physical security techniques, such as network intrusion detection systems (NIDSs) for CPSs, is to ensure continuous reliable operation even in exposed network environments. But the validation of such methods goes well beyond standard network analysis, since meaningful tests must also integrate realistic understanding of the physical systems behavior and response to the network activity. Our goal in this paper is to showcase an example of a testbed environment that can support such validation. In it, real network traffic, emulating and industrial control network, interacts with simulated physical models in real-time, extending and leveraging ""hardware-in-the-loop"" and ""cyber-in-the-loop"" capabilities. The testbed is a bridge between theory and practice and offers a number of features, including network communications, data management, as well as the virtualization of cyber-physical state analytics performed by the NIDS. The traffic is captured by real network taps and is forwarded to a real data management environment, receiving also the data reports from the simulated industrial control environment. To illustrate the capabilities of our testbed we show how the data are cross-checked by a ""physics aware"" NIDS, identifying network traffic that does not comply with its cyber-physical security rules.",A Real-Time Testbed Environment for Cyber-Physical Security on the Power Grid
"['Frank Loll', 'Niels Pinkwart', 'Oliver Scheuer', 'Bruce M. McLaren']","Supporting students in the acquisition of argumentation skills is an important goal of educational technology. However, there has not been much work done towards developing generic and reusable software architectures for collaborative argumentation that could reduce the development time for distributed argumentation learning systems. Based on a survey of more than 50 different argumentation systems, this paper presents a requirements analysis for a generic collaborative intelligent tutoring system for argumentation.",Towards a Flexible Intelligent Tutoring System for Argumentation
"['Yang Cai', 'Ingo Snel', 'B. Suman Bharathi', 'Clementine Klein', 'Judith Klein-Seetharaman']","Biomedical systems involve complex interactions between diverse components. Problem solving in such systems requires insight, i.e. the capability to make non-obvious connections. In this paper, we present a game-based problem solving environment, where users can explore biological interactions with navigation on atomic to macroscopic scales, role-play, and networked collaboration. The study investigates the system architecture of the biological game, bio-morphing characters, and bio-interactions with biosensing and biodynamics. The prototype has been implemented on PC and tested in a preschool environment where users have little knowledge in biology. The experiment shows that the game greatly inspired users both in concept learning and entertainment.",Towards biomedical problem solving in a game environment
"['Ferdinando Bedeschi', 'Chiara Boffino', 'Edoardo Bonizzoni', 'Osama Khouri', 'Giorgio Pollaccia', 'Claudio Resta', 'Guido Torelli']","This paper presents a low-ripple voltage tripler charge-pump intended to provide a stable supply voltage to the cascaded circuits, thus allowing relaxed design specifications of the latter. The structure, which is implemented by means of an array of charge pump modules sequentially enabled, was developed for a phase-change memory (PCM) device, but it is also suitable for use in other applications. The circuit was integrated in a PCM test-chip, fabricated in 0.35-/spl mu/m CMOS technology. An output voltage ripple of only 16 mV has been experimentally demonstrated.",A low-ripple voltage tripler
"['Jundong Liu', 'Baba C. Vemuri', 'Jose L. Marroquin']","Automatic registration of multimodal images involves algorithmically estimating the coordinate transformation required to align the data sets. Most existing methods in the literature are unable to cope with registration of image pairs with large nonoverlapping field of view (FOV). We propose a robust algorithm, based on matching dominant local frequency image representations, which can cope with image pairs with large nonoverlapping FOV The local frequency representation naturally allows for processing the data at different scales/resolutions, a very desirable property from a computational efficiency view point. Our algorithm involves minimizing-over all rigid/affine transformations-the integral of the squared error (ISE or L/sub 2/E) between a Gaussian model of the residual and its true density function. The residual here refers to the difference between the local frequency representations of the transformed (by an unknown transformation) source and target data. We present implementation results for image data sets, which are misaligned magnetic resonance (MR) brain scans obtained using different image acquisition protocols as well as misaligned MR-computed tomography scans. We experimently show that our L/sub 2/E-based scheme yields better accuracy over the normalized mutual information.",Local frequency representations for robust multimodal image registration
"['Yu Nejigane', 'Masamichi Shimosaka', 'Taketoshi Mori', 'Tomomasa Sato']","In this paper, we propose wrapped boosting that is extension of boosting algorithm for robust online action recognition. Boosting algorithm is one of ensemble learning algorithm and is also known as a feature selector. In our previous work utilizing boosting, we achieved automatic feature selection and robust model-based action classifiers which had very small calculation cost based on posture information of human body joints. However, which joints we should allocate posture sensors to must be given by humans in advance. Our new learning framework of wrapped boosting provides not only automatic feature selection but also automatic sensor allocation to proper joints of humans for target actions. We evaluated our algorithm targeting gait motion based on motion data fetched by motion capturing system. In consequence, wrapped boosting was able to select proper joints to which limited sensors should be attached, and to construct more robust classifiers compared to constructing classifiers with all joints available. Classifiers constructed only with existing boosting algorithm were subject to over-fitting to training data.",Online action recognition with wrapped boosting
"['Sara Lana-Serrano', 'Jos?? Miguel Go?Òi-Menoyo', 'Jos?? Carlos Gonz?≠lez Crist??bal']",This paper presents the 2005 MIRACLEÉ??s team approach to Cross-Language Geographical Retrieval (GeoCLEF). The main goal of the GeoCLEF participation of the MIRACLE team was to test the effect that geographical information retrieval techniques cause to information retrieval. The baseline approach is based on the development of named entity recognition and geospatial information retrieval tools and on its combination with linguistic techniques to perform indexing and retrieval tasks.,MIRACLEÉ??s 2005 Approach to Geographical Information Retrieval
"['J??ferson Luis de Moraes Machado', 'Isabel Cristina Giehl', 'Nance Beyer Nardi', 'Luis Alberto dos Santos']","Growth of cells in 3-D porous scaffolds has gained importance in the field of tissue engineering. The scaffolds guide cellular growth, synthesize extracellular matrix and other biological molecules, and make the formation of tissues and functional organs easier. The aim of this study is to use ?Ò-tricalcium phosphate cement in order to obtain new types of scaffolds with the aid of paraffin spheres as pore generators. The porosity of the scaffolds produced with paraffin spheres was analyzed and compared to the literature, and the study of scaffold permeability using the Forchheimer equation allowed the analysis of pore interconnectivity. In vitro tests showed the behavior of scaffolds in solutions of simulated body fluid, and viability and cell proliferation were also evaluated. The results show the potential use of the materials developed for scaffolds for use in tissue engineering applications.",Evaluation of Scaffolds based on ?Ò-Tricalcium Phosphate Cements for Tissue Engineering Applications
"['Roman Vacul??n', 'Katia P. Sycara']","The framework for automatic mediation of two process models composed of semantically annotated Web services is presented. Process mediation is hard because of many possible mismatches between process models. We introduce algorithms for the process models analysis to find possible mappings between provider's and requester's process models, or to identify incompatibilities that cannot be reconciled with given set of available data mediators and external services. Results of the analysis phase are used in the mediator runtime component. In particular, we show how the workflow and dataflow mismatches can be resolved.",Towards automatic mediation of OWL-S process models
"['Ivo Lattenberg', 'Kamil Vrba']","The paper deals with the novel filtration techniques for analog high-speed data signal preprocessing working on the base of a pure current mode filters. Because of better current-mode properties and frequency features, and due to the tendency to lower supply voltages, which is given by the possibilities of the technology used, where the dynamic range in the voltage area goes down due to the use of the voltage mode, a fast development in the field of current-mode application design can be witnessed. The paper shows possibilities of current-mode filter realizations using only current amplifiers. The paper shows also various possibilities of current amplifier realization such as basic building blocks for circuits working in the current-mode. Current amplifier realization possibilities are presented using both the existing elements, e.g. current conveyors and OTA elements, and those based on the transistor level. The currentmode filter with only current amplifier, without any active element with voltage terminal is designed.",Filters with Current Amplifiers for High-speed Communication
"['Nor Surayahani Suriani', 'Aini Hussain', 'Mohd Asyraf Zulkifley']","Event recognition is one of the most active research areas in video surveillance fields. Advancement in event recognition systems mainly aims to provide convenience, safety and an efficient lifestyle for humanity. A precise, accurate and robust approach is necessary to enable event recognition systems to respond to sudden changes in various uncontrolled environments, such as the case of an emergency, physical threat and a fire or bomb alert. The performance of sudden event recognition systems depends heavily on the accuracy of low level processing, like detection, recognition, tracking and machine learning algorithms. This survey aims to detect and characterize a sudden event, which is a subset of an abnormal event in several video surveillance applications. This paper discusses the following in detail: (1) the importance of a sudden event over a general anomalous event; (2) frameworks used in sudden event recognition; (3) the requirements and comparative studies of a sudden event recognition system and (4) various decision-making approaches for sudden event recognition. The advantages and drawbacks of using 3D images from multiple cameras for real-time application are also discussed. The paper concludes with suggestions for future research directions in sudden event recognition.",Sudden event recognition: a survey
"['Fred Harris', 'Xiaofei Chen', 'Elettra Venosa', 'Bhaskar D. Rao']","This paper investigates the filter bank (FB) based selection diversity combining as well as linear equalization for single carrier (SC)transmissions over frequency selective channels. In contrast to the multicarrier carrier (MC) transmissions, e.g., OFDM, the FB based approach avoids the use of cyclic prefix (CP) or guard band and offers a number of superior properties such as synchronization, and peak to average power ratio (PAPR), etc. However, due to the lack of practical diversity techniques and cost effective equalizers, the broadband SC signal can hardly be deployed under highly dispersive channel environment. We propose a practical FB based selection diversity based on non-maximally decimated filter banks with perfect reconstruction support (PR-NMDFB) with its companion linear equalizer in the FB transformed domain. We shall give detailed minimum mean square error (MMSE) and bit error rate (BER) analysis and compare it to the optimal maximum ratio combining (MRC) solution.",Selection diversity and linear equalization over frequency selective channels for single carrier filter bank-based transmissions
"['Karol Gregor', 'Yann LeCun']","In Sparse Coding (SC), input vectors are reconstructed using a sparse linear combination of basis vectors. SC has become a popular method for extracting features from data. For a given input, SC minimizes a quadratic reconstruction error with an L1 penalty term on the code. The process is often too slow for applications such as real-time pattern recognition. We proposed two versions of a very fast algorithm that produces approximate estimates of the sparse code that can be used to compute good visual features, or to initialize exact iterative algorithms. The main idea is to train a non-linear, feed-forward predictor with a specific architecture and a fixed depth to produce the best possible approximation of the sparse code. A version of the method, which can be seen as a trainable version of Li and OsherÉ??s coordinate descent method, is shown to produce approximate solutions with 10 times less computation than Li and OsherÉ??s for the same approximation error. Unlike previous proposals for sparse code predictors, the system allows a kind of approximate É??explaining awayÉ?ù to take place during inference. The resulting predictor is differentiable and can be included into globallytrained recognition systems.",Learning Fast Approximations of Sparse Coding
"['Ingo Wald', 'Thomas Kollig', 'Carsten Benthin', 'Alexander Keller', 'Philipp Slusallek']","Rasterization hardware provides interactive frame rates for rendering dynamic scenes, but lacks the ability of ray tracing required for efficient global illumination simulation. Existing ray tracing based methods yield high quality renderings but are far too slow for interactive use. We present a new parallel global illumination algorithm that perfectly scales, has minimal preprocessing and communication overhead, applies highly efficient sampling techniques based on randomized quasi-Monte Carlo integration, and benefits from a fast parallel ray tracing implementation by shooting coherent groups of rays. Thus a performance is achieved that allows for applying arbitrary changes to the scene, while simulating global illumination including shadows from area light sources, indirect illumination, specular effects, and caustics at interactive frame rates. Ceasing interaction rapidly provides high quality renderings.",Interactive global illumination using fast ray tracing
['Zhengyou Zhang'],"A heuristic method has been developed for registering two sets of 3-D curves obtained by using an edge-based stereo system, or two dense 3-D maps obtained by using a correlation-based stereo system. Geometric matching in general is a difficult unsolved problem in computer vision. Fortunately, in many practical applications, some a priori knowledge exists which considerably simplifies the problem. In visual navigation, for example, the motion between successive positions is usually approximately known. From this initial estimate, our algorithm computes observer motion with very good precision, which is required for environment modeling (e.g., building a Digital Elevation Map). Objects are represented by a set of 3-D points, which are considered as the samples of a surface. No constraint is imposed on the form of the objects. The proposed algorithm is based on iteratively matching points in one set to the closest points in the other. A statistical method based on the distance distribution is used to deal with outliers, occlusion, appearance and disappearance, which allows us to do subset-subset matching. A least-squares technique is used to estimate 3-D motion from the point correspondences, which reduces the average distance between points in the two sets. Both synthetic and real data have been used to test the algorithm, and the results show that it is efficient and robust, and yields an accurate motion estimate.",Iterative point matching for registration of free-form curves and surfaces
"['Louay Jalloul', 'Raj M. Misra']","The effect of channel estimation error on the performance of a wideband code division multiple access link using a pilot channel multiplexed with the data channel is analyzed. The gain achievable by improving the channel estimation algorithm is studied. Analysis and simulation results are presented for a data-aided channel estimation scheme that effectively extends the duration of the pilot signal. Using the maximum likelihood detection principle, it is shown that making tentative minimum mean-squared error decisions on the data bits using estimates obtained from the pilot channel can extend the number of samples used in the channel estimation, thus improving the channel estimate quality. Simulations show that a gain of up to 1.5 dB in block-error rate performance is achievable using the proposed channel estimation scheme for both Convolutional and Turbo codes. It is shown that the performance gain depends on the information bit rate, the channel characteristics, including the Doppler frequency, and the multipath fading channel power delay profile. Simulation results also show that for weak paths, data-aided channel estimation benefits significantly from diversity combining.",Data-aided channel estimation for wideband CDMA
"['Hong Shin Jun', 'Sung Soo Chung', 'Sang Hyeon Baeg']","This work presents a new methodology that removes JTAG bottlenecks in system interconnect test. JTAG test has a limitation by targeting only low-speed testing. But, the system interconnect test requires the test to be run at system clock speed through the cluster of the network and also needs to diagnose the skew and delay characteristics of the cluster. Resolving the synchronization issue between a high-speed pattern clock and TCK, the proposed technique enables high frequency interconnection testing, cluster testing, and delay testing. Experimental results with test vehicles show that the test technique can be used with complex interconnections including differential signal lines, AC coupling, latency, and optical signal interconnections.",Removing JTAG bottlenecks in system interconnect test
"['Yan Zhang', 'Andreas F. Koschan', 'Mongi A. Abidi']","Superquadrics are able to represent a large variety of objects with only a few parameters and a single equation. We present a superquadric representation strategy for automotive parts composed of 3-D triangle meshes. Our strategy consists of two ma- jor steps of part decomposition and superquadric fitting. The origi- nalities of this approach include the following two features. First, our approach can represent multipart objects with superquadrics suc- cessfully by applying part decomposition. Second, superquadrics re- covered from our approach have the highest confidence and accu- racy due to the 3-D watertight surfaces utilized. A novel, generic 3-D part decomposition algorithm based on curvature analysis is also proposed. Experimental results demonstrate that the proposed part decomposition algorithm is able to segment multipart objects into meaningful single parts efficiently. The proposed superquadric rep- resentation strategy can then represent each individual part of the original objects with a superquadric model successfully. ?? 2004",Superquadric representation of automotive parts applying part decomposition
"['Aurobrata Ghosh', 'Rachid Deriche', 'Maher Moakher']","In Diffusion Magnetic Resonance Imaging (D-MRI), the 2nd order diffusion tensor has given rise to a widely used tool É?? Diffusion Tensor Imaging (DTI). However, it is known that DTI is limited to a single prominent diffusion direction and is inaccurate in regions of complex fiber structures such as crossings. Various other approaches have been introduced to recover such complex tissue micro-geometries, one of which is Higher Order Cartesian Tensors. Estimating a positive diffusion function has also been emphasised mathematically, since diffusion is a physical quantity. Recently there have been efforts to estimate 4th order diffusion tensors from Diffusion Weighted Images (DWIs), which are capable of describing crossing configurations with the added property of a positive diffusion function. We take up one such, the Ternary Quartic approach, and reformulate the estimation equation to facilitate the estimation of the non-negative 4th order diffusion tensor. With our modified approach we test on synthetic, phantom and real data and confirm previous results.",Ternary Quartic approach for positive 4th order diffusion tensors revisited
"['P. Domokos', 'Istv?≠n Majzik']",Aspect-oriented modeling is proposed to design the architecture of fault tolerant systems. Notations are introduced that support the separate and modularized design of functional and dependability aspects in UML class diagrams. This notation designates sensitive parts of the architecture and selected architecture patterns that implement common redundancy techniques. A model weaver is presented that constructs both the integrated model of the system and the dependability model on the basis of the analysis sub-models attached to the architecture patterns. In this way fault tolerance mechanisms can be systematically analyzed when they are integrated into the system.,Design and analysis of fault tolerant architectures by model weaving
"['T. Bheemarjuna Reddy', 'John P. John', 'C. Siva Ram Murthy']","Ad hoc wireless networks with their widespread deployment, now need to support applications that generate multimedia and real-time traffic. Video, audio, real-time voice over IP, and other multimedia applications require the network to provide guarantees on the Quality of Service (QoS) of the connection. The 802.11e Medium Access Control (MAC) protocol was proposed with the aim of providing QoS support at the MAC layer. The 802.11e performs well in wireless LANs due to the presence of Access Points (APs), but in ad hoc networks, especially multi-hop ones, it is still incapable of supporting multimedia traffic.#R##N##R##N#One of the most important QoS parameters for multimedia and real-time traffic is delay. Our primary goal is to reduce the end-to-end delay, thereby improving the Packet Delivery Ratio of multimedia traffic, that is, the proportion of packets that reach the destination within the deadline, in 802.11e based multi-hop ad hoc wireless networks.#R##N##R##N#Our contribution is threefold: first we propose dynamic ReAllocative Priority (REAP) scheme, wherein the priorities of packets in the MAC queues are not fixed, but keep changing dynamically. We use the laxity and the hop length information to decide the priority of the packet. ReAP improves the PDR by over 28% in comparison with 802.11e, especially under heavy loads. Second, we introduce Adaptive-TXOP (A-TXOP), where transmission opportunity (TXOP) is the time interval during which a node has the right to initiate transmissions. This scheme reduces the delay of video traffic by reducing the number of channel accesses required to transmit large video frames. It involves modifying the TXOP interval dynamically based on the packets in the queue, so that fragments of the same packet are sent in the same TXOP interval. A-TXOP is implemented over ReAP to further improve the performance of video traffic. ReAP with A-TXOP helps in reducing the delay of video traffic by over 27% and further improves the quality of video in comparison with ReAP without A-TXOP. Finally, we have TXOP-sharing, which is aimed at reducing the delay of voice traffic. It involves using the TXOP to transmit to multiple receivers, in order to utilize the TXOP interval fully. It reduces the number of contentions to the channel and thereby reduces the delay of voice traffic by over 14%. A-TXOP is implemented over ReAP to further improve the performance of voice traffic. The three schemes (REAP, A-TXOP, and TXOP-sharing) work together to improve the performance of multimedia traffic in 802.11e based multi-hop ad hoc wireless networks.",Providing MAC QoS for multimedia traffic in 802.11e based multi-hop ad hoc wireless networks
"['Richard Bowden', 'Liam F. Ellis', 'Josef Kittler', 'Mikhail Shevchenko', 'David Windridge']",In conventional computer vision systems symbol grounding is invariably established via supervised learning. We investigate unsupervised symbol grounding mechanisms that rely on perception action coupling. The mechanisms involve unsupervised clustering of observed actions and percepts. Their association gives rise to behaviours that emulate human action. The capability of the system is demonstrated on the problem of mimicking shape puzzle solving. It is argued that the same mechanisms support unsupervised cognitive bootstrapping in cognitive vision.,Unsupervised symbol grounding and cognitive bootstrapping in cognitive vision
"['Mazen Abi-Hussein', 'Corinne Berland', 'Olivier Venard']","This paper presents a novel system simulation platform dedicated to the derivation of 3G receiver specifications in duplex transmission mode. The TX signal continuously present at the input of the receiver, and the eventual presence of in-band and out-of-band blocking signals degrade the signal to noise ratio through various phenomena. In order to find the optimal specifications of the system in this RF complex environment, a first stage of simulations is essential. The system simulator presented in this paper can handle complex scenarios simulations while tracking the distortion contribution of every block in the receive chain.",Novel simulation approach for 3G W-CDMA receivers
['Robert W. Numrich'],"The author describes a simulation study of interprocessor memory contention for a shared-memory, vector multiprocessor. When programs execute together on such a system, each program's performance, relative to its performance on a single dedicated processor, degrades because of contention among processors for shared memory. From the results of the simulation study, the author proposes analytic forms for the asymptotic steady-state behavior of throughput, time delay, and efficiency as functions of hardware parameters. The results suggest criteria for evaluating hardware designs and an index of quality for comparing different designs. >",Memory contention for shared memory vector multiprocessors
"['Andrew Lewis', 'David Abramson', 'Tom Peachey']","This paper describes a method of parallelisation of the popular Nelder-Mead simplex optimization algorithms that can lead to enhanced performance on parallel and distributed computing resources. A reducing set of simplex vertices are used to derive search directions generally closely aligned with the local gradient. When tested on a range of problems drawn from real-world applications in science and engineering, this reducing set concurrent simplex (RSCS) variant of the Nelder-Mead algorithm compared favourably with the original algorithm, and also with the inherently parallel multidirectional search algorithm (MDS). All algorithms were implemented and tested in a general-purpose, grid-enabled optimization toolset.",RSCS: a parallel simplex algorithm for the Nimrod/O optimization toolset
"['Ursula Redmond', 'P?≠draig Cunningham']","Temporal information is increasingly available with network data sets. This information can expose underlying processes in the data via sequences of link activations. Examples range from the propagation of ideas through a scientific collaboration network, to the spread of disease via contacts between infected and susceptible individuals. We focus on the flow of funds through an online financial transaction network, in which given patterns might signify suspicious behaviour. The search for these patterns may be formulated as a temporally constrained subgraph isomorphism problem. We compare two algorithms which use temporal data at different stages during the search, and empirically demonstrate one to be significantly more efficient.",Temporal subgraph isomorphism
"['Kiyotaka Izumi', 'Keigo Watanabe', 'Sangho Jin']","Intelligent control is being widely studied in the field of robotics. A construction method of an intelligent control system is the fuzzy behavior-based control which decomposes a task into each elemental behavior like a subsumption architecture, and each elemental behavior is realized by fuzzy reasoning. In the paper, a module learning method is proposed for such a system, because the robot will be able to get more general knowledge or fuzzy reasoning than a central learning method. In particular, the module learning method is applied to an obstacle avoidance problem of a mobile robot. The effectiveness of the present method is illustrated through some simulations.",Obstacle avoidance of mobile robot using fuzzy behaviour-based control with module learning
"['Baopu Li', 'Max Q.-H. Meng']","Wireless capsule endoscopy (WCE) has been gradually applied in hospitals due to its great advantage that it can directly view the entire small bowel in human body compared with traditional endoscopies and other imaging techniques for gastrointestinal diseases. However, a challenging problem with this new technology is that too many images produced by WCE causes a tough task to doctors, so it is very significant to help and relief the clinicians if we can develop computer based automatic detection system to prescreen the collected large amount of images and identify the images with potential problems. In this paper, we propose a new scheme aimed for small bowel tumor detection of WCE images. This new scheme utilizes texture feature, also a powerful clue used by physicians, to detect tumor images with support vector machine. We put forward a new idea of wavelet based local binary pattern as the textural features to discriminate tumor regions from normal regions, which take advantage of wavelet transform and uniform local binary pattern. With support vector machine as the classifier, three-fold cross validation experiments on our present image data verify that it is promising to employ the proposed texture features to recognize the small bowel tumor regions.",Small bowel tumor detection for wireless capsule endoscopy images using textural features and support vector machine
"['Congdao Han', 'Zhiyu Xiang', 'Jilin Liu', 'Eryong Wu']","Simultaneous localization and mapping is a well studied problem as it is considered by many to be an essential capability for autonomous robots. In this paper, we present an algorithm using a stereo camera based on Rao-Blackwelltion particle filter, It can realize three dimensional stereo vision SLAM for mobile robot in unknown outdoor environments. Firstly, we determine the initial motion estimation between two adjacent frames through the multiple view geometry utilizing the matched SIFT point pairs. Then, the 3D positions of landmarks are constructed directly through triangulation methodology. With accurate data association, the camera's state and the landmark positions are updated recursively. Finally, an efficient particle resamping algorithm MPR (the modified particle resampling algorithm) is proposed to deal with the degeneracy of particles.",Stereo vision based SLAM in outdoor environments
"['Keyhan Kobravi', 'Reza Iravani', 'Hassan Kojori']","Part I of this paper has proposed a duty-cycle-space-vector (DCSV)-based modulation strategy for three- and four-leg matrix converters (MCs). This paper develops a digital DCSV-based modulation strategy for the three- and four-leg MCs based on the results of Part I of this paper. The digital DCSV-based modulation strategy is implemented in a DSP-field programmable gate array (FPGA) platform. The objective is to reduce the calculation time of the modulation algorithm by performing the following: 1) reducing the total number of instructions needed to implement the modulation algorithm in the DSP and 2) implementing parts of the modulation algorithm in an FPGA to generate the switching pattern. Therefore, the calculation process of the modulation algorithm is split between the DSP and the FPGA.",Three-Leg/Four-Leg Matrix Converter Generalized Modulation StrategyÉ??Part II: Implementation and Verification
"['Hye-Jin Min', 'Jong C. Park']","We propose categories of finer-grained polarity for a more effective aspect-based sentiment summary, and describe linguistic and ontological clues that may affect such fine-grained polarity. We argue that relevance for satisfaction, contrastive weight clues, and certain adver-bials work to affect the polarity, as evidenced by the statistical analysis.",Toward finer-grained sentiment identification in product reviews through linguistic and ontological analyses
"['D. K. Arvind', 'Vinod E. F. Rebello']",This paper investigates issues which impinge on the design of static instruction schedulers for micronet-based asynchronous processor (MAP) architectures. The micronet model exposes both temporal and spatial concurrency within a processor. A list scheduling algorithm is described which has been optimised with MAP-specific heuristics. Their performance on some program graphs are presented and conclusions are drawn on the suitability of MAP as targets for ILP compilers.,Static scheduling of instructions on micronet-based asynchronous processors
['Junpei Kawamoto'],"We introduce a filtering methodology based on locality-sensitive hashing LSH and whitening transformation to reduce candidate tuples between which encrypted vector databases EVDBs must compute similarity for query processing. The LSH hashing methodology is efficient for estimating similarities between two vectors. It hashes a vector space using randomly chosen vectors. We can filter vectors that are less similar to the querying vectors by recording which hashed space each vector belongs to. However, if vectors in EVDBs are found locally, then most vectors are in the same hashed space, so the filter will not work. Because we can treat those cases using whitening transformation to distribute the vectors broadly, our proposed filtering methodology will work effectively on any vector space. We also show that our filter reduces the server's query processing cost.",A Locality Sensitive Hashing Filter for Encrypted Vector Databases
"['Yasutaka Matsuo', 'Sumio Yano']","Ultrahigh-definition television (UHDTV) is the next-generation ultrahigh-resolution video system with more than 4000 lines, and 7680 horizontal pixels ?? 4320 vertical lines at 60 f/s (frames per second) in video format. We propose a method of converting high-quality UHDTV video to digital cinema video that is 3840 horizontal pixels ?? 2160 vertical lines at 24 f/s. Here, it is important to detect motion vectors extremely accurately to convert the frame rate. We introduced two new algorithms to increase the accuracy of detecting motion vectors. First, we introduced a method of interpolation frames using 1/2 block-shift bi-directional motion estimation by expanding the time axis. The experimental results revealed that our new method produced higher quality video images than conventional methods of video conversion using forward-directional and overlapped block bi-directional motion estimation. Next, we focused on Bayer pattern sampling in existing UHDTV, and introduced a motion-compensated method of interpolation where the first algorithm utilizes the high-frequency components from green signals. The experimental results demonstrated that our proposed method achieves higher quality video conversions than conventional methods using only the first algorithm for motion-compensated interpolation, linear interpolation, and 5:2 frame skipping.",Converting Ultrahigh-Definition Video Into Digital Cinema by Using Time-Expanding Bi-Directional Motion Estimation and Higher Green Frequency
"['Alexandru Dancu', 'Zlatko Franjcic', 'Morten Fjeld']","While mobile phones affect our behavior and tend to separate us from our physical environment, this very environment could instead become a responsive part of the information domain. For navigation using a map while cycling in an urban environment, we studied two alternative solutions: smartphone display and projection on the road. This paper firstly demonstrates by proof-of-concept a GPS-based map navigation using a bike-mounted projector. Secondly, it implements a prototype using both a projector and a smartphone mounted on a bike, comparing them for use in a navigation system for nighttime cycling. Thirdly, it examines how visuo-spatial factors influence navigation. We believe that our findings will be useful for designing navigation systems for bikes and even for cars, helping cyclists and drivers be more attentive to their environment while navigating, and to provide useful information while moving.",Smart flashlight: map navigation using a bike-mounted projector
"['Yllias Chali', 'Sadid A. Hasan', 'Shafiq R. Joty']","In this paper, we analyze the impact of different automatic annotation methods on the performance of supervised approaches to the complex question answering problem (defined in the DUC-2007 main task). Huge amount of annotated or labeled data is a prerequisite for supervised training. The task of labeling can be accomplished either by humans or by computer programs. When humans are employed, the whole process becomes time consuming and expensive. So, in order to produce a large set of labeled data we prefer the automatic annotation strategy. We apply five different automatic annotation techniques to produce labeled data using ROUGE similarity measure, Basic Element (BE) overlap, syntactic similarity measure, semantic similarity measure, and Extended String Subsequence Kernel (ESSK). The representative supervised methods we use are Support Vector Machines (SVM), Conditional Random Fields (CRF), Hidden Markov Models (HMM), and Maximum Entropy (Max-Ent). Evaluation results are presented to show the impact.",Do Automatic Annotation Techniques Have Any Impact on Supervised Complex Question Answering
"['Xuemin Zhang', 'Wen Shan', 'Qin Xu', 'Bin Yang', 'Yunfeng Zhang']","The studies on user and mobile phone interaction have been an important problem in small screen interface design. The present study was intend to investigate the impact of three common mobile phone menu displays: matrix, tree, and page-to-page - and differential organizations of their sub-menus and functions by reaction time experiment. We also used rating scale to investigate users' preference of different mobile phone menu displays. Reaction time for accurately completing these operations was fastest when participants were presented with the matrix menu and fastest when participants were presented organizations emphasizing logical categorization rather than non-logical categorization. The result also showed consistency between users' operating efficiency and mobile phone users' interface preference. Finally, the implications of these findings for mobile phone design are discussed.",An Ergonomics Study of Menu-Operation on Mobile Phone Interface
"['Jianhua Guo', 'Liang Chu', 'Feikun Zhou', 'Libo Cao']","In this paper, an integrated vehicle dynamics control system is designed to improve vehicle handling and stability by coordinating control of Variable Torque Distribution (VTD) and Electronic Stability Program (ESP). The control system includes a coordinated controller in the upper layer and two subsystems controller in the lower layer. The control algorithm is based on the slip angle (?˝-?˝) phase plan to identify the driving situations and a rule based integrated scheme is employed to determine and allocate the control tasks between two subsystems. The simulation results demonstrate that the proposed control strategy is able to increase the tracking performance of the reference yaw rate and improve the driving steer ability of the vehicle.",Integrated control of variable torque distribution and electronic stability program based on slip angle phase
"['Yigang Wang', 'Danwei Wang', 'Bin Zhang', 'Keliang Zhou']","In repetitive control system, the period of exogenous signals must be the integer number of sampling points. However, it can not be always satisfied in real applications. In this paper, a systematic approach for non-integer delay repetitive control system with fixed sampling rate is proposed. The proposed fractional delay based repetitive control scheme employs the design techniques in digital signal processing theory and two different implementation structures are presented. One is easy to design and the other has optimized performance. Application of the proposed method to PWM DC/AC converter systems is studied to illustrate the design procedure and performance. Experimental results demonstrate the effectiveness of the proposed approach.",Fractional Delay Based Repetitive Control with Application to PWM DC/AC Converters
"['Panayotis Mertikopoulos', 'Aris L. Moustakas']","We study the distribution of traffic in networks whose users try to minimise their delays by adhering to a simple learning scheme inspired by the replicator dynamics of evolutionary game theory. The stable steady states of these dynamics coincide with the network's Wardrop equilibria and form a convex polytope whose dimension is determined by the network's redundancy (an important concept which measures the ""linear dependence"" of the users' paths). Despite this abundance of stationary points, we show that the long-term behaviour of the replicator dynamics is remarkably simple: every solution orbit converges to a Wardrop equilibrium. On the other hand, a major challenge occurs when the users' delays fluctuate unpredictably due to random external factors. In that case, interior equilibria are no longer stationary, but strict equilibria remain stochastically stable irrespective of the fluctuations' magnitude. In fact, if the network has no redundancy and the users are patient enough, we show that the long-term average of the users' traffic flows converges to the vicinity of an equilibrium, and we also estimate the corresponding invariant distribution.","Balancing traffic in networks: redundancy, learning, and the effect of stochastic fluctuations"
"['Ruediger Oehlmann', 'Balpreet Gill']","Typically interpersonal relationships in teams are investigated with questionnaires, interviews or observational studies, methods which lack either depth or are very costly. This affects the chances for improving such relationships that can be discovered. The first part of the paper describes a system that takes a diagrammatic approach to acquiring data on interpersonal relationships in teams. It will be argued that this approach is suitable to acquire data with less cost than conventional methods. The second part describes the analysis of relationships in a security team that utilizes the approach. The results of the analysis clearly identify strength and weaknesses of the intra-team relationships, and indicate that the approach is useful in organizational settings but also as research tool for investigating intra-group relationships in social psychology.",A diagrammatic approach to discovering chances in team relationships
"['Bongjun Ko', 'Dan Rubenstein']","Quality of service for high-bandwidth or delay-sensitive applications in the Internet, such as streaming media and online games, can be significantly improved by replicating server content. We present a decentralized algorithm that allocates server resources to replicated servers in large-scale client-server networks to reduce network distance between each client and the nearby replicated server hosting the resources of interest to that client. Preliminary simulation results show that our algorithm converges quickly to an allocation that reduces the expected client-server distance by almost half compared to the distance when the assignment of replicated servers is done at random.",Distributed server replication in large scale networks
"['Rui-Sheng Wang', 'Guangxu Jin', 'Xiang-Sun Zhang', 'Luonan Chen']","Transcriptional regulation is a fundamental process in biological systems, where transcription factors (TFs) play crucial roles. Except for TFs, an increasing number of small non-coding RNAs (ncRNAs) have been shown to mediate post-transcriptional processes in both prokaryotes and eukaryotes. In this work, we propose a novel approach to infer the activities of regulators including TFs and ncRNAs by exploring target gene expression profiles and (post) transcriptional regulatory relationships. The inference process is efficiently achieved by an iteration algorithm, in which two linear programming models are iteratively solved. In contrast to the existing works, for the first time, the effects of ncRNAs on transcription process are considered and thus more reasonable inference can be expected. Experiments on a model system of E. coli carbon source transition from glucose to acetate illustrate the effectiveness of our method.",Reconstruction of Regulator Activity in E. coli Post-Transcription Processes
"['Yiu Wai Lai', 'Joshua E.-Y. Lee']","Metal resistive heater on dielectric membrane structures are common in MEMS. In this paper, the evolution of the surface topography of this type of structure during operation is studied by in situ digital holographic microscopy with nanometer-scale resolution. Devices of a typical design with platinum resistive heater lying on 200 nm silicon nitride membrane were fabricated by standard MEMS processes. A permanent out-of-plane surface deformation up to 200 nm could be detected when applying heating cycles via real-time in situ images of the device surface profile. Such deformation bears the risk of failure in the thin membrane device.",In situ study of thermal deformation of metal resistive heater on silicon nitride membrane by digital holographic microscopy
"['Rui Zhang', 'Ying-Chang Liang']","A new form of multiuser diversity, named multiuser interference diversity, is investigated for opportunistic communications in cognitive radio (CR) networks by exploiting the mutual interference between the CR and the existing primary radio (PR) links. The multiuser diversity gain and ergodic throughput are analyzed for different types of CR networks and compared against those in the conventional networks without the PR link.",Investigation on multiuser diversity in spectrum sharing based cognitive radio networks
['Evgeny Kharitonov'],"Matrix factorization methods have proved to be very efficient in collaborative filtering tasks. Regularized empirical risk minimization with squared error loss function and L2-regularization and optimization performed via stochastic gradient descent (SGD) is one of the most widely used approaches.#R##N##R##N#The aim of the paper is to experimentally compare some modifications of this approach. Namely, we compare Huber's, smooth e-insensitive and squared error loss functions. Moreover, we investigate a possibility to improve the results by applying a more sophisticated optimization technique - stochastic meta-descent (SMD) instead of SGD.",Empirical study of matrix factorization methods for collaborative filtering
"['James R. Kroes', 'Yuwen Chen', 'Paul Mangiameli']","The Port of Davisville, located at Quonset Point, Rhode Island, is a former US Navy facility that was turned over to Rhode Island for commercial development when the naval base closed in 1974. Since then, a number of proposals have been put forth to expand the portÉ??s operations to include the handling of containerized cargo. The Port of DavisvilleÉ??s managing organization, the Quonset Development Corporation QDC, partnered with this academic research team to objectively analyze the viability of three proposals: 1 a major expansion of the port to make it an international container megaport, 2 a lesser investment to make it a regional international port of entry for containers, and 3 a minor expansion to make it a short-sea shipping container port. We estimated the potential demand for each expansion option using transportation cost optimization models. QDC used our studyÉ??s demand estimation in its request for grant funds from the US Department of TransportationÉ??s Transportation Investment Generating Economic Recovery program. As a result, QDC received $22.3 million to support the development of short-sea container freight shipping services at the Port of Davisville.",Estimating Demand for Container Freight Service at the Port of Davisville
"['Fan Zhang', 'Ciprian Docan', 'Manish Parashar', 'Scott Klasky', 'Norbert Podhorszki', 'Hasan Abbasi']","Emerging scientific application workflows are composed of heterogeneous coupled component applications that simulate different aspects of the physical phenomena being modeled, and that interact and exchange significant volumes of data at runtime. With the increasing performance gap between on-chip data sharing and off-chip data transfers in current systems based on multicore processors, moving large volumes of data using communication network fabric can significantly impact performance. As a result, minimizing the amount of inter-application data exchanges that are across compute nodes and use the network is critical to achieving overall application performance and system efficiency. In this paper, we investigate the in-situ execution of the coupled components of a scientific application workflow so as to maximize on-chip exchange of data. Specifically, we present a distributed data sharing and task execution framework that (1) employs data-centric task placement to map computations from the coupled applications onto processor cores so that a large portion of the data exchanges can be performed using the intra-node shared memory, (2) provides a shared space programming abstraction that supplements existing parallel programming models (e.g., message passing) with specialized one-sided asynchronous data access operators and can be used to express coordination and data exchanges between the coupled components. We also present the implementation of the framework and its experimental evaluation on the Jaguar Cray XT5 at Oak Ridge National Laboratory.",Enabling In-situ Execution of Coupled Scientific Workflow on Multi-core Platform
"['Yee Seng Chan', 'Hwee Tou Ng']","When a word sense disambiguation (WSD) system is trained on one domain but applied to a different domain, a drop in accuracy is frequently observed. This highlights the importance of domain adaptation for word sense disambiguation. In this paper, we first show that an active learning approach can be successfully used to perform domain adaptation of WSD systems. Then, by using the predominant sense predicted by expectation-maximization (EM) and adopting a count-merging technique, we improve the effectiveness of the original adaptation process achieved by the basic active learning approach.",Domain Adaptation with Active Learning for Word Sense Disambiguation
"['Yi Zhang', 'Li Zeng', 'Yanhua Li', 'Quanjie Liu']","With the development of the multi-robot coordination, the working efficiency of the multi-robot system is improved, and the tasks can be finished better. Multi-robot coordination is one of the important issues for the research of mobile robot. The research of formation control of multi-robot will improve the efficiency of the coordination of multi-robot. So the problem of multi-robot formation is a typical problem of the system of robot team formation, and also communications play an important role in mobile robot systems able to address real world applications. Mobile ad-hoc networks are characterized by self-organization, rapid deployment and fault tolerance. A multi-robot formation supported by mobile Ad-hoc networks is suitable to some special situations where the communication devices of mobile networks cannot be preinstalled. The technology of leader-follower method has been very mature and the effect is good, and could be fast respond. However one issue with the typical leader-follower strategy is the lack of inter-robot information feedback throughout the group. And at present, the research of formation control only based on communication in real-time is not high, and the effect is bad. As the reason mentioned above, in this paper, we combined the leader-follower method with ad-hoc network to solve the problem of real-time formation. Then the robots can be in their lines correctly and quickly and arrive at the goal position. Computerized simulation results show that the approach is feasible.",Multi-robot formation control using leader-follower for MANET
['Pascal Bondon'],The problem of minimum mean-square infinite extent interpolation for discrete-time stationary complex stochastic processes is studied. The interpolator consists of linear combinations of samples of the process and of their complex conjugate. The expressions of the interpolator and of the approximation error are derived and various consequences are examined. It is shown in particular that the approximation error may be zero while the interpolation error obtained when using only linear combinations of the samples is maximum.,Interpolation of complex stationary processes
"['Kil-Woong Jang', 'Ki Jun Han']","In this paper, we propose a new channel assignment scheme, which is a hybrid of the channel reservation scheme and the channel carrying scheme. It is designed to efficiently carry out handoffs in wireless mobile networks. We evaluated our scheme with a two-cell model using both Markov analysis and computer simulation. The analytical and simulation results indicate our scheme may offer better performance than the conventional reservation scheme and channel carrying scheme in terms of handoff blocking probability and channel utilization.",A Channel Assignment Scheme for Handoff in Wireless Mobile Networks
"['Tomaso Aste', 'N. Rivier']","The authors model the structure of space-filling disordered cellular systems. These systems are cellular networks with minimum incidence numbers (D+1 edges incident on a vertex in D-dimension). In the literature such systems are known as froths since the soap froth is the archetype of these structures. They present a method where the structure of froths is analyzed as organized in concentric layers of cells around a given, arbitrary, central cell. A simple map gives, by recursion, the number of cells in each layer. The map has one parameter, given as a function of the average topological properties of the cells in the neighbouring layers. From the behaviour of the number of cells per layer with the topological distance, one obtains the curvature of the space tiled by the froth. By using the map it is therefore possible to characterize the shape of the manifold tiled by the froth in term of the topological arrangements of its tiles. In two dimensions, they propose a method to deduce the Gaussian curvature of surfaces from a set of sampled points. In three dimensions, they use the map to investigate the freedom in constructing disordered Euclidean cellular structures. Among the closed packed structures, they find the average shape of the cells that maximize this freedom in filling space.",Topological modelling of disordered cellular structures
"['Giacomo Bucci', 'B. Fedeli', 'Enrico Vicario']","A modeling and validation approach extending the formalism of timed Petri nets (TPN) for the analysis of real time systems with flexible scheduling capabilities is introduced. The new formalism is called AdaptiveTPNs. State space analysis of the model supports exhaustive prediction of the time needed to complete critical functions, and permits automatic identification of loading conditions which determine the reduction of the quality of produced results.",Predicting timeliness of reactive systems under flexible scheduling
"['Xuanzhe Liu', 'Li Zhou', 'Gang Huang', 'Hong Mei']","Nowadays, there are a number of similar Web services over the Internet or intranet. They provide consumers with more choices according to their personalized QoS requirements. However, in current web service discovery and subscription, it takes consumers too much time on manual selection and cannot easily benefit from the wide QoS spectrum brought by the proliferating services. In this paper, we propose a QoS-aware discovery and subscription approach to free consumers from time-consuming human computer interactions as well as help them negotiate QoS with multiple service providers. The core idea of this approach is to build up a ""virtual service"" grouping function similar services together (called service pool) and dispatching consumer requests to the proper service in terms of QoS requirements. This paper makes contributions for the aggregation and usage of similar web services in a ""consumer-centric"" manner. Such manner is on-demand, user-friendly and efficient. On-demand means the aggregate is driven by consumers instead of providers. User-friendly means consumers do not select services, handle different WSDL of similar services and switch between services at runtime any longer. Efficient means it integrates an efficient service search engine, reduces the incorrect services by some filter, discover the aggregate by a polynomial complex algorithm.",Consumer-Centric Web Services Discovery and Subscription
"['Sarah P. Everett', 'Michael D. Byrne']","Users of modern GUIs routinely engage in visual searches for various control items, such as buttons and icons. Because this is so ubiquitous, it is important that the visual properties of user interfaces support such searches. The current research is aimed at deepening our understanding of how the visual spacing between icons affects visual search times. We constructed an experiment based on previous icon sets [8] where spacing between icons was systematically manipulated, and for which we had a computational cognitive model that predicted performance. In particular, the model predicted that larger spacing would lead to slower search times. While this prediction was borne out, there was an unanticipated finding: users in this new experiment were substantially slower than in previous similar experiments with smaller spacing. In fact, results from this new experiment were better fit with a model that employed a fundamentally different, and less efficient, search strategy. A second experiment was conducted to explicitly test the surprising result that this varied and larger icon spacing would lead to increased search times. Results were consistent with this hypothesis. These results imply that while small differences in visual layout may not intrinsically produce large differences in user performance, they may cause users to adopt suboptimal strategies that do produce such differences.",Unintended effects: varying icon spacing changes users' visual search strategy
"['Sriram V. Pemmaraju', 'Imran A. Pirwani']","Using a dominating set as a coordinator in wireless networks has been proposed in many papers as an energy conservation technique. Since the nodes in a dominating set have the extra burden of coordination, energy resources in such nodes will drain out more quickly than in other nodes. To maximize the lifetime of nodes in the network,it has been proposed that the role of coordinators be rotated among the nodes in the network. One abstraction that has been considered for the problem of picking a collection of coordinators and cycling through them, is the  domatic partition problem . This is the problem of partitioning the set of the nodes of the network into dominating sets with the aim of maximizing the number of dominating sets. In this paper,we consider the  k -domatic partition problem . A  k -dominating set  is a subset D of nodes such that every node in the network is at distance at most  k  from  D . The  k-domatic partition problem  seeks to partition the network into maximum number of  k -dominating sets.We point out that from the point of view of saving energy,it may be better to construct a  k -domatic partition for  k  >1.We present three deterministic, distributed algorithms for finding large  k -domatic partitions for  k  > 1. Each of our algorithms constructs a  k -domatic partition of size at least a constant fraction of the largest possible ( k  1)-domatic partition. Our first algorithm runs in constant time on unit ball graphs (UBGs) in Euclidean space assuming that all nodes know their positions in a global coordinate system. Our second algorithm drops knowledge of global coordinates and instead assumes that pairwise distances between neighboring nodes are known. This algorithm runs in  O (log*  n  ) time on UBGs in a metric space with constant doubling dimension. Our third algorithm drops all reliance on geometric information, using connectivity information only. This algorithm runs in  O (log ?? ?˙ log * n ) time on growth-bounded graphs. Euclidean UBGs, UBGs in metric spaces with constant doubling dimension, and growth-bounded graphs are successively more general models of wireless networks and all three models include the well-known, but somewhat simplistic wireless network models such as unit disk graphs.",Energy conservation via domatic partitions
"['Peter Hammarberg', 'Fredrik Rusek', 'Pierluigi Salvo Rossi', 'Ove Edfors']","In this paper we evaluate, by means of Extrinsic Information Transfer (EXIT) charts, an iterative receiver that has emerged as a promising candidate for non-coherent multi-user multi-antenna OFDM systems. The receiver performs parallel interference cancellation (followed by linear filtering) and channel estimation, using soft symbols obtained from a bank of single-user decoders. For the sake of conceptual clarity we study a system with two single antenna users and a receiver with two antennas, and we demonstrate how the convergence behavior of the receiver can be visualized using paired three dimensional EXIT surfaces. Our results show that the actual decoder trajectories obtained through simulations are well predicted from the EXIT charts.#R##N##R##N#For the iterative receiver under investigation we identify a very specific problem with EXIT chart generation; the EXIT curve for the inner component decoder depends on the outer encoder. To handle this problem we propose a modification to the iterative receiver which solves the aforementioned problem; the performance degradation is demonstrated to be small.",EXIT Chart Evaluation of a Receiver Structure for Multi-User Multi-Antenna OFDM Systems
"['Karin Edvardsson', 'Sven Ove Hansson']","In decision theory goals are usually taken as given inputs to the analysis, and the focus is on finding the most efficient means to achieve the goals. But where goals are set with the purpose of ac ...",When is a goal rational
"['Adrian Kosowski', 'Ichiro Suzuki', 'Pawel Zylinski']","Consider an orthogonal grid of streets and avenues in a Manhattan-like city populated by stationary sensor modules at some crossings and mobile robots that can serve as relays of information that the modules exchange. Both module-module and module-robot communication is limited to a straight line of sight along a row or a column of the grid. We present a number of distributed algorithms for the robots to establish a connected network of a given set S of modules by moving to suitable locations in the grid and serving as relays. It is shown that the number of robots required to connect the modules depends not only on the number c of connected components in the visibility graph of S, but also on the degree of symmetry in S. In most cases, our algorithms use the worst case optimal number of robots for a given c.",Forming a connected network in a grid by asynchronous and oblivious robots
"['Sajina Pradhan', 'Sun-Kuk Noh', 'Dong-You Choi']","In communication systems, there are various types of microstrip antenna that can be used for many applications. This paper mainly focuses on the simple design of an inset rectangular microstrip patch antenna to operate at a frequency of 2.45 GHz for rectenna design. The study involves using an high frequency structure simulator to design the antenna dimensions and to determine its performance. This antenna is based on a thickness of 1.6 mm flame retardant 4 (FR-4) substrate having a dielectric constant of approximately 4.7, an inset feed, and a ground plane. After simulation, the antenna performance characteristics such as its return loss, voltage standing wave ratio, gain, and radiation pattern were obtained and compared with the fabricated measured antenna.",Design of Inset Microstrip Patch Antenna for Wireless Power Transmission at 2.45 GHz
"['Marco Antonio Montes de Oca', 'Thomas St?¨tzle', 'Ken Van den Enden', 'Marco Dorigo']","Incremental social learning (ISL) was proposed as a way to improve the scalability of systems composed of multiple learning agents. In this paper, we show that ISL can be very useful to improve the performance of population-based optimization algorithms. Our study focuses on two particle swarm optimization (PSO) algorithms: a) the incremental particle swarm optimizer (IPSO), which is a PSO algorithm with a growing population size in which the initial position of new particles is biased toward the best-so-far solution, and b) the incremental particle swarm optimizer with local search (IPSOLS), in which solutions are further improved through a local search procedure. We first derive analytically the probability density function induced by the proposed initialization rule applied to new particles. Then, we compare the performance of IPSO and IPSOLS on a set of benchmark functions with that of other PSO algorithms (with and without local search) and a random restart local search algorithm. Finally, we measure the benefits of using incremental social learning on PSO algorithms by running IPSO and IPSOLS on problems with different fitness distance correlations.",Incremental Social Learning in Particle Swarms
"['Bo G??ransson', 'Bo Hagerman', 'Sven Petersson', 'Joakim Sorelius']","This contribution presents some link and system level simulation results obtained by a WCDMA base station equipped with an adaptive antenna system. The results show that the performance gain obtained by an advanced antenna system could be substantial compared to an ordinary sector covering system. The link simulations show that for a given SIR target in a single cell system, the capacity is increased considerably compared to a conventional single sector covering antenna system. It is also shown that the difference between the different adaptive antenna algorithms is marginal. The system level simulations have shown that the capacity performance is basically proportional to the number of orthogonal beams used in the base station. In cases where the conventional three sector reference system is not truly interference limited, the performance gain can be even higher. The system simulations have also shown that using more beam positions than those defining the set of orthogonal fixed beams slightly improves the performance. However the number of beam positions can be kept relatively small, in the order of twice the number of orthogonal beams, without significant loss.",Advanced antenna systems for WCDMA: link and system level results
"['Emin Kugu', 'Jiang Li', 'Frederic D. McKenzie', 'Ozgur Koray Sahingoz']","A crowd is a group of people attending a public gathering with some joint purpose, such as protesting against the government or celebrating an event. In some countries, these kinds of activities are the only way to express public displeasure with their government. The government's reactions to such activities may or may not be tolerant. For this reason, such situations must be eliminated by recognizing when and how they are likely to occur, and then providing guidelines to mitigate them. In urban areas, police and military forces use non-lethal weapons (NLWs), such as rubber bullets or clubs, to control a violent and destructive crowd. In order to estimate the results of this engagement, ensuring minimum injuries and reaching an optimal end state, simulating such actions in a virtual environment is necessary. In this work, a fuzzy logic-based crowd injury model for determining the physical effects of NLWs is proposed. Fuzzy logic concepts can be applied to a problem by using linguistic rules, which are determined by problem domain experts. A group of police and military officers were consulted for a set of injury model rules, and those rules were then included in the simulation platform. Sensitivity analysis has been conducted to analyze parameters in the model. As a proof of the concept, a prototype system was implemented using the Repast Simphony agent-based simulation toolkit. Simulation results illustrated the effectiveness of the simulation framework.",Fuzzy logic approach and sensitivity analysis for agent-based crowd injury modeling
"['Dapeng Li', 'Jing Liu', 'Youyun Xu', 'Xinbing Wang', 'Wen Chen']","In this paper, we address the incentive-based relay-selection problem over multi-source and multi-relay wireless networks. A two-side market game approach is employed to jointly consider the benefits of all sources and relays. The equilibrium concept in such games is called core. The outcomes in the core of the game cannot be improved upon by any subset of players. These outcomes correspond exactly to the price-lists that competitively balance the benefits of all sources and relays. When the price assumes only discrete values, the core of the game is defined as discrete core. The Distributed Source-Relay Assignment (DSRA) algorithm is proposed for competitive price adjustment and converges to the discrete core of the game. With small enough measurement of price, the algorithm can achieve the optimal performance compared with centralized one in terms of total profit of the system.",Distributed Relay-Source Matching for Cooperative Wireless Networks Using Two-Sided Market Games
"['Aaron Halfaker', 'John Riedl']","Bots and cyborgs are more than tools to better manage content quality on Wikipedia-through their interaction with humans, they're fundamentally changing its culture.",Bots and Cyborgs: Wikipedia's Immune System
"['Ceren Budak', 'Divyakant Agrawal', 'Amr El Abbadi']","The identification of popular and important topics discussed in social networks is crucial for a better understanding of societal concerns. It is also useful for users to stay on top of trends without having to sift through vast amounts of shared information. Trend detection methods introduced so far have not used the network topology and has thus not been able to distinguish viral topics from topics that are diffused mostly through the news media. To address this gap, we propose two novel structural trend definitions we call coordinated and uncoordinated trends that use friendship information to identify topics that are discussed among clustered and distributed users respectively. Our analyses and experiments show that structural trends are significantly different from traditional trends and provide new insights into the way people share information online. We also propose a sampling technique for structural trend detection and prove that the solution yields in a gain in efficiency and is within an acceptable error bound. Experiments performed on a Twitter data set of 41.7 million nodes and 417 million posts show that even with a sampling rate of 0.005, the average precision is 0.93 for coordinated trends and 1 for uncoordinated trends.",Structural trend analysis for online social networks
"['Parijat Dube', 'Rahul Jain']","This paper is motivated by study of the economics of Quality of Service (QoS) of congestible services. We introduce a queueing game framework to study such problems. We consider multiple competing providers, each offering a queued service. Users are sensitive to both access price and expected delay, and pick providers with the smallest price plus delay cost. We study equilibrium of the pricing (Bertrand) game between the congestible network service providers. We establish the existence of a Nash equilibrium under some natural assumptions. We then consider a setting with multiple classes of differentiated service. Differentiated Services (DiffServ) technologies of the Internet that can provide QoS guarantees have failed to catch on, primarily due to economic impediments. Each provider is now modeled as operating a multi-class queue. We provide sufficient conditions for the existence of a Nash equilibrium in the Bertrand (pricing) game between the providers. We characterize the inefficiency (price of anarchy) due to strategic pricing to be 2/3. Surprisingly, the price of anarchy for the multi-class setting is the same as for the single-class setting.",Bertrand equilibria and efficiency in markets for congestible network services
"['Aaron Block', 'James H. Anderson']","We consider the problem of task reweighting in fair-scheduled multiprocessor systems wherein each task's processor share is specified using a weight. The responsiveness of a reweighting scheme can be assessed by comparing its allocations to those of an ideal scheduler that instantly reweights task. A reweighting scheme is fine-grained if the per-task ""error"" (in comparison to an ideal allocation) caused by a reweighting event is constant, and coarse-grained, otherwise. When the number of tasks N is larger than the number of processors M, the worst-case time complexity for fine-grained reweighting, /spl Omega/(NlogN), is larger than that of coarse-grained reweighting, /spl Theta/(MlogN). In this paper, we construct two new reweighting algorithms that are hybrids of fine- and coarse-grained reweighting that have time complexity less than /spl Theta/(NlogN), and produce less error than current coarse-grained techniques. We also present experiments to compare relative advantages of all schemes.",Task reweighting on multiprocessors: efficiency versus accuracy
"['Mohammad Izadi', 'Marcello M. Bonsangue']","Constraint automata have been proposed as the operational semantics of Reo, a glue-code language for the exogenous composition and orchestration of components in a software system. In this paper we recast the theory of constraint automata into that of Buchi automata on infinite strings of records. We use records to express simultaneity constraints of I/O operations and show that every constraint automaton can be expressed as a Buchi automaton on an appropriate alphabet of records. Further, we give examples of component compositions that are expressible as Buchi automata but not as constraint automata. Finally, we show that the join composition operator for constraint automata and its counterpart for Buchi automata of records can be expressed as two basic operations on Buchi automata: alphabet extension and product.",Recasting Constraint Automata into B?¨chi Automata
"['Michel Hurfin', 'Frederic Tronel']","Chandra and Toueg (1996) have proposed a new approach to overcome the impossibility of deterministically reaching consensus in asynchronous systems subject to crash failures. They augment the asynchronous model with unreliable failure detectors. We present an extension of an algorithm that they proposed to solve consensus using /spl square/S failure detectors. We argue that this extension is a simple and efficient building block which can be used to solve various agreement problems. We consider a particular agreement problem, namely the non-blocking atomic commitment problem and we show the advantages of our solution by comparing it to other classical approaches.",A solution to atomic commitment based on an extended consensus protocol
"['Xiaobo Long', 'Biplab Sikdar']","In wireless communications, shadow fading can cause at least 6 dB power loss for 10% of the time [1]. Early detection of shadow fading plays an important part in facilitating the design of adaptive data transmission schemes. We propose an accurate, on-line detection mechanism to detect a receiver entering or leaving shadow regions using simply signal strength measurements. The method is based on wavelet decomposition of signal strength time series into independent fading components. Our measurements indicate that fast fading signals have a scale invariant nature. This scale invariance of fading signals is destroyed when shadow fading, which is log-normal distributed and is independent of fast fading components, is added to the signal. An online detection mechanism is then proposed which exploits this phenomenon. Real measurements of signal strength traces are used to validate the detection mechanism.",Wavelet Based Detection of Shadow Fading in Wireless Networks
"['Kar Wai Lim', 'Wray L. Buntine']","Bibliographic analysis considers authorÉ??s research areas, the citation network and paper content among other things. In this paper, we combine these three in a topic model that produces a bibliographic model of authors, topics and documents using a non-parametric extension of a combination of the Poisson mixed-topic link model and the author-topic model. We propose a novel and ecient inference algorithm for the model to explore subsets of research publications from CiteSeer X . Our model demonstrates improved performance in both model tting and a clustering task compared to several baselines.",Bibliographic Analysis with the Citation Network Topic Model
"['Myra Spiliopoulou', 'Carsten Pohle']","For many companies, competitiveness in e-commerce requires a successful presence on the web. Web sites are used to establish the company's image, to promote and sell goods and to provide customer support. The success of a web site affects and reflects directly the success of the company in the electronic market. In this study, we propose a methodology to improve the É??successÉ?ù of web sites, based on the exploitation of navigation pattern discovery. In particular, we present a theory, in which success is modelled on the basis of the navigation behaviour of the site's users. We then exploit WUM, a navigation pattern discovery miner, to study how the success of a site is reflected in the users' behaviour. With WUM we measure the success of a site's components and obtain concrete indications of how the site should be improved. We report on our first experiments with an online catalog, the success of which we have studied. Our mining analysis has shown very promising results, on the basis of which the site is currently undergoing concrete improvements.",Data Mining for Measuring and Improving the Success of Web Sites
"['A.R. Calderbank', 'Lawrence H. Ozarow']","Signaling schemes for the Gaussian channel based on finite-dimensional lattices are considered. The signal constellation consists of all lattice points within a region R, and the shape of this region determines the average signal power. Spherical signal constellations minimize average signal power, and in the limit as N to infinity , the shape gain of the N-sphere over the N-cube approaches pi e/6 approximately=1.53 dB. A nonequiprobable signaling scheme is described that approaches this full asymptotic shape gain in any fixed dimension. A signal constellation, Omega is partitioned into T subconstellations Omega /sub 0/, . . ., Omega /sub tau -1/ of equal size by scaling a basic region R. Signal points in the same subconstellation are used equiprobably, and a shaping code selects the subconstellation Omega /sub i/ with frequency f/sub i/. Shaping codes make it possible to achieve any desired fractional bit rate. The schemes presented are compared with equiprobable signaling schemes based on Voronoi regions of multidimensional lattices. For comparable shape gain and constellation expansion ratio, the peak to average power ratio of the schemes presented is superior. Furthermore, a simple table lookup is all that is required to address points in the constellations. It is also shown that it is possible to integrate coding and nonequiprobable signaling within a common multilevel framework. >",Nonequiprobable signaling on the Gaussian channel
"['Scott A. Sallberg', 'Peter S. Maybeck', 'Mark E. Oxley']","In this paper we apply the infinite-dimensional sampled-data Kalman filter (ISKF) [1], [2] to a system characterized by the stochastic heat equation for the purpose of estimating the temperature distribution along a slender (one-dimensional) cylindrical rod using a simple linear measurement model. The key to applying the ISKF is the development of an essentially equivalent finite-dimensional discrete-time model from an infinite-dimensional continuous-time dynamics model. In addition to estimating the temperature of the rod, we employ a bank of elemental filters via the multiple model adaptive estimation (MMAE) technique to estimate unknown model parameters such as the thermal diffusivity constant of the slender cylindrical rod.",Infinite-dimensional sampled-data Kalman filtering and the stochastic heat equation
"['Babita Pandey', 'R. B. Mishra']","Knowledge-based systems (KBS) and intelligent computing systems have been used in the medical planning, diagnosis and treatment. The KBS consists of rule-based reasoning (RBR), case-based reasoning (CBR) and model-based reasoning (MBR) whereas intelligent computing method (ICM) encompasses genetic algorithm (GA), artificial neural network (ANN), fuzzy logic (FL) and others. The combination of methods in KBS such as CBR-RBR, CBR-MBR and RBR-CBR-MBR and the combination of methods in ICM is ANN-GA, fuzzy-ANN, fuzzy-GA and fuzzy-ANN-GA. The combination of methods from KBS to ICM is RBR-ANN, CBR-ANN, RBR-CBR-ANN, fuzzy-RBR, fuzzy-CBR and fuzzy-CBR-ANN. In this paper, we have made a study of different singular and combined methods (185 in number) applicable to medical domain from mid 1970s to 2008. The study is presented in tabular form, showing the methods and its salient features, processes and application areas in medical domain (diagnosis, treatment and planning). It is observed that most of the methods are used in medical diagnosis very few are used for planning and moderate number in treatment. The study and its presentation in this context would be helpful for novice researchers in the area of medical expert system.",Knowledge and intelligent computing system in medicine
"['Andreas Leitner', 'Ilinca Ciupa', 'Manuel Oriol', 'Bertrand Meyer', 'Arno Fiva']","Although unit tests are recognized as an important tool in software development, programmers prefer to write code, rather than unit tests. Despite the emergence of tools like JUnit which automate part of the process, unit testing remains a time-consuming, resource-intensive, and not particularly appealing activity.This paper introduces a new development method, called Contract Driven Development. This development method is based on a novel mechanism that extracts test cases from failure-producing runs that the programmers trigger. It exploits actions that developers perform anyway as part of their normal process of writing code. Thus, it takes the task of writing unit tests off the developers' shoulders, while still taking advantage of their knowledge of the intended semantics and structure of the code. The approach is based on the presence of contracts in code, which act as the oracle of the test cases. The test cases are extracted completely automatically, are run in the background, and can easily be maintained over versions. The tool implementing this methodology is called Cdd and is available both in binary and in source form.",Contract driven development = test driven development - writing test cases
['Mihailo Stojnic'],"Recently, [3], [7] theoretically analyzed the success of a polynomial l 1  optimization algorithm in solving an under-determined system of linear equations. In a large dimensional and statistical context [3], [7] proved that if the number of equations (measurements in the compressed sensing terminology) in the system is proportional to the length of the unknown vector then there is a sparsity (number of non-zero elements of the unknown vector) also proportional to the length of the unknown vector such that l 1  optimization succeeds in solving the system. In this paper, we consider the same problem while additionally assuming that all non-zero elements elements are equal to each other. We provide a performance analysis of a slightly modified l 1  optimization. As expected, the obtained recoverable sparsity proportionality constants improve on the equivalent ones that can be obtained if no information about the non-zero elements is available. In addition, we conducted a sequence of numerical experiments and observed that the obtained theoretical proportionality constants are in a solid agreement with the ones obtained experimentally.",Recovery thresholds for É?? 1 optimization in binary compressed sensing
"['Guido Boella', 'Leonardo Lesmo', 'Rossana Damiano']","This article describes an ontological model of norms. The basic assumption is that a substantial part of a legal system is grounded on the concept of agency. Since a legal system aims at regulating a society, then its goal can be achieved only by affecting the behaviour of the members of the society. We assume that a society is made up of agents (which can be individuals, institutions, software programs, etc.), that agents have beliefs, goals and preferences, and that they commit to intentions in order to choose a line of behaviour. The role of norms, within a legal system, is to specify how and when the chosen behaviour agrees with the basic principles of the legal system. In this article, we show how a model based on plans can be the basis for the ontological representation of norms, which are expressed as constraints on the possible plans an agent may choose to guide its behaviour. Moreover, the paper describes how the proposed model can be linked to the upper level of a philosophically well-founded ontology (DOLCE); in this way, the model is set in a wider perspective, which opens the way to further developments.",On the Ontological Status of Plans and Norms
['Kevin C. Baird'],"This software tool, developed in Max/MSP, presents performers with image files consisting of traditional notation as well as conducting in the form of video playback. The impetus for this work was the desire to allow the musical material for each performer of a given piece to differ with regard to content and tempo.",Multi-conductor: an onscreen polymetrical conducting and notation display system
"['Yang Yang', 'Xuesong Qiu', 'Luoming Meng', 'Lanlan Rui']","In a clustering-based MANETs, task allocation has posed increasing research challenges because the needs of management and coordination are accentuated by complicated demands of cluster members. A self-adaptive method of task allocation is designed to facilitate self-planning and self-negotiation for nodes during tasks being distributed and executed. The method is composed of two parts: for one part, the cluster head works out an integrated schedule for tasks, including selecting different sets of execution nodes and defining their functions according to task types. Cooperative group towards synergetic task is formed by policies of filtering and voting. Assignment modes based on either polling or mobile agents are also involved, the latter adopts an improved Ant Colony Optimization (ACO) algorithm to plan a migration path. For another, if a cluster member fails to accomplish a task, it could negotiate as a tenderee with other nodes using a revised contract net protocol. In addition, we employ a stimulation mechanism of distributing virtual task experience in connection with QoS guarantees to offer compensation for nodes' energy consumption and extra load. Simulation results demonstrate performance benefits of our self-adaptive method can efficaciously alleviate load of the cluster head, balance loads of nodes in consideration of energy restriction, and prolong the lifecycle of the cluster.",A self-adaptive method of task allocation in clustering-based MANETs
"['Jae-Bok Kim', 'Jeong-Sik Park', 'Yung-Hwan Oh']","This paper proposes a new Speech Emotion Recognition (SER) framework. Compared to the speaker-independent emotion models, speaker-adapted models constructed by using a speaker's emotional speech data can represent the speaker's emotional characteristics more precisely, thus improving SER accuracy. However, it is hard to collect a sufficient amount of personal emotional data at once. For this reason, we propose an MLLR-based online speaker adaptation technique using accumulated personal data. Compared to speech models, it is relatively difficult to construct reliable emotion models applicable to MLLR due to the domain-oriented characteristics. Thus, we modify the conventional MLLR procedure by using selective label refinement, which categorizes newly accumulated adaptation data into discriminative and non-discriminative data, and only refines the labels of the discriminative data. On SER experiments based on an LDC emotion corpus, our approach exhibited superior performance when compared to conventional adaptation techniques as well as the speaker-independent model framework. 1",On-line speaker adaptation based emotion recognition using incremental emotional information
"['Darrell W. Starks', 'Todd C. Whyte']","This tutorial article addresses the use of simulation in the hospitality industry, in particular the use of simulation in the fast food restaurant industry. Although the application of simulation to any system follows the same basic series of steps in an iterative fashion, there are unique aspects of the fast food restaurant industry that must be addressed when applying simulation to that industry's needs. In particular in the data collection, model building and in the validation steps there seem to be somewhat unique problems due to the wide variability of the same restaurant in different locations. Another aspect that is addressed is the use of simulation in one of two ways, either as a planning tool at a central location in a particular company versus a field based tool. All in all simulation is just coming into widespread use in this industry and will be a valuable analysis tool for an industry with wide variability among its various components.",Tutorial: simulation in the hospitality industry
"['Sajjad Baloch', 'Hamid Krim', 'Irina A. Kogan', 'Dmitry V. Zenkov']","In this paper, we propose a numerical algorithm for extracting the topology of a three-dimensional object (2 dimensional surface) embedded in a three-dimensional space /spl Ropf//sup 3/. The method is based on capturing the topology of a modified Reeb graph by tracking the critical points of a distance function. As such, the approach employs Morse theory in the study of translation, rotation, and scale invariant skeletal graphs. The latter are useful in the representation and classification of objects in /spl Ropf//sup 3/.",Rotation invariant topology coding of 2D and 3D objects using Morse theory
"['Rosangela H. Loschi', 'Frederico R. B. Cruz']","The multiple change point identification problem may be encountered in many subject areas, including disease mapping, medical diagnosis, industrial control, and finance. One appealing way of tackling the problem is through the product partition model (PPM), a Bayesian approach. Nowadays, practical applications of Bayesian methods have attracted attention perhaps because of the generalized use of powerful and inexpensive personal computers. A Gibbs sampling scheme, simple and easy to implement, is used to obtain the estimates. We apply the algorithm to the analysis of two important stock market data in Brazil. The results show that the method is efficient and effective in analyzing change point problems.",APPLYING THE PRODUCT PARTITION MODEL TO THE IDENTIFICATION OF MULTIPLE CHANGE POINTS
"['Prem C. Pandey', 'Milind S. Shah']","Production of vowel-oral stop consonant-vowel utterances involves movement of articulators from the articulatory position of the initial vowel towards that of the oral stop closure, and then to that of the final vowel. As the closure segments have zero or low signal energy, linear predictive coding (LPC)-based estimation of vocal tract shape fails during stop closure. This paper reports a technique for estimation of place of articulation during stop closures by performing bivariate polynomial modeling on vocal tract area values during transition segments preceding and following the closure. The technique with second-degree polynomial modeling was found to be suitable for estimating the place of maximum constriction during stop closure segments of vowel-consonant-vowel utterances with bilabial, alveolar, and velar stops. The estimated places compared well with the actual constriction locations observed from the articulatory data. The technique may be useful for improving effectiveness of speech-training aids for production of stop consonants by providing visual feedback of the estimated place of articulation.",Estimation of Place of Articulation During Stop Closures of VowelÉ??ConsonantÉ??Vowel Utterances
"['Wuei-He Tsai', 'Wen-Whei Chang']","This study focuses on the parametric stochastic modeling of characteristic sound features that distinguish languages from one another. A new stochastic model, the so-called Gaussian mixture bigram model (GMBM), that allows exploitation of the acoustic feature bigram statistics without requiring transcribed training data is introduced. For greater efficiency, a minimum classification error (MCE) algorithm is employed to accomplish discriminative training of a GMBM-based Chinese dialect identification system. Simulation results demonstrate the effectiveness of the GMBM for dialect-specific acoustic modeling, and use of this model allows the proposed system to distinguish between the three major Chinese dialects spoken in Taiwan with 94.4% accuracy.",Discriminative training of Gaussian mixture bigram models with application to Chinese dialect identification
"['Thadpong Pongthawornkamol', 'Indranil Gupta']","Today?s large-scale distributed systems consist of collections of nodes that have highly variable availability - a phenomenon sometimes called churn. This availability variation is often a hindrance to achieving reliability and performance for distributed applications such as multicast. This paper looks into utilizing and leveraging availability information in order to provide availability-dependent message reliability for multicast receivers. An application (e.g., a publish-subscribe system) may want to scale the multicast message reliability at each receiver according to that receiver?s availability (in terms of the fraction of time that receiver is online) - different options are that the reliability is independent of the availability, or proportional to it.. We propose several gossip-based algorithms to support several such predicates. These techniques rely on each node?s availability being monitored in a distributed manner by a small group of other nodes in such a way that the monitoring load is evenly distributed in the system. Our techniques are light-weight, scalable, and are space- and timeefficient. We analyze our algorithms and evaluate them experimentally by injecting availability traces collected from real peer-to-peer systems.",AVCast : New Approaches For Implementing Availability-Dependent Reliability for Multicast Receivers
"['Marguerite C. Murphy', 'Doron Rotem']","A practical join processing strategy that allows effective utilization of arbitrary degrees of parallelism in both the I/O subsystem and join processing subsystems is presented. Analytic bounds on the minimum execution time, minimum number of processors, and processor utilization are presented along with bounds on the execution time, given a fixed number of processors. These bounds assume that sufficient buffers are available. An analytic lower bound on buffer requirements as well as a practical heuristic for use in limited buffer environments are also presented. A sampling of corroborative simulation results are included. >",Multiprocessor join scheduling
"['John Plaice', 'Blanca Mancilla']","We present a new approach for context-oriented programming in which the context is represented by a set of ( dimension, value ) pairs. This tuple parameterizes the environment, and it can be referred to either as a single entity or as a composed entity, parts of which can independently be accessed. The context is also an  index  into any programmable entity, in our model the  hyperdatons , which are in turn, arbitrary-dimensional arrays of arbitrary extent.   The context may have privileged dimensions and one such dimension is time, which has as well a physical interpretation. The importance of this dimension relies on the fact that its proper handling will allow the control of software evolution, of systems, and of system instances or views; partial changes or updates to specific parts of a system; and synchronous communications between heterogenous components or even systems. In fact, it is our tool to create synchronous Cartesian systems, essential for context-aware distributed systems.   The implementation of a Cartesian distributed system may rely on the behavior of several subsystems, all running on an internal clock necessarily infinitely faster than the external one, since a bunch of tasks in a subsystem, corresponds to one tick of the system. These subsystems all run with respect to a shared context called an  aether , which facilitates communication by broadcasting between systems at possibly different levels. The aether in this case is an active context.",The Cartesian approach to context
"['Andrew K. Rider', 'Geoffrey Siwo', 'Nitesh V. Chawla', 'Michael T. Ferdig', 'Scott J. Emrich']","Background: Complexity and noise in expression quantitative trait loci (eQTL) studies make it difficult to distinguish potential regulatory relationships among the many interactions. The predominant method of identifying eQTLs finds associations that are significant at a genome-wide level. The vast number of statistical tests carried out on these data make false negatives very likely. Corrections for multiple testing error render genome-wide eQTL techniques unable to detect modest regulatory effects. We propose an alternative method to identify eQTLs that builds on traditional approaches. In contrast to genomewide techniques, our method determines the significance of an association between an expression trait and a locus with respect to the set of all associations to the expression trait. The use of this specific information facilitates identification of expression traits that have an expression profile that is characterized by a single exceptional association to a locus. Our approach identifies expression traits that have exceptional associations regardless of the genome-wide significance of those associations. This property facilitates the identification of possible false negatives for genome-wide significance. Further, our approach has the property of prioritizing expression traits that are affected by few strong associations. Expression traits identified by this method may warrant additional study because their expression level may be affected by targeting genes near a single locus. Results: We demonstrate our method by identifying eQTL hotspots in Plasmodium falciparum (malaria) and Saccharomyces cerevisiae (yeast). We demonstrate the prioritization of traits with few strong genetic effects through Gene Ontology (GO) analysis of Yeast. Our results are strongly consistent with results gathered using genome-wide methods and identify additional hotspots and eQTLs. Conclusions: New eQTLs and hotspots found with this method may represent regions of the genome or biological processes that are controlled through few relatively strong genetic interactions. These points of interest warrant experimental investigation.",A statistical approach to finding overlooked genetic associations
"['Daniel Grund', 'Jan Reineke', 'Gernot Gebhard']","One step in the verification of hard real-time systems is to determine upper bounds on the worst-case execution times (WCET) of tasks. To obtain tight bounds, a WCET analysis has to consider micro-architectural features like caches, branch prediction, and branch target buffers (BTB). We propose a modular WCET analysis framework for branch target buffers, which allows for easy adaptability to different BTBs. As an example, we investigate the Motorola PowerPC 56x family (MPC56x), which is used in automotive and avionic systems. On a set of avionic and compiler benchmarks, our analysis improves WCET bounds on average by 17% over no BTB analysis. Capitalizing on the modularity of our framework, we explore alternative hardware designs. We propose more predictable designs, which improve obtainable WCET bounds by up to 20%, reduce analysis time considerably, and simplify the analysis. We generalize our findings and give advice concerning hardware used in real-time systems.",Branch target buffers: WCET analysis framework and timing predictability
"['Jacques Colinge', 'Alexandre Masselot', 'J??r??me Magnin']","GeneProt Inc., Pr??e de la Fontaine 2, CH-1219 Meyrin, SwitzerlandJacques.Colinge@geneprot.comAbstract. Tandem mass spectrometry has become central in proteo-mics projects. In particular, it is of prime importance to design sensitiveand selective score functions to reliably identify peptides in databases.By using a huge collection of 140000+ peptide MS/MS spectra, we sys-tematically study the importance of many characteristics of a match(peptide sequence/spectrum) to include in a score function. Besides clas-sical matchcharacteristics, weinvestigatethevalueofnewcharacteristicssuchas amino acid dependenceandconsecutive fragment matches. Weã™Å-nally select a combination of promising characteristics and show that thecorresponding score function achieves very low false positive rates whilebeing very sensitive, thereby enabling highly automated peptide identi-ã™Åcation in large proteomics projects. We compare our results to widelyused protein identiã™Åcation systems and show a signiã™Åcant reduction infalse positives.",A Systematic Statistical Analysis of Ion Trap Tandem Mass Spectra in View of Peptide Scoring
"['Ma H', 'Qiming Qin', 'Shihong Du', 'Lin Wang', 'Chuan Jin']","Research on road extraction from digital imagery is motivated by the need for data acquisition and update for geographic information systems (GIS). Roads usually have parallelism of road sides, and on the images, especially the edge map, there are dual edges for each road. In this paper, we propose an approach for automatically extracting road from ETM panchromatic image with a resolution of 15 meters based on Dual-Edge Following. Our approach uses the edge detector with embedded confidence (EDEC, Peter Meer, 2001) to detect road edge, then traces road to generate road candidates by Dual-Edge Following, next exploits the perceptual organization based on probability to link the road segments. Dual-Ddge Following use edge information of both road sides to search for road segments which can improve the precision of road segments. The experiment with ETM panchromatic image in Xinjiang, China shows the validity of the approach.",Road extraction from ETM panchromatic image based on Dual-Edge Following
"['Rachel M. McLaren', 'Denise Haunani Solomon', 'Jennifer S. Priem']","The authors use the relational turbulence model to derive hypotheses linking characteristics of relationships and reactions to hypothetical hurtful messages from a romantic relationship partner. It was hypothesized that relational uncertainty and perceptions of goal interference and facilitation from a partner predict perceptions of relational turbulence, which in turn predicts the intensity of hurt feelings, negative emotions, and the perceived intentionality of hurt evoked by hypothetical scenarios involving that partner. Participants in a web-based survey ( N = 381) completed measures of relationship qualities and recorded responses to five hypothetical scenarios that described their romantic partner delivering a potentially hurtful message. As anticipated, relationship uncertainty and interference from a partner predicted increased relational turbulence, whereas facilitation from a partner was associated with less turbulence; relational turbulence significantly predicted all three reactions to hurtful...",Explaining Variation in Contemporaneous Responses to Hurt in Premarital Romantic Relationships: A Relational Turbulence Model Perspective
"['Rafael Pous', 'Joan Meli?ˇ-Segu??', 'Anna Carreras', 'Marc Morenza-Cinos', 'Zulqarnain Rashid']","The popularization of  eCommerce  has led to effective customer shopping experiences. Pervasive computing could bring the benefits of eCommerce to brick and mortar stores, merging both online and physical worlds into a unique system. We define  crick  as the extension of the (c)lick and b(rick) concept, by means of pervasive technologies. In this paper, we summarize our work-in-progress research on using pervasive Radio Frequency Identification (RFID) to sense human-product interaction. These  cricks  can be performed through diverse interfaces in the retail domain, and automatically receive feedback in different manners. We believe that integrating RFID and other pervasive technologies in retail stores is the next step to obtain comprehensive customer's user models and preferences. Retail management improvement, or personal and collaborative recommendations, are envisioned to be successful applications of  cricking .",Cricking: customer-product interaction in retail using pervasive technologies
['David W. Knapp'],"A description is given of new capabilities of the RLEXT register level exploration tool. RLEXT is an interactive tool that takes a datapath design and allows a user to modify it freely, using transformations that do not themselves preserve correctness. By maintaining a representation of the desired behavior and timing as well as structure, RLEXT is able to 'repair' the design when the user creases modifying, so that the ability of the design to express the specified behavior is once again guaranteed. A description is also given of RLEXT's support for manual changes of schedule in the presence of an existing data-path structure. Such schedule changes invalidate the structure, but the structure is incrementally repaired rather than just thrown out. The author knows of no other high-level synthesis tool that supports this kind of functionality. >",Manual rescheduling and incremental repair of register-level datapaths
"['Omar Sosa-Tzec', 'Martin A. Siegel']","This paper introduces an approach for evaluating user interfaces built on visual rhetoric and the rhetorical notion of function. A personal informatics mobile application has been selected to exemplify the application of this approach. Through the results of this example evaluation, this paper discusses the consequence of applying a rhetorical evaluation to a user interface. In this discussion, it is observed that inspecting the function performed by interface components takes into account experiences, communication, and meaning. In addition, it fosters reflection and criticism.",Rhetorical evaluation of user interfaces
"['Noga Alon', 'Paul Erd??s']","Answering an old question in combinatorial geometry, we show that any configuration consisting of a setV ofn points in general position in the plane and a set of 6n ? 5 closed straight line segments whose endpoints lie inV, contains three pairwise disjoint line segments.",Disjoint edges in geometric graphs
"['Bin Xu', 'Chenyang Yang', 'Shiyi Mao']","An improved signal subspace estimation method is proposed and applied to blind multiuser detection. Firstly, a rough signal subspace is estimated from the received signal by direct eigendecomposition or subspace tracking. Then, the estimated signal subspace is modified with the desired user's signature waveform that is generally known. The new multiuser detector based on the improved signal subspace can offer substantial performance improvement over recently proposed multiuser detectors based on signal subspace estimation with a little attendant increase in computational complexity. Moreover, the estimated dimension of the signal subspace can be significantly reduced with little performance reduction when there are some interference signals weaker than the desired signal. Numerical simulation results are provided to support our claims.",An improved signal subspace estimation method and its application to multiuser detection
"['Feng Zhao', 'Qingming Huang', 'Wen Gao']","Correlation is widely used as an effective similarity measure in matching tasks. However, traditional correlation based matching methods are limited to the short baseline case. In this paper we propose a new correlation based method for matching two images with large camera motion. Our method is based on the rotation and scale invariant normalized cross-correlation. Both the size and the orientation of the correlation windows are determined according to the characteristic scale and the dominant direction of the interest points. Experimental results on real images demonstrate that the new method is effective for matching image pairs with significant rotation and scale changes as well as other common imaging conditions.",Image Matching by Normalized Cross-Correlation
"['Ezzat G. Bakhoum', 'Marvin H. M. Cheng']","A new type of pressure sensors with extremely high sensitivity is introduced. Unlike piezoresistive, capacitive, and linear-variable-differential-transformer-based pressure sensors, the new sensor is based on a technique for substantially changing the inductance of a coil. The prototype device has demonstrated a change in inductance of approximately 34.5 mH over a pressure range of 10 kPa. The sensor offers a number of desirable features, including linearity, low temperature, and pressure hysteresis, in addition to small size.",High-Sensitivity Inductive Pressure Sensor
"['Zhaolong Shen', 'Sean B. Andersson']","The fluoroBancroft (FB) algorithm is an analytical solution to the position estimation problem in single molecule fluorescence microscopy. In this paper we derive a theoretical description of the bias and precision of the estimator for three dimensional estimation based on a stack of charge-coupled device (CCD) images and illustrate the results through realistic simulations. The results indicate that the algorithm exhibits a small bias that is driven primarily by modeling error and is dependent on the location of the source particle relative to the set of pixels used for estimation. In the shot noise limited case, the precision scales approximately as the inverse square root of the number of photons detected and as the inverse of the number of photons detected in the background noise limited case. The results are compared through simulation to the maximum-likelihood (ML) estimator based on the theoretical point spread function and found to have a similar performance. In general, the ML estimate had lower bias and variance, though at the lowest signal-to-noise ratio (SNR), FB outperformed ML. The FB algorithm executes approximately three to four orders-of-magnitude faster than the ML estimator and is well-suited for applications in which real-time results are needed.",Bias and Precision of the fluoroBancroft Algorithm for Single Particle Localization in Fluorescence Microscopy
"['Norsyidah Mat Saat', 'Dalbir Singh']","The basic purpose of selection in human resource management is to choose the right candidates and to eliminate unsuitable ones. It is difficult to evaluate and to select one person over another, even for an experienced interviewer. The evaluation among the interviewers also varies according to their background, experience, personal judgment and observation. In the meantime, the recruitment system in the Malaysian public service is seen as too academic oriented, focuses more on academic qualification and does not cater for the quality of a candidate as a whole. To overcome this notion, Suruhanjaya Perkhidmatan Awam Malaysia (SPA) as the leading recruiting agency in Malaysian public service has adopted the profiling recruitment model which profile candidates in terms of personality, interests, attitudes, knowledge and skills. It is hoped that with a more comprehensive and descriptive profile, interviewers can make a better, fair, consistent and objective evaluation of a candidate and thus make more accurate hiring decision. However, in current state, there is no proper profiling done to include all the decision criteria. The purpose of this paper is to propose a decision support system (DSS) framework which is not only able to profile candidates according to the decision criteria but able to evaluate suitability of candidates to be appointed based on profile matching. This paper discusses the problem in-depth with data analysis from initial survey, the literature review on application of DSS in selection and product review from existing profiling system in the market. Finally, the paper proposed a DSS framework to assess suitability of candidates for selection and generate candidates' profiling report based on the profile matching between the job profile and the candidate's profile.",Assessing suitability of candidates for selection using candidates' profiling report
"['Xinfan Lin', 'Anna G. Stefanopoulou', 'Yonghua Li', 'R. Dyche Anderson']","The voltage of lithium ion batteries is usually monitored to prevent overcharge and overdischarge. For battery packs consisting of hundreds of cells, monitoring the voltage of every single cell adds significant cost and complexity to the battery management system (BMS). Reducing voltage sensing by only measuring the total voltage of multiple cells in series connection is desirable if the state of charge (SOC) of individual cells can be correctly estimated. Such goal cannot be achieved by an extended Kalman filter, because the cell SOCs are not observable in the linearized battery string model. In this paper, an observer based on solving simultaneously multiple nonlinear equations along the trajectory of SOC evolution is used for the estimation problem. Existence of the solution depends on the nonlinearity of the battery voltage-SOC relationship. The observer is applied to a LiFePO 4 /graphite battery string with 2 cells, where the individual cell SOCs are observable in low and high SOC ranges. Experimental results show good convergence of SOC and voltage estimation, indicating that this new methodology can be applied to, at least, halve the voltage sensing in a battery pack.",State of charge estimation of cells in series connection by using only the total voltage measurement
"['Karunaharan Ratnam', 'Ibrahim Matta']","The transmission control protocol (TCP) has been mainly designed assuming a relatively reliable wireline network. It is known to perform poorly in the presence of wireless links because of its basic assumption that any loss of a data segment is due to congestion and consequently it invokes congestion control measures. However, on wireless access links, a large number of segment losses will occur more often because of wireless link errors or host mobility. For this reason, many proposals have recently appeared to improve TCP performance in such environment. They usually rely on the wireless access points (base stations) to locally re-transmit the data in order to hide wireless losses from TCP. The authors use extensive simulations to evaluate TCP performance in the presence of congestion and wireless losses when the base station employs one of two proposals, namely Snoop and WTCP. The results show that WTCP significantly improves the throughput of TCP connections due to its unique feature of hiding the time spent by the base station to locally recover from wireless link errors so that TCP's round trip time estimation at the source is not affected. This proved to be critical since otherwise the ability of the source to effectively detect congestion in the fixed wireline network is hindered.",Effect of local retransmission at wireless access points on the round trip time estimation of TCP
"['Martin Pelikan', 'Kumara Sastry', 'David E. Goldberg']","This paper describes a scalable algorithm for solving multiobjective decomposable problems by combining the hierarchical Bayesian optimization algorithm (hBOA) with the nondominated sorting genetic algorithm (NSGA-II) and clustering in the objective space. It is first argued that for good scalability, clustering or some other form of niching in the objective space is necessary and the size of each niche should be approximately equal. Multiobjective hBOA (mohBOA) is then described that combines hBOA, NSGA-II and clustering in the objective space. The algorithm mohBOA differs from the multiobjective variants of BOA and hBOA proposed in the past by including clustering in the objective space and allocating an approximately equally sized portion of the population to each cluster. The algorithm mohBOA is shown to scale up well on a number of problems on which standard multiobjective evolutionary algorithms perform poorly.","Multiobjective hBOA, clustering, and scalability"
"['C.W. Chan', 'W.C. Chan', 'A. W. Jayawardena', 'C.J. Harris']","Neurofuzzy networks are being used increasingly to model non-linear dynamic systems, since they have the approximating ability of neural networks and the transparency of fuzzy systems. However, good generalization results can only be obtained if the structure of the network is suitably chosen. It is shown here that the structure of neurofuzzy networks with scatter partitioning can be obtained from the support vectors (SV) of the Support Vector Regression (SVR), since the SVR can be transformed to a neurofuzzy network. The main advantage of this approach is that the structure of the neurofuzzy networks can now be objectively chosen, as the SV are obtained by constrained optimization for a given modelling error bound. Since neurofuzzy networks are linear-inweights networks, the estimate of the weights of the networks can be obtained by the linear least-squares method. The properties of neurofuzzy networks based on the SV are derived, and its performance is illustrated by a simulation example involving a non...",Structure selection of neurofuzzy networks based on support vector regression
"['Luis Fernando Martins Carlos', 'Jo?úo Lu??s Garcia Rosa']",,Face recognition through a chaotic neural network model
"['A. De Lucia', 'Rocco Oliveto', 'Genny Tortora']","We present the results of a controlled experiment aiming at analysing the role played by the approach adopted during an IR-based traceability recovery process. In particular, we compare the tracing performances achieved by subjects using the ""one-shot"" process, where the full ranked list of candidate links is proposed, and the incremental process, where a similarity threshold is used to cut the ranked list and the links are classified step-by-step. The analysis of the achieved results shows that, in general, the incremental process improves the tracing accuracy and reduces the effort to analyse the proposed links.","IR-Based Traceability Recovery Processes: An Empirical Comparison of ""One-Shot"" and Incremental Processes"
"['Wenxia Shi', 'Jagath Samarabandu']","The capability of a mobile robot to negotiate corridors is essential for autonomous navigation in an indoor environment. An approach is proposed for determining the corridor line locations and the vanishing point in a corridor environment using a single camera, based on hypotheses generation/verification and a feedback control strategy. A corridor line is the intersection line between a wall and the floor, which is, the farthest lateral position the autonomous robot can safely navigate in a corridor. There have been numerous approaches described in the literature which detect corridor edges and vanishing point; however, no solution has been reported to detect true corridor line locations in the presence of many spurious linear features around the corridor line. The proposed method consists of low, medium, and high level processing stages which correspond to the extraction of features, the formation of hypotheses, and the verification of hypotheses using a feedback mechanism, respectively. The system has been tested on a large number of real corridor images captured by a moving robot in a corridor. The experimental results demonstrated the reliability and robustness of the approach with respect to different viewpoints, reflection variations and different illumination conditions.",Corridor Line Detection for Vision Based Indoor Robot Navigation
"['S. Di Martino', 'Filomena Ferrucci', 'Carmine Gravino', 'Emilia Mendes']","Size represents one of the most important attribute of software products used to predict software development effort. In the past nine years, several measures have been proposed to estimate the size of Web applications, and it is important to determine which one is most effective to predict Web development effort. To this aim in this paper we report on an empirical analysis where, using data from 15 Web projects developed by a software company, we compare four sets of size measures, using two prediction techniques, namely Forward Stepwise Regression (SWR) and Case-Based Reasoning (CBR). All the measures provided good predictions in terms of MMRE, MdMRE, and Pred(0.25) statistics, for both SWR and CBR. Moreover, when using SWR, length measures and Web Objects gave significant better results than Functional measures, however presented similar results to the Tukutuku measures. As for CBR, results did not show any significant differences amongst the four sets of size measures.",Comparing Size Measures for Predicting Web Application Development Effort: A Case Study
"['T. Mioch', 'Wietse Ledegang', 'R.T. Paulissen', 'Mark A. Neerincx', 'Jurriaan van Diggelen']","Sharing and re-using design knowledge is a challenge for the diverse multi-disciplinary research and development teams that work on complex and highly automated systems. For this purpose, a situated Cognitive Engineering (sCE) methodology was proposed that specifies and assesses the functional user requirements with their design rationale in a coherent and concise way. This paper presents this approach for the development of human-robot collaboration, focusing on a recently added component: the application of interaction design patterns to capture and share design knowledge on the shape of the human-robot interaction (i.e., the communication level). The sCE case study in the urban search and rescue domain provided the specification and assessment of functions and shape of a team-awareness display. Twenty fire fighters participated as operator of a ground or aerial robot, in several realistic earth quake scenarios to assess the functions and shapes of this display in different settings. It showed that the functions (i.e., the task level requirements and rationale) were valid, while the shape (communication level) was (yet) sub-optimal. Based on this evaluation result, a design improvement on the communication level has been proposed without the need to adjust the task-level design solution.",Interaction design patterns for coherent and re-usable shape specifications of human-robot collaboration
"['Nabil Abu-Khader', 'Pepe Siy']","In this paper, we present a pipelined inversion/division circuit in Galois field using AB circuit technique (where both A and B are elements in the finite field). We use composite Galois fields in a multiple-valued logic (MVL) approach to minimize the inversion/division circuit needed for binary Galois fields. The overall design, which connects basic cells in a systolic manner, thereby making effective use of pipelining, is shown. The fact that less literals are used speeds up the calculation operation. Also, our circuit shows a significant amount of savings in both transistor count and connections, which is so important in VLSI.",Inversion/Division in Galois Field Using Multiple-Valued Logic
"['Andreas Muller', 'Joachim Speidel']","We determine the exact symbol error probability of M-ary phase shift keying (M-PSK) for multihop communication systems with regenerative relays, where the source terminal transmits data to the destination terminal via a set of intermediate relay stations, which perform hard decisions on the received symbols before forwarding them to their respective successor node. Both, time-invariant additive white Gaussian noise channels as well as frequency-flat fading channels are considered and we derive generic expressions, which might be easily evaluated numerically or even be given in closed-form for various cases.",Exact symbol error probability of m-psk for multihop transmission with regenerative relays
"['Saskia Bakker', 'Elise van den Hoven', 'Berry Eggen']","This paper presents a research-through-design study into interactive systems for a primary school setting to support teachers' everyday tasks. We developed an open-ended interactive system called FireFlies, which is intended to be interacted with in the periphery of the teacher's attention and thereby become an integral part of everyday routines. FireFlies uses light-objects and audio as a (background) information display. Furthermore, teachers can manipulate the light and audio through physical interaction. A working prototype of FireFlies was deployed in four different classrooms for six weeks. Qualitative results reveal that all teachers found a relevant way of working with FireFlies, which they applied every day of the evaluation. After the study had ended and the systems were removed from the schools, the teachers kept reaching for the devices and mentioned they missed FireFlies, which shows that it had become part of their everyday routine.",FireFlies: physical peripheral interaction design for the everyday routine of primary school teachers
"['Warren Ferguson', 'Tom Brightman']","A technique for computing monotonicity preserving approximations F/sub a/(x) of a function F(x) is presented. This technique involves computing an extra precise approximation of F(x) that is rounded to produce the value of F/sub a/(x). For example, only a few extra bits of precision are used to make the accurate transcendental functions found on the Cyrix FasMath line of 80387 compatible math coprocessors monotonic. >",Accurate and monotone approximations of some transcendental functions
"['Andreas Bulling', 'Florian Alt', 'A. Schmidt']","With computers being used ever more ubiquitously in situations where privacy is important, secure user authentication is a central requirement. Gaze-based graphical passwords are a particularly promising means for shoulder-surfing-resistant authentication, but selecting secure passwords remains challenging. In this paper, we present a novel gaze-based authentication scheme that makes use of cued-recall graphical passwords on a single image. In order to increase password security, our approach uses a computational model of visual attention to mask those areas of the image that are most likely to attract visual attention. We create a realistic threat model for attacks that may occur in public settings, such as filming the user's interaction while drawing money from an ATM. Based on a 12-participant user study, we show that our approach is significantly more secure than a standard image-based authentication and gaze-based 4-digit PIN entry.",Increasing the security of gaze-based cued-recall graphical passwords using saliency masks
"['Rawad Zgheib', 'Gilles Fleury', 'Elisabeth Lahalle']","This paper deals with the problem of adaptive reconstruction and identification of nonstationary AR processes with randomly missing observations. Existent methods use a direct realization of the filter. Therefore, the estimated parameters may not correspond to a stable all-pole filter. In addition, when the probability of missing a sample is high, existent methods may converge slowly or even fail to converge. We propose, at our knowledge, the first algorithm based on the lattice structure for online processing of signals with missing samples. It is an extension of the RLSL algorithm to the case of missing observations, using a Kalman filter for the prediction of missing samples. The estimated parameters guarantee the stability of the corresponding all-pole filter. In addition it is robust to high probabilities of missing a sample. It offers a fast parameter tracking even for high probabilities of missing a sample. It is compared to the Kalman pseudolinear RLS algorithm, an already proposed algorithm using a direct realization of the filter. The proposed algorithm shows better performance in reconstruction of audio signals.",Lattice Algorithm for Adaptive Stable Identification and Robust Reconstruction of Nonstationary AR Processes With Missing Observations
"['V. De La Luz', 'M. Kandemir', 'Anand Sivasubramaniam', 'Mary Jane Irwin']","Data compression in caches has been studied from the performance and energy consumption points of view. In this paper, we study the possible benefits of operating with compressed operands. Operating in the compressed domain (i.e., executing instructions with compressed operands) as far as possible can offer several advantages. First, since it is less reliant on decompression, it may be possible to employ a fancier compression/decompression strategy, without the associated performance/power penalties, that can offer higher compression rates to further boost cache locality. Second, the transfers between cache and datapath can use fewer bits (to transmit codes instead of data values) and provide dynamic energy savings in the corresponding bus. In this paper, we demonstrate that there are cases where operations can be executed in the compressed domain, which can lead to performance improvements and energy savings. In particular, we show that by operating in the compressed domain, the effectiveness of prior techniques can be further improved.",Exploring the Possibility of Operating in the Compressed Domain
"['Minseok Kwon', 'Sonia Fahmy']","We propose an application level multicast approach, Topology Aware Grouping (TAG), which exploits underlying network topology information to build efficient overlay networks among multicast group members. TAG uses information about path overlap among members to construct a tree that reduces the overlay relative delay penalty, and reduces the number of duplicate copies of a packet on the same link. We study the properties of TAG, and model and experiment with its economies of scale factor to quantify its benefits compared to unicast and IP multicast. We also compare the TAG approach with the ESM approach in a variety of simulation configurations including a number of real Internet topologies and generated topologies. Our results indicate the effectiveness of the algorithm in reducing delays and duplicate packets, with reasonable algorithm time and space complexities.",Topology-aware overlay networks for group communication
"['In Kee Kim', 'Sung Ho Jang', 'Jong Sik Lee']","The mobile grid introduces various research challenges distinguished from existing grid computing systems. They are low bandwidth, low processing power, low battery capacity, frequent disconnectivity, and mobility. Mobility of the grid node increases the system load of the mobile grid in a constrained operating environment by increasing the number of communication messages required to confirm the location between the grid broker and mobile grid node. Therefore, this paper proposes an adaptive distance filter that can effectively reduce communication traffic between the mobile grid node and grid broker. This filter constructs clusters based on the mobility and velocity of the grid node and filters the location updates. However, the reduction of location updates generates location errors, which occur when the grid broker cannot acquire the exact location of mobile nodes. To solve this problem, if the location updates are filtered, the grid broker can estimate the location of the mobile node using a statistical estimation method. For the performance evaluation of the adaptive distance filter, we modeled the mobility of the grid nodes. We then measured the reduction in location updates and location errors. In these experiments, we prove that the adaptive distance filter is an effective scheme for reducing location updates and the grid broker can reduce location errors through location estimation.",Adaptive Distance Filter-based Traffic Reduction for Mobile Grid
"['Yue Zhang', 'Mark Panahi', 'Kwei-Jay Lin']","Service-oriented architecture (SOA) provides a flexible paradigm to compose dynamic service processes using individual services. However, service processes can be vastly complex, involving many service partners, thereby giving rise to difficulties in terms of pinpointing the service(s) responsible for problematic outcomes. In this research, we study efficient and effective mechanisms to deploy agents to monitor and detect undesirable services in a service process. We model the agent deployment problem as the classic weighted set covering (WSC) problem and present agent selection solutions at different stages of service process deployment. We propose the MASS (merit based agent and service selection) algorithm that considers agent cost during QoS-based service composition by using a meritagent_cost heuristic metric. We also propose the IGA (incremental greedy algorithm) to achieve fast agent selection when a service process is reconfigured after service failures. The performance study shows that our proposed algorithms are effective on saving agent cost and efficient on execution time.",Deployment of Accountability Monitoring Agents in Service-Oriented Architectures
"['Francesca Bovolo', 'Lorenzo Bruzzone']","This paper addresses unsupervised change detection by proposing a proper framework for a formal definition and a theoretical study of the change vector analysis (CVA) technique. This framework, which is based on the representation of the CVA in polar coordinates, aims at: 1) introducing a set of formal definitions in the polar domain (which are linked to the properties of the data) for a better general description (and thus understanding) of the information present in spectral change vectors; 2) analyzing from a theoretical point of view the distributions of changed and unchanged pixels in the polar domain (also according to possible simplifying assumptions); 3) driving the implementation of proper preprocessing procedures to be applied to multitemporal images on the basis of the results of the theoretical study on the distributions; and 4) defining a solid background for the development of advanced and accurate automatic change-detection algorithms in the polar domain. The findings derived from the theoretical analysis on the statistical models of classes have been validated on real multispectral and multitemporal remote sensing images according to both qualitative and quantitative analyses. The results obtained confirm the interest of the proposed framework and the validity of the related theoretical analysis",A Theoretical Framework for Unsupervised Change Detection Based on Change Vector Analysis in the Polar Domain
"['Sarath Indrakanti', 'Vijay Varadharajan', 'Ritesh Agarwal']","This paper proposes an authorisation architecture for web services. It describes the architectural framework, the administration and runtime aspects of our architecture and its components for secure authorisation of web services as well as the support for the management of authorisation information. The paper then describes the implementation aspects of the architecture. The architecture has been implemented and integrated within the.NET framework. The authorisation architecture for web services is demonstrated using a case study in the healthcare domain. The proposed architecture has several benefits. First and foremost, the architecture supports multiple access control models and mechanisms; it supports legacy applications exposed as web services as well as new web service-based applications built to leverage the benefits offered by the Service-Oriented Architecture; it is decentralised and distributed and provides flexible management and administration of web services and related authorisation information. The proposed architecture can be integrated into existing middleware platforms to provide enhanced security to web services deployed on those platforms.","On the design, implementation and application of an authorisation architecture for web services"
['Paul McMullan'],"The material presented in this work will outline the advances in the development of existing and new techniques produced from research, to complex scheduling problems encountered in industrial and commercial settings. The work outlines the success of using primitive or simple heuristics to achieve fast, efficient and good quality solutions to complex constraint satisfaction problems. The techniques used highlight the robustness and generality observed, with comparisons to approaches and results from existing research in the current literature, and application to a number of varied problem domains. It is intended to demonstrate the unique approach and subsequent advantages of application of this research. These include (a) efficiency, in terms of increasing the likelihood of obtaining high-quality solutions, (b) generality, in that current techniques can be applied quickly and successfully to various problem domains, (c) scope, in which the satisfaction of more complex constraints to problem areas which up until now have been limited to specific subsets of the problem, can be achieved. The latter can be demonstrated by case studies with commercial feedback from sources of application. The work described expands on well-tried benchmarks, exploiting cutting edge research-based techniques in order to gauge effectiveness on wider-scope problems within varied problem domains. A common goal for most problem domains is achieving efficiency (cost) savings to the provider while maintaining satisfaction for those recipients influenced or affected by any solution. Almost as important as the quality of any technique, is speed in achieving solutions; it is important to allow the quick production of high-quality solutions, allowing many scenarios with multiple varied objectives to be explored. This work will be presented in two sections, i.e. the research work undertaken and benchmark results obtained, followed by the application of this work to the real-world commercial environment including the challenges posed and feedback from this. Initially an introduction to the main research area will be given, covering problem definitions and current work in the literature. Each problem domain will be described, with results demonstrating the success of the approach in solving complex problems under examination, while maintaining generality. This will be expanded to highlight the scope and depth of the Real-World issues of these areas as faced in Industry, with the challenges and goals involved, advances made and feedback obtained. The work will conclude with a discussion of future plans for application to other areas with wider goals and more complex requirements.",The application of primitive heuristics to constraint satisfaction for complex real world optimisation
['Todd W. Neller'],"Given a heuristic estimate of the relative safety of a hybrid dynamical system trajectory, we transform the initial safety problem for dynamical systems into a global optimization problem. We compare untuned performance of several Simulated Annealing and Multi Level Single Linkage method variants, and discuss the dynamic use of knowledge gained during optimization.",Heuristic Optimizaton and Dynamical System Safety Verification
['Behrooz Bagheri Gh'],"Let G1 and G2 be two labeled graphs of order n. For any permutation ?Ò É?? Sn the (G1,G2)-permutation graph of labeled graphs G1 and G2 is the union of G1 and G2 together with the edges joining the vertex ui É?? V (G1) to the vertex v?Ò(i) É?? V (G2). This operation on graphs is useful to produce a large class of networks with approximately the same properties as one of the original networks or even smaller. In this work we consider some properties of the permutation graph P?Ò(G1,G2), for labeled graph G1 and G2 of the same order. We provide bounds for the parameters radius, diameter, total distance, connectivity, edge-connectivity, chromatic number, and edge-chromatic number.","(G1,G2)-permutation graphs"
"['Jordi Llosa', 'Ignasi Vilajosana', 'Xavier Vilajosana', 'Joan Manuel Marqu??s']","Wireless Sensor Networks (WSN) have been applied to gather data from a large number of scenarios. WSN provide the potential to collect data at spatial and temporal scales that are many times not feasible with past instrumentation. Although many applications have been already developed, little has been done using WSN as infrastructure for sports oriented commercial applications.This paper studies the use of WSN to develop motion detectors. Particularly REMOTE, a sensor network based application that provides a detailed picture of a boat movement, rowerÉ??s individual performance, or his/her performance compared to other crew members is proposed. The application aims to analyze data gathered with a WSN strategically deployed over a boat to obtain information on the boat and oar movements. The paper presents the architecture, software design and hardware infrastructure used to develop the application.",Design of a Motion Detector to Monitor Rowing Performance Based on Wireless Sensor Networks
"['Shijian Lu', 'Chew Lim Tan']","For document images captured by a digital camera, perspective and geometric distortions make it hard to recognize the document content properly. In this paper, we propose an integrated document restoration technique, which is capable of removing perspective and geometric distortions, and producing a flattened and fronto-parallel text image that is friendly to the generic OCR systems. The proposed document restoration is accomplished through grid modeling, which divides camera images into multiple quadrilateral grids using vertical text directions and the x lines and base lines. The global distortions are then removed through grid regularization that transforms the quadrilateral grids together with the pixel contents to the regular square grids. Experimental results show the proposed method is fast and easy for implementation.",Document Flattening through Grid Modeling and Regularization
"['Xiaohu Ge', 'Chengqian Cao', 'Minho Jo', 'Min Chen', 'Jinzhong Hu', 'Iztok Humar']","In this paper, the relationship between the energy efficiency and spectrum efficiency in a two-cell cellular network is obtained, and the impact of multi-antenna on the energy efficiency of cellular network is analyzed and modeled based on two-state Markovian wireless channels. Then, the energy efficiency of multi-cell cellular networks with co-channel interference is investigated. Simulation results verify the proposed model and the energy-spectrum efficiency tradeoffs in cellular networks with multi-antenna and co-channel interference.",Energy Efficiency Modelling and Analyzing Based on Multi-cell and Multi-antenna Cellular Networks
"['Martin Stridh', 'Leif S??rnmo']","A new method for QRST cancellation is presented for the analysis of atrial fibrillation in the surface electrocardiogram (ECG). The method is based on a spatiotemporal signal model which accounts for dynamic changes in QRS morphology caused, e.g., by variations in the electrical axis of the heart. Using simulated atrial fibrillation signals added to normal ECGs, the results show that the spatiotemporal method performs considerably better than does straightforward average beat subtraction (ABS). In comparison to the ABS method, the average QRST-related error was reduced to 58 percent. The results obtained from ECGs with atrial fibrillation agreed very well with those from simulated fibrillation signals.",Spatiotemporal QRST cancellation techniques for analysis of atrial fibrillation
"['Shih-Chien Chou', 'Chyan-Goei Chung']","A new OOA model and methodology are proposed. The proposed OOA model is composed of an object model, which specifies objects and the relationships between them, and a function model, which specifies system functions and their operation logic. The proposed OOA methodology develops the specifications of objects and system functions in parallel by following a functional refinement process. Since system functions are specified clearly in the OOA model, the user can easily understand what the system can do by tracing its specification. The object model and function model are developed in parallel, so the consistency between system functions and objects are kept throughout the OOA process. >",An OOA model with system function specifications
"['Nemanja Memarovic', 'Keith Cheverst', 'Marc Langheinrich', 'Ivan Elhart', 'Florian Alt']","Many design decisions need to be made when creating situated public displays that aim to serve a community. One such decision concerns access to its contents: should users be able to access content remotely, e.g., via a web page, or should this be limited to users who are co-located with the display? A similar decision has to be made for community content upload: do posters need to be co-located with the display or can posts be made from any location? In other words, content display and creation can be 'tethered' to a display or it can be 'free to roam', i.e., accessible from anywhere. In this paper we analyze prior community display deployments in an attempt to explore this space and produce a taxonomy that highlights the inherent design choices. Furthermore, we discuss some of the reasons that may underlie these choices and identify opportunities for design.",Tethered or free to roam: the design space of limiting content access on community displays
"['Nazila Yavari', 'Jeroen Dam', 'Johan Antonsson', 'Karin W?ùrdell', 'Stefan Andersson-Engels']","Knowledge of the optical properties of tissues can be applied in numerous medical and scientific fields, including cancer diagnostics and therapy. There are many different ways of determining the optical properties of turbid media. The paper describes measurements of the optical properties of porcine brain tissue using novel instrumentation for simultaneous absorption and scattering characterisation of small turbid samples. Integrating sphere measurements are widely used as a reference method for determination of the optical properties of relatively thin turbid samples. However, this technique is associated with bulky equipment, complicated measuring techniques, interference compensation techniques and inconvenient sample handling. It is believed that the sphere for some applications can be replaced by a new, compact device, called the combined angular and spatially resolved head sensor, to measure the optical properties of thin turbid samples. The results compare very well with data obtained with an integrating sphere for well-defined samples. The instrument was shown to be accurate to within 12% for mu(a) and 1% for mu'(s) in measurements of intralipid-ink samples. The corresponding variations of data were 17% and 2%, respectively. The reduced scattering coefficient for porcine white matter was measured to be 100 cm(-1) at 633 nm, and the value for coagulated brain tissue was 65 cm(-1). The corresponding absorption coefficients were 2 and 3 cm(-1), respectively. (Less)",In vitro measurements of optical properties of porcine brain using a novel compact device
"['Thommen Korah', 'Jason Wither', 'Yun-Ta Tsai', 'Ronald Azuma']",This work introduces techniques to facilitate large-scale Augmented Reality (AR) experiences in unprepared outdoor environments. We develop a shape-based object detection framework that works with limited texture and can robustly handle extreme illumination and occlusion issues. The contribution of this work is a purely geometric approach for detecting marker-like objects under difficult and realistic outdoor conditions. We demonstrate these techniques for mobile AR experiences by detecting and tracking star-shaped pentagrams embedded in the Hollywood Walk of Fame at 30Hz on a Nokia N900 phone.,Mobile Augmented Reality at the Hollywood Walk of Fame
"['Tudor Murgan', 'A. Ortiz', 'Mihail Petrov', 'Manfred Glesner']","This work introduces a linear model for high-level prediction of delay in capacitively and inductively coupled very deep sub-micron (VDSM) on-chip interconnects. The proposed estimation model approximates the signal delay as a linear combination of the contributions induced by each other aggressor line. It accurately predicts the delay in both capacitively and inductively coupled lines for the complete set of the switching patterns, and not only for the worst case, as in previous works. Therefore, it is suitable for fast yet efficient high-level analysis of bus encoding schemes envisaging delay minimisation. The accuracy of the model has been assessed by means of extensive experiments using 3D field solvers and SPICE simulations.",A linear model for high-level delay estimation in VDSM on-chip interconnects
"['Lianru Gao', 'Qiong Ran', 'Bing Zhang', 'Yaobin Chi', 'Zhiyong Wang']","Compression has become a must for efficient storing and transmission of data acquired by satellites with increasing resolution and swath. However, for compression of raw data satellite images, impact of striping noise is inevitable. Variances of Digital Number (DN) values introduced by striping noise will surely impair continuity and smoothness of satellite image and reduce the efficiency of onboard compression. In this paper, using Beijing-1 small satellite images, origin and characteristics of striping noise caused by double channel linear CCD and its impacts on the compression process are analyzed. Then, based on properties of striping noise, an improved method for compression of raw data satellite images is proposed. The new compression method is applied to Beijing-1 small satellite raw data images and yields significant boost in compression performance. Ideas of the proposed algorithm can be easily realized with circuit modification and no adaptation is needed for post processing of the compressed images.",New thoughts for onboard compression of satellite images
"['O. Abdoun', 'S??bastien Joucla', 'Claire Mazzocco', 'Blaise Yvert']","A major characteristic of neural networks is the complexity of their organization at various spatial scales, from microscopic local circuits to macroscopic brain-scale areas. Understanding how neural information is processed thus entails the ability to study them at multiple scales simultaneously. This is made possible using microelectrodes array (MEA) technology. Indeed, high-density MEAs provide large-scale covering (several mm?˝) of whole neural structures combined with microscopic resolution (about 50?Êm) of unit activity. Yet, current options for spatiotemporal representation of MEA-collected data remain limited. Here we present NeuroMap, a new interactive Matlab-based software for spatiotemporal mapping of MEA data. NeuroMap uses thin plate spline interpolation, which provides several assets with respect to conventional mapping methods used currently. First, any MEA design can be considered, including 2D or 3D, regular or irregular, arrangements of electrodes. Second, spline interpolation allows the estimation of activity across the tissue with local extrema not necessarily at recording sites. Finally, this interpolation approach provides a straightforward analytical estimation of the spatial Laplacian for better current sources localization. In this software, coregistration of 2D MEA data on the anatomy of the neural tissue is made possible by fine matching of anatomical data with electrode positions using rigid deformation based correction of anatomical pictures. Overall, NeuroMap provides substantial material for detailed spatiotemporal analysis of MEA data. The package is distributed under GNU General Public License (GPL) and available at http://sites.google.com/site/neuromapsoftware.",NeuroMap: a spline-based interactive open-source software for spatiotemporal mapping of 2D and 3D MEA data
"['Rajan Shankaran', 'Vijay Varadharajan', 'Michael Hitchens']","With the increasing growth in mobile computing devices and wireless networks, users are able to access information from anywhere and at anytime. In such situations, the issues of location management for mobile hosts are becoming increasingly significant. Different location management schemes such as Columbia University's mobile IP scheme and IETF Mobile IP have been proposed. In this paper, we propose a distributed location management scheme and discuss the advantages of the proposed scheme over the others. It seems that the proposed scheme is easy to implement and achieves optimal routing. The paper then considers the issues of multicasting in the proposed scheme. Finally, the paper describes security protocols for authentication and data privacy for the proposed architecture.",Secure distributed location management scheme for mobile hosts
"['Alexander Berman', 'Olga A. Nickolaychuk', 'Alexander Yu. Yurin']","An approach for control of failure analysis for unique mechanical systems on the basis of production rules is proposed. This approach has been employed for creation of an intelligent planner (control subsystem), which provides for dynamic generation of the plan of failure analysis. The intelligent planner is part of the hybrid expert system. The efficiency of this expert system with the intelligent planner has been verified in analysis of real failures.",Intelligent planner for control of failures analysis of unique mechanical systems
"['Kevin Knight', 'Daniel Marcu']","When humans produce summaries of documents, they do not simply extract sentences and concatenate them. Rather, they create new sentences that are grammatical, that cohere with one another, and that capture the most salient pieces of information in the original document. Given that large collections of text/abstract pairs are available online, it is now possible to envision algorithms that are trained to mimic this process. In this paper, we focus on sentence compression, a simpler version of this larger challenge. We aim to achieve two goals simultaneously:our compressions should be grammatical, and they should retain the most important pieces of information. These two goals can conflict. We devise both noisy-channel and decision-tree approaches to the problem, and we evaluate results against manual compressions and a simple baseline.",Statistics-Based Summarization - Step One: Sentence Compression
"['Yanjing Wang', 'Qinxiang Cao']","In the literature, different axiomatizations of Public Announcement Logic (PAL) have been proposed. Most of these axiomatizations share a ""core set"" of the so-called ""reduction axioms"". In this paper, by designing non-standard Kripke semantics for the language of PAL, we show that the proof system based on this core set of axioms does not completely axiomatize PAL without additional axioms and rules. In fact, many of the intuitive axioms and rules we took for granted could not be derived from the core set. Moreover, we also propose and advocate an alternative yet meaningful axiomatization of PAL without the reduction axioms. The completeness is proved directly by a detour method using the canonical model where announcements are treated as merely labels for modalities as in normal modal logics. This new axiomatization and its completeness proof may sharpen our understanding of PAL and can be adapted to other dynamic epistemic logics.",On Axiomatizations of Public Announcement Logic
"['Yu Cao', 'P. P. B. Eggermont', 'Susan Terebey']","We present a multiplicative algorithm for image reconstruction, together with a partial convergence proof. The iterative scheme aims to maximize cross Burg entropy between modeled and measured data. Its application to infrared astronomical satellite (IRAS) data shows reduced ringing around point sources, compared to the EM (Richardson-Lucy) algorithm.",Cross Burg entropy maximization and its application to ringing suppression in image reconstruction
"['Li Shen', 'Iouri Chepelev', 'Jie Liu', 'Wei Wang']","Background#R##N#An exciting application of genetic network is to predict phenotypic consequences for environmental cues or genetic perturbations. However, de novo prediction for quantitative phenotypes based on network topology is always a challenging task.",Prediction of quantitative phenotypes based on genetic networks: a case study in yeast sporulation
"['Hui Deng', 'Feng Wang', 'Bo Liang']","Optical mark recognition (OMR) is a traditional data input technique and an important human computer interaction technique which is widely used in education testing. Aimed at the drawbacks of current OMR technique, a new image-based low cost OMR technique is presented in the paper. The new technique is capable of processing thin papers and low-printing precision answer sheets. The system key techniques and relevant implementations, which include the image scan, tilt correction, scanning error correction, regional deformation correction and mark recognition, are presented. This new technique is proved robust and effective by the processing results of large amount of questionnaires.",A Low-Cost OMR Solution for Educational Applications
"['Qingchun Ren', 'Qilian Liang']","In this paper, we propose an efficient MAC protocol: the throughput maximized MAC protocol (TM-MAC), inspired by the availability that a number of ultrawideband (UWB) transmission parameters can be tuned to better match the requirements of data flow. In TM-MAC, we implement a concurrent multiuser access scheme instead of a mutual exclusion method such as TDMA and random access. For multiuser interference, we establish a model to adaptively adjust the data transmission rate to generate the expected signal to interference noise ratio (SINR) at the receiver side for reliable communications. We also analyze the relationship among the theoretical maximum channel capacity, achievable maximum channel capacity, and data transmission rate. According to network topology, TM-MAC redivides each piconet into several subsets in which communication pairs can make communication simultaneously and achieve the maximum throughput using the highest data rate. In subset formation, we propose a general analytical framework that captures the unique characteristics of shared wireless channel and throughput variance, as well as allows the modeling of a large class of systemwide throughput maximization via the specification of the per-link utilization function. For algorithm essential parameters design, we consider the influence of traffic type on the system performance. Heavy tailed distribution, compared to Poisson distribution for most existing work, is exploited to accurately model the real traffic to achieve the adaptation of our algorithm. Simulation results show that our algorithm can maximize throughput to achieve short latency.",Throughput and Energy-Efficiency-Aware Protocol for Ultrawideband Communication in Wireless Sensor Networks: A Cross-Layer Approach
"['Beneyam B. Haile', 'Edward Mutafungwa', 'Jyri H??m??l??inen']","The heterogeneous deployment of high-power macro cells and low-power nodes (LPNs) is now widely acknowledged as an essential requirement towards meeting the continued demand for mobile data capacity. The selection of the optimum backhaul solution for the LPNs obliges operators to consider not only the capacity of the backhaul but also other key factors so as to fully leverage the benefits provided by LPNs: the cost of the backhauling may limit the density of LPN deployments and the backhaul configuration requirements impact on the flexibility of LPN deployment. To that end, self-backhauling of LPNs via the existing macro radio access network (RAN) provides an attractive solution, particularly for deployment scenarios that are very cost-sensitive and/or require high flexibility. However, use of self-backhauling usually makes backhaul as a bottleneck due to the a) limited bandwidth allocated for legacy macro RAN, b) the need to share resources with macro user equipment (UE), and c) the high-intercell interference particularly in the macro cell edge. In this paper, we provide an overview of self-backhauled LPNs and investigate possible performance enhancements through the use of coordinated multi-point (CoMP) transmission to relax the downlink backhaul capacity bottleneck for self-backhauled LPNs. To that end, we carry out analytical studies for a practical limited-feedback CoMP technique and numerically verify the derived capacity outage expressions. Furthermore, we implement a simulation study for an exemplary heterogeneous network deployment in a realistic radio propagation environment. The results of the studies demonstrate that significant spectral efficiency and throughput gains for the LPN backhaul are achievable through the use of selected CoMP technique under realizable feedback overhead, even under feedback bit error. The achieved relaxation in the backhaul bottleneck is observed providing improved performance for the UEs served by the LPNs. Furthermore, more resources will be available for macro UEs leading to overall performance gains compared to the case without CoMP.",Coordinated multi-point transmission for relaxation of self-backhauling bottlenecks in heterogeneous networks
"['Marko J. Moisio', 'Keijo V????n??nen']",Two recursive algorithms for computing the weight distributions of certain binary irreducible cyclic codes of length n in the so-called index 2 case are presented. The running times of these algorithms are smaller than O(log/sup 2/r) where r=2/sup m/ and n is a factor of r-1.,Two recursive algorithms for computing the weight distribution of certain irreducible cyclic codes
"['Shuning Wang', 'Xiaolin Huang', 'Yeung Yam']","Smooth hinging hyperplane (SHH) has been proposed as an improvement over the well-known hinging hyperplane (HH) by the fact that it retains the useful features of HH while overcoming HH's drawback of nondifferentiability. This paper introduces a formal characterization of smooth hinge function (SHF), which can be used to generate SHH as a neural network. A method for the general construction of SHF is also given. Furthermore, the work proves that SHH is better than HH in functional approximation, i.e., the optimal error of SHH approximating a general function is always smaller or equal to that of HH. Particularly, in the case that the SHF is generated via the integration of a class of sigmoidal functions, it is further proven that the corresponding SHH of the 2 m  SHFs would outperform a neural network with  m  of the sigmoidal function from which the SHF is derived. Any upper bound established on the approximation error of a neural network of  m  sigmoidal activation functions can hence be translated to the SHH of  m  SHFs by replacing  m  with  m /2. The work also includes an algorithm for the identification of SHH making use of its differentiability property. Simulation experiments are presented to validate the theoretical conclusions to possible extent.",A Neural Network of Smooth Hinge Functions
"['Blaise Thomson', 'Jost Schatzmann', 'Steve J. Young']","This paper presents a new framework for accumulating beliefs in spoken dialogue systems. The technique is based on updating a Bayesian network that represents the underlying state of a partially observable Markov decision process (POMDP). POMDP models provide a principled approach to handling uncertainty in dialogue but generally scale poorly with the size of the state and action space. The framework proposed, on the other hand, scales well and can be extended to handle complex dialogues. Learning is achieved with a factored summarising function that is applicable for many slot-filling type dialogues. The framework also provides a good structure from which to build hand-crafted policies. For very complex dialogues, this allows the POMDP's principled approach to uncertainty to be incorporated without requiring computationally intensive learning algorithms. Simulations show that the proposed framework outperforms standard techniques whenever errors increase.",Bayesian update of dialogue state for robust dialogue systems
"['Xiaomeng Su', 'Damiano Bolzoni', 'van Pascal Eck']","In this paper we present an approach for specifying and prioritizing information security requirements in organizations. It is important to prioritize security requirements since hundred per cent security is not achievable and the limited resources available should be directed to satisfy the most important ones. We propose to explicitly link security requirements with the organization's business vision, i.e. to provide business rationale for security requirements. The rationale is then used as a basis for comparing the importance of different security requirements. Furthermore we discuss how to integrate the aforementioned solution concepts into a service level management process for security services, which is an important step in IT Governance. We validate our approach by way of a focus group session.",Understanding and Specifying Information Security Needs to Support the Delivery of High Quality Security Services
"['Liansheng Zhuang', 'Haoyuan Gao', 'Zhouchen Lin', 'Yi Ma', 'Xin Zhang', 'Nenghai Yu']","Constructing a good graph to represent data structures is critical for many important machine learning tasks such as clustering and classification. This paper proposes a novel non-negative low-rank and sparse (NNLRS) graph for semi-supervised learning. The weights of edges in the graph are obtained by seeking a nonnegative low-rank and sparse matrix that represents each data sample as a linear combination of others. The so-obtained NNLRS-graph can capture both the global mixture of subspaces structure (by the low rankness) and the locally linear structure (by the sparseness) of the data, hence is both generative and discriminative. We demonstrate the effectiveness of NNLRS-graph in semi-supervised classification and discriminative analysis. Extensive experiments testify to the significant advantages of NNLRS-graph over graphs obtained through conventional means.",Non-negative low rank and sparse graph for semi-supervised learning
['David K. Y. Chiu'],"The notion of consensus structure, a special form of random graph, is formally defined. The consensus structure represents an extended higher-order representation of the random n-tuple for statistical and structural pattern recognition. An algorithm inferring the consensus structure from a random n-tuple is designed based on the detection of statistical interdependency under certain structural constraints. In applying structural constraints, a circular diagram indicating variable interaction is used to extract global structural features in a macromolecular modeling problem. >",A consensus structure inference algorithm
"['Jiong Jin', 'Wei-Hua Wang', 'Marimuthu Palaniswami']","Motivated by the limitations of current optimal flow control approach, we develop a new utility max-min flow control framework using classic sliding mode control. It consists of a source algorithm and a binary congestion feedback mechanism, in which only the sources with the highest utility at each congested link are required to reduce their transmission rates. It can be directly applied to a multi-service network with heterogeneous applications that have different QoS characteristics. The proposed framework achieves the utility max-min fairness among applications efficiently in the sense of low overhead and rapid convergence. Rigorously, the system is proven to be asymptotically stable by means of Lyapunov's theorem.",A simple framework of utility max-min flow control using sliding mode approach
"['Liam Rourke', 'Heather Kanuka']","This qualitative case study illustrates barriers to informal argumentation and reasoned debate, i.e., critical discourse, in online forums. The case is the computer conference of a 15-week, graduate-level humanities course offered entirely at a distance. Twelve students, all with families and careers, were enrolled in the course. We read all messages as they were posted and interviewed five of the students several times during the course. The students provided three insights into our interpretation that the forums contained little critical discourse: (1) The students did not orient to the conference as a forum for critical discourse, and worse, they had competing orientations; (2) they perceived critiques as personal attacks; and (3) they realized early on that critical discourse was a bothersome means to obtain their participation marks. Certain practices may ease some of these difficulties, including (1) well-structured learning activities with clearly defined roles for teachers and students, and (2) a method of assessing studentsÉ?? participation that reflects the time and effort required to engage in critical discourse.",Barriers to online critical discourse
"['S Suzana Andova', 'Mark van den Brand', 'Luc Engelen']","A formal definition of the semantics of a domain-specific language (DSL) is a key prerequisite for the verification of the correctness of models specified using such a DSL and of transformations applied to these models. For this reason, we implemented a prototype of the semantics of a DSL for the specification of systems consisting of concurrent, communicating objects. Using this prototype, models specified in the DSL can be transformed to labeled transition systems (LTS). This approach of transforming models to LTSs allows us to apply existing tools for visualization and verification to models with little or no further effort. The prototype is implemented using the ASF+SDF Meta-Environment, an IDE for the algebraic specification language ASF+SDF, which offers efficient execution of the transformation as well as the ability to read models and produce LTSs without any additional pre or post processing.",Prototyping the Semantics of a DSL using ASF+SDF: Link to Formal Verification of DSL Models
"['Boqiang Huang', 'Yuanyuan Wang', 'Jianhua Chen']","A novel 2-D compression scheme of ECG signals is proposed, which employs 1-D discrete wavelet transform, the region of interest mask, and the conditional entropy coding based on context models. Experimental results on records selected from the Massachusetts Institute of Technology-Beth Israel Hospital arrhythmia database show that the proposed method outperforms some existing compression schemes.",2-D Compression of ECG Signals Using ROI Mask and Conditional Entropy Coding
"['Hai-ping Feng', 'Jun Zhao']","For the characteristics of dial board images, this paper proposes an accomplishment method of auto-calibration system of dial gauges. Uses computer vision technology into the calibration of dial gauges, substitutes the traditional calibration method which need human, and accomplishes automatic dial gauge calibration. The dial gauge calibration process consists of three parts: image pre-processing, scale marks recognition, and the pointer indication interpretation. This paper uses image subtraction method, dynamic threshold segmentation method to process in image pre-processing part. In the other two parts, this paper proposes a new method named as region-segmentation, to partition the dial image, only process the useful blocks of dial image, and not process the other areas. This method reduces the computation amount greatly, and improves the processing speed effectively. Then, uses least squares fitting lines method and central projection method to compute scale marks data and detected pointer value. Prove through experiments, this paper accomplishes an intelligent and rapidly dial gauge calibration system.",Application Research of Computer Vision in the Auto-Calibration of Dial Gauges
['Victoria Lebed'],"In the first part we recall two famous sources of solutions to the Yang-Baxter equationÉ??R-matrices and Yetter-Drinfel0d (=YD) modulesÉ??and an interpretation of the former as a particular case of the latter. We show that this result holds true in the more general case of weak R-matrices, introduced here. In the second part we continue exploring the É??braidedÉ?ù aspects of YD module structure, exhibiting a braided system encoding all the axioms from the definition of YD modules. The functoriality and several generalizations of this construction are studied using the original machinery of YD systems. As consequences, we get a conceptual interpretation of the tensor product structures for YD modules, and a generalization of the deformation cohomology of YD modules. This homology theory is thus included into the unifying framework of braided homologies, which contains among others Hochschild, Chevalley-Eilenberg, Gerstenhaber-Schack and quandle homologies.","R-Matrices, Yetter-Drinfel$'$d Modules and Yang-Baxter Equation"
"['J. A. Becerra', 'Francisco Bellas', 'J. Santos', 'Richard J. Duro']","Combining previous experience and knowledge to contemplate tasks of increasing complexity is one of the most interesting problems in autonomous robotics. Here we present an ANN based modular architecture that uses the concept of modulation to increase the possibilities of reusing previously obtained modules. A first approximation to the modulation of the actuators was tested in a previous paper where we showed how it was useful to obtain more complex behaviours that those obtained using only activation / inhibition. In this paper we extend the concept to sensor modulation, which enables the architecture to easily modify the required behaviour for a module, we show how both types of modulation can be used at the same time and how the activation / inhibition can be seen as a particular case of modulation. Some examples in a real robot illustrate the capabilities of the whole architecture.",Complex behaviours through modulation in autonomous robot control
"['Jos?? L. Neira', 'Juan D. Tard??s', 'Jos?? A. Castellanos']","In this paper we propose an algorithm to deter- mine the location of a vehicle in an environment represented by a stochastic map, given a set of environment measure- ments obtained by a sensor mounted on the vehicle. We show that the combined use of (1) geometric constraints considering feature correlation, (2) joint compatibility, (3) random sampling and (4) locality, make this algorithm lin- ear with both the size of the stochastic map and the number of measurements. We demonstrate the practicality and ro- bustness of our approach with experiments in an outdoor environment.",Linear time vehicle relocation in SLAM
"['Alexandr M. Kuzminskiy', 'Hamid Reza Karimi']","The potential of multi-antenna interference cancellation receiver algorithms for increasing the uplink throughput in WLAN systems such as 802.11 is investigated. The medium access control (MAC) in such systems is based on carrier sensing multiple-access with collision avoidance (CSMA/CA), which itself is a powerful tool for the mitigation of intra-system interference. However, due to the spatial dependence of received signal strengths, it is possible for the collision avoidance mechanism to fail, resulting in packet collisions at the receiver and a reduction in system throughput. The CSMA/CA MAC protocol can be complemented in such scenarios by interference cancellation algorithms at the physical (PHY) layer. The corresponding gains in throughput are a result of the complex interplay between the PHY and MAC layers. It is shown that semi-blind interference cancellation techniques are essential for mitigating the impact of interference bursts, in particular since these are typically asynchronous with respect to the desired signal burst. This is illustrated by cross-layer simulations in open access network (OAN) scenarios involving intra- and (impolite) inter-system interference.",Cross-Layer Design of Uplink Multiple-Antenna Interference Cancellation for WLAN with CSMA/CA in Open Access Networks
"['Raymond W. Lam', 'Henry C. B. Chan', 'Victor O. K. Li', 'Tharam S. Dillon', 'Victor C. M. Leung']","A new routing method, known as active routing, has been emerging. This involves using active packets to configure customized network paths. Based on a Markov decision model, this paper presents an active routing service for active networks in general and the next generation network, called ISDN3, in particular. Our aim is to determine the active routing policy so as to minimize the network cost. Theoretical analysis is presented to show the advantages of our proposal as compared with three other approaches.",Active routing service for the next-generation network/ISDN3
"['Lili Wan', 'Zhenjiang Miao']","This paper presents a fuzzy KNN and Bayesian decision based classification method for determining whether a 3D object belongs to human class. To achieve efficiency and simplicity, the view having maximum area is used to substitute a 3D shape, which is front-side view for human models. View features and structural features are utilized to describe those selected views. For shape feature can not distinguish one class from the others crisply, fuzzy KNN is adopted to estimate the probability that the object belongs to human class using view feature extracted from an unknown object and view features of training objects. Structural features are represented as adjacent graphs after shape decompositions, while characteristic structures are predefined for human class. Next priori probabilities that the characteristic structure exist in human and other classes are gained from training objects. For an unknown object, an adjacent graph is checked whether the characteristic structure exists, and then fuzzy classification results and priori probabilities are used together to classify the unknown object by Bayesian rule. Experimental results show that our approach have good accuracy for classifications.",3D Object Classification by Fuzzy KNN and Bayesian Decision
"['Mart??n L??pez-Nores', 'Jos?? J. Pazos-Arias', 'Jorge Garc??a-Duque', 'Belen Barragans-Martinez']","The development of requirements specifications has to face the imprecise and changeable knowledge available about the desired systems at the early stages of development. In this paper, we take advantage of that changeability to introduce an agile approach that helps identify suitable evolutions of a specification. This approach provides a solution with low computational cost to achieve frequent interaction with the stakeholders, this being the key to support the eminently creative task of developing requirements specifications.",An agile approach to support incremental development of requirements specifications
"['Dong-Gil Ko', 'Laurie J. Kirsch', 'William R. King']","Enterprise resource planning (ERP) systems and other complex information systems represent critical organizational resources. For such systems, firms typically use consultants to aid in the implementation process. Client firms expect consultants to transfer their implementation knowledge to their employees so that they can contribute to successful implementations and learn to maintain the systems independent of the consultants. This study examines the antecedents of knowledge transfer in the context of such an interfirm complex information systems implementation environment. Drawing from the knowledge transfer, information systems, and communication literatures, an integrated theoretical model is developed that posits that knowledge transfer is influenced by knowledge-related, motivational, and communication-related factors. Data were collected from consultant-and-client matched-pair samples from 96 ERP implementation projects. Unlike most prior studies, a behavioral measure of knowledge transfer that incorporates the application of knowledge was used. The analysis suggests that all three groups of factors influence knowledge transfer, and provides support for 9 of the 13 hypotheses. The analysis also confirms two mediating relationships. These results (1) adapt prior research, primarily done in non-IS contexts, to the ERP implementation context, (2) enhance prior findings by confirming the significance of an antecedent that has previously shown mixed results, and (3) incorporate new IS-related constructs and measures in developing an integrated model that should be broadly applicable to the interfirm IS implementation context and other IS situations. Managerial and research implications are discussed.",Antecedents of knowledge transfer from consultants to clients in enterprise system implementations
"['Sukomal Pal', 'Mandar Mitra', 'Jaap Kamps']","The Initiative for the Evaluation of XML retrieval (INEX) provides a TREC-like platform for evaluating content-oriented XML retrieval systems. Since 2007, INEX has been using a set of precision-recall based metrics for its ad hoc tasks. The authors investigate the reliability and robustness of these focused retrieval measures, and of the INEX pooling method. They explore four specific questions: How reliable are the metrics when assessments are incomplete, or when query sets are small? What is the minimum pool/query-set size that can be used to reliably evaluate systems? Can the INEX collections be used to fairly evaluate É??newÉ?ù systems that did not participate in the pooling process? And, for a fixed amount of assessment effort, would this effort be better spent in thoroughly judging a few queries, or in judging many queries relatively superficially? The authors' findings validate properties of precision-recall-based metrics observed in document retrieval settings. Early precision measures are found to be more error-prone and less stable under incomplete judgments and small topic-set sizes. They also find that system rankings remain largely unaffected even when assessment effort is substantially (but systematically) reduced, and confirm that the INEX collections remain usable when evaluating nonparticipating systems. Finally, they observe that for a fixed amount of effort, judging shallow pools for many queries is better than judging deep pools for a smaller set of queries. However, when judging only a random sample of a pool, it is better to completely judge fewer topics than to partially judge many topics. This result confirms the effectiveness of pooling methods. ?? 2011 Wiley Periodicals, Inc.","Evaluation effort, reliability and reusability in XML retrieval"
['Zhang Daohai'],"Emergency is an important factor resulting in supply disruption risk. From the angle of risk management, we build supply disruption management model of supply chain, and discuss whether decision-making mechanism of the case-based reasoning, in this paper, can bring the better effect by using computational experiment for supply disruption. The main results show that risk assessment, risk identify, risk control and risk evaluation mechanism based on the case-based reasoning can effectively deal with supply disruption risk, bringing more profit and service level for enterprises.",Study on supply disruption management of supply chain based on case-based reasoning
"['Edoardo Pignotti', 'Peter Edwards', 'Alun D. Preece', 'J. Gareth Polhill', 'Nicholas Mark Gotts']","In this paper we explore the use of proposed semantic grid standards and methodology through deployment of a land use modelling service. The FEARLUS-G service architecture is presented which allows large scale simulation experiments to be distributed over the grid. We also discuss ontology support for simulation parameters, hypotheses and results that facilitates sharing and re-use of such resources among land-use scientists. This leads to a description of infrastructure for semantic data management which integrates Jena2 and the ELDAS data access service.",Semantic support for computational land-use modelling
"['Antti Anttonen', 'Ilkka Moilanen', 'Adrian Kotelba', 'Pertti J??rvensivu']","The challenge of the adaptive modulation and coding (AMC) scheme is to make a decision when to switch between different modulation and coding scheme. This is difficult because the performance of the AMC system is very sensitive to channel quality estimation errors and other channel impairments. In this paper, block error rate (BLER) performance of the wideband code division multiple access (WCDMA) system using AMC techniques are studied with nonideal channel and signal-to-interference-and-noise ratio (SINR) estimation and with the presence of multiuser interference. Also, the throughput of the adaptive system is simulated.",Effect of nonideal channel and SINR estimation to performance of WCDMA with adaptive modulation and coding
['David Kirsh'],"The question of how to conceive and represent the context of work is explored from the theoretical perspective of distributed cognition. It is argued that to understand the office work context we need to go beyond tracking superficial physical attributes such as who or what is where and when and consider the state of digital resources, people's concepts, task state, social relations, and the local work culture, to name a few. In analyzing an office more deeply, three concepts are especially helpful: entry points, action landscapes, and coordinating mechanisms. An entry point is a structure or cue that represents an invitation to enter an information space or office task. An activity landscape is part mental construct and part physical; it is the space users interactively construct out of the resources they find when trying to accomplish a task. A coordinating mechanism is an artifact, such as a schedule or clock, or an environmental structure such as the layout of papers to be signed, which helps a user manage the complexity of his task. Using these three concepts we can abstract away from many of the surface attributes of work context and define the deep structure of a setting--the invariant structure that many office settings share. A long-term challenge for context-aware computing is to operationalize these analytic concepts.",The context of work
"['Shoji Kajita', 'F. Itakura']","In this paper, the Subband-Autocorrelation (SBCOR) analysis technique is proposed, and applied to speech recognition. Although the SBCOR analysis is a simplified version of the auditory model proposed by Seneff, it is not an auditory model, but a signal analysis technique based on filter bank and autocorrelation analysis. The SBCOR analysis system is evaluated for five types of filter banks and three autocorrelation detectors using a speaker dependent DTW word recognition system. The experimental results show that the SBCOR spectrum performs as well as the smoothed group delay spectrum under clean condition, and much better under noisy conditions. Finally, it is shown that the optimum filter bank is a fixed Q filter bank whose center frequencies are equally spaced on the Bark scale, and the optimum autocorrelation analysis is a conventional autocorrelation detection without controlling weak signals. An analysis example of the SBCOR is also shown. >",Subband-Autocorrelation analysis and its application for speech recognition
"['Maira Marques', 'Sergio F. Ochoa']","The software industry and the academia have recognized the importance of teamwork as a driver to succeed in software projects. Therefore, the industry expects that new engineers are able to work in teams. Unfortunately, teamwork is a skill that cannot be transferred in a simple way, and there is not a clear recipe for doing that. This paper proposes the use of particular ThinkLets (a process pattern to address collaboration recurrent problems) to help overcome particular problems that jeopardize teamwork. This proposal has been evaluated through software developments in the academia involving computer science undergraduate students.",Improving teamwork in students software projects
"['Simon Oechsner', 'Frank Lehrieder', 'Tobias Hossfeld', 'Florian Metzger', 'Dirk Staehle', 'Konstantin Pussep']","Locality promotion in P2P content distribution networks is currently a major research topic. One of the goals of all discussed approaches is to reduce the interdomain traffic that causes high costs for ISPs. However, the focus of the work in this field is generally on the type of locality information that is provided to the overlay and on the entities that exchange this information. An aspect that is mostly neglected is how this information is used by the peers. In this paper, we consider the predominant approach of Biased Neighbor Selection and compare it with Biased Unchoking, which is an alternative locality aware peer selection strategy that we propose in this paper. We show that both mechanisms complement each other for the BitTorrent file sharing application and achieve the best performance when combined.",Pushing the performance of Biased Neighbor Selection through Biased Unchoking
"['Greg Barish', 'Craig A. Knoblock']","Software agents can be used to automate many of the tedious, time-consuming information processing tasks that humans currently have to complete manually. However, to do so, agent plans must be capable of representing the myriad of actions and control flows required to perform those tasks. In addition, since these tasks can require integrating multiple sources of remote information -- typically, a slow, I/O-bound process -- it is desirable to make execution as efficient as possible. To address both of these needs, we present a flexible software agent plan language and a highly parallel execution system that enable the efficient execution of expressive agent plans. The plan language allows complex tasks to be more easily expressed by providing a variety of operators for flexibly processing the data as well as supporting subplans (for modularity) and recursion (for indeterminate looping). The executor is based on a streaming dataflow model of execution to maximize the amount of operator and data parallelism possible at runtime. We have implemented both the language and executor in a system called THESEUS. Our results from testing THESEUS show that streaming dataflow execution can yield significant speedups over both traditional serial (von Neumann) as well as nonstreaming dataflow-style execution that existing software and robot agent execution systems currently support. In addition, we show how plans written in the language we present can represent certain types of subtasks that cannot be accomplished using the languages supported by network query engines. Finally, we demonstrate that the increased expressivity of our plan language does not hamper performance; specifically, we show how data can be integrated from multiple remote sources just as efficiently using our architecture as is possible with a state-of-the-art streaming-dataflow network query engine.",An expressive language and efficient execution system for software agents
"['S. Gutierrez', 'Gr??gory Valigiani', 'Y. Jamont', 'P. Collet', 'C. Delgado Kloos']","This paper describes a possible application of swarm intelligence techniques in e-learning: an auditing tool for pedagogical planning. Swarm intelligence techniques can be applied to a web system thanks to the fact that the available online material can be organized in a graph by means of hyperlinks. In this case, the swarm that moves on the graph is composed of students who unconsciously leave pheromones in the environment depending on their success or failure. The paper presents the system and shows its capacity to serve as an auditing tool for courses designed by a pedagogical team.",A Swarm Approach for Automatic Auditing of Pedagogical Planning
"['Sue Newell', 'Jacky Swan', 'Maxine Robertson']",,Contextualising knowledge management for innovation
"['Johannes Koskinen', 'Jari Peltonen', 'Petri Selonen', 'Tarja Syst??', 'Kai Koskimies']","The Unified Modeling Language (UML) provides several diagram types, viewing a system from different perspectives. In this research, we exploit the logical relationships between different UML models. We propose operations to compare, merge, slice and synthesize UML diagrams based on these relationships. In a formal demonstration, we show how statechart diagrams can be synthesized semi-automatically from a set of sequence diagrams using an interactive algorithm called MAS. We also demonstrate how a class diagram, annotated with pseudocode presentations of key operations, can be synthesized from sequence diagrams, and how class diagrams and sequence diagrams can be sliced against each other.",Model processing tools in UML
"['Sanjoy Dasgupta', 'Yoav Freund']",We present a simple variant of the k-d tree which automatically adapts to intrinsic low dimensional structure in data without having to explicitly learn this structure.,Random projection trees and low dimensional manifolds
['Jonathan Ginzburg'],Deciding what is the content of an utterance in dialogue is a potentially tricky business: should it be an entity computed using (solely/primarily) grammatical information or is it determined by recognition of participant intention using domain level inference? The decisions one makes on this score play a crucial role in any model of the interaction involved in grounding an utterance. Integrating the clarificatory potential of an utterance into the grounding process transforms the issue of content recognition into a more concrete issue: grammatically determined content has markedly distinct clarificatory potential from content determined using domain level inference. This leads to a new challenge: how to integrate the two types of content in such a way that both enables their distinct clarificatory potential to be maintained and allows content determined by domain level inference to feature in grounding. My talk will address this challenge.,Content Recognition in Dialogue
['Annie Saunders'],"The Princeton University Help Desk KnowledgeBase (KB) is a searchable online information system that publishes Princeton-specific computer solutions to better serve the University community. Heavily used internally by all Office of Information Technology (OIT) support staff, the KB is also marketed and publicized to the entire University community to promote online self-help.    Over the past six years the KnowledgeBase has been molded to consolidate and streamline the documentation provided by OIT, gaining recognition and respect for its usefulness. The Help Desk has been able to increase productivity and its success rate of solving customer problems, with quantifiable results in the numbers of users serviced via the Help Desk web site. This presentation will provide a current look at the Help Desk KnowledgeBase, how it is used, and the plans for future development.",Online solutions: looking to the future of knowledgeBase management
"['Long Cheng', 'Sajal K. Das', 'Canfeng Chen', 'Jian Ma', 'Wendong Wang']","Wireless ad hoc networks can experience significant performance degradation under fading channels. In this work, we present a robust forwarding extension (RFE) for reactive routing protocols in wireless ad hoc networks. RFE is designed to enhance existing reactive routing protocols to provide reliable and energy- efficient packet delivery against the unreliable wireless links. Specifically, we introduce a biased backoff scheme during the route discovery phase to find a robust virtual path, which can provide more cooperative forwarding opportunities. Along this virtual path, data packets are greedily progressed toward the destination through nodes cooperation. We extend the widely used AODV routing protocol with RFE to study its performance. Through extensive simulations, we demonstrate that AODV-RFE effectively improves the reliability, end-to-end energy efficiency and latency.",Robust Forwarding for Reactive Routing Protocols in Wireless Ad Hoc Networks with Unreliable Links
"['Bogdan S. Chlebus', 'Anna Gambin', 'Piotr Indyk']",,PRAM Computations Resilient to Memory Faults
"['K. C. Ho', 'Haiqin Liu', 'Liang Hong']","Modulation identification has many applications, such as interference detection in wireless communication systems. This paper studies the use of wavelet transform to distinguish between CDMA signal and GSM signal. The modulation types of these two signals are different. The former has QPSK modulation while the latter has GMSK modulation. The approach is to use wavelet transform to extract the different transient behaviors of the two signals resulted from their modulation types, and apply template matching in wavelet transform domain for identification. Previous work only applies the wavelet transform at a single scale to an input signal. This paper improves the accuracy of the identifier by combining the identification results at different scales. When E/sub b//N/sub 0/ is -13 dB and the observation period is about 0.5 ms, simulation shows that the percentage of correct identification is 100% for GSM signal and 97.5% for CDMA signal.",On improving the accuracy of a wavelet based identifier to classify CDMA signal and GSM signal
"['Kiarash Bazargan', 'Abhishek Ranjan', 'Majid Sarrafzadeh']","In many applications such as high-level synthesis (HLS) and logic synthesis and possibly engineering change order (ECO) we would like to get fast and accurate estimations of different performance measures of the chip, namely area, delay and power consumption. These measures cannot be estimated with high accuracy unless a fairly detailed layout of the chip, including the floorplan and routing is available, which in turn are very costly processes in terms of running time. As we have entered the deep sub-micron era, we have to deal with designs which contain million gates and up. Not only we should consider the area occupied by the modules, but we also have to consider the wiring congestion. In this paper we propose a cost function that is, in addition to other parameters, a function of the wiring area. We also propose a method, to avoid running the floorplanning process after  every  change in the design, by considering the possible changes in advance and generating a floorplan which is  tolerant  to these modifications, i.e., the changes in the netlist does not dramatically change the performance measures of the chip. Experiments are done in the high-level synthesis domain, but the method can be applied to logic synthesis and ECO as well. We gain speedups of 184% on the average over the traditional estimation methods used in HLS.",Fast and accurate estimation of floorplans in logic/high-level synthesis
"['Edward E. Aftandilian', 'Samuel Z. Guyer']","This paper introduces  GC assertions , a system interface that programmers can use to check for errors, such as data structure invariant violations, and to diagnose performance problems, such as memory leaks. GC assertions are checked by the garbage collector, which is in a unique position to gather information and answer questions about the lifetime and connectivity of objects in the heap. By piggybacking on existing garbage collector computations, our system is able to check heap properties with very low overhead -- around 3% of total execution time -- low enough for use in a deployed setting.   We introduce several kinds of GC assertions and describe how they are implemented in the collector. We also describe our reporting mechanism, which provides a complete path through the heap to the offending objects. We report results on both the performance of our system and the experience of using our assertions to find and repair errors in real-world programs.",GC assertions: using the garbage collector to check heap properties
"['Xiao Cai', 'Syahrulanuar Ngah', 'Hui Zhu', 'Yuji Tanabe', 'Takaaki Baba']","This paper develops pipeline architecture of particle swarm optimization with random time-varying inertia weight and acceleration coefficients (PSO-RTVIWAC) based on existing serial architecture. The proposed architecture incorporated two features to reduce calculation error and keep high speed calculation with acceptable chip area cost. First, actual hardware design is simplified. Then the pipeline mechanism is introduced. The developed system is implemented in field programmable gate array (FPGA) and achieves high stability with slightly increased speed.",Pipeline Architecture of Particle Swarm Optimization
"['Miquel Grau-S?≠nchez', 'Jos?? Luis D??az-Barrero']",A zero-finding technique in which the order of convergence is improved and nonlinear equations are solved more efficiently than they are solved by traditional iterative methods is derived. Composing a modified ChebyshevÉ??Halley method with a variant of this method that just introduces one evaluation of the function the iterative methods presented are obtained. By carrying out this procedure the output numerical results show that the new methods compete in both order and efficiency with the modified ChebyshevÉ??Halley methods.,Some sixth order zero-finding variants of ChebyshevÉ??Halley methods
"['Amir Shahzad', 'Bryn Ll. Jones', 'Eric C. Kerrigan', 'George A. Constantinides']",Descriptor systems consisting of a large number of differential-algebraic equations (DAEs) usually arise from the discretization of partial differential-algebraic equations. This paper presents an efficient algorithm for solving the coupled Sylvester equation that arises in converting a system of linear DAEs to ordinary differential equations. A significant computational advantage is obtained by exploiting the structure of the involved matrices. The proposed algorithm removes the need to solve a standard Sylvester equation or to invert a matrix. The improved performance of this new method over existing techniques is demonstrated by comparing the number of floating-point operations and via numerical examples.,An efficient algorithm for the solution of a coupled Sylvester equation appearing in descriptor systems
['Guoliang Xue'],"We study end-to-end routing in a communication system where there is a link bandwidth and a link propagation delay associated with each link, as well as a queuing delay associate with each intermediate node. We present a polynomial time algorithm for computing an optimal multi-path end-to-end routing to transmit a given message. Examples are also given to show that a previously published path-based algorithm for this problem is suboptimal.",Optimal multi-path end-to-end data transmission in networks
"['Rainer Koster', 'Thorsten Kramp']","Components of a complex multimedia application typically work reactively processing events such as notifications from other threads, signals, and network packets. To better support these applications we have built a message-based threading platform providing more flexibilty than event handling with one thread and easier synchronisation than conventional multithreading approaches. Reuse and reconfiguration are facilitated by using a uniform message interface for all types of events. Moreover, scheduling can be based on timing constraints attached to messages rather than to threads. The reimplementation of a multi-stream video player shows the benefits of this approach.",Using message-based threading for multimedia applications
"['Krishnendu Mukherjee', 'Bijan Sarkar', 'Ardhendu Bhattacharya']","To maximize the total cost of savings, a mixed-integer programming model for remanufacturing proposed by Kim et al. [Kim, K., Song, I., Kim, J., & Jeong, B. (2006). Supply planning model for remanufacturing system in reverse logistics environment. Computers & Industrial Engineering, 51, 279-287] was published in Computers & Industrial Engineering journal to decide optimally the quantity of parts to be processed at each remanufacturing facilities, the number of purchased parts from subcontractor. In this work, comments and suggestions are given to the published model of Kim, Song, Kim, and Jeong (2006). Comments like partial and complete disassembly, disassembly cost, optimum level of disassembly and finally, the modification of constraints are suggested in this work.",Comments on the erratum to Supply planning model for remanufacturing system in reverse logistics environment [Comput. Ind. Eng. 51 (2006) 279-287]
"['Jin Gu Kang', 'Seung Won Yang', 'Jong Yeol Lee']","This paper describes a debugging method for library functions written in assembly language without a target debugging tool. The proposed method can reduce the time required to port codes written in assembly language such as library functions and OS kernels. In the proposed method, an assembly library function is translated into a C function with debugging information which can be used in a host debugging tool. Since a hand-written assembly code is different from a compiler-generated code in the point of translation, a new analysis method is proposed. When translating an assembly code, the debugging information that links the assembly code with the generated C code is inserted as directives in the generated C code. By executing the host executable compiled from the generated C code on a host debugger which exploits the debugging information directives, we can debug the input assembly code. To verify the proposed method, we translate the assembly library functions in libgcc into C codes and execute the generated C code on a host debugger, GDB.",Debugging of assembly library functions without target debugging tools
"['John C. Pendergrass', 'Karen Heart', 'C. Ranganathan', 'V. N. Venkatakrishnan']","Information security within healthcare is paramount and telemedicine applications present unique security challenges. Technology is giving rise to new and advanced telemedicine applications and understanding the security threats to these applications is needed to ensure, among other things, the privacy of patient information. This paper proposes a threat table approach to assess security threats pertaining to telemedicine applications. The concept and its usefulness are illustrated using a case study. This case study focuses on the capture and representation of salient security threats in telemedicine. To analyze the security threats to an application, it presents a threat modeling framework utilizing a table driven approach. The study reveals that even in a highly controlled environment with static locations, the security risks posed by telemedicine applications are significant, and that using a threat table approach provides an easy-to-use and effective method for managing these threats.",A Threat Table Based Assessment of Information Security in Telemedicine
['Christoph Aschwanden'],"This paper focuses on simulating the infectious process on a computer. To simulate how an infectious viral disease spreads not only shows how people get sick, it can also be a powerful tool in disease prevention. You can test actions such as to isolate people that get sick and analyze if the disease spread can be circumscribed or stopped. Special focus is given to SARS and the Common Flu.",Spatial simulation model for infectious viral diseases with focus on SARS and the common flu
"['Barry Bond', 'Kerry Hammil', 'Lubomir Litchev', 'Satnam Singh']",This paper describes the techniques used to describe and synthesize FPGA circuits expressed in a data-parallel domain specific language (DSL) called Accelerator. We identify the subset of data-parallel descriptions that are supported by our system and explain how we track memory access patterns which allow us to generate efficient FPGA circuits.,FPGA Circuit Synthesis of Accelerator Data-Parallel Programs
"['Cheng Zheng', 'Weiming Shen', 'Hamada H. Ghenniwa']","Service discovery and integration has been a very active research field attracting many researchers around the world. Current technologies in service discovery like WSDL and UDDI have deficiencies which prevent their wide applications, especially outside of enterprise areas. Intents is an emerging technology aimed at interconnecting various services. Unlike its counterparts, services in Intents are single topic and given more informative descriptions. This paper examines the essentials of service discovery and integration applied in Intents, presents an architecture of Intents-based systems, and discusses the advantages of Intents over other existing technologies.",An intents-based approach for service discovery and integration
"['Anja Soldan', 'Yunglin Gazes', 'H. John Hilton', 'Yaakov Stern']","This study examined how aging affects the spatial patterns of repetition effects associated with perceptual priming of unfamiliar visual objects. Healthy young (n = 14) and elderly adults (n = 13) viewed four repetitions of structurally possible and impossible figures while being scanned with blood oxygenation level-dependent functional magnetic resonance imaging. Although explicit recognition memory for the figures was reduced in the elder subjects, repetition priming did not differ across the two age groups. Using multivariate linear modeling, we found that the spatial networks of regions that demonstrated repetition-related increases and decreases in activity were identical in both age groups, although there was a trend for smaller magnitude repetition effects in these networks in the elder adults for objects that had been repeated thrice. Furthermore, repetition-related reductions in activity in the left inferior frontal cortex for possible objects correlated with repetition-related facilitation in reaction time across both young and elder subjects. Repetition-related increases of an initially negative response were observed for both object types in both age groups in parts of the default network, suggesting that less attention was required for processing repeated stimuli. These findings extend prior studies using verbal and semantic picture priming tasks and support the view that perceptual repetition priming remains intact in later adulthood because the same spatial networks of regions continue to show repetition-related neural plasticity across the adult life span.",Aging does not affect brain patterns of repetition effects associated with perceptual priming of novel objects
"['Yang Feng', 'Zhenyu Chen']","Software behavior learning is an important task in software engineering. Software behavior is usually represented as a program execution. It is expected that similar executions have similar behavior, i.e. revealing the same faults. Single-label learning has been used to assign a single label (fault) to a failing execution in the existing efforts. However, a failing execution may be caused by several faults simultaneously. Hence, it needs to assign multiple labels to support software engineering tasks in practice. In this paper, we present multi-label software behavior learning. A well-known multi-label learning algorithm ML-KNN is introduced to achieve comprehensive learning of software behavior. We conducted a preliminary experiment on two industrial programs: flex and grep. The experimental results show that multi-label learning can produce more precise and complete results than single-label learning.",Multi-label software behavior learning
"['John Morgan', 'Henrik Orzen', 'Martin Sefton']","This paper presents theory and experiments to investigate how network architecture influences route-choice behavior. We consider changes to networks that, theoretically, exhibit the Pigou-Knight-Downs and Braess Paradoxes. We show that these paradoxes are specific examples of more general classes of network change properties that we term the ""least congestible route"" and ""size"" principles, respectively. We find that technical improvements to networks induce adjustments in traffic flows. In the case of network changes based on the Pigou-Knight-Downs Paradox, these adjustments undermine short-term payoff improvements. In the case of network changes based on the Braess Paradox, these adjustments reinforce the counter-intuitive, but theoretically predicted, effect of reducing payoffs to network users. Although aggregate traffic flows are close to equilibrium levels, we see some systematic deviations from equilibrium. We show that the qualitative features of these discrepancies can be accounted for by a simple reinforcement learning model.",Network architecture and traffic flows: Experiments on the Pigou-Knight-Downs and Braess Paradoxes
"['James Lanagan', 'Paul Ferguson', ""Neil O'Hare"", 'Alan F. Smeaton']","In this paper we examine the effects of noise when creating a real-world weblog corpus for information retrieval. We focus on the DiffPost (Lee et al. 2008) approach to noise removal from blog pages, examining the difficulties encountered when crawling the blogosphere during the creation of a real-world corpus of blog pages. We introduce and evaluate a number of enhancements to the original DiffPost approach in order to increase the robustness of the algorithm. We then extend DiffPost by looking at the anchor-text to text ratio, and discover that the time-interval between crawls is more important to the successful application of noise-removal algorithms within the blog context, than any additional improvements to the removal algorithm itself.",Coping With Noise in a Real-World Weblog Crawler and Retrieval System
['Sanna Liimatainen'],"One new type of a security system is a decentralized authorization system that allows an owner of a resource to decide who can use this resource - even without trust towards external third parties. The idea itself is fascinating, but how about its usability? Security systems are generally considered to be difficult to understand and use. This paper compares the usability features and problems of three decentralized authorization systems: PolicyMaker, KeyNote and SPKI/SDSI. The analysis is based on the Heuristic Evaluation and Cognitive Walkthrough usability test methods. The analysis suggests that approaches of public key infrastructure type, such as the SPKI/SDSI authorization system, have the best usability features but even then the current state of the art is far from the end-user requirements. Designing and measuring usability of these systems is hard, complicated by the fact that security is only a means to achieve the user goal, not the goal as such.",Usability of Decentralized Authorization Systems - A Comparative Study
"['Daniel C. Asmar', 'John S. Zelek', 'Samer M. Abdallah']","In the absence of absolute localization tools such as GPS, a robot can still successfully navigate by conducting simultaneous localization and mapping (SLAM). All SLAM algorithms to date can only be applied in one environment at a time. In this paper we propose to extend SLAM to multi-environments. In SmartSLAM, the robot first classifies its entourage using environment recognition code and then performs SLAM using landmarks that are appropriate for its surrounding milieu. One thousand images of various indoor and outdoor environments were collected and used as training data for a three-layered feedforward backpropagation neural network. This neural network was then tested on two sets of query images of indoor environments and another two sets of outdoor environments, yielding 83% and 95% correct classification rates for the indoor images and 80% and 79% success rates for the outdoor images.",SmartSLAM: localization and mapping across multi-environments
"['Guilhem Chalancon', 'Mickey Kosloff', 'Hatice Ulku Osmanbeyoglu', 'Saras Saraswathi']","The annual international conference on Intelligent Systems for Molecular Biology (ISMB) is the largest meeting of the International Society for Computational Biology (ISCB). In 2010 it was held in Boston, United States, July 11É??13. What follows are four conference postcards that reflect different activities considered exciting and important by younger attendees. Postcards, as the name suggests, are brief reports on the talks and other events that interested attendees. You can read more about the idea of conference postcards at http://www.ploscompbiol.org/doi/pcbi.1000746, and if you are a graduate student or postdoctoral fellow, please consider contributing postcards at any future meetings of interest to the PLoS Computational Biology readership. We want to hear your view of the science being presented.",PLoS Computational Biology conference postcards from ISMB 2010.
"['Ryoulhee Kwak', 'John M. Cioffi']","This paper considers the subchannel-allocation problem to maximize the sum-rate in orthogonal frequency- division multiple-access (OFDMA) regenerative relaying downlink systems with a total power constraint. Based on the Lagrange dual decomposition and the optimal condition of the problem, this paper proposes three heuristic algorithms: an extended modified inverse subchannel signal-to-noise ratio algorithm, a bit accumulation algorithm, and a bit ratio algorithm. Simulation results show that relaying systems with proposed algorithms outperform optimally resource-allocated OFDMA systems without relay stations by almost 100% ~ 270% depending on the system power, which confirms that subchannel-allocation is a key factor in obtaining a huge performance gain.",The Subchannel-Allocation for OFDMA Relaying Downlink Systems with Total Power Constraint
"['Miao Jin', 'Feng Luo', 'Xianfeng David Gu']","Systematically generalizing planar geometric algorithms to manifold domains is of fundamental importance in computer aided design field. This paper proposes a novel theoretic framework, geometric structure, to conquer this problem. In order to discover the intrinsic geometric structures of general surfaces, we developed a theoretic rigorous and practical efficient method, Discrete Variational Ricci flow. Different geometries study the invariants under the corresponding transformation groups. The same geometry can be defined on various manifolds, whereas the same manifold allows different geometries. Geometric structures allow different geometries to be defined on various manifolds, therefore algorithms based on the corresponding geometric invariants can be applied on the manifold domains directly. Surfaces have natural geometric structures, such as spherical structure, affine structure, projective structure, hyperbolic structure and conformal structure. Therefore planar algorithms based on these geometries can be defined on surfaces straightforwardly. Computing the general geometric structures on surfaces has been a long lasting open problem. We solve the problem by introducing a novel method based on discrete variational Ricci flow. We thoroughly explain both theoretical and practical aspects of the computational methodology for geometric structures based on Ricci flow, and demonstrate several important applications of geometric structures: generalizing Voronoi diagram algorithms to surfaces via Euclidean structure, cross global parametrization between high genus surfaces via hyperbolic structure, generalizing planar splines to manifolds via affine structure. The experimental results show that our method is rigorous and efficient and the framework of geometric structures is general and powerful.",Computing general geometric structures on surfaces using Ricci flow
['James C. McKim'],"Teaching Object-Oriented Programming and Design James C. McKim Jr., Hartford Graduate Center  A course in object-oriented programming and design orientedparadigm, namely that it promotes reuse, should address the claimsmade for the object- models the problem space, facilitatesmaintenance, incorporates changes easily, and shortens thedevelopment lifecycle. One way (perhaps the only way) for studentsto test such claims is to build a small but high quality product aspart of the course.  This tutorial shows how to have students in a conventionalcomputer science program build such a product, and addresses suchissues as how to pick good projects, whether and how studentsshould work together in teams, how to keep students on schedule,and how to divide a project into a sequence of deliverables withinthe context of a one semester course.  The tutorial also describes how to scale back the approach sothat students in short courses, common in industry, can get maximumbenefit in the limited time that is allowed.",Teaching object-oriented programming and design (abstract)
"['Gianpiero Cabodi', 'Stefano Quer', 'Paolo Enrico Camurati']",Computing equivalence classes for FSMs has several applications to synthesis and verification problems. Symbolic traversal techniques are applicable to medium-small circuits. This paper extends their use to large FSMs by means of cofactor-based enhancements to the state-of-the-art approaches and of underestimations of equivalence classes. The key to success is pruning the search space by constraining it. Experimental results on some of the larger ISCAS'89 and MCNC circuits show its applicability.,Computing subsets of equivalence classes for large FSMs
"['Junyan Yi', 'Gang Yang', 'Yuki Todo', 'Zheng Tang']","Edge linking is a fundamental computer-vision task, viewed as a constrained optimization problem, it is NP hard- being isomorphic to the classical traveling salesman problem. In this paper, we propose an efficient Elastic Net method for edge linking of images. A dynamic parameter strategy is introduced into the Elastic Net, which enable the network to have superior search ability for edge points and converge sooner to optimal or near-optimal solutions. Simulations are conducted on a series of artificial images. The results confirm that this method effectively improves both the solution quality and convergence speed of the classical Elastic Net.",An efficient Elastic Net method for edge linking of images
"['Kumar Yelamarthi', 'Chien-In Henry Chen']","Performance variation is one of the primary concerns in nanometer-scale dynamic CMOS circuits. This performance variation is worse in circuits with multiple timing paths such as those used in microprocessors. In this paper, a Process Variation-aware Transistor (PVT) sizing algorithm is proposed, which is capable of significantly reducing worst-case delay, delay uncertainty, and delay sensitivity to process variations in dynamic CMOS circuits. The proposed algorithm is based on identifying the significance of all timing paths in the design, increasing the sizes of transistors that appear in most number of paths to reduce delays of most paths. In parallel, it minimizes the channel load by reducing the size of transistors in the interacting paths, which will lead to a power saving. Additional advantages in this algorithm include its simplicity, accuracy, independent of the transistor order, and initial sizing factors. Using 90 nm CMOS process, the proposed algorithm has demonstrated an average improvement in worst-case delay by 36.9%, delay uncertainty by 44.1%, delay sensitivity by 19.8%, and power-delay-product by 35.3% when compared to their initial performances.",Delay optimization considering power saving in dynamic CMOS circuits
['Ayman Farahat'],"We describe an optimize-and-dispatch approach for delivering pay-per-impression advertisements in online advertising. The platform provider for an advertising network commits to showing advertisers' banner ads while capping the number of advertising message shown to a unique user as the user transitions through the network. The traditional approach for enforcing frequency caps has been to use cross-site cookies to track users. However,cross-site cookies and other tracking mechanisms can infringe on the user privacy. In this paper, we propose a novel linear programming approach that decides when to show an ad to the user based solely on the page currently viewed by the users. We show that the frequency caps are fulfilled in expectation. We show the efficacy of that approach using simulation results.",Privacy preserving frequency capping in internet banner advertising
"['Arnaud Clerentin', 'Laurent Delahoche', 'Eric Brassart']","In this paper, an absolute localization paradigm based on the cooperation of an omnidirectional vision system composed of a conical mirror and a CCD camera and a low cost panoramic range finder system is reported. These two sensors, which have been used independently until now, provide some complementary data. This association enables us to build a robust sensorial model which integrates an important number of significant primitives. We can thus realize an absolute localization of the mobile robot in particular configurations, like symmetric environments, where it is not possible to determine the position with the use of only one of the two sensors. In a first part, we present our global perception system. In a second part, we describe our sensorial model building approach. Finally we present an absolute localization method which uses three matching criteria fused thanks to the combination rules of the Dempster-Shafer theory. The basic probability assignment got for each primitive matching enables to estimate the reliability of the localization. We test our global absolute localization system on several robots' elementary moves in an indoor and symmetric environment.",Cooperation between two omnidirectional perception systems for mobile robot localization
"['Michele Scarpiniti', 'Daniele Vigliano', 'Raffaele Parisi', 'Aurelio Uncini']","In this paper a natural gradient approach to blind source separation in complex environment is presented. It is shown that signals can be successfully reconstructed by a network based on the so called generalized splitting activation function (GSAF). This activation function, whose shape is modified during the learning process, is based on a couple of bi-dimensional spline functions, one for the real and one for the imaginary part of the input, thus avoiding the restriction due to the Louiville's theorem. In addition recent learning metrics are compared with the classical ones in order to improve the speed convergence.#R##N##R##N#Several experimental results are shown to demonstrate the effectiveness of the proposed method.",A Flexible Natural Gradient Approach to Blind Separation of Complex Signals
"['Balkaran S. Gill', 'Michael Nicolaidis', 'Francis G. Wolff', 'Christos A. Papachristou', 'Steven L. Garverick']","We propose a new built-in current sensor (BICS) to detect single event upsets (SEUs) in SRAM. The BICS is designed and validated for 100 nm process technology. The BICS reliability analysis is provided for process, voltage and temperature variations, and power supply noise. The BICS detects various shapes of current pulses generated due to particle strike. The BICS power consumption and area overhead are also provided. The BICS is found to be very reliable for process, voltage and temperature variations and under stringent noise conditions.",An Efficient BICS Design for SEUs Detection and Correction in Semiconductor Memories
"['Li Li', 'Changqing Xu', 'Pingzhi Fan', 'Jian He']","In this paper, resource allocation problem in orthogonal frequency division multiple access-based cognitive radio CR systems to maintain minimum transmission rate constraints of CR users CRUs with the specified interference thresholds is investigated. Firstly, a single primary user PU CR system is considered, and a suboptimal resource allocation algorithm to maximize the sum transmission rate of all CRUs is proposed. Secondly, the single-PU scenario is extended to multiple-PU case, and an asymptotically optimal resource allocation algorithm is proposed using dual methods subject to constraints on both interference thresholds of PUs and total transmit power of all CRUs. Analysis and numerical results show that, in contrast to classical resource allocation algorithms, the proposed algorithm can achieve higher transmission rate and guarantee each CRU's minimum transmission rate in both scenarios. Copyright ?? 2012 John Wiley & Sons, Ltd.",Resource allocation in orthogonal frequency division multiple access-based cognitive radio systems with minimum rate constraints
['Jerry M. Mendel'],"Computing the centroid of a type-2 fuzzy set (T2 FS) is an important operation for such sets. For an interval T2 FS, the centroid can be computed by using two iterative procedures that were developed by Karnik and Mendel [2]. In this paper, we prove that if the footprint of uncertainty for an interval T2 FS is symmetrical about the primary variable y at y = m, then the centroid is also symmetrical about y = m and its defuzzified value equals m. As a consequence of this, computation of the centroid for such a T2 FS is reduced by 50%, and the importance of obtaining a non-symmetrical interval T2 FS prior to defuzzification is demonstrated.",On a 50% savings in the computation of the centroid of a symmetrical interval type-2 fuzzy set
"['Xiang He', 'Aylin Yener']","In this work, we derive the secrecy degrees of freedom (s.d.o.f.) region of the Gaussian two-way wiretap channel in which the eavesdropper channel state is arbitrarily varying and is unknown to the legitimate nodes. We prove that the s.d.o.f. region is identical to that when the eavesdropper channel is fixed and globally known. A multi-stage coding scheme that combines secret key generation and confidential message transmission is developed to prove achievability. The confidentiality guarantee provided in this work is in the sense of strong secrecy.",Gaussian two-way wiretap channel with an arbitrarily varying eavesdropper
"['Jonathan Fiene', 'G?¨nter Niemeyer']","In this paper we propose an integrated motion control architecture based on a full third order motor model. We investigate the advantages and feasibility of controlling a motor using very fast (MHz) switching using digital components in place of traditional amplifiers. A switching controller combines the current and encoder (position) feedback paths into a single loop, and a model based observer estimates the unsensed motor velocity. When compared to second order controllers built with traditional amplifiers, the proposed design promises increased performance, better efficiency, and unproved velocity estimation. Advances in semiconductor technology facilitate the implementation of these fast switches and the required logic using relatively standard components. Preliminary experiments verify good behavior of a DC motor driven up to 5 MHz.",Switching motor control: an integrated amplifier design for improved velocity estimation and feedback
"['H. Hannoun', 'Mickae??l Hilairet', 'Claude Marchand']","This paper presents an innovative torque control strategy for a switched reluctance machine (SRM) operating in continuous-conduction mode (CCM). The proposed strategy was first designed to control the torque in discontinuous-conduction mode and then extended to operate in CCM as well. With CCM, the torque can be increased at high speed without modifying the number of turns, the source voltage, or the converter; it is achieved through the control. Reviews of past control strategies, as well as the principle of this mode and its influence on the machine performance, are presented. The advantages and drawbacks of the proposed strategy are outlined. In particular, the power and torque densities of the SRM are improved so that this machine becomes comparable with ac machines without the drawback of field weakening. A new control law has been studied and implemented; simulation and experimental results are provided to demonstrate the effectiveness of the proposed control.",Experimental Validation of a Switched Reluctance Machine Operating in Continuous-Conduction Mode
"['Amotz Bar-Noy', 'Zohar Naor']","The issue of tracking a group of users is discussed in this study. Given the condition that the search is over only after all the users in the group are found, this problem is called the conference call search (CCS) problem. The goal is to design efficient CCS strategies under delay and bandwidth constraints. While the problem of tracking a single user has been addressed by many studies, to the best of our knowledge, this study is one of the first attempts to reduce the search cost for multiple users. Moreover, as oppose to the single user tracking, for which one can always reduce the expected search delay by increasing the expected search cost, for a multiple users search the dependency between the delay and the search cost is more complicated, as demonstrated in this study. We identify the key factors affecting the search efficiency, and the dependency between them and the search delay. Our analysis shows that under tight bandwidth constraints, the CCS problem is NP-hard. We therefore propose a search method that is not optimal, but has a low computational complexity. In addition, the proposed strategy yields a low search delay as well as a low search cost. The performance of the proposed search strategy is superior to the implementation of an optimal single user search on a group of users.",Establishing a mobile conference call under delay and bandwidth constraints
"['Lisa Anthony', 'Jie Yang', 'Kenneth R. Koedinger']","This paper presents the interaction design of, and demonstration of technical feasibility for, intelligent tutoring systems that can accept handwriting input from students. Handwriting and pen input offer several affordances for students that traditional typing-based interactions do not. To illustrate these affordances, we present evidence, from tutoring mathematics, that the ability to enter problem solutions via pen input enables students to record algebraic equations more quickly, more smoothly (fewer errors), and with increased transfer to non-computer-based tasks. Furthermore our evidence shows that students tend to like pen input for these types of problems more than typing. However, a clear downside to introducing handwriting input into intelligent tutors is that the recognition of such input is not reliable. In our work, we have found that handwriting input is more likely to be useful and reliable when context is considered, for example, the context of the problem being solved. We present an intelligent tutoring system for algebra equation solving via pen-based input that is able to use context to decrease recognition errors by 18% and to reduce recognition error recovery interactions to occur on one out of every four problems. We applied user-centered design principles to reduce the negative impact of recognition errors in the following ways: (1) though students handwrite their problem-solving process, they type their final answer to reduce ambiguity for tutoring purposes, and (2) in the small number of cases in which the system must involve the student in recognition error recovery, the interaction focuses on identifying the student's problem-solving error to keep the emphasis on tutoring. Many potential recognition errors can thus be ignored and distracting interactions are avoided. This work can inform the design of future systems for students using pen and sketch input for math or other topics by motivating the use of context and pragmatics to decrease the impact of recognition errors and put user focus on the task at hand.",A paradigm for handwriting-based intelligent tutors
"['Mark J. W. Lee', 'Catherine McLoughlin', 'Anthony Chan']","Podcasting allows audio content from one or more user-selected feeds or channels to be automatically downloaded to oneÉ??s computer as it becomes available, then later transferred to a portable player for consumption at a convenient time and place. It is enjoying phenomenal growth in mainstream society, alongside other Web 2.0 technologies that enable Internet users to author and distribute rich media content quickly and easily. Instead of using the technology for the mere recording and dissemination of lectures and other instructor-centred information, the project reported on in this article focused on enabling students to create their own podcasts for distribution to their peers. The article describes how engaging in the podcasting exercise promoted collaborative knowledge building among the student-producers, as evidenced through focus-group interviewing and an analysis of the products of their shared dialogue and reflection. The findings suggest that the collaborative development of audio learning objects enabling student conceptualisations of disciplinary content to be shared with peers is a powerful way of stimulating both individual and collective learning, as well as supporting social processes of perspective-taking and negotiation of meaning that underpin knowledge creation. Background and introduction The aim of the study described in the present paper was to enable a group of volunteer undergraduate students to create digital audio clips, structured as weekly, 3 to 5 minute talkback radio-style É??showsÉ??, and share them with their peers through podcasting. It was hoped that by listening to background material and by being exposed to the terminology of the subject area, members of the student audience would be better prepared to participate in classes. The authors believed that in this way, the podcasts could be part of an effective solution to help alleviate the listenersÉ?? preclass anxiety and allay their",Talk the talk: LearnerÉ?êgenerated podcasts as catalysts for knowledge creation
['Roderic D. M. Page'],"Summary: GeneTree is a program for comparing gene and species trees using reconciled trees. The program can compute the cost of embedding a gene tree within a species tree, visually display the location and number of gene duplications and losses, and search for optimal species trees. Availability: The program is free and is available at {{http://taxonomy.zoology.gla.ac.uk/rod/genetree/gene-tree.html}}. Contact: r.page@bio.gla.ac.uk",GeneTree: comparing gene and species phylogenies using reconciled trees.
"['Mustafa M. Matalgah', 'Redha M. Radaydeh']","In this paper, we propose a design for the transceiver of a new modulation scheme that is based on combining frequency and polarization modulated signals. This is referred to as hybrid frequency-polarization shift keying (FPoISK) modulation. This modulation enables representing the signal points over multidimensional constellation points space, which ensures increasing the geometric distances between signal points, and in turn, improving the system power efficiency. To demonstrate the efficiency of our proposed technique, we derive an exact expression of the symbol error probability, and conduct a performance comparison between the proposed modulation technique and previously developed ones. Our results reveal that the proposed modulation scheme performs better than L-PoISK, L-DPSK, and L-FSK modulation schemes in terms of power efficiency.",Power-efficient multi-level modulation scheme for high-speed optical communications
"['Andr?? Valente', 'Joost Breuker']",,ON-LINE: an architecture for modelling legal information
"['Talha Zahir', 'Kamran Arshad', 'Atsushi Nakata', 'Klaus Moessner']","Increase in system capacity and data rates can be achieved efficiently in a wireless system by getting the transmitter and receiver closer to each other. Femtocells deployed in the macrocell significantly improve the indoor coverage and provide better user experience. The femtocell base station called as Femtocell Access Point (FAP) is fully user deployed and hence reduces the infrastructure, maintenance and operational cost of the operator while at the same time providing good Quality of Service (QoS) to the end user and high network capacity gains. However, the mass deployment of femtocell faces a number of challenges, among which interference management is of much importance, as the fundamental limits of capacity and achievable data rates mainly depends on the interference faced by the femtocell network. To cope with the technical challenges including interference management faced by the femtocells, researchers have suggested a variety of solutions. These solutions vary depending on the physical layer technology and the specific scenarios considered. Furthermore, the cognitive capabilities, as a functionality of femtocell have also been discussed in this survey. This article summarises the main concepts of femtocells that are covered in literature and the major challenges faced in its large scale deployment. The main challenge of interference management is discussed in detail with its types in femtocells and the solutions proposed over the years to manage interference have been summarised. In addition an overview of the current femtocell standardisation and the future research direction of femtocells have also been provided.",Interference Management in Femtocells
"['Lin Ma', 'Roger D. Chamberlain']","Graphics engines are excellent execution platforms for high-throughput computations that exploit a large degree of available parallelism. The achieved performance is, however, highly dependent on the access patterns that the applicationimposes on the memory subsystem. Here, we propose an analytic model that helps improve the understanding of the performance of memory-limited kernels that employ randommemory access schemes, especially as impacted by cache andvarious configuration parameters that can be used to tunekernel execution, such as the number of blocks and the number of threads per block. The analytic model is first explored through the use of a synthetic micro-benchmark, which is then followed by an empirical validation using a pair of production applications used in computational biology.",A Performance Model for Memory Bandwidth Constrained Applications on Graphics Engines
"['Dan Chen', 'Hong Ji', 'Victor C. M. Leung']","In cognitive radio (CR) networks, cooperative relaying is emerging as a key technology to improve the performance of secondary users (SUs), while ensuring the quality of service of primary transmissions. Most previous work considers maximizing physical layer throughput as a design criterion. However, the end-to-end Transmission Control Protocol (TCP) performance perceived by SUs is largely ignored. In this paper, we take a cross-layer design approach to jointly consider optimal relay selection strategy, physical layer adaptive modulation and coding and data-link layer frame size to maximize the TCP throughput in CR networks. Specifically, we formulate the CR relay network as a restless bandit system, where the finite-state Markov channel model is used to characterize the time-varying channel state. With this stochastic optimization formulation, the optimal policy has indexability property that dramatically reduces the computation and implementation complexity. Simulation results are presented to show the effectiveness of the proposed scheme.",Distributed Optimal Relay Selection for Improving TCP Throughput over Cognitive Radio Networks: A Cross-Layer Design Approach
['Juergen Schmidhuber'],"Do you want your neural net algorithm to learn sequences? Do not limit yourself to conventional gradient descent (or approximations thereof). Instead, use your sequence learning algorithm (any will do) to implement the following method for history compression. No matter what your final goals are, train a network to predict its next input from the previous ones. Since only unpredictable inputs convey new information, ignore all predictable inputs but let all unexpected inputs (plus information about the time step at which they occurred) become inputs to a higher-level network of the same kind (working on a slower, self-adjusting time scale). Go on building a hierarchy of such networks. This principle reduces the descriptions of event sequences without loss of information, thus easing supervised or reinforcement learning tasks. Alternatively, you may use two recurrent networks to collapse a multi-level predictor hierarchy into a single recurrent net. Experiments show that systems based on these principles can require less computation per time step and many fewer training sequences than conventional training algorithms for recurrent nets. Finally you can modify the above method such that predictability is not defined in a yes-or-no fashion but in a continuous fashion.",Learning Unambiguous Reduced Sequence Descriptions
"['Emily B. Fox', 'Michael I. Jordan', 'Erik B. Sudderth', 'Alan S. Willsky']","We propose a Bayesian nonparametric approach to the problem of modeling related time series. Using a beta process prior, our approach is based on the discovery of a set of latent dynamical behaviors that are shared among multiple time series. The size of the set and the sharing pattern are both inferred from data. We develop an efficient Markov chain Monte Carlo inference method that is based on the Indian buffet process representation of the predictive distribution of the beta process. In particular, our approach uses the sum-product algorithm to efficiently compute Metropolis-Hastings acceptance probabilities, and explores new dynamical behaviors via birth/death proposals. We validate our sampling algorithm using several synthetic datasets, and also demonstrate promising results on unsupervised segmentation of visual motion capture data.",Sharing Features among Dynamical Systems with Beta Processes
"['Hongwei Ding', 'Lyes Benyoucef', 'Xiaolan Xie']","A simulation-based multiobjective optimization method is proposed in this paper for joint decision-making on strategic sourcing and inventory replenishment. More specifically, a multiobjective genetic algorithm is developed to determine the optimal supplier portfolio and inventory control parameters in order to reach best compromise of two conflicting criteria: costs and demand fill-rate. Discrete-event simulation is used to provide faithful evaluation of these two criteria. Numerical results on a variant of a real case study are presented.",A multiobjective optimization method for strategic sourcing and inventory replenishment
"['Dustin McIntire', 'Thanos Stathopoulos', 'Sasank Reddy', 'Thomas Schmidt', 'William J. Kaiser']","A broad range of embedded networked sensing (ENS) applications have appeared for large-scale systems, introducing new requirements leading to new embedded architectures, associated algorithms, and supporting software systems. These new requirements include the need for diverse and complex sensor systems that present demands for energy and computational resources, as well as for broadband communication. To satisfy application demands while maintaining critical support for low-energy operation, a new multiprocessor node hardware and software architecture, Low Power Energy Aware Processing (LEAP), has been developed. In this article, we described the LEAP design approach, in which the system is able to adaptively select the most energy-efficient hardware components matching an applicationÉ??s needs. The LEAP platform supports highly dynamic requirements in sensing fidelity, computational load, storage media, and network bandwidth. It focuses on episodic operation of each component and considers the energy dissipation for each platform task by integrating fine-grained energy-dissipation monitoring and sophisticated power-control scheduling for all subsystems, including sensors. In addition to the LEAP platformÉ??s unique hardware capabilities, its software architecture has been designed to provide an easy way to use power management interface and a robust, fault-tolerant operating environment and to enable remote upgrade of all software components. LEAP platform capabilities are demonstrated by example implementations, such as a network protocol design and a light source event detection algorithm. Through the use of a distributed node testbed, we demonstrate that by exploiting high energy-efficiency components and enabling proper on-demand scheduling, the LEAP architecture may meet both sensing performance and energy dissipation objectives for a broad class of applications.","Energy-Efficient Sensing with the Low Power, Energy Aware Processing (LEAP) Architecture"
"['Amit Bahl', 'Brian P. Brunk', 'Ross L. Coppel', 'Jonathan Crabtree', 'Sharon J. Diskin', 'Martin J. Fraunholz', 'Gregory R. Grant', 'Dinesh Gupta', 'Robert Huestis', 'Jessica C. Kissinger', 'Philip Labo', 'Li Li', 'Shannon McWeeney', 'Arthur J. Milgram', 'David S. Roos', 'Jonathan Schug', 'J Christian Stoeckert']","PlasmoDB (http://PlasmoDB.org) is the official database of the Plasmodium falciparum genome sequencing consortium. This resource incorporates finished and draft genome sequence data and annotation emerging from Plasmodium sequencing projects. PlasmoDB currently houses information from five parasite species and provides tools for cross-species comparisons. Sequence information is also integrated with other genomic-scale data emerging from the Plasmodium research community, including gene expression analysis from EST, SAGE and microarray projects. The relational schemas used to build PlasmoDB [Genomics Unified Schema (GUS) and RNA Abundance Database (RAD)] employ a highly structured format to accommodate the diverse data types generated by sequence and expression projects. A variety of tools allow researchers to formulate complex, biologically based queries of the database. A version of the database is also available on CD-ROM (Plasmodium GenePlot), facilitating access to the data in situations where Internet access is difficult (e.g. by malaria researchers working in the field). The goal of PlasmoDB is to enhance utilization of the vast quantities of data emerging from genome-scale projects by the global malaria research community.","PlasmoDB: the Plasmodium genome resource. An integrated database providing tools for accessing, analyzing and mapping expression and sequence data (both finished and unfinished)"
['Lukasz Debowski'],"This paper concerns the rates of power law growth of mutual information computed for a stationary measure or for a universal code. The rates are called Hilberg exponents, and four such quantities are defined for each measure and each code: two random exponents and two expected exponents. A particularly interesting case arises for the conditional algorithmic mutual information. In this case, the random Hilberg exponents are almost surely constant on ergodic sources and are bounded by the expected Hilberg exponents. This property is the second-order analog of the ShannonÉ??McMillanÉ??Breiman theorem, proved without invoking the ergodic theorem. It carries over to Hilberg exponents for the underlying probability measure via ShannonÉ??Fano coding and Barron inequality. Moreover, the expected Hilberg exponents can be linked for different universal codes. Namely, if one code dominates another, the expected Hilberg exponents are greater for the former than for the latter. This paper is concluded by an evaluation of Hilberg exponents for certain sources, such as the mixture Bernoulli process and the Santa Fe processes.",Hilberg Exponents: New Measures of Long Memory in the Process
"['Hyun-Chul Choi', 'Se-Young Oh']","This paper proposes an efficient human-like memory and memory management which utilizes Walsh-based distributed associative memory in reducing the computer storage and processing for pattern recognition. As a verification example, a memory storing 26 binary alphabet images takes only the physical space needed to store 8 patterns and yet capable of perfect recognition. Further, the experimental results show that the proposed memory management strategy can nicely deal with data transfer from short-term (working) memory to long-term memory.",Efficient Human-like Memory Management based on Walsh-based Associative Memory for Real-time Pattern Recognition
"['Marius Pachitariu', 'Adam M Packer', 'Noah Pettit', 'Henry Dalgleish', 'Michael H??usser', 'Maneesh Sahani']","Biological tissue is often composed of cells with similar morphologies replicated throughout large volumes and many biological applications rely on the accurate identification of these cells and their locations from image data. Here we develop a generative model that captures the regularities present in images composed of repeating elements of a few different types. Formally, the model can be described as convolutional sparse block coding. For inference we use a variant of convolutional matching pursuit adapted to block-based representations. We extend the K-SVD learning algorithm to subspaces by retaining several principal vectors from the SVD decomposition instead of just one. Good models with little cross-talk between subspaces can be obtained by learning the blocks incrementally. We perform extensive experiments on simulated images and the inference algorithm consistently recovers a large proportion of the cells with a small number of false positives. We fit the convolutional model to noisy GCaMP6 two-photon images of spiking neurons and to Nissl-stained slices of cortical tissue and show that it recovers cell body locations without supervision. The flexibility of the block-based representation is reflected in the variability of the recovered cell shapes.",Extracting regions of interest from biological images with convolutional sparse block coding
['Craig Boutiller'],"Belief revision and belief update have been proposed as two types of belief change serving differ ent purposes Belief revision is intended to capture changes of an agent's belief state reflecting new information about a static world Belief update is intended to capture changes of belief in response to a changing world. We argue that both belief revision and belief update are too restrictive, routine belief change involves elements of both. We present a model for generalized update that allows updates in response to external changes to inform the agent about its prior beliefs. This model of update combines aspects of revision and update, providing a more realistic characterization of belief change. We show that, under certain assumptions, the original update postulates are satisfied. We also demonstrate that plain revision and plain update are special cases of our model, in a way that formally verifies the intuition that revision is suitable for ""static"" belief change.",Generalized update: belief change in dynamic settings
"['Yuri Demchenko', 'Cees de Laat', 'Jeroen van der Ham', 'Mattijs Ghijsen', 'Volodymyr Yakovenko', 'Mihai Cristea']","Effective use of existing network and IT infrastructure can be achieved by providing combined network and IT resources on-demand as infrastructure services that are capable of supporting complex technological processes, scientific experiments, and collaborative groups of researchers and applications. This paper provides a short overview of existing standards and technologies and refers to ongoing projects. We also describe experiences in developing an architectural framework and tools for combined on-demand network and Grid/Cloud service provisioning. The paper proposes an architectural framework for on-demand infrastructure service provisioning comprising of three main components: the Composable Services Architecture (CSA) that intends to provide a conceptual and methodological framework for developing dynamically configurable virtualised infrastructure services; the Infrastructure Services Modeling Framework (ISMF) that provides a basis for the infrastructure resources virtualisation and management, including description, discovery, modeling, composition and monitoring; and the Service Delivery Framework (SDF) that provides a basis for defining the whole composable services life cycle management and supporting infrastructure services. We discuss implementation suggestions for the defined architectural components and provides information about the ongoing developments of the GEMBus which is considered as a middleware framework for CSA.",On-demand provisioning of Cloud and Grid based infrastructure services for collaborative projects and groups
"['Carlos Prieto', 'Javier De Las Rivas']","Agile Protein Interaction DataAnalyzer (APID) is an interactive bioinformatics web tool developed to integrate and analyze in a unified and comparative platform main currently known information about proteinÉ??protein interactions demonstrated by specific small-scale or large-scale experimental methods. At present, the application includes information coming from five main source databases enclosing an unified sever to explore .35 000 different proteins and 111 000 different proven interactions. The web includes search tools to query and browse upon the data, allowing selection of the interaction pairs based in calculated parameters that weight and qualify the reliability of each given protein interaction. Such parameters are for the É??proteinsÉ??: connectivity, cluster coefficient, Gene Ontology (GO) functional environment, GO environment enrichment; and for the É??interactionsÉ??: number of methods, GO overlapping, iPfam domainÉ??domain interaction. APID also includes a graphic interactive tool to visualize selected sub-networks and to navigate on them or along the whole interaction network. The application is available open access at http://bioinfow.dep.usal. es/apid/.",APID: Agile Protein Interaction DataAnalyzer
"['Z. M. Ma', 'Wenjun Zhang', 'Fatma Mili']","Abstract#R##N##R##N#In this article, we focus on the issues of fuzzy data dependencies. After introducing the notion of semantic equivalence degree, fuzzy functional and multivalued dependencies are defined. A set of sound and complete inference rules, similar to Armstrong's axioms for classic cases, for fuzzy functional dependencies (FFDs) and fuzzy multivalued dependencies (FMVDs) are proposed. The strategies and approaches for compressing fuzzy values by FFDs and FMVDs are investigated. By such processing, the unnecessary elements are eliminated from a fuzzy value and its range is compressed. ?? 2002 Wiley Periodicals, Inc.",Fuzzy data compression based on data dependencies
"['C??dric Archambeau', 'Nicolas Delannay', 'Michel Verleysen']","Principal components and canonical correlations are at the root of many exploratory data mining techniques and provide standard pre-processing tools in machine learning. Lately, probabilistic reformulations of these methods have been proposed (Roweis, 1998; Tipping & Bishop, 1999b; Bach & Jordan, 2005). They are based on a Gaussian density model and are therefore, like their non-probabilistic counterpart, very sensitive to atypical observations. In this paper, we introduce robust probabilistic principal component analysis and robust probabilistic canonical correlation analysis. Both are based on a Student- t  density model. The resulting probabilistic reformulations are more suitable in practice as they handle outliers in a natural way. We compute maximum likelihood estimates of the parameters by means of the EM algorithm.",Robust probabilistic projections
"['In?¶s M. Cec??lio', 'James R. Ottewill', 'Harald Fretheim', 'Nina F. Thornhill']","This paper presents a method to detect transient disturbances in a multivariate context, and an extension of that method to handle multirate systems. Both methods are based on a time series analysis technique known as nearest neighbors, and on multivariate statistics implemented as a singular value decomposition. The motivation for these developments is that there is an increasing industrial requirement for the analysis of data sets comprising measurements from industrial processes together with their associated electrical and mechanical equipment. These systems are increasingly affected by transient disturbances, and their measurements are commonly sampled at different rates. This paper demonstrates superior results with the multivariate method in comparison with the univariate approach, and with the multirate method in comparison to a unirate method, for which the fast-sampled measurements had to be downsampled. The method is demonstrated on experimental and industrial case studies.",Multivariate Detection of Transient Disturbances for Uni- and Multirate Systems
"['Christian A. Larsen', 'Thomas B. Moeslund']","3D reconstruction and texturing of buildings have a large number of applications and have therefore been the focus of much attention in recent years. One aspect that is still lacking, however, is a way to reconstruct recessed features such as windows and doors. These may have little value when seen from a frontal viewpoint. But when the reconstructed model is rotated and zoomed the lack of details will leap out. In this work we therefore aim at reconstructing a 3D model with refined details. To this end we apply a structure from motion approach based on bottom up bundle adjustment to first estimate a 3D point cloud of a building. Next, a rectified texture of the facade is extracted and analyzed in order to detect recessed features and their depths, and enhance the 3D model accordingly. For evaluation we apply the method to a number of different buildings.",3D reconstruction of buildings with automatic facade refinement
"['J Nyhan', 'Oliver Duke-Williams']","The stereotype of the multi-authored Digital Humanities paper is well known but has not, until now, been empirically investigated. Here we present the results of a statistical analysis of collaborative publishing patterns in Computers and the Humanities (CHum) (1966-2004); Literary and Linguistic Computing (LLC) (1986-2011); and, as a control, the Annals of the Association of American Geographers (AAAG) (1966-2013) in order to take a first step towards investigat- ing concepts of 'collaboration' in Digital Humanities. We demonstrate that in two core Digital Humanities journals, CHum and LLC, single-authored papers predominate. In AAAG, single-authored papers are also predominant. In regard to multi-authored papers the statistically significant increases are more wide- ranging in AAAG than in either LLC or CHum, with increases in all forms of multi-authorship. The author connectivity scores show that in CHum, LLC, and AAAG, there is a relatively small cohort of authors who co-publish with a wide set of other authors, and a longer tail of authors for whom co-publishing is less common.",Joint and multi-authored publication patterns in the Digital Humanities
"['Raymond E. Barnett', 'Jin Liu']","A 0.8V nano-power relaxation oscillator for EPC standard UHF RFID transponder is presented. A low-voltage inverted mirror feedback V GS /R reference is proposed to provide correlated current and voltage references for the oscillator. As a result, the oscillator frequency is solely determined by the resistor in the reference and the timing capacitor to meet the frequency tolerance specification. Meanwhile, to minimize the power consumption, a minimum-supply-voltage-constraint (MSVC) design criterion is proposed to minimize the required supply voltage. The inverted mirror feedback technique reduces the headroom requirement of the traditional V GS  /R to meet the MSVC. Measurement results show that the entire oscillator requires a minimum supply voltage of 0.8V in the prototype chip fabricated in CMOS 0.13mum technologies. The measured oscillation frequency is 1.52MHz with 400nA total current consumption. The chip area is 13400mum 2",A 0.8V 1.52MHz MSVC Relaxation Oscillator with Inverted Mirror Feedback Reference for UHF RFID
"['Jonathan Eckstein', 'Avigdor Gal', 'Sarit Reiner']","We describe scheduling algorithms for monitoring a single information source whose contents change at times modeled by a nonhomogeneous Poisson process. In a given time period of length T, we enforce a server-side politeness constraint that we may only probe the source at most n times. This constraint, along with an optional constraint that no two probes may be spaced less than ?? time units apart, is intended to prevent the monitor from being classified as a nuisance to be É??locked outÉ?ù of the information source. To develop our algorithms, we use a portion of the cost model developed in our earlier work. Our first algorithm assumes a discrete set of N >n possible update times, and uses dynamic programming to identify a provably optimal subset of n of these times at which to probe the server. Our second algorithm is a simple direct search for locally improving any continuous-time schedule with respect to the same cost model. In particular, this improvement procedure may be applied to the schedule obtained from our first algorithm. We evaluate our algorithms using real-world data feeds.",Monitoring an Information Source Under a Politeness Constraint
"['Peter Buchholz', 'Peter Kemper']","The analysis of stochastic marked graphs is considered. The underlying idea is to decompose the marked graph into subnets, to generate state spaces and transition matrices for these isolated parts and then to represent the generator matrix underlying the complete net by means of much smaller subnet matrices combined via tensor operations. Based on this matrix representation, efficient numerical analysis techniques can be used to compute the stationary solution. Furthermore we propose on approximation technique which is similar to known approximate solution techniques for this kind of nets, but our approach is completely integrated in the structured description of the generator matrix. This allows an estimation of the approximation error and the usage of the approximate results as an initial guess for a subsequent iterative analysis, such that the number of required iterations is often significantly reduced.",Numerical analysis of stochastic marked graph nets
"['Neil Robertson', 'Ian D. Reid 0001']","In this paper we develop a system for human behaviour recognition in video sequences. Human behaviour is modelled as a stochastic sequence of actions. Actions are described by a feature vector comprising both trajectory information (position and velocity), and a set of local motion descriptors. Action recognition is achieved via probabilistic search of image feature databases representing previously seen actions. A HMM which encodes the rules of the scene is used to smooth sequences of actions. High-level behaviour recognition is achieved by computing the likelihood that a set of predefined hidden Markov models explains the current action sequence. Thus, human actions and behaviour are represented using a hierarchy of abstraction: from simple actions, to actions with spatio-temporal context, to action sequences and finally general behaviours. While the upper levels all use (parametric) Bayes networks and belief propagation, the lowest level uses nonparametric sampling from a previously learned database of actions. The combined method represents a general framework for human behaviour modelling. In this paper we demonstrate the results chiefly on broadcast tennis sequences for automated video annotation.",Behaviour understanding in video: a combined method
"['Karen Kapur', 'Hui Jiang', 'Yi Xing', 'Wing Hung Wong']","Motivation: Microarray designs have become increasingly probe-rich, enabling targeting of specific features, such as individual exons or single nucleotide polymorphisms. These arrays have the potential to achieve quantitative high-throughput estimates of transcript abundances, but currently these estimates are affected by biases due to cross-hybridization, in which probes hybridize to off-target transcripts.#R##N##R##N#Results: To study cross-hybridization, we map Affymetrix exon array probes to a set of annotated mRNA transcripts, allowing a small number of mismatches or insertion/deletions between the two sequences. Based on a systematic study of the degree to which probes with a given match type to a transcript are affected by cross-hybridization, we developed a strategy to correct for cross-hybridization biases of gene-level expression estimates. Comparison with Solexa ultra high-throughput sequencing data demonstrates that correction for cross-hybridization leads to a significant improve-ment of gene expression estimates.#R##N##R##N#Availability: We provide mappings between human and mouse exon array probes and off-target transcripts and provide software extending the GeneBASE program for generating gene-level expression estimates including the cross-hybridization correction http://biogibbs.stanford.edu/~kkapur/GeneBase/.#R##N##R##N#Contact: whwong@stanford.edu#R##N##R##N#Supplementary information:Supplementary data are available at Bioinformatics online.",Cross-hybridization modeling on Affymetrix exon arrays
"['Christo Wilson', 'Alessandra Sala', 'Krishna P. N. Puttaswamy', 'Ben Y. Zhao']","Social networks are popular platforms for interaction, communication, and collaboration between friends. Researchers have recently proposed an emerging class of applications that leverage relationships from social networks to improve security and performance in applications such as email, Web browsing, and overlay routing. While these applications often cite social network connectivity statistics to support their designs, researchers in psychology and sociology have repeatedly cast doubt on the practice of inferring meaningful relationships from social network connections alone. This leads to the question: É??Are social links valid indicators of real user interaction? If not, then how can we quantify these factors to form a more accurate model for evaluating socially enhanced applications?É?ù In this article, we address this question through a detailed study of user interactions in the Facebook social network. We propose the use of É??interaction graphsÉ?ù to impart meaning to online social links by quantifying user interactions. We analyze interaction graphs derived from Facebook user traces and show that they exhibit significantly lower levels of the É??small-worldÉ?ù properties present in their social graph counterparts. This means that these graphs have fewer É??supernodesÉ?ù with extremely high degree, and overall graph diameter increases significantly as a result. To quantify the impact of our observations, we use both types of graphs to validate several well-known social-based applications that rely on graph properties to infuse new functionality into Internet applications, including Reliable Email (RE), SybilGuard, and the weighted cascade influence maximization algorithm. The results reveal new insights into each of these systems, and confirm our hypothesis that to obtain realistic and accurate results, ongoing research on social network applications studies of social applications should use real indicators of user interactions in lieu of social graphs.",Beyond Social Graphs: User Interactions in Online Social Networks and their Implications
"['K. N. Sridhar', 'Lillykutty Jacob']","In this paper a performance study of the service differentiation mechanism 'stateless wireless adhoc networks (SWAN)', proposed for mobile adhoc networks (MANETs), is first carried out with adhoc on-demand distance vector (AODV), dynamic source routing (DSR) and associativity based routing (ABR) protocols. The paper then investigates any possible performance advantage by making SWAN benefit out of the features of the routing protocol The end-to-end probing mechanism of SWAN is reduced to probing the routing protocol within the source node. The burden of estimating the bottleneck bandwidth is thus passed on to the routing protocol.",Interplay of service differentiation and routing mechanisms in MANETs: a performance study
['Chee-Keng Yap'],"The door width of a simple polygon (a chair) is defined and an O(n^{2}) algorithm for computing its door width is given. It is first shown that all passages of the chair through the door can be reduced to a sequence of certain elementary motions. The technique of constraint analysis in characterizing elementary motions is introduced. Our algorithm actually constructs a motion of the chair through a door, and thus is a ""local expert"" for planning motion through doors. Such algorithms have applications in more general motion-planning systems in robotics.",How to move a chair through a door
"['Stavros K. Filippidis', 'Ioannis A. Tsoukalas']","The purpose of this paper is to compare the use of two methods of instruction where adaptable multimedia content is used in teaching an informatics course (spreadsheets, to be more specific). One method makes use of adaptable instructional images, meaning that the images used in the learning material change according to students' preference. The second method makes use of adaptable instructional video in a similar way to the images. In these methods we choose to change (adapt) the volume of the used media, but we do not change the type of media. Both methods had been applied to high school students in two different experiments conducted and this paper gives a comparison of the two experiments as well as the results of a post-research survey among the students (subjects) of the two experiments. Statistical analysis and results are given as well as the explaining of these results.",Adaptable Instructional Images versus Adaptable Instructional Video: Comparison of the Two Methods of Instruction and Results of a Post-Research Survey
"['Erwan Bigorgne', 'Catherine Achard', 'Jean Devars']","Invariance to photometric changes is implicitly required for a view-based object recognition sysThe definition of reliable local signal characteri- tem. zations is of great importance for many computer vision tasks as mosai'cing, 3D-scene reconstruction or more recently in applications like content-based image retrieval systems. The following study concerns this last general pattern. Aiming at this, we present the use of Full-Zernike moments as a local characterization of the image signal. Their computation allows us to construct an invariant vector, of which the projection in an index table (feature space) provides a vote for some model-images. This approach is based on the quasi-invariant theory applied to perspective transformations and is an extension of a standard point to point matching between two images. It addresses the problem of similarity search in high dimensional space (d > 20).",Local Zernike Moments Vector for Content-Based Queries in Image Data Base
"['Zeng Lertmanorat', 'Dominique M. Durand']","Electrical extracellular stimulation of peripheral nerve activates the large-diameter motor fibers before the small ones, a recruitment order opposite the physiological recruitment of myelinated motor fibers during voluntary muscle contraction. Current methods to solve this problem require a long-duration stimulus pulse which could lead to electrode corrosion and nerve damage. The hypothesis that the excitability of specific diameter fibers can be suppressed by reshaping the profile of extracellular potential along the axon using multiple electrodes is tested using computer simulations in two different volume conductors. Simulations in a homogenous medium with a nine-contact electrode array show that the current excitation threshold (I/sub th/) of large diameter axons (13-17 /spl mu/m) (0.6-3.0 mA) is higher than that of small-diameter axons (2-7 /spl mu/m) (0.4-0.7 mA) with 200-/spl mu/m axon-electrode distance and 10-/spl mu/s stimulus pulse. The electrode array is also tested in a three-dimensional finite-element model of the sacral root model of dog (ventral root of S3). A single cathode activates large-diameter axons before activating small axons. However, a nine-electrode array activates 50% of small axons while recruiting only 10% of large ones and activates 90% of small axons while recruiting only 50% of large ones. The simulations suggest that the near-physiological recruitment order can be achieved with an electrode array. The diameter selectivity of the electrode array can be controlled by the electrode separation and the method is independent of pulse width.",A novel electrode array for diameter-dependent control of axonal excitability: a Simulation study
"['George V. Meghabghab', 'Abraham Kandel']","A hierarchical data structure for analyzing visual motion is presented. Although the literature on perception is abundant with studies on visual motion, none of the studies investigated the importance of a hierarchical model in the analysis of visual motion. The model was implemented on a supercomputer (Cyber 205). The algorithms of hierarchical correlation were performed on binary images. The results are compared with those obtained using similar serial algorithms. The impact of such a hierarchy on component directional selectivity and on pattern directional selectivity is studied. >",Hierarchical analysis of visual motion
"['Weirong Chen', 'Guoqing Zhou']","The intra-urban variation in surface temperature and its related natural and social variables region by region within a city was investigated in the study. The study area is Washington DC, USA. Data sources include one EOS Terra ASTER scene, census data and high spatial resolution (1m) color infrared DOQQ. The census tracts were used to partition the city into different regions. Variables extracted and considered in the study are: (1) urban surface temperature, (2) population density, (3) NDVI, and (4) land use/cover, particularly the percentage of urban surfaces. Mean values of each variable were calculated based on the census tracts, and their interrelationships were examined using a correlation matrix. Results indicate that urban surface temperature is mostly correlated with the percentage of urban surfaces (r = 0.857). Whereas, NDVI shows strong negative correlation with surface temperature (r= - 0.817), and its coefficient is further subject to the influence of available water bodies (i.e., water bodies have low NDVI but can lower surface temperature). Positive relation was found between population density and temperature with a reduced degree of correlation (r = 0.417), but still significant (p-value <= 0.0001 at 95% confidence level)",Comparison of the effects of selected variables on urban surface temperature
"['Daniel Bienstock', 'Abhinav Verma']","Given a power grid modeled by a network together with equations describing power flows, power generation and consumption, the so-called $N-k$ problem asks whether there exists a set of $k$ or fewer arcs whose removal will cause the system to fail. The case where $k$ is small is of practical interest. We present theoretical and numerical results involving a mixed-integer linear model and a continuous nonlinear model related to this problem.","The $N-k$ Problem in Power Grids: New Models, Formulations, and Numerical Experiments"
"['Thomas Lundgaard Hansen', 'Mihai Alin Badiu', 'Bernard Henri Fleury', 'Bhaskar D. Rao']","This paper concerns sparse decomposition of a noisy signal into atoms which are specified by unknown continuous- valued parameters. An example could be estimation of the model order, frequencies and amplitudes of a superposition of complex sinusoids. The common approach is to reduce the continuous parameter space to a fixed grid of points, thus restricting the solution space. In this work, we avoid discretization by working directly with the signal model containing parameterized atoms. Inspired by the ""fast inference scheme"" by Tipping and Faul we develop a novel sparse Bayesian learning (SBL) algorithm, which estimates the atom parameters along with the model order and weighting coefficients. Numerical experiments for spectral estimation with closely-spaced frequency components, show that the proposed SBL algorithm outperforms state-of-the- art subspace and compressed sensing methods.",A sparse Bayesian learning algorithm with dictionary parameter estimation
"['Ivan T. Bowman', 'Peter Bumbulis', 'Dan Farrar', 'Anil Kumar Goel', 'Brendan Lucier', 'Anisoara Nica', 'G. N. Paulley', 'John Smirnios', 'Matthew Young-Lai']","In this paper we present an overview of the self-management features of SQL anywhere, a full-function relational database system designed for frontline business environments with minimal administration. SQL Anywhere can serve as a high-performance workgroup server, an embedded database that is installed along with an application, or as a mobile database installed on a handheld device that provides full database services, including two-way synchronization, to applications when the device is disconnected from the corporate intranet. We illustrate how the various self-management features work in concert to provide a robust data management solution in zero-administration environments.",SQL Anywhere: A Holistic Approach to Database Self-management
"['Alfredo Pironti', 'Riccardo Sisto']","Spi2Java is a tool that enables semi-automatic generation of cryptographic protocol implementations, starting from verified formal models. This paper shows how the last version of spi2Java has been enhanced in order to enable interoperability of the generated implementations. The new features that have been added to spi2Java are reported here. A case study on the SSH transport layer protocol, along with some experiments and measures on the generated code, is also provided. The case study shows, with facts, that reliable and interoperable implementations of standard security protocols can indeed be obtained by using a code generation tool like spi2Java.",An Experiment in Interoperable Cryptographic Protocol Implementation Using Automatic Code Generation
"['Lei Lei', 'Hua Zhong', 'W. Art Chaovalitwongse']","The integrated production and distribution problem with bidirectional flows is a complicated optimization problem, usually with large problem sizes when encountered in practice. In this study, we propose a partial linear programming relaxation-based heuristic approach to solve a variation of this problem. The approach is called a partial relaxation in the sense that it relaxes the integer requirements only on selected variables. We also report on the gaps between the optimal solution and the heuristic solution provided by this partial relaxation, including analytical gaps for a special case and empirical gaps for randomly generated test cases. Our study of this problem was motivated by the operational planning problem of a medical equipment leasing network that involves a forward flow for new and refurbished devices and a reverse flow for used devices to be returned to suppliers over a multiple time-period planning horizon.",On the Integrated Production and Distribution Problem with Bidirectional Flows
"['Ian M. Lyons', 'Daniel Ansari']","Although significant insights into the neural basis of numerical and mathematical processing have been made, the neural processes that enable abstract symbols to become numerical remain largely unexplored in humans. In the present study, adult participants were trained to associate novel symbols with nonsymbolic numerical magnitudes (arrays of dots). Functional magnetic resonance imaging was used to examine the neural correlates of numerical comparison versus recognition of the novel symbols after each of two training stages. A left-lateralized fronto-parietal network, including the intraparietal sulcus, the precuneus, and the dorsal prefrontal cortex, was more active during numerical comparison than during perceptual recognition. In contrast, a network including bilateral temporal-occipital regions was more active during recognition than comparison. A whole-brain three-way interaction revealed that those individuals who had higher scores on a postscan numerical task (measuring their understanding of the global numerical organization of the novel symbols) exhibited increasing segregation between the two tasks in the bilateral intraparietal sulci as a function of increased training. Furthermore, whole-brain regression analysis showed that activity in the left intraparietal sulcus was systematically related to the effect of numerical distance on accuracy. These data provide converging evidence that parietal and left prefrontal cortices are involved in learning to map numerical quantities onto visual symbols. Only the parietal cortex, however, appeared systematically related to the degree to which individuals learned to associate novel symbols with their numerical referents. We conclude that the left parietal cortex, in particular, may play a central role in imbuing visual symbols with numerical meaning.",The cerebral basis of mapping nonsymbolic numerical quantities onto abstract symbols: An fmri training study
"['Patrick P. K. Chan', 'Zhi-Chun Huang', 'Wing W. Y. Ng', 'Daniel S. Yeung']","In order to improve the retrieval accuracy of image retrieval, semantic-based image retrieval has become popular in the recent years. It is because this kind of retrieval method could narrow É??semantic gapÉ?ù between the visual features and the high-level semantic features. However, most of the existing methods of the semantic-based image retrieval are limited to fixed number of semantic features. A dynamic hierarchical semantic network method is proposed in this paper to overcome this limitation. The proposed dynamic hierarchical semantic network is constructed by relevance feedback. The number of semantic features can be dynamically changed according to user feedbacks. Moreover, the semantic features are allowed to have different levels of abstraction. In addition, the proposed method also integrates low-level visual feature-based image retrieval style, which could full use of the advantages of visual feature-based image retrieval and semantic-based image retrieval. Experimental results show that the proposed method achieves higher retrieval accuracy than fixed number of semantic feature method and only one level semantic feature method.",Dynamic hierarchical semantic network based image retrieval using relevance feedback
"['Ingemar J. Cox', 'Joseph B. Kruskal']",,On The Congruence Of Noisy Images To Line Segment Models
"['Michelangelo Diligenti', 'Marco Gori', 'Marco Maggini']","Page ranking is a fundamental step towards the construction of effective search engines for both generic ( horizontal ) and focused ( vertical ) search. Ranking schemes for horizontal search like the PageRank algorithm used by Google operate on the topology of the graph, regardless of the page content. On the other hand, the recent development of vertical portals ( vortals ) makes it useful to adopt scoring systems focussed on the topic and taking the page content into account.In this paper, we propose a general framework for Web Page Scoring Systems (WPSS) which incorporates and extends many of the relevant models proposed in the literature. Finally, experimental results are given to assess the features of the proposed scoring systems with special emphasis on vertical search.",Web page scoring systems for horizontal and vertical search
"['Maliwan Buranapatana', 'Felicia Zhang']","This paper reports on a study which evaluates the effect of a language teaching approach called the Somatically-Enhanced Approach [1] in the teaching of Thai language to a group of Vietnamese learners in Vietnam. The teaching methodology deals with training students' perceptual mechanisms to enable them to have better pronunciation in an L2 from the beginning. Teaching innovations include: the use of relaxation techniques to relax students; the use of humming, clapping and physical gestures to emphasize the rhythm of the Thai language; the use of a Speech comparison tool (Sptool) [2] for self-study; and the provision of all learning materials on CD-ROMs. The results show that after 12 face-to-face contact hours, Vietnamese students who undertook a course in SEA are as fluent as their fellow students who studied Thai for 1 year using the traditional approach. In this paper, the results of the study, both quantitative and qualitative, will be reported. An evaluation of the Speech comparison tool will also be included.",The Contribution of a Speech Comparison Tool in Teaching Thai as a Foreign Language
"['Roni Khardon', 'Dan Roth', 'Rocco A. Servedio']","We study online learning in Boolean domains using kernels which capture feature expansions equivalent to using conjunctions over basic features. We demonstrate a tradeoff between the computational efficiency with which these kernels can be computed and the generalization ability of the resulting classifier. We first describe several kernel functions which capture either limited forms of conjunctions or all conjunctions. We show that these kernels can be used to efficiently run the Percep-tron algorithm over an exponential number of conjunctions; however we also prove that using such kernels the Perceptron algorithm can make an exponential number of mistakes even when learning simple functions. We also consider an analogous use of kernel functions to run the multiplicative-update Winnow algorithm over an expanded feature space of exponentially many conjunctions. While known upper bounds imply that Winnow can learn DNF formulae with a polynomial mistake bound in this setting, we prove that it is computationally hard to simulate Win-now's behavior for learning DNF over such a feature set, and thus that such kernel functions for Winnow are not efficiently computable.",Efficiency versus Convergence of Boolean Kernels for On-Line Learning Algorithms
"['Alex K. Jones', 'Raymond R. Hoare', 'Dara Kusic', 'Justin Stander', 'Gayatri Mehta', 'Joshua Fazekas']","This brief presents a heterogeneous multicore embedded processor architecture designed to exceed performance of traditional embedded processors while reducing the power consumed compared to low-power embedded processors. At the heart of this architecture is a multicore very large instruction word (VLIW) containing homogeneous execution cores/functional units. Additionally, heterogeneous combinational hardware function cores are tightly integrated to the VLIW core providing an opportunity for improved performance and reduced energy consumption. Our processor has been synthesized for both a 90-nm Stratix II field-programmable gate array and a 160-nm cell-based application-specific integrated circuit from Oki each operating at a core frequency of 167 MHz. For selected multimedia and signal processing benchmarks, we show how this processor provides kernel performance improvements averaging 179X over an Intel StrongARM and 36X over an Intel XScale leading to application speedups averaging 30X over StrongARM and 10X over XScale",A VLIW Processor With Hardware Functions: Increasing Performance While Reducing Power
"['Maximo A. Roa', 'Ra?ßl Su?≠rez']","This paper presents an efficient algorithm to compute independent contact regions on the surface of complex 3D objects such that a finger contact anywhere inside each of these regions assures a force-closure grasp despite the exact contact position. Independent contact regions provide robustness in front of finger positioning errors during an object grasping, and give relevant information for finger repositioning during the object manipulation. The object is described with a mesh of surface points, so the procedure is applicable to objects of any arbitrary shape. The proposed approach uses information from the wrench space, and generates the independent regions by growing them around the contact points of a given starting grasp. A two-phase approach is also provided to find a locally optimum force-closure grasp that serves as starting grasp, considering as grasp quality measure the largest perturbation wrench that the grasp can resist with independence of the perturbation direction. The approach has been implemented and several examples are provided to illustrate its performance.",Independent contact regions for frictional grasps on 3D objects
"['Peter Bailey', 'Nick Craswell', 'David Hawking']","Past research into text retrieval methods for the Web has been restricted by the lack of a test collection capable of supporting experiments which are both realistic and reproducible. The 1.69 million document WT10g collection is proposed as a multi-purpose testbed for experiments with these attributes, in distributed IR, hyperlink algorithms and conventional ad hoc retrieval.WT10g was constructed by selecting from a superset of documents in such a way that desirable corpus properties were preserved or optimised. These properties include: a high degree of inter-server connectivity, integrity of server holdings, inclusion of documents related to a very wide spread of likely queries, and a realistic distribution of server holding sizes. We confirm that WT10g contains exploitable link information using a site (homepage) finding experiment. Our results show that, on this task, Okapi BM25 works better on propagated link anchor text than on full text.WT10g was used in TREC-9 and TREC-2000 and both topic relevance and homepage finding queries and judgments are available.",Engineering a multi-purpose test collection for web retrieval experiments
"['Shinsuke Ibi', 'Tad Matsumoto', 'Reiner S. Thoma', 'Seiichi Sampei', 'Norihiko Morinaga']","This paper proposes an adaptive coding (AC) scheme for multilevel bit-interleaved coded modulation (ML-BICM) with minimum mean-square error (MMSE) turbo equalization in frequency-selective multiple-input-multiple-output (MIMO) channels. The aim of this paper is to minimize the information rate loss due to the mismatch between channel realization and channel coding. With the aid of the knowledge about extrinsic information transfer characteristics at the receiver, code parameters such as code rates and/or generator polynomials are adaptively selected independently for each ML-BICM layer. Model-based simulation results show that an achievable average throughput can be significantly improved with the proposed AC technique over automatic repeat request with fixed coding rate. Furthermore, the advantageous points of the proposed scheme are verified through field-measurement-data-based simulations.",EXIT Chart-Aided Adaptive Coding for Multilevel BICM With Turbo Equalization in Frequency-Selective MIMO Channels
"['Khaled M. Rabie', 'Emad Alsusa']","Impulsive noise (IN) is the most dominant factor degrading the performance of communication systems over powerlines. Many IN mitigation techniques have been introduced in the literature such as the application of a blanker at the front-end of the receiver which sets the incoming signal to zero when it exceeds a certain threshold. Determining this threshold is the key for achieving best performance in this technique. Most studies dealing with threshold optimization require prior knowledge about the IN characteristics. However, if the peak to average power ratio (PAPR) of the OFDM symbols can be determined at the receiver, this will allow optimal blanking of IN without requiring any knowledge about the noise parameters. In this paper we propose a look-up table (LUT) based algorithm with uniform quantization to estimate the peak of OFDM symbols. We also investigate the impact of quantization bits on the system performance in terms of the signal-to-noise-ratio (SNR) at the output of the blanking device, in two different scenarios, weakly and heavily disturbed IN environments. Simulation results reveal that a 5 bit LUT is sufficient for reliable performance.",Impulsive Noise Blanking Using Quantized PAPR Estimates in Powerline Communications
"['Chris Loadman', 'Zhizhang Chen']",The paper attempts to analyze the performance of a heterodyne phase conjugate retrodirective transceiver for full duplex communications in the presence of multipath signals and amplitude and phase perturbations across a four element antenna array. Simulation results are given and it is shown that the retrodirective array improves multipath fading in a full duplex scheme even under amplitude and phase imperfections across the array.,A study of retrodirective array performance in the presence of multipath
['Soemon Takakuwa'],"A method of modeling large-scale AS/RS is proposed in an attempt to precisely describe the AS/RS in which a lot of items are stored and retrieved. The system considered in this study consists of a large-scale AS/RS with stacker cranes, a material handling system, and aisle conveyors connecting these two systems. In the proposed model, every item number put on each rack, the corresponding number of cases for each item, the time of arriving in the warehouse, and the time of departing from the system are stored and recorded. A framework of constructing simulation models is proposed, and an example is illustrated. It is shown that simulation proposed in this study can be used as a monitor of the real system for decision making. In addition, regarding a large-scale AS/RS system, four particular types of storing/retrieving systems are analyzed to examine their performance under various operation conditions.",Precise modeling and analysis of large-scale AS/RS
"['Hideyuki Nakanishi', 'Satoshi Koizumi', 'Toru Ishida', 'Hideaki Ito']","Many studies have been conducted on supporting communication in home and office spaces, but relatively few studies have explored supporting communication in large-scale public spaces, despite the importance of such environments in our daily lives. We propose a transcendent means of communication as an emerging style in this pervasive computing era: a system that allows administrative staff to effectively help visitors in large-scale public spaces. The visitors' context is used to provide a bird's-eye view of a simulated public space for the staff to grasp the situation and point at a particular location within the view to indicate the visitors they intend to address. The results of an experiment showed synergic effects between the bird's-eye view and the first-person one in determining the spatial movements of people. In indoor and outdoor large-scale public spaces, a central railway station and a park, we installed our prototypes and learned the implications of its use.",Transcendent communication: location-based guidance for large-scale public spaces
"['Farhad Bayat', 'Tor Arne Johansen', 'Ali Akbar Jalali']","The online computational burden of linear model predictive control (MPC) can be moved offline by using multi-parametric programming, so-called explicit MPC. The solution to the explicit MPC problem is a piecewise affine (PWA) state feedback function defined over a polyhedral subdivision of the set of feasible states. The online evaluation of such a control law needs to determine the polyhedral region in which the current state lies. This procedure is called point location; its computational complexity is challenging, and determines the minimum possible sampling time of the system. A new flexible algorithm is proposed which enables the designer to trade off between time and storage complexities. Utilizing the concept of hash tables and the associated hash functions, the proposed method solves an aggregated point location problem that overcomes prohibitive complexity growth with the number of polyhedral regions, while the storage-processing trade-off can be optimized via scaling parameters. The flexibility and power of this approach is supported by several numerical examples.",Brief paper: Using hash tables to manage the time-storage complexity in a point location problem: Application to explicit model predictive control
"['Cheng Wang', 'Shaojie Tang', 'Xiang-Yang Li', 'Changjun Jiang', 'Yunhao Liu']","We study the multicast capacity for hybrid wireless networks consisting of ordinary wireless nodes and base stations under Gaussian Channel model, which generalizes both the unicast capacity and broadcast capacity for hybrid wireless networks. We simply consider the hybrid extended network, where the ordinary wireless nodes are placed in the square region An with side-length sqrt n according to a Poisson point process?ˇ?ˇwith unit intensity. In addition, $m$ additional base stations (BSs) serving as the relay gateway are placed regularly in the region An and they are connected by a high-bandwidth wired network. Three broad categories of multicast strategies are proposed in this paper. According to the different scenarios in terms of m, n and n_d, we select the optimal scheme from the three categories of strategies, and derive the achievable multicast throughput based on the optimal decision.",Multicast Throughput of Hybrid Wireless Networks Under Gaussian Channel Model
"['Davide Cerotti', 'Susanna Donatelli', 'Andr?≠s Horv?≠th', 'Jeremy Sproston']","This paper presents a Continuous Stochastic Logic (CSL) model-checking algorithm for Generalized Stochastic Petri Nets (GSPNs). CSL is a temporal logic defined over Continuous Time Markov Chains (CTMCs). GSPNs are a class of Stochastic Petri Nets in which sojourn times in states are either exponentially distributed (tangible states) or deterministically zero (vanishing states). Although vanishing states have zero probabilities, they can be relevant for the definition of system properties expressed as CSL formulae: the semantics of CSL is therefore modified accordingly. The paper then shows how the set of GSPN states which satisfy a CSL formula can be computed through the solution of CTMCs produced from a series of embedded Discrete Time Markov Chains modified according to the formula being checked.",CSL Model Checking for Generalized Stochastic Petri Nets
"['Yan Zhou', 'Guyang Matthew Huang', 'Liping Wei']","Summary: More and more often, a gene is epitomized by a large number of sequences in GenBank. This high redundancy makes it very difficult to identify a unique best match for a query sequence from its BLAST results. We developed a novel program UniBLAST that filters out uninformative hits, clusters the redundant hits, groups the hits by LocusLink, and graphically displays the results. We also implemented a scoring function in UniBLAST to assign a unique gene name to a query sequence. UniBLAST significantly increases the efficiency of gene annotation. Availability: The program is available at http://south.","UniBLAST: a system to filter, cluster, and display BLAST results and assign unique gene annotation"
"['M.E. Tuna', 'Kamlesh Rath', 'Steven D. Johnson']","Bounded indirection is a restricted form of pointers, for system specification. It provides a mechanism for compact descriptions of many complex control structures, such as interrupts, continuations, and dynamic connections between machines. We describe three kinds of indirection-control state, value and net indirection-for use in different aspects of system description. Transformations on indirection representations and methods for synthesizing bounded indirection within the framework of behavior tables are presented.",Specification and synthesis of bounded indirection
"['Zai Yang', 'Lihua Xie', 'Cishen Zhang']",Compressed sensing (CS) studies the recovery of a high dimensional signal from its low dimensional linear measurements under a sparsity prior. This paper is focused on the CS problem with quantized measurements. An algorithm is proposed based on a Bayesian perspective that treats measurement noises and quantization errors separately and allows data saturation. It is shown to improve the recovery accuracy in comparison with existing approaches by numerical simulations.,Accurate signal recovery in quantized compressed sensing
"['Farzad Kamalabadi', 'Behzad Sharif']","A robust method for tomographic image reconstruction from limited-angle noisy measurements is proposed which builds upon a combination of regularization theory and the method of projections onto convex sets (POCS). Two specific formulations of the proposed method, namely, Tikhonov-POCS and TV-POCS, are introduced and investigated. A statistical framework is developed that provides insight into the behavior of the two algorithms. The inclusion of a reference image is approached by either a coarse reconstruction or a model generated background image. The method is validated in the context of simulations for the reconstruction of highly structured images from partial projections. Results demonstrate significant improvement over conventional regularization methods in situations where the conventional techniques are inadequate.",Robust regularized tomographic imaging with convex projections
"['Relja Arandjelovic', 'Andrew Zisserman']",We describe a scalable approach to 3D smooth object retrieval which searches for and localizes all the occurrences of a user outlined object in a dataset of images in real time. The approach is illustrated on sculptures.,Smooth object retrieval using a bag of boundaries
['B. Wayne Bequette'],"Pursuit of a closed-loop artificial pancreas that automatically controls the blood glucose of individuals with type 1 diabetes has intensified during the past six years. Here we discuss the progress and challenges in the major steps towards a closed-loop system. Continuous insulin infusion pumps have been widely available for over two decades, but É??smart pumpÉ?ù technology has made the devices easier to use and more powerful. Continuous glucose monitoring (CGM) technology has improved and the devices are more widely available. A number of approaches are currently under study for fully closed-loop systems; most manipulate only insulin, while others manipulate insulin and glucagon. Algorithms include on-off (for prevention of overnight hypoglycemia), proportional-integral-derivative (PID), model predictive control (MPC) and fuzzy logic based learning control. Meals cause the major É??disturbanceÉ?ù to blood glucose, and we focus on approaches that our group has developed to predict when a meal is likely to be consumed and its effect.",Challenges and progress in the development of a closed-loop artificial pancreas
"['Andy M. Connor', 'Jim Buchan', 'Krassie Petrova']","In this paper, we introduce the concept of the research-practice gap as it is perceived in the field of software requirements engineering. An analysis of this gap has shown that two key causes for the research-practice gap are lack of effective communication and the relatively light coverage of requirements engineering material in University programmes. We discuss the design and delivery of a Masters course in Software Requirements Engineering (SRE) that is designed to overcome some of the issues that have caused the research-practice gap. By encouraging students to share their experiences in a peer learning environment, we aim to improve shared understanding between students (many of whom are current industry practitioners) and researchers (including academic staff members) to improve the potential for effective collaborations, whilst simultaneously developing the requirements engineering skill sets of the enrolled students. Feedback from students in the course is discussed and directions for the future development of the curriculum and learning strategies are given.",Bridging the Research-Practice Gap in Requirements Engineering through Effective Teaching and Peer Learning
"['Imdad Ullah', 'Zawar Shah', 'Madeeha Owais', 'Adeel Baig']","Wireless VoIP is becoming an increasingly important application in recent years. This fact, coupled with the increasing interest in location based services, strongly suggest that tracking of wireless VoIP clients will become a widely deployed feature in emerging wireless applications. In this paper, we evaluate the capacity of WiFi networks carrying VoIP and tracking sessions for our novel architectures developed specifically to support such applications. We first present an upper bound on a maximum number of users who can be supported by the proposed architecture for tracking only applications via simulations and experiments. We then vary the transmission frequency of tracking data and observe that it has a significant impact on the tracking capacity. Finally, we utilize one of our other locations tracking architecture that is designed for wireless VoIP to investigate how the transmission frequency affects the capacity of combined VoIP and tracking sessions. We develop an insight that at high packetization intervals (e.g. 60 ms) of VoIP traffic, there is a 30% decrease in combined VoIP and tracking capacity in comparison to VoIP only capacity. We further evaluate the capacity of proposed architecture using UDP (User Datagram Protocol) and DCCP (Datagram Congestion Control Protocol) in the presence of TCP (Transmission Control Protocol) traffic. Our studies suggest that compared to UDP, DCCP not only improves the combined VoIP and tracking capacity but also enables TCP to get a reasonable bandwidth share.",VoIP and Tracking Capacity over WiFi Networks
"['Alison S. Tomlin', 'S. Ghorai', 'G. Hart', 'Martin Berzins']","High resolution models of air pollution transport and transformation are necessary in order to test possible abatement strategies based on pollution control and to forecast high pollution episodes. Models are especially relevant for secondary pollutants like ozone and nitrogen dioxide which are formed in the atmosphere through nonlinear chemical reactions involving primary pollutant species far from their sources. Often we are trying to resolve the interactions between plumes from point sources such as power stations and regional pollution tides of ozone formed in other European countries. One method of tackling this problem of different scales is with different grid sizes, using highly resolved grids in regions where the structure is very fine. This paper describes the use of 3-D adaptive gridding models for pollution transport and reaction using both a layered and a fully adaptive 3-D tetrahedral approach.",3-D Multi-scale air pollution modelling using adaptive unstructured meshes
"['M.J. Geuzebroek', 'J.T. van der Linden', 'A. J. van de Goor']","Efficient production testing is frequently hampered because (cores in) current complex digital circuit designs require too large test sets, even with powerful ATPG tools that generate compact test sets. Built-in Self-Test approaches often suffer from fault coverage problems, due to random-resistant faults, which can be improved successfully by means of Test Point Insertion (TPI). In this paper, we evaluate the effect of TPI for BIST on the compactness of ATPG generated test sets and it turns out that most often a significant test set size reduction can be obtained. We also propose a novel TPI method, specifically aimed at facilitating compact test generation, based on the ''test counting' technique. Experimental results indicate that the proposed method results in even larger and moreover more consistent reduction of test set sizes.",Test point insertion for compact test sets
"['Nagarajan Venkateswaran', 'Aditya Krishnan', 'S. Niranjan Kumar', 'Arrvindh Shriraman', 'Srinivas Sridharan']","The Von-Neumann bottleneck is a major impediment towards attaining higher performance. Novel merged logic-memory architectures such as the Processor In Memory (PIM) approach seek to delay the Von-Neumann bottleneck. This paper introduces the concept of Memory In Processor (MIP) architecture, which overcomes this bottleneck by providing a logical and physical integration of the memory into the functional units of the processor thereby creating a memory like organization. This is unlike the PIM, which purely involves physical integration. The MIP node employs High-Level Functional Units (HLF units) like matrix multipliers, matrix inverters, sorter units and graph algorithm solvers all of which are designed to be memory like. This integration has led to the equivalence of functional unit density with the SRAM, initiating a new memory-based metric for quantifying the HLF unit capability in terms of bytes. The 2 MB MIP node operates on 128 bit data, and has been shown to be equivalent to the performance of a uniprocessor 3D torus cluster of 5*5*4 nodes. Thereby achieving supercomputing on a multi billion-device chip. The MIP cluster markedly deviates from conventional approaches of cluster based supercomputing and attains high performance while maintaining a smaller node count.",Memory in processor: a novel design paradigm for supercomputing architectures
"['Vladimir Salnikov', 'Daniel Cho??', 'Philippe Karamian-Surville']","In this paper we describe efficient methods of generation of representative volume elements (RVEs) suitable for producing the samples for analysis of effective properties of composite materials via and for stochastic homogenization. We are interested in composites reinforced by a mixture of spherical and cylindrical inclusions. For these geometries we give explicit conditions of intersection in a convenient form for verification. Based on those conditions we present two methods to generate RVEs: one is based on the random sequential adsorption scheme, the other one on the time driven molecular dynamics. We test the efficiency of these methods and show that the first one is extremely powerful for low volume fraction of inclusions, while the second one allows us to construct denser configurations. All the algorithms are given explicitly so they can be implemented directly.",On efficient and reliable stochastic generation of RVEs for analysis of composites within the framework of homogenization
"['Iok-Fai Leong', 'Yain-Whar Si', 'Simon Fong', 'Robert P. Biuk-Aghai']","Departmental workflows within a digital business ecosystem are often executed concurrently and required to share limited number of resources. However, unexpected events from the business environment and delay in activities can cause temporal exceptions in these workflows. Predicting temporal exceptions is a complex task since a workflow can be implemented with various types of control flow patterns. In this paper, we describe a critical path based approach for predicting temporal exceptions in concurrent workflows which are required to share limited resources. Our approach allows predicting temporal exceptions in multiple attempts while workflows are being executed.",Critical path based approach for predicting temporal exceptions in resource constrained concurrent workflows
"['Matthew Brenneman', 'Yu Morton']",The Tracy-Widom distribution is used to determine the false alarm rate of information theoretic methods that statistically estimate the number of sources in a multichannel receiver input. The Tracy-Widom distribution is the limiting distribution for the largest eigenvalue of a covariancematrix having a central whiteWishart distribution. Such covariance matrices are produced by the output of multi-channel receivers whose signals can be characterized as zero-mean Gaussian processes. The Tracy-Widom distribution is used to estimate the false alarm rate of the Akaike Information Criterion and Minimum Description Length methods when no external sources are present. The Tracy-Widom distribution along with the eigenvalue inclusion principle is used to obtain an upper bound on the false alarm rate of the Akaike Information Criterion and Minimum Description Length when one external source is present. Monte-Carlo simulations were performed to demonstrate the effectiveness of both methods for cases where both the array and data sample sizes are small.,False alarm rate estimation for information-theoretic-based source enumeration methods
"['Yanyan Guo', 'Guixia Kang', 'Yang Yu', 'Ping Zhang']","In this paper, we propose a relay selection scheme based on cooperative MIMO (Multi-input-multi-output) communication for network lifetime maximization in energy- constrained sensor networks. Compared with existing work, our distributions are: cooperative nodes choice is not based on minimizing energy consumption per communication but balancing remaining energy between participants including source and cooperators based on one special application. We validate the efficiency of the proposed algorithm by simulation by comparing it with the direct communication and minimizing energy consumption communication schemes.",A Relay Selection Cooperative MIMO Communication Scheme for Network Lifetime Maximization
"['Tuncer C. Aysal', 'Kenneth E. Barner']","Linear combinations of polynomial terms yield poor performance in environments characterized by heavy-tailed distributions. Weighted myriad (WMy) filters, however, are well known for their outlier suppression and detail preservation properties. It is shown here that the WMy methodology is naturally extended to the polynomial sample case, yielding filter structures that exploit the higher order statistics of the observed samples while simultaneously being robust to outliers for heavy-tailed distributions environments. Moreover, the introduced power weighted myriad (PWMy) filter class is well motivated by analysis of cross- and square-term statistics of heavy-tailed distributions. The effectiveness of the proposed filter is evaluated through simulations",Myriad-Type Polynomial Filtering
"['Mohsen Lesani', 'Jens Palsberg']","Many concurrent programming models enable both transactional memory and message passing. For such models, researchers have built increasingly efficient implementations and defined reasonable correctness criteria, while it remains an open problem to obtain the best of both worlds. We present a programming model that is the first to have opaque transactions, safe asynchronous message passing, and an efficient implementation. Our semantics uses tentative message passing and keeps track of dependencies to enable undo of message passing in case a transaction aborts. We can program communication idioms such as barrier and rendezvous that do not deadlock when used in an atomic block. Our experiments show that our model adds little overhead to pure transactions, and that it is significantly more efficient than Transactional Events. We use a novel definition of safe message passing that may be of independent interest.",Communicating memory transactions
"['Kleanthis Psarris', 'Konstantinos Kyriakopoulos']","Optimizing compilers rely upon program analysis techniques to detect data dependences between program statements. Data dependence information captures the essential ordering constraints of the statements in a program that need to be preserved in order to produce valid optimized and parallel code. Data dependence testing is very important for automatic parallelization, vectorization, and any other code transformation. In this paper, we examine the impact of data dependence analysis in practice. A number of data dependence tests have been proposed in the literature. In each test, there are different trade offs between accuracy and efficiency. We present an experimental evaluation of several data dependence tests, including the Banerjee test, the I-Test, and the Omega test. We compare these tests in terms of data dependence accuracy, compilation efficiency, effectiveness in parallelization, and program execution performance. We analyze the reasons why a data dependence test can be inexact and we explain how the examined tests handle such cases. We run various experiments using the Perfect Club Benchmarks and the scientific library Lapack. We present the measured accuracy of each test and the reasons for any approximation. We compare these tests in term's of efficiency and we analyze the trade offs between accuracy and efficiency. We also determine the impact of each data dependence test on the total compilation time. Finally, we measure the number of loops parallelized by each test and we compare the execution performance of each benchmark on a multiprocessor. Our results indicate that the Omega test is more accurate, but also very inefficient in the cases where the other two tests are inaccurate. In general, the cost of the Omega test is high and uses a significant percentage of the total compilation time. Furthermore, the difference in accuracy of the Omega test over the Banerjee test and the l-Test does not improve parallelization and program execution performance.",An experimental evaluation of data dependence analysis techniques
['Mei Wang'],"Two schools of thoughts have emerged over the recent debate on internet router buffer sizing. One school argues that the presence of a large number of flows leads to traffic desynchronization which therefore requires a small buffer size. The other school, however, argues that doing so creates instability in the network and thus causes degradation in throughput. In this work, we use theoretical analysis based on a mean-field theory to demonstrate that the missing link between the above two arguments is the fairness in packet dropping at the buffer. This mean-field theory provides a simple and yet quantitative tool to analyze the dynamics between the TCP flows and the queue length. Our analysis shows that, for the widely deployed drop-tail queue management scheme, there is a trade-off between the desynchronization among the flows and the fairness in the packet dropping process.",Mean-Field Analysis of Buffer Sizing
"['Carmine Clemente', 'Maurizio di Bisceglie', 'Michele Di Santo', 'Nadia Ranaldo', 'Marcello Spinelli']","Synthetic aperture radar processing is a complex task that involves advanced signal processing techniques and intense computational effort. While the first issue has now reached a mature stage, the question of how to produce accurately focused images in real-time, without mainframe facilities, is still under debate. The recent introduction of general-purpose graphic processing units seems to be quite promising in this view, especially for the decreased per-core cost barrier and for the affordable programming complexity. The authors explain, in this work, the main computational features of a range-Doppler Synthetic Aperture Radar (SAR) processor, trying to disclose the degree of parallelism in the operations at the light of the CUDA programming model. Given the extremely flexible structure of the Single Instruction Multiple Threads (SIMT) model, the authors show that the optimization of a SAR processing unit cannot reduce to an FFT optimization, although this is a quite extensively used kernel. Actually, it is noticeable that the most significant advantage is obtained in the range cell migration correction kernel where a complex interpolation stage is performed very efficiently exploiting the SIMT model. Performance show that, using a single Nvidia Tesla-C1060 GPU board, the obtained processing time is more than fifteen time better than our test workstation.",Processing of synthetic Aperture Radar data with GPGPU
"['Z. Khan', 'Saad Liaquat Kiani', 'Kamran Soomro']","Background#R##N#In the context of smart cities, public participation and citizen science are key ingredients for informed and intelligent planning decisions and policy-making. However, citizens face a practical challenge in formulating coherent information sets from the large volumes of data available to them. These large data volumes materialise due to the increased utilisation of information and communication technologies in urban settings and local authoritiesÉ?? reliance on such technologies to govern urban settlements efficiently. To encourage effective public participation in urban governance of smart cities, the public needs to be facilitated with the right contextual information about the characteristics and processes of their urban surroundings in order to contribute to the aspects of urban governance that affect them such as socio-economic activities, quality of life, citizens well-being etc. The cities on the other hand face challenges in terms of crowd sourcing with quality data collection and standardisation, services inter-operability, provisioning of computational and data storage infrastructure.",A framework for cloud-based context-aware information services for citizens in smart cities
"['Oron Levin', 'Joseph Mizrahi', 'Moshe Shoham']","In this study, a model for the estimation of the dynamics of the lower extremities in standing sway from force plate data only is presented. A three-dimensional, five-segment, four-joint model of the human body was used to describe postural standing sway dynamics. Force-plate data of the reactive forces and centers of pressure were measured bilaterally. By applying the equations of motion to these data, the transversal trajectory of the center of gravity (CG) of the body was resolved in the sagittal and coronal planes. An inverse kinematics algorithm was used to evaluate the kinematics of the body segments. The dynamics of the segments was then resolved by using the Newton-Euler equations, and the model's estimated dynamic quantities of the distal segments were compared with those actually measured. DiAerences between model and measured dynamics were calculated and minimized, using an iterative algorithm to re-estimate joint positioning and anthropometric properties. The above method was tested with a group of 11 able-bodied subjects, and the results indicated that the relative errors obtained in the final iteration were of the same order of magnitude as those reported for closed loop problems involved in direct kinematics measurements of human gait.",Standing sway: iterative estimation of the kinematics and dynamics of the lower extremities from force-plate measurements
"['Zhongbao Zhou', 'Guang Jin', 'Doudou Dong', 'Jinglun Zhou']","Due to the difficulty of reliability analysis of multistate systems, a new method based on Bayesian networks is proposed through an example. Reliability block diagram and logic operators are firstly established according to the hierarchy of structure and function of multistate systems, and the Bayesian networks is constructed based on the reliability block diagram, distribution of components and logic operators, then the analysis is performed. The paper shows that the new method based on Bayesian networks could model generic multistate systems and basic inference techniques on Bayesian networks may be used to obtain classical parameters in reliability analysis (i.e. probability of the top event or of any subsystem, etc.). Moreover, by using Bayesian networks, some additional information can be obtained at the analysis level which could be used to perform diagnosis and complex common cause problems can be handled conveniently. The comparison with traditional method is carried out by means of an example of radar system taken from literature.",Reliability analysis of multistate systems based on Bayesian networks
"['Boumediene Hamzi', 'Arthur J. Krener', 'Wei Kang']","In this paper, we introduce the Controlled center dynamics for nonlinear discrete time systems with uncontrollable linearization. This is a reduced order control system whose dimension is the number of uncontrollable modes and whose stabilizability properties determine the stabilizability properties of the full order system. After reducing the order of the system, the synthesis of a stabilizing controller is performed based on the reduced order control system. By changing the feedback, the stability properties of the controlled center dynamics will change, and thus the stability properties of the full order system will change too. Thus, choosing a feedback that stabilizes the controlled center dynamics allows stabilizing the full order system. This approach is a reduction technique for some classes of controlled differential equations.",The controlled center dynamics of discrete time control bifurcations
"['N. Scheinberg', 'Erlan H. Feria', 'J. Barba', 'D. L. Schilling']",This paper describes a one-stage look-ahead algorithm for adaptive delta modulators. The algorithm does not require the calculation of two encoding paths nor does it require the decision circuitry to choose the optimum path.,A One-Stage Look-Ahead Algorithm for Delta Modulators
"['Hsiang-Cheh Huang', 'Yueh-Hong Chen', 'Guan-Yu Lin']","Digital watermarking has been a popular topic in both researches and applications in the last decade. Based on previous experiences in literature, designing one good watermarking algorithm is desired, and how to assess how good the algorithm is should be based on the preset requirements. Here we use one optimization technique named bacterial foraging and we employ the concept in fuzzy theory to design an effective fitness function with the predetermined requirements. Unlike conventional scheme to fix the components in fitness function, by using the fuzzy concept, better results can be obtained. Simulation results demonstrate the advantages of the proposed algorithm over existing ones in literature.",Fuzzy-Based Bacterial Foraging for Watermarking Applications
"['Honglin Wu', 'Amir Gourgy', 'Ted H. Szymanski']","Optoelectronic integrated circuits can support thousands of integrated optical laser diodes and photodetectors bonded to a high-performance CMOS substrate, and can be used in the design of multi-Terabit optical local area networks. This paper describes the design of an integrated optoelectronic CMOS crossbar switch to interconnect approx. 128 parallel fiber ribbon optical links, each with 12 channels clocked at 2.5 Gigabit/sec, to achieve a local area network (LAN) with an aggregate capacity of 3.84 Terabits/second. A prototype switch core has been designed in 0.18 /spl mu/m CMOS technology. Logic optimization and synthesis was performed using the synopsis logic optimization tools, and VLSI layout was performed using the Cadence 2002 tools. It is shown that using 0.18 /spl mu/m CMOS technology, a 3.84 Terabit crossbar switch for an optoelectronic LAN occupies approx. 1.78 sq. cm of real estate, and consumes approx. 90 watts of power.",An optoelectronic multi-Terabit CMOS switch core for local area networks
"['Ismet Bayraktaroglu', 'Alex Orailoglu']",A methodology for the determination of decompression hardware that guarantees complete fault coverage for a unified compaction/compression scheme is proposed. Test cube information is utilized for the determination of a near optimal decompression hardware. The proposed scheme attains simultaneously high compression levels and reduced pattern counts through a linear decompression hardware. Significant test volume and test application time reductions are delivered through the scheme we propose while a highly cost effective hardware implementation is retained.,Decompression hardware determination for test volume and time reduction through unified test pattern compaction and compression
"['John W. Daly', 'Andrew Brooks', 'James Miller', 'Marc Roper', 'Murray Wood']","The empirical study was undertaken as part of a programme of research to explore unsupported claims about the object-oriented paradigm: a series of experiments tested the effect of inheritance on the maintainability of object-oriented software. Subjects were asked to modify object-oriented software with a hierarchy of 3 levels of inheritance depth and equivalent object-based software with no inheritance. The collected timing data showed that subjects maintaining object-oriented software using inheritance performed the modification tasks, on average, approximately 20% quicker than those maintaining equivalent object-based software with no inheritance. An initial inductive analysis revealed that 2 out of 3 subjects performed faster when maintaining the object-oriented software with inheritance. The findings are sufficiently important that attempts to verify the results should be made by independent researchers. Subsequent studies should seek to scale up the findings to the maintenance of more complex software by professional programmers.",The effect of inheritance on the maintainability of object-oriented software: an empirical study
"['Shaowei Lin', 'Winston W. L. Ho', 'Ying-Chang Liang']","Variable-rate coding for a multiple input multiple output (MIMO) channel increases transceiver complexity. In single user case, the uniform channel decomposition (UCD) scheme overcomes this problem by generating decoupled subchannels with identical SNRs so that equal-rate (ER) coding can be applied. In this paper, the solution is extended to the multi-user broadcast case. The block-diagonal geometric mean decomposition (BD-GMD) is used to design a capacity-achieving scheme, called the block-diagonal UCD (BD-UCD). It allows each user to apply ER coding on its own subchannels. An efficient near-optimal algorithm for multiuser uplink beamforming with SINR constraints is also proposed. Using this and duality, a scheme that allows ER coding to be applied to every subchannel of every user is designed. Simulations have shown that both schemes have superior BER performance and higher achievable sum-rates than conventional schemes.",MIMO Broadcast Communications Using Block-Diagonal Uniform Channel Decomposition (BD-UCD)
"['Kai M. Hynna', 'Kwabena Boahen']","Thalamic relay cells express distinctive response modes based on the state of a low-threshold calcium channel (T-channel). When the channel is fully active (burst mode), the cell responds to inputs with a high-frequency burst of spikes; with the channel inactive (tonic mode), the cell responds at a rate proportional to the input. Due to the T-channel's dynamics, we expect the cell's response to become more nonlinear as the channel becomes more active. To test this hypothesis, we study the response of an in silico relay cell to Poisson spike trains. We first validate our model cell by comparing its responses with in vitro responses. To characterize the model cell's nonlinearity, we calculate Poisson kernels, an approach akin to white noise analysis but using the randomness of Poisson input spikes instead of Gaussian white noise. We find that a relay cell with active T-channels requires at least a third-order system to achieve a characterization as good as a second-order system for a relay cell without T-channels.",Nonlinear Influence of T-Channels in an in silico Relay Neuron
"['Jo?úo Abreu', 'Jos?? Luiz Fiadeiro']","We present a formal model for the coordination of interactions in service-oriented systems. This model provides a declarative semantics for the language SRML that is being developed under the FET-GC2 project SENSORIA for modelling and reasoning about complex services at the abstract business level. In SRML, interactions are conversational in the sense that they involve a number of correlated events that capture phenomena that are typical of SOC like committing to a pledge or revoking the effects of a deal. Events are exchanged across wires that connect the parties involved in the provision of the service.",A coordination model for service-oriented interactions
"['Mattia Salnitri', 'Achim D. Brucker', 'Paolo Giorgini']","Making todays' systems secure is an extremely difficult and challenging problem. Socio and technical issues interplay and contribute in creating vulnerabilities that cannot be easily prevented without a comprehensive engineering method. This paper presents a novel approach to support process-aware secure systems modeling and automated generation of secure artifact-centric implementations. It combines social and technical perspectives in developing secure complex systems. This work is the result of an academic and industrial collaboration, where SecBPMN2, a research prototype, has been integrated with SAP River, an industrial artifact-centric language.",From Secure Business Process Models to Secure Artifact-Centric Specifications
"['Hakem Beitollahi', 'Geert Deconinck']","Denial of service (DoS) attacks are major threat against availability in the Internet. A large number of countermeasure techniques try to detect attack and then filter out DoS attack packets. Unfortunately these techniques that filter DoS traffic by looking at known attack patterns or statistical anomalies in the traffic patterns can be defeated by changing the attack patterns and masking the anomalies that are sought by the filter. Hence, detecting DoS traffic is one of the main challenges for filtering techniques. Furthermore techniques that drop any malicious packet need to process the packet and processing is time-consuming. This paper addresses how an efficient and good filter can be designed by helping an overlay network layer to mitigate DoS attacks. Fosel (filtering by helping an overlay security layer) filter is independent from DoS attack types, so we do not worry about the changing attack patterns. Furthermore it reduces processing time noticeably. Through simulation this paper shows by employing Fosel filter, DoS attacks have a negligible chance to saturate the target by malicious packets. Our simulation demonstrates that Fosel architecture reduces the probability of successful attack to minuscule levels. Furthermore Fosel is between 10% and 50% faster than SOS (secure overlay services) (Keromytis et al., 2002) architecture to drop malicious packets based on attack rate.",FOSeL: Filtering by Helping an Overlay Security Layer to Mitigate DoS Attacks
"['King-Hong Chung', 'Yuk-Hee Chan']","In most digital cameras, Bayer color filter array (CFA) images are captured and demosaicing is generally carried out before compression. Recently, it was found that compression-first schemes outperform the conventional demosaicing-first schemes in terms of output image quality. An efficient prediction-based lossless compression scheme for Bayer CFA images is proposed in this paper. It exploits a context matching technique to rank the neighboring pixels when predicting a pixel, an adaptive color difference estimation scheme to remove the color spectral redundancy when handling red and blue samples, and an adaptive codeword generation technique to adjust the divisor of Rice code for encoding the prediction residues. Simulation results show that the proposed compression scheme can achieve a better compression performance than conventional lossless CFA image coding schemes.",A Lossless Compression Scheme for Bayer Color Filter Array Images
"['Jos?? Ignacio Alvarez-Hamelin', ""Luca Dall'Asta"", 'Alain Barrat', 'Alessandro Vespignani']","We use the k-core decomposition to develop algorithms for the analysis of large scale complex networks. This decomposition, based on a recursive pruning of the least connected vertices, allows to disentangle the hierarchical structure of networks by progressively focusing on their central cores. By using this strategy we develop a general visualization algorithm that can be used to compare the structural properties of various networks and highlight their hierarchical structure. The low computational complexity of the algorithm, O(n + e), where n is the size of the network, and e is the number of edges, makes it suitable for the visualization of very large sparse networks. We show how the proposed visualization tool allows to find specific structural fingerprints of networks.",Large scale networks fingerprinting and visualization using the k-core decomposition
"['Young Deok Chun', 'Sang Yong Seo', 'Nam Chul Kim']","We propose two new texture features, block difference of inverse probabilities (BDIP) and block variation of local correlation coefficients (BVLC), for content-based image retrieval (CBIR) and then present an image retrieval method based on the combination of BDIP and BVLC moments. BDIP uses local probabilities in image blocks to measure an image's local brightness variations well. BVLC uses variations of local correlation coefficients in image blocks to measure local texture smoothness of an image well. Experimental results show that the presented retrieval method yields about 12% better performance in precision versus recall and about 0.13 in average normalized modified retrieval rank (ANMRR) than the method using wavelet moments.",Image retrieval using BDIP and BVLC moments
"['Florin Lohan', 'Irek Def??e']","The coming generation of home multimedia terminals will be evolving in the direction of open platforms based on complete PC architecture and broadband networking. Such terminals will take over the role of home entertainment, information database and communication centers. We analyze here the internal architecture of open media terminals and point to the need of modularity based on public interfaces. This will enable creation of open application environment and development of new information and communication services.",Modularity in open media terminal system architecture
['Drasko M. Sotirovski'],"""Software engineering has produced no effective methods to eradicate latent software faults. "" This sentence is, of course, a stereotype, but it is as true as a stereotype can get. And yet, it begs some questions. If it is not possible to construct a large software system without residual faults, is it at least possible to construct it to degrade gracefully if and when a latent fault is encountered? This paper presents the approach adopted on CAATS (Canadian Automated Air Traffic System), and argues that OO design and certain architectural properties are the enabling elements towards a true fault-tolerant software architecture.",Towards fault-tolerant software architectures
['Yunpeng Pan'],"In combinatorial auctions, prospective buyers bid on bundles of items for sale, including but not limited to singleton bundles. The bid price given by a buyer on a particular bundle reflects his/her perceived utility of the bundle of items as a whole. After collecting all the bids, the auctioneer determines the revenue-maximizing assignment of winning bidders to bundles subject to nonoverlapping of bundles. To accomplish this, the auctioneer needs to solve a winner determination problem (WDP). The exactly same way of thinking can be taken to the context of min-sum scheduling, where jobs can be viewed as bidders who bid on bundles of discrete time periods on machines. Particular problems often permit only a subset of bundles. By putting appropriate restrictions on the collection of permissible bundles, we can derive from the WDP, various integer programming (IP) formulations for nonpreemptive as well as preemptive min-sum scheduling problems. We thus obtain the well-known time-indexed IP formulation in the nonpreemptive case, and further, a new strong IP formulation in the preemptive case.",A combinatorial auctions perspective on min-sum scheduling problems
"['Siong-Hoe Lau', 'Peter Charles Woods']","This study empirically evaluates the technology acceptance model drawn from InformationSystems(IS)literaturetoinvestigatehowuserbeliefsandattitudes influence learning-object use among higher education learners by evaluating the relationships between perceived usefulness, perceived ease of use, attitude, behavioural intentions and actual use. In the study, 601 potential learningobject users were presented with an introductory demonstration of learning objectsforaDigitalSystemscourse.Followingthedemonstrationandpractice, data on user beliefs, attitudes and intention to use learning objects were gathered, while data on actual use of learning objects was collected at the end of the semester. Subjects with prior experience using the learning objects were eliminated from further analysis, resulting in a final sample of 481 users. structural equation modelling was employed to test the hypothesised study model. The analysis showed that both the user beliefs and attitudes have significantpositiverelationshipswithbehaviouralintentionandthatbehavioural intention accurately predicted the actual use of learning objects. The results extend the validity of the TAM into a learning object context and clearly pointed out that it can be used to predict usersÉ?? future behaviour.",An Investigation of User Perceptions and Attitudes towards Learning Objects.
"['Eduardo P??rez', 'Lewis Ntaimo', 'Carla Bailey', 'Peter McCormack']","Increased demand for specialized healthcare services has been identified as one of the causes of increased healthcare costs in the US. Nuclear medicine, a sub-specialty of radiology, uses relatively new technology to diagnose and treat patients. Procedures (tests) in nuclear medicine require the use of radiopharmaceuticals with a limited half-life and involve several steps that are constrained by strict time windows and require multiple resources for completion. Consequently, managing patient service in nuclear medicine is a very challenging problem that has received very little research attention. In this paper, we present a discrete event system specification (DEVS) simulation model for nuclear medicine patient service management that considers both patient and management perspectives. DEVS is a formal modeling and simulation framework based on dynamical systems theory and provides well-defined concepts for coupling components, hierarchical and modular model construction, and an object-oriented substrate supporting repository reuse. We report on simulation results based on historical data using both patient and management performance measures. The results provide useful insights regarding the management of patient service in nuclear medicine. While this work focuses on nuclear medicine, results will find generality in other healthcare settings.",Modeling and Simulation of Nuclear Medicine Patient Service Management in DEVS
"['Hiroaki Mukaidani', 'Hua Xu']","This paper investigates the design of static output feedback controllers for a class of uncertain stochastic systems with multiple decision makers. The necessary conditions, which are determined from Karush-Kuhn-Tucker (KKT) conditions, for the existence of a guaranteed cost controller have been derived on the basis of the solutions of cross-coupled stochastic algebraic Riccati equations (CSAREs). It is shown that, if the solution of the CSAREs exists, the closed-loop system is exponentially mean square stable (EMSS) and its performance has an upper cost bound under uncertainties. In order to obtain the strategy set, Newton's method and a new sequential algorithm for solving the CSAREs are developed. Finally, a numerical example for a practical megawatt-frequency control problem shows that the proposed methods can help in attaining an adequate cost bound.",Guaranteed cost control for uncertain stochastic systems with multiple decision makers via static output feedback
"['Carsten R?¨tz', 'Julien Schmaltz']","Several theories have been proposed for timed model-based testing, but only few case-studies have been reported. In this paper, we report about our experience in using UPPAAL-TRON to test the conformance of an industrial implementation Auto trust of Automatic Trust Anchor Updating, a protocol to help securing DNS. We created models for specific parts of the protocol focussing on security key states and critical timing behaviours. We developed testing environments to test one or multiple keys and to check a security-relevant feature. This case-study also illustrates several challenges when testing timed systems, namely, quiescence, latency, and coverage.",An Experience Report on an Industrial Case-Study about Timed Model-Based Testing with UPPAAL-TRON
['Charles R. Graham'],"The technological pedagogical content knowledge (TPACK) framework is increasing in use by educational technology researchers around the world who are interested in issues related to technology integration. Much that is good can be found in the TPACK framework; however considerable theoretical work needs to be done if TPACK research is to cohere and constructively strengthen the field of educational technology. This paper uses Whetten's (1989) criteria for theory building as a lens for examining the TPACK framework. Specific weaknesses are identified, which in turn suggest areas needing theoretical development. This paper calls for researchers to increase emphasis on using research findings to constructively build common definitions and understandings of the TPACK constructs and the boundaries between them.",Theoretical considerations for understanding technological pedagogical content knowledge (TPACK)
"['Zhiqing Wei', 'Qixun Zhang', 'Zhiyong Feng', 'Wei Li', 'T. Aaron Gulliver']","The Radio Environment Map (REM) provides an effective approach to Dynamic Spectrum Access (DSA) in Cognitive Radio Networks (CRNs). Previous results on REM construction show that there exists a tradeoff between the number of measurements (sensors) and REM accuracy. In this paper, we analyze this tradeoff and determine that the REM error is a decreasing and convex function of the number of measurements (sensors). The concept of geographic entropy is introduced to quantify this relationship. And the influence of sensor deployment on REM accuracy is examined using information theory techniques. The results obtained in this paper are applicable not only for the REM, but also for wireless sensor network deployment.",On the construction of Radio Environment Maps for Cognitive Radio Networks
"['Giorgio Fumera', 'Ignazio Pillai', 'Fabio Roli']","In recent years anti-spam filters have become necessary tools for Internet service providers to face up to the continuously growing spam phenomenon. Current server-side anti-spam filters are made up of several modules aimed at detecting different features of spam e-mails. In particular, text categorisation techniques have been investigated by researchers for the design of modules for the analysis of the semantic content of e-mails, due to their potentially higher generalisation capability with respect to manually derived classification rules used in current server-side filters. However, very recently spammers introduced a new trick consisting of embedding the spam message into attached images, which can make all current techniques based on the analysis of digital text in the subject and body fields of e-mails ineffective. In this paper we propose an approach to anti-spam filtering which exploits the text information embedded into images sent as attachments. Our approach is based on the application of state-of-the-art text categorisation techniques to the analysis of text extracted by OCR tools from images attached to e-mails. The effectiveness of the proposed approach is experimentally evaluated on two large corpora of spam e-mails.",Spam Filtering Based On The Analysis Of Text Information Embedded Into Images
"['Chunjie Zhang', 'Yifan Zhang', 'Shuhui Wang', 'Junbiao Pang', 'Chao Liang', 'Qingming Huang', 'Qi Tian']","The bag of visual words model (BoW) and its variants have demonstrate their effectiveness for visual applications and have been widely used by researchers. The BoW model first extracts local features and generates the corresponding codebook, the elements of a codebook are viewed as visual words. The local features within each image are then encoded to get the final histogram representation. However, the codebook is dataset dependent and has to be generated for each image dataset. This costs a lot of computational time and weakens the generalization power of the BoW model. To solve these problems, in this paper, we propose to undo the dataset bias by codebook linear transformation. To represent every points within the local feature space using Euclidean distance, the number of bases should be no less than the space dimensions. Hence, each codebook can be viewed as a linear transformation of these bases. In this way, we can transform the pre-learned codebooks for a new dataset. However, not all of the visual words are equally important for the new dataset, it would be more effective if we can make some selection using sparsity constraints and choose the most discriminative visual words for transformation. We propose an alternative optimization algorithm to jointly search for the optimal linear transformation matrixes and the encoding parameters. Image classification experimental results on several image datasets show the effectiveness of the proposed method.",Undo the codebook bias by linear transformation for visual applications
"['Alfons G. Hoekstra', 'Peter M. A. Sloot', 'F.P.G.M. van der Linden', 'M. van Muiswinkel', 'J.J.J. Vesseur', 'Louis O. Hertzberger']",,Native and Generic Parallel Programming Environments on a Transputer and a PowerPC Platform
"['Hyung Lee-Kwang', 'Choong Ho Cho']","In this paper, a hierarchical reduction method of hypergraphs is proposed. A macro-vertex in a reduced hypergraph corresponds to an edge of the original hypergraph, and thus a reduced hypergraph can provide a partition of a system. The reduction is realized by the iterations and the sequence of hierarchical reduction gives a sequence of hierarchical partitions. The proposed method allows to reduce and decompose the complexity of the system represented by hypergraphs.",Hierarchical reduction and partition of hypergraph
['Ken J. McDonell'],"Input-output subsystem architectures have evolved over the past 20-odd years to the point where two divergent approaches have found acceptance in current computer systems; the 'IBM channel' is the archetype of the lower level alternative, while the functionally more complex techniques involve a wide spectrum of distributed processor architectures supporting database and/or storage management functions independently with respect to the central processor. The paper traces the historical development of support (outside central processor based software) for input-output functions and concludes with a preliminary comparison of the relative merits of the software interfaces provided by the alternative input-output subsystem architectures.",Trends in non-software support for input-output functions
"['H?ùvard Berland', 'B?ùrd Skaflestad', 'William Matthew. Wright']","Recently, a great deal of attention has been focused on the construction of exponential integrators for semilinear problems. In this article we describe a MATLAB 1  package which aims to facilitate the quick deployment and testing of exponential integrators, of Runge--Kutta, multistep, and general linear type. A large number of integrators are included in this package along with several well-known examples. The so-called p functions and their evaluation is crucial for accuracy, stability, and efficiency of exponential integrators, and the approach taken here is through a modification of the scaling and squaring technique, the most common approach used for computing the matrix exponential.",EXPINT---A MATLAB package for exponential integrators
"['Hu Xiong', 'Zhiguang Qin', 'Fagen Li']","Privacy and anonymity have become two factors of increasing importance in auction protocol. This paper provides an eÉ?êcient sealed-bid electronic auction protocol based on the technique of ring signature and veriflable technique of encryption key chain. The peculiar characteristics of our protocol are non-repudiation of bidders but preserving their anonymity and allowing the auctioneer to determine the wining bid without revealing the losing bid. Our protocol has additional characteristics such as public veriflability, unforgeability, correctness and fairness.",An Anonymous Sealed-bid Electronic Auction Based on Ring Signature
"['C. Mohan', 'Hamid Pirahesh']","A method called ARIES-RRH (algorithm for recovery and isolation exploiting semantics with restricted repeating of history) is presented which is a modified version of the ARIES transaction recovery and concurrency control method implemented to varying degrees in Starburst, QuickSilver, the OS/2 Extended Edition Database Manager, DB2 V2, Workstation Data Save Facility/VM and the Gamma database machine. The repeating history paradigm of ARIES is analyzed to propose a more efficient handling of redos, especially when the smallest granularity of locking is not less than a page, by combining the paradigm of selective redo from DB2 V1. Even with fine-granularity locking, it is not always the case that all the unapplied but logged changes needed to be redone. ARIES-RRH, which incorporates these changes, still retains all the good properties of ARIES-avoiding undo of undos, single-pass media recovery, nested top actions, etc. The fundamentals behind why DB2 V1's selective redo works, in spite of failures during restart recovery, are also explained. >",ARIES-RRH: restricted repeating of history in the ARIES transaction recovery method
"['Sachio Teramoto', 'Erik D. Demaine', 'Ryuhei Uehara']","The Voronoi game is a two-person game which is a model for competitive facility location. The game is played on a continuous domain, and only two special cases (the 1-dimensional case and the 1-round case) are well investigated. We introduce the discrete Voronoi game in which the game arena is given as a graph. We first show the best strategy when the game arena is a large complete k-ary tree. Next we show that the discrete Voronoi game is intractable in general. Even in the 1-round case, and the place occupied by the first player is fixed, the game is NP-complete in general. We also show that the game is PSPACE-complete in general case.",Voronoi game on graphs and its complexity
"['Rongxing Lu', 'Zhenfu Cao', 'Xiaolei Dong', 'Renwang Su']","A proxy signature enables an original signer to delegate her signing capability to a proxy signer and the proxy signer can sign a message on behalf of the original signer. Later, anyone accessible to the public keys of the original signer and proxy signer and the corresponding delegation warrant is able to verify the authenticity of a purported proxy signature. Obviously, the ""public-verifiable"" property here is quite suitable in most application scenarios, however, it is not applicable for some other applications, as in the case that a proxy signed message may be personally or commercially sensitive, for example, in a bill of tax or a bill of health, etc. Thereby, aim at these applications, several designated verifier proxy signature schemes have recently been suggested. However, to our best knowledge, none of these schemes has provided the provable security proof. Therefore, in this paper, based on the bilinear pairing, we would like to propose a new designated verifier proxy signature scheme, and apply the techniques from provable security to analyze its security.",Designated Verifier Proxy Signature Scheme from Bilinear Pairings
"['Naveen Kumar', 'Jason Bellorado', 'Marcus Marrow', 'Kai Keung Chan']","The enormous growth of consumer electronic industry is driving the need for achieving ultra-high areal storage densities. Shingled magnetic recording (SMR) is a promising high density storage technique where shingles are created by overlapping tracks for increasing the track per inch (TPI) and achieving higher areal densities. High TPI values, however, introduce sidetrack interference in the equalized output of the center track sensed through the read head. The problem of removing interference from the center track signal can be formulated as estimating pulse shapes of sidetracks assuming sidetracks are decodable. In this paper, we first propose a solution using the correlation method for estimating the pulse shape and the phase-misalignment. The correlation method fails when there is a frequency offset present from sidetracks. Furthermore, we devise a solution based on the least mean square (LMS) algorithm which can provide time varying estimates of the sidetrack pulse shapes and remove the interference from the center track signal. Experimental and simulation results which corroborate the analysis of the proposed algorithm, demonstrate that the proposed algorithm outperforms other techniques and gives the minimum bit error rate (BER). The complexity of the proposed algorithm is low and it can be easily implemented on the hardware.",Inter-track interference cancelation in presence of frequency offset for shingled magnetic recording
"['Sam L. Phillips', 'William Craelius']","We studied the pressure patterns in the residual limbs of transradial amputees during their voluntary commands for finger taps. Topographic maps of pressures exerted against the hard prosthetic socket were registered with an array of 32 pressure sensors, to produce residual kinetic images (RKIs) of the limb. Results with 2 untrained subjects demonstrated that RKIs are reliable decoders of efferent commands. Coupled with a trained filter, RKIs can provide biomimetic control over multiple degrees of freedom.",Residual kinetic imaging: a versatile interface for prosthetic control
"['Yong Li', 'Qingyu Miao', 'Mugen Peng', 'Wenbo Wang']","Frequency allocation is a critical issue for cellular relaying networks. In this paper we present spectrum-efficient frequency allocation policies that are applicable for fixed two-hop cellular relaying networks. The frequency allocation problem is decomposed into two sub-problems: inter-cell and intra-cell frequency allocation. The goal is to make more users to be served with satisfactory service quality. Through numerical analysis, the basic inter-cell frequency allocation scheme is proposed, including frequency reuse pattern and reuse factors for each type of links. Then we propose a method to realize safe intra-cell frequency reuse, in the sense that higher spectrum efficiency is achieved through this tighter frequency reuse while the signal- to-interference (SIR) quality for the links of concern is still guaranteed. Simulations show that our schemes are beneficial to obtain more satisfied users and low outage ratio, when compared to the cases without intra-cell frequency reuse scheme and the cases with fixed intra-cell frequency reuse scheme.",Frequency Allocation in Two-Hop Cellular Relaying Networks
['Elena Yanovskaya'],"Consistency properties of game solutions connect between themselves the solution sets of games with different sets of players. In the paper, the strongly consistent solutions with respect to the DavisÉ??Maschler definition of the reduced games to the class of balanced cooperative TU games with finite sets of players are considered. A cooperative game solution ?ü to a class of a TU cooperative game is called strongly consistent if for any and , where is the reduced game of ?? on the player set S and with respect to x. Evidently, all consistent single-valued solutions are strongly consistent. In the paper, we characterise anonymous, covariant bounded and strongly consistent to the class of balanced games. The core, its relative interior and the prenucleolus are among them. However, they are not unique solutions satisfying these axioms. Thus, more axioms are necessary in order to characterise these solutions with strong consistency. One of such axioms is the definition of a solution for the class of balanced two-person games. It is sufficient for the axiomatisation of the prenucleolus without the single-valuedness axiom. If we add the closed graph property of the solution correspondence to the given axioms, then the system characterises only the core. The two axiomatisations are the main result of the paper. An example of a strongly consistent solution different from the prenucleolus, the core and its relative interior is given.",STRONGLY CONSISTENT SOLUTIONS TO BALANCED TU GAMES
"['Noa Arad', 'Yuval Shavitt']","Geographic ad hoc networks use position information for routing. They often utilize stateless greedy forwarding and require the use of recovery algorithms when the greedy approach fails. We propose a novel idea based on virtual repositioning of nodes that allows to increase the efficiency of greedy routing and significantly increase the success of the recovery algorithm based on local information alone. We explain the problem of predicting dead ends which the greedy algorithm may reach and bypassing voids in the network, and introduce NEAR, node elevation ad-hoc routing, a solution that incorporates both virtual positioning and routing algorithms that improve performance in ad-hoc networks containing voids. We demonstrate by simulations the advantages of our algorithm over other geographic ad-hoc routing solutions.",Minimizing Recovery State in Geographic Ad Hoc Routing
"['Xiaoxuan Zhu', 'Mingyang Shi', 'Jing Zhang', 'Xiaofeng Tao', 'Ping Zhang']","The non-line-of-sight (NLOS) propagation has become a key obstacle to high accuracy location in urban environment. In this paper, a novel algorithm is proposed based on classical scattering models to mitigate the NLOS errors in location estimation. With distributed multi-antenna, the location of the scatterer that causes the NLOS errors is estimated, and the distance between the mobile station (MS) and the scatterer is also obtained. Then the estimated scatterers are used as virtual base stations (BSs) to locate MSs. Simulation results show that the proposed NLOS mitigation algorithm outperforms other algorithms, which indicates our NLOS mitigating method can be applied in NLOS environment effectively.",A Scattering Model based Non-Line-of-Sight Error Mitigating Algorithm Via Distributed Multi-Antenna
"['Weirong Jiang', 'Viktor K. Prasanna']","Multi-field packet classification is a fundamental function that enables routers to support a variety of network services. Most of the existing multi-field packet classification algorithms can be divided into two classes: cutting-based and merging-based solutions. However, neither of them is scalable with respect to memory requirement for all rule sets with various characteristics. This paper makes several observations on real- life rule sets and proposes a novel hybrid scheme to leverage the desirable features of the two classes of algorithms. We propose a SRAM-based parallel multi-pipeline architecture to achieve high throughput. Several challenges in mapping the hybrid algorithm onto the architecture are addressed. Extensive simulations and FPGA implementation results show that the proposed scheme sustains 80 Gbps throughput for minimum size (40 bytes) packets while consuming a small amount of on-chip resources for large rule sets consisting of up to 10K unique entries.",Scalable Packet Classification: Cutting or Merging?
"['Martin Svensson', 'Kristina H????k', 'Rickard C??ster']","The idea of social navigation is to aid users to navigate information spaces through making the collective, aggregated, or individual actions of others visible and useful as a basis for making decisions on where to go next and what to choose. These social markers should also help in turning the navigation experience into a social and pleasurable one rather than the tedious, boring, frustrating, and sometimes even scary experience of a lonely traveler. To evaluate whether it is possible to design for social navigation, we built the food recipe system  Kala s. It includes several different forms of aggregated trails of user actions and means of communication between users: recommender system functionality (recommendations computed from others' choices), real-time broadcasting of concurrent user activity in the interface, possibilities to comment and vote on recipes, the number of downloads per recipe, and chatting facilities. Recipe author was also included in the recipe description.Kalas was tried with 302 users during six months, and 73 of the users answered a final questionnaire. The overall impression was that users liked and acted on aggregated trails and navigated differently because of them. 18p of the selected recipes came from the list of recommended recipes. About half of the 73 users understood that recommendations were computed from their own and others actions, while the rest had not reflected upon it or had erroneous beliefs. Interestingly, both groups selected a large proportion of their recipes from the recommendations.Unfortunately, there were not enough users to populate the space at every occasion, and thus both chatting and following other users moving in the space was for the most part not possible, but when possible, users move to the space where most other users could be found. Of the other social textures, users themselves claimed to be most influenced by other users' comments attached to the recipes and less by recipe author or number of downloads. Users are more positive to the possibility of expressing themselves in terms of comments and voting than seeing the comments and votes of others.It was noted that users did not pick more recommended recipes towards the end of the study period when the accuracy of recommendations should have been higher. More or less from the start, they picked recommended recipes and went on doing so throughout the whole period.",Designing and evaluating kalas: A social navigation system for food recipes
"['M. Sarhangzadeh', 'Seyed Hossein Hosseini', 'Mohammad Bagher Bannae Sharifian', 'Gevorg B. Gharehpetian', 'O. Sarhangzadeh']","This paper presents a dynamic nonlinear model for Bidirectional Multi-Input DC-AC Converter and is used to meet the requirement of many Distributed Generation (DG) systems, such as Photovoltaic (PV) and Fuel Cell (FC) systems. The converter is analyzed step by step to show the principles of the operation in providing energy management based on duty cycle variations. The system capability in different operation conditions has been simulated. Dynamic analysis and control design of the bidirectional DC-AC converter is presented based on the steady state and small signal studies.",Multi-input direct DC-AC converter
"['Xiaoqing Zhu', 'Piyush Agrawal', 'Jatinder Pal Singh', 'Tansu Alpcan', 'Bernd Girod']","Contemporary wireless devices integrate multiple networking technologies, such as cellular, WiMax and IEEE 802.11a/b/g, as alternative means of accessing the Internet. Efficient utilization of available bandwidth over heterogeneous access networks is important, especially for media streaming applications with high data rates and stringent delay requirements. In this work we consider the problem of rate allocation among multiple video streaming sessions sharing multiple access networks. We develop and evaluate an analytical framework for optimal video rate allocation, based on observed available bit rate (ABR) and round trip time (RTT) over each access network, as well as the video distortion-rate (DR) characteristics. The rate allocation is formulated as a convex optimization problem that minimizes the sum of expected distortion of all video streams. We then present a distributed approximation of the optimization, which enables autonomous rate allocation at each device in a media- and network-aware fashion. Performance of the proposed allocation scheme is compared against robust rate control based on HÉ?? optimal control and two heuristic schemes employing TCP style additive-increase-multiplicative-decrease (AIMD) principles. Wesimulate in NS-2 [1] simultaneous streaming of multiple high-definition(HD) video streams over multiple access networks, using ABR and RTT traces collected on Ethernet, IEEE 802.11g, and IEEE 802.11b networks deployed in a corporate environment. In comparison with heuristic AIMD-based schemes, rate allocation from both the media-aware convex optimization scheme and HÉ?? optimal control benefit from proactive avoidance of network congestion, and can reduce the average packet loss ratio from 27% to below 2%, while improving the average received video quality by 3.3 - 4.5 dB in PSNR.",Rate allocation for multi-user video streaming over heterogenous access networks
"['Necmiye Ozay', 'Constantino M. Lagoa', 'Mario Sznaier']","This paper addresses the problem of robust identification of a class of discrete-time linear hybrid systems, switched linear models, in a set membership framework. Given a finite collection of input/output data from a noisy process the objective is twofold: (i) establish whether this data was generated by a system that switches amongst an a priori known number of subsystems, and (ii) in that case identify a suitable set of linear models along with a switching sequence that can explain the available experimental information. Our main result shows that these problems are equivalent to minimizing the rank of a matrix whose entries are affine in the optimization variables, subject to a convex constraint imposing that these variables are the moments of an (unknown) Borel measure with finite support. The use of well known (tight) convex relaxations of rank allows for further reducing the problem to a semidefinite optimization that can be efficiently solved. In the second part of the paper we extend these results to handle sensor failures that result in corrupted input/output measurements. Assuming that these failures are infrequent, we show that the problem can be recast into an optimization form where the objective is to simultaneously minimize the rank of a matrix and the number of nonzero rows of a second one. In both cases, appealing to well known convex relaxations of rank and sparsity leads to overall semidefinite optimization problems that can be efficiently solved. These results are illustrated with multiple examples showing substantially improved identification performance in the presence of noise and sensor faults.",Set membership identification of switched linear systems with known number of subsystems
"['Igor Kharitonenko', 'Xing Zhang', 'Sue Twelves']","This paper presents a low-complexity wavelet transform that utilizes point-symmetric extension at the image tile boundaries. The proposed approach is considered as an alternative to the well-known symmetric signal extension in dealing with the boundary artifacts that manifest when tiles of images are lossy compressed. The new solution developed for odd-length filters preserves the perfect reconstruction property of the filter banks and deals efficiently with blocking artifacts without requiring any post processing technique or additional information from the neighboring tiles. It is shown that point-symmetric extension at the tile boundaries does not need to be applied explicitly, but instead the equivalent boundary filters can be derived. The lifting-based implementation of the filters provides a very simple way of changing filter parameters at the boundaries that suits both hardware and software platforms.",A wavelet transform with point-symmetric extension at tile boundaries
"['Mehrdad Yaghoobi', 'Thomas Blumensath', 'Mike E. Davies']","Sparse coding is a new field in signal processing with possible applications to source coding. In this paper we present a new method that combines the problems of sparse signal approximation with coefficient quantization. This method uses overcomplete dictionaries and exploits signal redundancy. The proposed method will be derived as an extension of a recently presented method (iterative thresholding) to find sparse representations of signals. Because in digital communication and storage we need a quantized representation of the signal, instead of quantization of sparse representations a posteriori, we propose a refined method that combines sparse approximation and quantization. To compare the proposed method to a posteriori quantization, we present an audio example.",Quantized Sparse Approximation with Iterative Thresholding for Audio Coding
"['Dohee Kim', 'William MacKunis', 'Norman Fitz-Coy', 'Warren E. Dixon']","An adaptive robust integrated power and attitude control system (IPACS) is presented for a variable speed control moment gyroscope (VSCMG)-actuated satellite. The developed IPACS method is capable of achieving precision attitude control while simultaneously achieving asymptotic power tracking for a rigid-body satellite in the presence of dynamic uncertainty in the VSCMG gimbals and wheels. In addition, the developed controller compensates for the effects of uncertain, time-varying satellite inertia properties. Some challenges encountered in the control design are that the control input is premultiplied by a non-square, time-varying, nonlinear, and uncertain matrix and is embedded in a discontinuous nonlinearity. Globally uniformly ultimately bounded attitude tracking and asymptotic power tracking results are proven via Lyapunov stability analysis.",Precision IPACS in the presence of dynamic uncertainty
"['Zixuan Shao', 'Zhiyuan Fang', 'Jie Yang']","Based on the analysis of the defects of the two popular E -business models currently used, and the study of the needs of current Chinese companies, the paper proposed a new B2B E -business model based onP2P. This paper discussed the architecture, information flow and key technologies of the new model, together with the structure of the market constructed on this model. It states that, given the incomplete credit system of China, the P2P based E -business model is feasible and advantageous, is a beneficial supplement to the existed.",A P2P-Based E-Business Portal Model
"['N??stor Cata?Òo', 'Radu I. Siminiceanu']","This paper presents the correctness proof of Saturation, analgorithm for generating state spaces of concurrent systems, implemented in the SMART tool. Unlike the Breadth First Search exploration algorithm, which is easy to understand and formalise, Saturation is a complex algorithm, employing a mutually-recursive pair of procedures that compute a series of non-trivial, nested local fixed points, corresponding to a chaotic fixed point strategy. A pencil-and-paper proof of Saturation exists, but a machine checked proof had never been attempted. The key element of the proof is the characterisation theorem of saturated nodes in decision diagrams, stating that a saturated node represents a set of states encoding a local fixed-point with respect to firing all events affecting only the nodeÉ??s level and levels below. For our purpose, we have employed the Prototype Verification System (PVS) for formalising the Saturation algorithm, its data structures, and for conducting the proofs.",A Machine-Checked Proof of A State-Space Construction Algorithm
"['Andreas Tolk', 'S. Diallo', 'Jose J. Padilla', 'Heber Herencia-Zapana']","This paper evaluates the implications of model theory for agent languages. The tasks of ambassador agents are to represent simulations and identify potential contributions, select the best solutions in light of the question, compose the selected best solutions to provide the new functionality, and orchestrate their execution. Model-based data engineering can help to identify the information that needs to be exchanged between systems, existential and transformational dependencies can be identified using graph theory, and Petri nets can represent the availability of required information. All structures can be computed and fall under the realm of formal languages. Model theory is a subset of mathematics that focuses on the study of formal languages and their interpretations. Interpreting the terms model, simulation, and data of the modeling and simulation community using model theoretic terms allows the application of model theoretic insights. This allows to formally and unambiguously capture requirements for interoperability and composability.",Model theoretic implications for agent languages in support of interoperability and composability
"['Wen-Sheng Wang', 'Jozo J. Dujmovic', 'W.R. Mathews']","This paper presents NPME/NT, a tool for performance measurement of networks that work under the Windows NT operating system. The tool is organized as a network performance measurement environment (NPME) that includes a network workload generator, and a control system that automatically distributes the workload to each node of the network and generates files containing performance indicators. We present the concepts of NPME, the implementation of NPME/NT, and experimental results of using the tool.",A tool for performance measurement of NT networks
"['Erica L. Plambeck', 'Qiong Wang']","People tend to lack the self-control to undergo an unpleasant service that would generate future benefits. This paper derives a tractable quasi-hyperbolic discounting model of that behavioral tendency (for a queueing system in which service time is short relative to the time horizon for its benefits). Planning in advance, people may naively overestimate their self-control. This paper shows how customers' lack of self-control and naivete affect optimal pricing and scheduling. The welfare-maximizing usage fee and the revenue-maximizing usage fee decrease with customers' lack of self-control. Charging for subscription, in addition to or instead of per use, increases revenue, especially when subscribers are naive. If the manager can charge for subscription or per use, subscription is optimal for revenue maximization, whereas usage-based pricing is optimal for welfare maximization. If customers are heterogeneous in their self-control and naivete, priority scheduling can dramatically increase welfare and revenu...",Implications of Hyperbolic Discounting for Optimal Pricing and Scheduling of Unpleasant Services That Generate Future Benefits
"['Lachlan J. Gunn', 'James M. Chappell', 'Andrew Allison', 'Derek Abbott']","While information-theoretic security is often associated with the one-time pad and quantum key distribution, noisy transport media leave room for classical techniques and even covert operation. Transit times across the public internet exhibit a degree of randomness, and cannot be determined noiselessly by an eavesdropper. We demonstrate the use of these measurements for information-theoretically secure communication over the public internet.",Physical-Layer Encryption on the Public Internet: a Stochastic Approach to the Kish-Sethuraman Cipher
"['Qiwen Yang', 'Y. Liu', 'Shubin Wang', 'Yunchan Xue']","Locating the neighborhood of global optimum (NGO) is critical for global optimization. For this purpose, the relationship between the barycenter and the global maximum is investigated in this paper. And a space transform technique is proposed such that the barycenter of the transformed space is more close to NGO. Consequently, NGO can be readily located by means of estimating the barycenter of search space. The validity of the proposed method is demonstrated by multimodal function optimization.",A method to locate neighborhood of global optimum
"['Baptiste Charmette', 'Eric Royer', 'Fr??d??ric Chausse']","Matching image features between an image and a map of landmarks is usually a time consuming process in mobile robot localization or Simultaneous Localisation And Mapping algorithms. The main problem is being able to match features in spite of viewpoint changes. Methods based on interest point descriptors such as SIFT have been implemented on GPUs to reach real time performance. In this paper, we present another way to match features with the use of a local 3D model of the features and a motion model of the robot. This matching algorithm dedicated to robot localization would be much too slow if executed on a CPU. Thanks to a GPU implementation, we show that it is possible to achieve real-time performance while offering more robustness than descriptor based methods.",Efficient planar features matching for robot localization using GPU
"['Huei Huang Chen', 'Shih Chih Chen']","Automotive telematics comprises the applications of Global Positioning System (GPS) navigation, multimedia entertainment, wireless communications and automatic driving assistance systems. This study examined users' acceptance of automotive telematics. Following the theory comparison approach, we evaluated including the Technology Acceptance Model (TAM), the Theory of Planned Behaviour (TPB) and Combined Technology Acceptance Model (TAM)-TPB model (C-TAM-TPB), that could explain users' automotive telematics acceptance decisions. The respective models were evaluated using web-based survey data collected from 345 users about their perceptions of automotive telematics. Overall, the results revealed that the effect of Perceived Ease Of Use, Attitude and Perceived Behavioural Control were very important but that usefulness and Subjective Norm did not influence an individual's Behavioural Intention. The implications of this study were also discussed.",The empirical study of automotive telematics acceptance in Taiwan: comparing three Technology Acceptance Models
"['Jason Stewart', 'Harry Mangalam', 'Jiaye Zhou']","Use of the Open Source Software (OSS) development model has been crucial in a number of recent technological areas, including operating systems, applications and bioinformatics. The rationale for why OSS is often a betterdevelopment model than proprietary development and some of the results of this model in the field of Gene Expression are reviewed. The paper concludes with a discussion of why funding agencies should endorse OSS and require funded software projects to be released Open Source.",Open Source Software meets gene expression
"['Zhichao Li', 'Amanpreet Mukker', 'Erez Zadok']","Modern storage systems are becoming more complex, combining different storage technologies with different behaviors. Performance alone is not enough to characterize storage systems: energy efficiency, durability, and more are becoming equally important. We posit that one must evaluate storage systems from a monetary cost perspective as well as performance. We believe that cost should consider the workloads used over the storage systems' expected lifetime. We designed and developed a versatile hybrid storage system under Linux that combines HDD and SSD. The SSD can be used as cache or as primary storage for hot data. Our system includes tunable parameters to enable trading off performance, energy use, and durability. We built a cost model and evaluated our system under a variety of workloads and parameters, to illustrate the importance of cost evaluations of storage systems.",On the importance of evaluating storage systems' $Costs
"['Amor Lazzez', 'Noureddine Boudriga', 'Mohammad S. Obaidat', 'S.G. El Fatmi']","Among the promising solutions for next generation Internet backbones, one can consider the optical burst switching (OBS) technology. One of the main aspects in the design of optical burst-switched networks is the development of a burst admission control protocol suitable for QoS provisioning. In this paper, we develop a method to address the call admission control (CAC) in OBS networks that is QoS-oriented. For this, an analytic model is developed for formulating the burst admission control problem. A QoS-constraints based burst admission control protocol is developed. Finally, simulation experiments are performed to validate the proposed schemes.",A QoS-Oriented Protocol for Burst Admission Control in OBS Networks
"['Andrea Szab??ov?≠', 'Ondè?ej Kuè?elka', 'E. Sergio Morales', 'Filip è´elezn?´', 'Jakub Tolar']","We contribute a novel, ball-histogram approach to DNA-binding propensity prediction of proteins. Unlike state-of-the-art methods based on constructing an ad-hoc set of features describing the charged patches of the proteins, the ball-histogram technique enables a systematic, Monte-Carlo exploration of the spatial distribution of charged amino acids, capturing joint probabilities of specified amino acids occurring in certain distances from each other. This exploration yields a model for the prediction of DNA binding propensity. We validate our method in prediction experiments, achieving favorable accuracies. Moreover, our method also provides interpretable features involving spatial distributions of selected amino acids.",Prediction of DNA-binding propensity of proteins by the ball-histogram method
"['Lei Jiang 0002', 'Alexander Borgida', 'John Mylopoulos']","We address the fundamental question: what does it mean for data in a database to be of high quality? We motivate our discussion with examples, where traditional views on data quality are found to be unsatisfactory. Our work is founded on the premise that data values are primarily linguistic signs that convey meaning from their producer to their user through senses and referents. In this setting, data quality issues arise when discrepancies occur during this communication. We sketch a theory of senses for individual values in a relational table based on its semantics expressed using some ontology. We use this to offer a compositional approach, where data quality is expressed in terms of a variety of primitive relationships among values and their senses. We evaluate our approach by accounting for quality attributes in other frameworks proposed in the literature. This exercise allows us to (i) reveal and differentiate multiple, sometimes conflicting, definitions of a quality attribute, (ii) accommodate competing views on how these attributes are related, and (iii) point to possible new definitions.",Towards a Compositional Semantic Account of Data Quality Attributes
"['Amir Hamed Mohsenian Rad', 'Vincent W. S. Wong']","The aggregate capacity of wireless ad-hoc networks can be substantially increased if each wireless node is equipped with multiple network interface cards (NICs) and each NIC operates over a distinct orthogonal frequency channel. Most of the recently proposed channel assignment algorithms are based on formulating combinatorial channel assignment problems. The key is to assign exactly one frequency channel to each NIC. However, combinatorial channel assignment models may result in computationally complicated algorithms as well as inefficient utilization of the available frequency spectrum. In this paper, we revisit channel assignment problem by formulating a novel continuous multi-interface multi-channel random access model. This includes elaborate modeling of the link data rates for various multi-interface multi-channel networking scenarios. We then propose a fast, fully distributed and easy to implement multi- interface multi-channel random access algorithm. Simulation results show that our proposed algorithm significantly outperforms combinatorial channel assignment algorithms in terms of achieved network utility and aggregate network throughput.",Distributed Multi-Interface Multi-Channel Random Access
"['Thanadech Thanakornworakij', 'Raja Nassar', 'Chokchai Leangsuksun', 'Mihaela Paun']","A high-performance computing (HPC) system, which is composed of a large number of components, is prone to failure. To maximize HPC system utilization, one should understand the failure behavior and the reliability of the system. Studies in the literature show that the time to failure of a node is best described by a Weibull distribution. In this study, we consider, without loss of generality, the Weibull as the distribution of time to failure and develop a reliability model for a system of k nodes where nodes can fail simultaneously. From this model, we develop expressions for the probability of failure of the system at any time t, for the failure rate, and for the mean time to failure. Also, we validate the model by using failure data from the Blue Gene/L logs obtained from the Lawrence Livermore National Laboratory. Results show that if failures of the components (nodes) in the system possess a degree of dependency, the system becomes less reliable, which means that the failure rate increases and the mean time to failure decreases. Also, an increase in the number of nodes decreases the reliability of the system.",Reliability model of a system of k nodes with simultaneous failures for high-performance computing applications
"['Tat-Seng Chua', 'Hung Keng Pung', 'Guojun Lu', 'Hee-Sen Jong']","Currently, most image retrieval systems rely on text annotation of pictures as the basis to index and retrieve image data. The main limitations of using free-text to describe images are the problems of completeness and consistency. Recent works have addressed the completeness problem by using the document space modification technique to update the image descriptions using the users' relevant feedback. However, the updating process can be inconsistent and that the terms used to update the image descriptions can be ambiguous. The authors describe the use of a concept model of the image collection as the basis to guide the retrieval and updating of image contents. Relevance feedbacks from the users during retrieval activities are used to update the concept representations and image descriptions. The concept structure helps to ensure that only meaningful terms in the domain are used to update the image contents, and to overcome the ambiguity of the meanings of terms in a free-text environment. The system has been tested on an image database in the domain of Singapore history and has been found to be feasible. >",A concept-based image retrieval system
['Dan R. Olsen'],"This article discusses a basic architecture for a user interface management system and the problem of updating a display in response to interactive commands. The basic architecture is then extended to include basic editing and browsing processes on arbitrary data structures. Editing templates are presented as a technique embodying the entire manipulation process for a particular data structure/data display combination. In conjuction with the user interface management system, Such templates are able to automatically provide a majority of the code required in an interactive applicaton.",Editing Templates: A User Interface Generation Tool
"['Hongjun Wu', 'Bart Preneel']","DECIM is a hardware oriented stream cipher with an 80-bit key and a 64-bit IV. In this paper, we point out two serious flaws in DECIM. One flaw is in the initialization of DECIM. It allows to recover about half of the key bits bit-by-bit when one key is used with about 220 random IVs; only the first two bytes of each keystream are needed in the attack. The amount of computation required in the attack is negligible. Another flaw is in the keystream generation algorithm of DECIM. The keystream is heavily biased: any two adjacent keystream bits are equal with probability about ${1 \over 2}+2^{-9}$. A message could be recovered from the ciphertext if that message is encrypted by DECIM for about 218 times. DECIM with an 80-bit key and an 80-bit IV is also vulnerable to these attacks.",Cryptanalysis of the stream cipher DECIM
"['Yvonne Eriksson', 'Peter E Johansson', 'Petera Bj??rndal']","One challenge for the global market is to overcome communication problems of different kinds. The largest communication problem is language, people speak different languages and have limited knowledge in other languages. This problem is central in manuals and instructions for assembly and installations. One hopeful solution is that pictures can replace verbal instructions. In this paper we will discuss how illustrations in flat perspective can be useful for showing action in drawings.",Showing Action in Pictures
"['Gregory E. Stewart', 'Dimitry Gorinevsky', 'Guy A. Dumont']",This paper reports on the development and implementation of an algorithm for the design of spatially distributed feedback controllers for the wide variety of physical processes that are included in cross-directional (CD) control of industrial paper machines. The spatial and temporal structure of this class of process models is exploited in the use of the 2D frequency domain for analysis and 2D loop shaping design of feedback controllers. This algorithm forms the basis of a software tool that has recently been implemented in a commercial product and its use is illustrated for tuning CD controllers on two different industrial paper machines. The first example describes the use of the tool in stabilizing an unstable closed-loop system by retuning the distributed controller. The second paper machine example exposes an underperforming controller. Subsequent retuning of the controller resulted in a dramatic performance improvement.,Feedback controller design for a spatially distributed system: the paper machine problem
"['Urs Niesen', 'Piyush Gupta', 'Devavrat Shah']","We consider the problem of characterizing per node throughput scaling in arbitrary extended wireless networks. Recently, Ozgur, Leveque, and Tse (2007) obtained a complete characterization of throughput scaling for random extended networks (i.e., nodes are placed in a square region uniformly at random) under a fast fading channel model. They proposed a hierarchical cooperative communication scheme to establish this result. However, their results (both the communication scheme and the proof technique) are strongly dependent on the ""regularity"" induced with high probability by the random node placement. As a main result of this paper, we propose a more general (and very different) hierarchical cooperative communication scheme that works for arbitrarily placed nodes (with a minimum- separation requirement). Under our scheme, we obtain exactly the same per node throughput scaling as in Ozgur et. al., showing that much less regularity is necessary for successful hierarchical cooperation. Our result holds under both fast and slow fading channel model. For small path-loss exponents alpha isin (2,3), we show that our scheme is order optimal for all node placements with minimum-separation requirement. We also show that for certain node placements, our scheme is order optimal for all alpha > 3 as well.",Hierarchical cooperation for arbitrary wireless networks
"['Bi Zhao', 'Vasilis Friderikos']","We are currently witnessing the emergence of two important trends in wireless networks, namely the increased usage of Internet like application many of them which are delay tolerant and cognitive radio techniques. In this paper, the focus is how to capitalize the delay tolerance of various applications (email, P2P networks, social networking & operating system updates, file transfers) to reduce the energy consumption in cognitive networks. Since it becomes feasible to estimate the Primary Users (PU) connections for the Secondary Users (SU) by contacting a trusted database containing the information of PU traffic, a scheme is proposed that explicitly utilize the distribution of SU traffic loads to provide load-balancing. By modeling a M/M/K/L queuing system, the performance of the SUs competition is analyzed under various traffic blocking thresholds and queuing delays. To this end, the number of SU connection that can be accommodated simultaneously is derived, which as a result optimize the frequency channel utilization. A wide set of numerical investigations reveal how message transmission delays and use of white spaces can reduce the energy consumption without affecting PU traffic.",Increased Energy Efficiency via Delay-Tolerant Transmissions in Cognitive Radio Networks
"['Matthias E. Futschik', 'Toni Crompton']","Summary: Microarray data are generated in complex experiments and frequently compromised by a variety of systematic errors. Subsequent data normalization aims to correct these errors. Although several normalization methods have recently been proposed, they frequently fail to account for the variability of systematic errors within and between microarray experiments. However, optimal adjustment of normalization procedures to the underlying data structure is crucial for the efficiency of normalization. To overcome this restriction of current methods, we have developed two normalization schemes based on iterative local regression combined with model selection. The schemes have been demonstrated to improve considerably the quality of normalization. They are implemented in a freely available R package. Additionally, functions for visualization and detection of systematic errors in microarray data have been incorporated in the software package. A graphical user interface is also available.#R##N##R##N#Availability: The R package can be downloaded from http://itb.biologie.hu-berlin.de/~futschik/software/R/OLIN. It underlies the GPL version 2.#R##N##R##N#Contact: m.futschik@biologie.hu-berlin.de#R##N##R##N#Supplementary information: Further information about the methods used in the OLIN software package can be found at http://itb.biologie.hu-berlin.de/~futschik/software/R/OLIN","OLIN: optimized normalization, visualization and quality testing of two-channel microarray data"
"['N. Rosenberg', 'Dan A. Simovici', 'Szymon Jaroszewicz']","We characterize measures on free Boolean algebras and we examine the relationships that exists between measures and binary tables in relational databases. It is shown that these measures are completely defined by their values on positive conjunctions and an algorithm that leads to the construction of measures starting from its values on a positive conjunction is also given, including a formula that allows the evaluation of measures for arbitrary polynomials. Finally, we study pairs of measures generated by ternary tables, i.e. by tables that contain missing or unknown values.",On functions defined on free Boolean algebras
"['Yongqing Wang', 'Fu-Chang Huang', 'Haibo Liu']",,Adaptive filtered x-least mean square algorithm with improved convergence for resonance suppression
"['La?Ætitia Matignon', 'Guillaume Laurent', 'Nadine Le Fort-Piat']","Recently, a great deal of interest has been developed in learning in multi-agent systems to achieve decentralized control. Machine learning is a popular approach to find controllers that are tailored exactly to the system without any prior model. In this paper, we propose a semi-decentralized reinforcement learning control approach in order to position and convey an object on a contact-free MEMS-based distributed-manipulation system. The experimental results validate the semi-decentralized reinforcement learning method as a way to design control laws for such distributed systems.",Design of semi-decentralized control laws for distributed-air-jet micromanipulators by reinforcement learning
"['Xiaozhen Niu', 'Akihiro Kishimoto', 'Martin M?¨ller']","Seki is a situation of coexistence in the game of Go, where neither player can profitably capture the opponent's stones. This paper presents a new method for deciding whether an enclosed area is or can become a seki. The method combines local search with global-level static analysis. Local search is used to identify possible seki, and reasoning on the global level is applied to determine which stones are safe with territory, which coexist in a seki and which are dead. Experimental results show that a safety-of-territory solver enhanced by this method can successfully recognize a large variety of local and global scale test positions related to seki. In contrast, the well-known program GNU Go can only solve easier problems from a test collection.",Recognizing seki in computer go
"['Xuegong Zhang', 'Xin Lu', 'Qian Shi', 'Xiuqin Xu', 'HonÉ?êchiu Eastwood Leung', 'Lyndsay Harris', 'James Dirk Iglehart', 'Alexander Miron', 'Jun S. Liu', 'Wing Hung Wong']","Background: Like microarray-based investigations, high-throughput proteomics techniques require machine learning algorithms to identify biomarkers that are informative for biological classification problems. Feature selection and classification algorithms need to be robust to noise and outliers in the data. Results: We developed a recursive support vector machine (R-SVM) algorithm to select important genes/biomarkers for the classification of noisy data. We compared its performance to a similar, state-of-the-art method (SVM recursive feature elimination or SVM-RFE), paying special attention to the ability of recovering the true informative genes/biomarkers and the robustness to outliers in the data. Simulation experiments show that a 5 %-~20 % improvement over SVM-RFE can be achieved regard to these properties. The SVM-based methods are also compared with a conventional univariate method and their respective strengths and weaknesses are discussed. RSVM was applied to two sets of SELDI-TOF-MS proteomics data, one from a human breast cancer study and the other from a study on rat liver cirrhosis. Important biomarkers found by the algorithm were validated by follow-up biological experiments. Conclusion: The proposed R-SVM method is suitable for analyzing noisy high-throughput proteomics and microarray data and it outperforms SVM-RFE in the robustness to noise and in the ability to recover informative features. The multivariate SVM-based method outperforms the univariate method in the classification performance, but univariate methods can reveal more of the differentially expressed features especially when there are correlations between the features.",Recursive SVM feature selection and sample classification for mass-spectrometry and microarray data
"['Antti Ylip????', 'Olli Yli-Harja', 'Wei Zhang', 'Matti Nykter']","Background#R##N#Cancer is a broad group of genetic diseases which account for millions of deaths worldwide each year. Cancers are classified by various clinical, pathological and molecular methods, but even within a well-characterized disease, there is a significant inter-patient variability in survival, response to treatment, and other parameters. Especially in molecular level, tumours of the same category can appear significantly dissimilar due to complex combinations of genetic aberrations leading to a similar malignancy. We extended the current classification methods by studying tumour heterogeneity at pathway level.",Characterization of aberrant pathways across human cancers
"['M. Majid Butt', 'Kimmo Kansanen', 'Ralf R. Muller']","In this work, an opportunistic scheduling scheme for a large multiuser system is proposed. A group of users with good channels are scheduled simultaneously for data transmission and separated by means of superposition coding. The proposed scheduling scheme is analyzed in the large system limit. Random packet arrivals are modeled as constant arrivals with random content size. Transmission thresholds are optimized in such a way that the system energy is minimized while obeying a strict upper bound on the packet delay. We find that the state space representations of systems with either constant or random arrivals are equivalent. Thus, the thresholds optimized for constant arrivals in earlier work are valid for systems with random arrivals as well.",Hard deadline constrained multiuser scheduling for random arrivals
"['Jungyoon Kim', 'Doo Hwan Bae']","SUMMARY While the way we build software affects significantly its maintenance in terms of the effort and cost, the experience level of the maintainer in a software acquirersÉ?? organization is also one of concern. In this context, often the maintainer is the user of the system. Unfortunately, it is quite possible to lose the trustworthiness of the software due to the inexperience of the maintainer, especially when the maintainer is without the help of the original developers. One remedy for providing security against the effects of the maintainerÉ??s software modifications is to restrict the access to software parts (modules) relative to the experience level of the maintainers. For such a remedy to be successful, the software should be constructed in such a way that its parts under maintenance affect others as little as possible. We propose an approach to software construction aligning the dependencies among software parts in one direction so that they are allocated to maintainers based on their experience level. Our approach decomposes the software into parts based on functionality and orders the parts by essentiality, which indicates how difficult it is to change each part. Then, we align the dependencies in such a way that the less essential functionality is dependent on the more essential functionality. Consequently, any modification on less essential functionality does not affect the essential functionalities. To demonstrate the feasibility of our proposed approach, we applied it to a military application and found that the constructed software enables us to confine maintainersÉ?? activity within a limited working area, and thus the software is safer against maintainersÉ?? modification. Copyright c ã®´ 2006 John Wiley & Sons, Ltd. Received 31 January 2005; Revised 30 September 2005; Accepted 14 November 2005",An approach to feature-based software construction for enhancing maintainability É??É??
"['Mohammed Alzaabi', 'Jawad Berri', 'Mohamed Jamal Zemerly']","This paper presents a mobile learning system that makes use of available multimedia web resources to manage the authoring and delivery of learning material for nomadic users. The system constructs in real time a personalized learning web for a mobile user who enquires about a particular topic to fit his/her curiosity while facing a real-life situation. The learning web is mapped from a hierarchical structure that is automatically generated based on information gathered from a specific location in the web. The learning content is packaged into preset layouts and presented to the user based on his/her profile and the capabilities of the mobile handset. The architecture of the system is web-based which is designed as a set of modules that i) gather multimedia information from different locations in the web, ii) package the content into lightweight learning objects (LO), iii) organize LOs into a learning web structure, and finally iv) deliver the whole to the user allowing him/her to navigate through the learning web according to his/her needs and constraints.",On the fly generation of mobile learning structured multimedia content from the web
"['F. Anthony San Lucas', 'G. Wang', 'Paul Scheet', 'Bo Peng']","Motivation: Storing, annotating and analyzing variants from next-generation sequencing projects can be difficult due to the availability of a wide array of data formats, tools and annotation sources, as well as the sheer size of the data files. Useful tools, including the GATK, ANNOVAR and BEDTools can be integrated into custom pipelines for annotating and analyzing sequence variants. However, building flexible pipelines that support the tracking of variants alongside their samples, while enabling updated annotation and reanalyses, is not a simple task.#R##N##R##N#Results: We have developed variant tools, a flexible annotation and analysis toolset that greatly simplifies the storage, annotation and filtering of variants and the analysis of the underlying samples. variant tools can be used to manage and analyze genetic variants obtained from sequence alignments, and the command-line driven toolset could be used as a foundation for building more sophisticated analytical methods.#R##N##R##N#Availability and implementation:variant tools consists of two command-line driven programs vtools and vtools_report. It is freely available at http://varianttools.sourceforge.net, distributed under a GPL license.#R##N##R##N#Contact: bpeng@mdanderson.org",Integrated annotation and analysis of genetic variants from next-generation sequencing studies with variant tools
"['Hyunyoung Kil', 'Wonhong Nam', 'Dongwon Lee']","A preliminary framework, termed as  DL2Go  , that enables  editable  and  portable  personal digital libraries is presented. For mobile offline users of digital libraries,  DL2Go  can: (1) package digital libraries into mobile storage devices such as flash drives, along with needed application softwares (e.g., wiki and DBMS), (2) (de-)compress contents of digital libraries to address storage constraints of mobile users when needed, (3) enables users to add, delete, and update entities of digital libraries using wiki framework, and (4) share/sync edited contents with other  DL2Go  users and the server using web services and RSS framework.",DL2Go: Editable Digital Libraries in the Pocket
"['Andrew Samuelson', 'Padmanabhan Seshaiyer']","The enlargement and rupture of intracranial and abdominal aortic aneurysms constitutes a major medical problem. It has been suggested that enlargement and rupture are due to mechanical instabilities of the associated complex fluid-solid interaction in the lesions. In this paper, we examine a coupled fluid-structure mathematical model for a cylindrical geometry representing an idealized aneurysm using both analytical and numerical techniques. A stability analysis for this subclass of aneurysms is presented. It is shown that this subclass of aneurysms is dynamically stable both with and without a viscoelastic contribution to the arterial wall.",Stability of Membrane Elastodynamics with Applications to Cylindrical Aneurysms
"['Leiguang Gong', 'Casimir A. Kulikowski']","This paper presents a new approach to the knowledge-based composition of processes for image interpretation and analysis. Its computer implementation in the VISIPLAN (Vision Planner) system provides a way of modeling the composition of image analysis processes within a unified, object-centered hierarchical planning framework. The approach has been tested and shown to be flexible in handling a reasonably broad class of multi-modality biomedical image analysis and interpretation problems. It provides a relatively general design or planning framework, within which problem specific image analysis and recognition processes can be generated more efficiently and effectively. In this way, generality is gained at the design and planning stages, even though the final implementation stage of interpretation processes is almost invariably problem- and domain-specific. >",Composition of image analysis processes through object-centered hierarchical planning
"['Juan J. Martinez-Espla', 'Tomas Martinez-Marin', 'Juan M. Lopez-Sanchez']","This work presents a phase unwrapping (PU) algorithm for SAR interferometry based on an approximate grid-based filter (GbF). This PU algorithm, which makes use of state space techniques, performs simultaneously noise filtering and phase unwrapping. The formulation of this technique provides independence from noise statistics and is not constrained by the non-linearity of the problem. Results obtained with synthetic and real data show a significant improvement with respect to conventional PU algorithms in some situations.",Introduction of a grid-based filter approach for InSAR phase filtering and unwrapping
"['Zoran Utkovski', 'Wenbin Li', 'Juergen Lindner']","A non-coherent distributed space-time coding scheme for wireless relay networks is presented. The scheme is the counterpart of non-coherent coding based on Grassmann codes, for cooperative wireless networks with relays. It is shown that special classes of Grassmann codes obtained from unitary codes such as Alamouti or Sp(n) codes, can be used as noncoherent distributed space-time codes. The scheme is extended to construct non-coherent distributed space-time codes of large block lengths in a recursive procedure.",Distributed Non-Coherent Grassmann Space-Time Codes for Wireless Relay Networks
"['Julia Kempe', 'Oded Regev']","We consider one-round games between a classical verifier and two provers. One of the main questions in this area is the \emph{parallel repetition question}: If the game is played $\ell$ times in parallel, does the maximum winning probability decay exponentially in $\ell$? In the classical setting, this question was answered in the affirmative by Raz. More recently the question arose whether the decay is of the form $(1-\Theta(\eps))^\ell$ where $1-\eps$ is the value of the game and $\ell$ is the number of repetitions. This question is known as the \emph{strong parallel repetition question} and was motivated by its connections to the unique games conjecture. It was resolved by Raz who showed that strong parallel repetition does \emph{not} hold, even in the very special case of games known as XOR games. This opens the question whether strong parallel repetition holds in the case when the provers share entanglement. Evidence for this is provided by the behavior of XOR games, which have strong (in fact \emph{perfect}) parallel repetition, and by the recently proved strong parallel repetition of linear unique games. A similar question was open for games with so-called non-signaling provers. Here the best known parallel repetition theorem is due to Holenstein, and is of the form $(1-\Theta(\eps^2))^\ell$. We show that strong parallel repetition holds neither with entangled provers nor with non-signaling provers. In particular we obtain that Holenstein's bound is tight. Along the way we also provide a tight characterization of the asymptotic behavior of the entangled value under parallel repetition of unique games in terms of a semidefinite program.",No Strong Parallel Repetition with Entangled and Non-signaling Provers
"['Tudor Marian', 'Daniel A. Freedman', 'Kenneth P. Birman', 'Hakim Weatherspoon']","High-bandwidth, semi-private optical lambda networks carry growing volumes of data on behalf of large data centers, both in cloud computing environments and for scientific, financial, defense, and other enterprises. This paper undertakes a careful examination of the end-to-end characteristics of an uncongested lambda network running at high speeds over long distances, identifying scenarios associated with loss, latency variations, and degraded throughput at attached end-hosts. We use identical fast commodity source and destination platforms, hence expect the destination to receive more or less what we send. We observe otherwise: degraded performance is common and easily provoked. In particular, the receiver loses packets even when the sender employs relatively low data rates. Data rates of future optical network components are projected to outpace clock speeds of commodity end-host processors, hence more and more end-to-end applications will confront the same issue we encounter. Our work thus poses a new challenge for those hoping to achieve dependable performance in higher-end networked settings.",Empirical characterization of uncongested optical lambda networks and 10GbE commodity endpoints
"['Maria A. Osorio', 'Fred Glover']","The exploitation by Osorio et al. (2002) of surrogate constraints and constraint pairing is strengthened to give better results in multidimensional knapsack problems, by excluding simple bounding constraints as component constraints. The surrogate constraint is obtained by weighting the original problem constraints by their associated dual values in the LP relaxation of the problem. This surrogate constraint is paired with the objective function to obtain a combined constraint where negative variables are replaced by complemented variables and the resulting constraint used to fix variables to zero or one.",Exploiting surrogate constraint analysis for fixing variables in both bounds for multidimensional knapsack problems
"['Matthew E. Taylor', 'Shimon Whiteson', 'Peter Stone']","Reinforcement learning (RL) methods have become popular in recent years because of their ability to solve complex tasks with minimal feedback. Both genetic algorithms (GAs) and temporal difference (TD) methods have proven effective at solving difficult RL problems, but few rigorous comparisons have been conducted. Thus, no general guidelines describing the methods' relative strengths and weaknesses are available. This paper summarizes a detailed empirical comparison between a GA and a TD method in Keepaway, a standard RL benchmark domain based on robot soccer. The results from this study help isolate the factors critical to the performance of each learning method and yield insights into their general strengths and weaknesses.",Temporal difference and policy search methods for reinforcement learning: an empirical comparison
"['R Rajesh', 'Vinod Sharma']","We address the problem of estimating a random field via a wireless sensor network. We use a Multiple Access Channel (MAC) as the basic building block for such a network. For Gaussian sources over Gaussian MACs, we show that Amplify and Forward scheme (AF) performs well in such sensor network scenarios where the battery power is at a premium. We then extend this result to the hierarchical network scenario and show that it can perform favourably to the Slepian-Wolf based source coding and independent channel coding scheme. Since AF is simple and scalable, a good performance makes it an attractive scheme.",Amplify and Forward for Correlated Data Gathering over Hierarchical Sensor Networks
['Merideth Leigh Gattis'],"Three experiments investigated whether the similarity of relational structures influences the interpretation of spatial representations. Adults were shown diagrams of hand gestures paired with simple statements and asked to judge the meaning of new gestures. In Experiment 1 the gestures were paired with active declarative statements. In Experiment 2, the gestures were paired with conjunctive and disjunctive relations. Experiment 3 used statements similar to those used in Experiment 1, but eliminated the initial object-to-object mapping provided in Experiments 1 and 2. In all three experiments, most participants chose an interpretation that set up a parallel relational structure between the gesture and its meaning: spatial elements were paired with conceptual elements and spatial relations were paired with conceptual relations. These results are consistent with the hypothesis that similarity of relational structures influences spatial reasoning, and have implications for analogical reasoning, diagrammatic reasoning, and language processing.",Mapping relational structure in spatial reasoning
"['Erica Mealy', 'Paul A. Strooper']","Up to 75% of the costs associated with the development of software systems occur post-deployment during maintenance and evolution. Software refactoring is a process that can significantly reduce the costs associated with software evolution. Refactoring is defined as internal modification of source code to improve system quality, without change to observable behaviour. Tool support for software refactoring attempts to further reduce evolution costs by automating manual, error-prone and tedious tasks. Although the process of refactoring is well-defined, tools supporting refactoring do not support the full process. Existing tools suffer from issues associated with the level of automation, the stages of the refactoring process supported or automated, the subset of refactorings that can be applied, and complexity of the supported refactorings. This paper presents a framework for evaluating software refactoring tool support based on the DESMET method. For the DESMET application, a functional analysis of the requirements for supporting software refactoring is used in conjunction with a case study. This evaluation was completed to assess the support provided by six Java refactoring tools and to evaluate the efficacy of using the DESMET method for evaluating refactoring tools.",Evaluating software refactoring tool support
"['Bogdan Smolka', 'Konstantinos N. Plataniotis', 'Rastislav Lukac', 'Anastasios N. Venetsanopoulos']",The paper presents a new filtering scheme for the removal of impulsive noise in color images. It is based on estimating the probability density function for color pixels in a filter window by means of the kernel density estimation method. A quantitative comparison of the proposed filter with the vector median filter shows its excellent ability to reduce noise while simultaneously preserving fine image details.,New class of impulsive noise reduction filters based on kernel density estimation
"['Yufei Bai', 'Xiujun Ma', 'Kunqing Xie', 'Cuo Cai', 'Ke Li']","Resource management and optimization among autonomous spatial databases in a grid environment is always a challenge task. In this paper, we introduce a BDI Agent model for cooperating complex spatial query optimization task in a grid environment. Spatial query agents coordinate with spatial database resource agents using FIPA auction protocol for dynamic spatial query execution. The experiment showed that BDI Agents achieve efficiency on complex spatial query optimization in grid environment.",Agent-Based Spatial Query Optimization in a Grid Environment
"['Feng Pan', 'Zhengguo Li', 'Keng Pang Lim', 'Dajun Wu', 'Rongshan Yu', 'Genan Feng']","With the recent development of third-generation communication technologies, encoding live video using a PDA and sharing it among friends has become a reality. However, the embedded processor inside a PDA is still not powerful enough and there are two major hurdles to overcome: (1) video coding needs to meet the rigorous constraint of the available computation capacity of a PDA; (2) In a PDA the computing power allocated to video coding may vary drastically (in bursts). In this paper, a new adaptive rate control algorithm is proposed for video coding over a PDA. This adaptive rate control scheme takes into account the time constraint of a PDA, and its bit allocation depends not only on the available data bits, but more importantly, on the available coding time. Experimental results show that, compared to the existing rate control scheme, the new algorithm can always achieve the maximum frame rate, maximize the utilization of the available bandwidth and computing power, increase the average PSNR, and improve the subjective perceptual quality of the reconstructed video.",An adaptive rate control algorithm for video coding over personal digital assistants (PDA)
"['Ronald J. Cosentino', 'Stephen J. Meehan']","Introduced here is a new technique for encrypting bandpass signals. The technique linearly filters a sample-masked signal, modifying the encryption component of the cryptogram to place its spectrum within the frequency band of the signal. Sample masking is a recognized means of encrypting baseband signals while containing the spectrum of the cryptogram within the signal bandwidth. A drawback occurs for bandpass signals, owing to the processing required in the mixing procedures used ordinarily to convert the signal from bandpass to baseband and back. In contrast, the linear filter modifies the encryption component of the cryptogram without processing the signal component, reducing the computational burden of encrypting and decrypting by an order of magnitude. When the maximum frequency is an integer multiple of the minimum frequency, the filter can output the same information symbols that appear at the input and replace the redundant symbols with symbols that control the passband of the output signal. For voice bandwidths of 300-3000 Hz sampled at 6000 Hz, redundancy symbols (one in every ten or 600 samples/s) are removed, the information symbols (nine in ten or 5400 samples/s) are encrypted, and the redundancy symbols are replaced by bandwidth-controlling symbols. Experimental results of clear voice transmitred over simulated telephone lines validate the technique.",An Efficient Technique for Sample-Masked Voice Transmission
['Fernando J. Barros'],"Although in most of the modeling methodologies the structure of the models is viewed as time-invariant, dynamic structure models offers a better paradigm to represent certain type of systems. We describe the Dynamic Structure Discrete Event System Specification (DSDEVS) formalism and its implementation, the DELTA modeling and simulation environment. The DSDEVS formalism is used to model the Eratosthenes Sieve for prime numbers. The sieve is modeled in two different perspectives: iterative and recursive. These approaches are compared with respect to performance and model expressiveness. A variation of the iterative solution will be extended to find relatively prime numbers in a scrambled list, showing the generality of the DELTA system.",Dynamic structure modeling and simulation of the Eratosthenes Sieve for prime numbers
"['Jacques Demongeot', 'Mathilde Noual', 'Sylvain Sen??']","In line with fields of theoretical computer science and biology that study Boolean automata networks often seen as models of regulation networks, we present some results concerning the dynamics of networks whose underlying interaction graphs are circuits, that is, Boolean automata circuits. In the context of biological regulation, former studies have highlighted the importance of circuits on the asymptotic dynamical behaviour of the biological networks that contain them. Our work focuses on the number of attractors of Boolean automata circuits. We prove how to obtain formally the exact value of the total number of attractors of a circuit of arbitrary size n as well as, for every positive integer p, the number of its attractors of period p depending on whether the circuit has an even or an odd number of inhibitions. As a consequence, we obtain that both numbers depend only on the parity of the number of inhibitions and not on their distribution along the circuit.",On the Number of Attractors of Positive and Negative Boolean Automata Circuits
"['Lionel Garnier', 'Sebti Foufou', 'Yohan D. Fougerolle']","In this article, we present a method to perform G 1 -continuous blends between a differentiable superquadric of revolution and a plane or a sphere using Dupin cyclides. These blends are patches delimited by four lines of curvature. They allow to avoid parameterization problems that may occur when parametric surfaces are used. Rational quadratic Bezier curves are used to approximate the principal circles of the Dupin cyclide blends and thus a complex 3D problem is now reduced to a simpler 2D problem. We present the necessary conditions to be satisfied to create the blending patches and illustrate our approach by a number of superellipsoid/plane and superellipsoid/sphere blending examples.",G1-Blend between a Differentiable Superquadric of Revolution and a Plane or a Sphere Using Dupin Cyclides
"['Iacopo Carreras', 'David Tacconi']","This demonstration presents U-Hopper, a user-centric heterogeneous opportunistic middleware specifically tailored to the diffusion of user centric information, such as contextual and entertainment data, in opportunistic environments. The proposed platform exploits proximity wireless interfaces available on most commercial mobile devices for disseminating data among mobile users. Such diffusion if driven by the specific interests of mobile users, combined with the intrinsic locality of data being generated in such pervasive environments. The prototype is developed over java-enabled smartphones and relies on Bluetooth connectivity for achieving proximity communications. In this paper, U-Hopper is described in all its functional components, together with the details of its software implementation.",U-Hopper: User-centric heteogeneous opportunistic middleware
"['Renato Machado', 'Bartolomeu F. Uchoa-Filho', 'Tolga M. Duman']","We propose a simple cooperative diversity scheme for a communication system consisting of two cooperating nodes that receive a single channel state information (CSI) bit from the destination node. Essentially, the feedback bit tells which cooperating node has the strongest channel, and this information is used appropriately to obtain cooperative diversity. A simple linear receiver is proposed and its performance is shown to be very close to the maximum-likelihood performance. An upper bound on the average error probability is derived for binary phase-shift keying (BPSK) in flat Rayleigh fading channels under the assumption of ideal inter-user channel. In addition, through computer simulations, it is verified that the proposed scheme presents a good error performance when the inter-user channel signal-to-noise ratio is high or when the inter-user channel has a well-defined line-of-sight component. In other words, the new scheme becomes interesting when the cooperating nodes are close to each other. Comparisons with a cooperative scheme based on the Alamouti code are provided.",A Cooperative Diversity Scheme with Partial Channel Knowledge at the Cooperating Nodes
"['Olusegun Folorunso', 'A. T. Akinwale', 'Aderonke Justina Ikuomola']","Network Intrusion Detection System (NIDS) is a security system that monitors the network traffic and analyzes activities for possible hostile attacks. A novel collaborative visual analytics application for cognitive overloaded site security officer (SSO) in the network intrusion detection environment is presented. The system was developed for site security officers who need to analyze heterogeneous, complex intrusion under time pressure, and then make predictions and time-critical decisions rapidly and correctly under a constant influx of intrusion alert/alarm. This purpose was achieved by designing system architecture of a Treemaps Visualization on NIDs. The Treemaps Network Intrusion Detection System was implemented using the Java platform. The results of an informal usability of the network system were evaluated by the security experts in the context of EndleyÉ??s three levels of situation awareness. The proposed visualization tool has some economic advantages by aiding NIDÉ??s SSO to dynamically discover intrusive zone which will reduce cost of manual analysis and high risks, efficient space utilization, interactivity, comprehension and esthetics.",Using Visual Analytics to Develop Situation Awareness in Network Intrusion Detection System
"['Ching-Hui Chen', 'C.C. Fung']","A bit error rate optimized unimode selection algorithm is proposed for MIMO-OFDM based spatial multiplexing systems using linear receivers. For a fixed number of antennas, the proposed scheme dynamically selects the number of subcarriers used for transmission. The proposed method increases the diversity advantage of spatial multiplexing systems without the use of computational expensive spacetime receivers such as ML and VBLAST. Simulation results show that the proposed unimode technique outperforms existing ones in terms of link reliability by performing selection of data streams across different subcarriers, and compensating significantly attenuated subcarriers due to deep fades.",Ber optimized unimode precoder design for MIMO-OFDM based spatial multiplexing systems
"['Edmond M. DuPont', 'Carl A. Moore', 'Rodney G. Roberts']","Unmanned ground vehicles (UGV's) commonly used in military applications must possess the capability to traverse various terrains that may largely affect the performance and controllability of the vehicle. A UGV that can autonomously perceive its terrain using navigational sensors can make necessary changes to its control strategy. The research presented uses the output of the induced vehicle's vibration measured by navigational sensors to classify the underlying terrain at multiple speeds. The classification algorithm incorporates Principal Component Analysis (PCA) for feature extraction and dimension reduction. The PCA transformation coefficients are then used to develop a manifold curve that uses these known coefficients to interpolate unknown coefficients of the terrains as the robot's speed changes. Experimental data is collected using two distinctly different unmanned ground vehicle platforms. Results demonstrate the performance of the method for classifying multi-differentiated terrains broadly classified as grass, asphalt, mud, and gravel.",Terrain classification for mobile robots traveling at various speeds: An eigenspace manifold approach
"['Vladimir Protasov', 'Rapha?Æl M. Jungers']","We analyse the problem of stability of a continuous time linear switching system (LSS) versus the stability of its Euler discretization. It is well-known that the existence of a positive ?? for which the corresponding discrete time system with stepsize ?? is stable implies the stability of the LSS. Our main goal is to obtain a converse statement, that is to estimate the discretization stepsize ?? > 0 up to a given accuracy e > 0. This would lead to a method for deciding the stability of a continuous time LSS with a guaranteed accuracy. As a first step towards the solution of this problem, we show that for systems of matrices with real spectrum the parameter ?? can be effectively estimated. We prove that in this special case, the discretized system is stable if and only if the Lyapunov exponent of the LSS is smaller than - C ??, where C is an effective constant depending on the system. The proofs are based on applying Markov-Bernstein type inequalities for systems of exponents.",Is switching systems stability harder for continuous time systems
"['Mehrdad Raisi', 'Selam T. Ahderom', 'Kamal Alameh', 'Kamran Eshraghian']",We propose a novel multiband tunable optical filter structure based on an Opto-VLSI processor. Tunability for each band is achieved by controlling the size and shape of holographic diffractive grating generated on the Opto-VLSI processor. Results for an experimental 3-passband tunable filter are presented indicating over 25 dB of dynamic range and passband bandwidth of 2 nm.,Multi-band MicroPhotonic tunable optical filter
"['Kathrin Maria Gerling', 'Frank P. Schulte', 'Maic Masuch']","Game research increasingly addresses human factors of gaming. Though more and more seniors become players, game design for frail elderly has rarely been explored. This paper addresses game design for senior citizens experiencing age-related changes, especially cognitive and physical limitations. We introduce and evaluate the case study  SilverPromenade , which is specifically aimed at providing institutionalized frail elderly with a new leisure activity.  SilverPromenade  allows players to go on virtual walks while accounting for special needs regarding game complexity, and simplistic interaction paradigms are provided using Nintendo's Wii Remote and the Balance Board for game control. Evaluation results suggest that despite age-related impairments, the game was generally accessible to elderly persons. Yet, differences between inexperienced and experienced players were observed which suggest that interaction problems may be reduced by engaging with games over a longer time. Findings also indicate that the engagement of elderly players transcends into their everyday life, and their social interaction increases among one another. Most importantly, the evaluation showed that games were perceived as enjoyable leisure activity, supporting the approach of applying digital games to raise the quality of life among frail elderly by fostering activity.",Designing and evaluating digital games for frail elderly persons
"['Yoshihiro Tanaka', 'Ryohei Sugimura', 'Akihito Sano', 'Hideo Fujimoto']","This paper presents the development of an active tactile sensor using fluid. Measurement of stiffness and surface conditions, such as sliminess, smoothness, and others, of body tissue is discussed. This sensor is made of a silicone balloon catheter. The balloon is contacted with a soft object and expanded by using fluid. In the process of the expansion, the balloon pushes the object and the contact surface of the balloon slips or sticks. These phenomena depend on stiffness and surface conditions of the object. The expansion can be evaluated by measuring the flow of the fluid. The balloon that is a sensing element is very soft and flexible and does not need electricity. This sensor is available as a medical sensor since it provides safety for human. Experiments using the sensor are carried out on soft samples with various stiffness and surface conditions. It is confirmed that the sensor output are different from each sample. Furthermore, some experiments under different conditions on inner pressure and load are conducted. The results show that the sensor has a potential to measure stiffness and surface conditions.",An active tactile sensor using fluid for body tissue
"['Udo Hahn', 'Martin Honeck', 'Stefan Schulz']","Document retrieval in languages with a rich and complex morphology - particularly in terms of derivation and (single-word) composition - suffers from serious performance degradation with the stemming-only query-term-to-text-word-matching paradigm. We propose an alternative approach in which morphologically complex word forms are segmented into relevant subwords (such as stems, prefixes, suffixes), and subwords constitute the basic unit for indexing and retrieval. We evaluate our approach on a large biomedical document collection.",Subword-based text retrieval
"['Laura Garach', 'J De Ona', 'Miguel Pasadas']","This paper presents a quick, simple and automatic approximation method that allows the alignments in a road (straights, curves or clothoids) and their respective curvature values to be identified from an approximation point set given by its UTM (Universal Transverse Mercator) coordinates obtained from field data. The method reconstructs the geometry of the road by a smoothing variational cubic spline, computes the curvature function of this spline and approximates the curvature function using a polygonal function formed by trapezoids on the abscises axis. This method permits to obtain alignments that can be used to study any road system that has certain characteristics.",Original Articles: Determination of alignments in existing roads by using spline techniques
"['Kai Hwang', 'Hai Jin', 'Roy S. C. Ho']","A new RAID-x (redundant array of inexpensive disks at level x) architecture is presented for distributed I/O processing on a serverless cluster of computers. The RAID-x architecture is based on a new concept of orthogonal striping and mirroring (OSM) across all distributed disks in the cluster. The primary advantages of this OSM approach lie in: (1) a significant improvement in parallel I/O bandwidth; (2) hiding disk mirroring overhead in the background; and (3) greatly enhanced scalability and reliability in cluster computing applications. All claimed advantages are substantiated with benchmark performance results on the Trojans cluster built at USC in 1999. The authors discuss the issues of scalable I/O performance, enhanced system reliability, and striped checkpointing on distributed RAID-x in a serverless cluster environment.",RAID-x: a new distributed disk array for I/O-centric cluster computing
"['Promila Agrawal', 'Dina Bitton', 'Keh-Chang Guh', 'Chengwen Liu', 'Clement T. Yu']","(i) (ii) (is) In the integrated strategy, one component decides which relation should remain fragmented at different sites. The other component decides which local operations, selections and projections, should be performed before the join operations. Our experimental results reveal that the choices made by the integrated algorithm in deciding which relation should remain fragmented and which local operations to be performed are valid. More precisely , the response times of queries processed by the algorithm are lower than those of the same queries processed using other strategies. These experimental results agree with the analytic cost model which we have previously proposed [YGC87]. In addition, our experiments provide insight on the relative cost VO, network, and processing cost in a real environment. This enables us to identify timeconsuming operations in distributed query processing, and suggest ways to improve perforniance.",A case study for distributed query processing
"['Maris Lapinsh', 'Peteris Prusis', 'Staffan Uhl??n', 'Jarl E. S. Wikberg']","Motivation: Proteochemometrics is a novel technology for the analysis of interactions of series of proteins with series of ligands. We have here customized it for analysis of large datasets and evaluated it for the modeling of the interaction of psychoactive organic amines with all the five known families of amine G protein-coupled receptors (GPCRs).#R##N##R##N#Results: The model exploited data for the binding of 22 compounds to 31 amine GPCRs, correlating chemical descriptions and cross-descriptions of compounds and receptors to binding affinity using a novel strategy. A highly valid model (q2 = 0.76) was obtained which was further validated by external predictions using data for 10 other entirely independent compounds, yielding the high q2ext = 0.67. Interpretation of the model reveals molecular interactions that govern psychoactive organic amines overall affinity for amine GPCRs, as well as their selectivity for particular amine GPCRs. The new modeling procedure allows us to obtain fully interpretable proteochemometrics models using essentially unlimited number of ligand and protein descriptors.#R##N##R##N#Contact: jarl.wikberg@farmbio.uu.se#R##N##R##N#Supplementary information: Supplementary data are available at Bioinformatics online.",Improved approach for proteochemometrics modeling: application to organic compound---amine G protein-coupled receptor interactions
"['Craig Macdonald', 'Iadh Ounis']","An expert search system assists users with their ""expertise need"" by suggesting people with relevant expertise to their query. Most systems work by ranking documents in response to the query, then rank- ing the candidates using information from this initial document ranking and known associations between documents and candidates. In this pa- per, we aim to determine whether we can approximate an evaluation of the expert search system using the underlying document ranking. We evaluate the accuracy of our document ranking evaluation by assessing how closely each measure correlates to the ground truth evaluation of the candidate ranking. Interestingly, we find that improving the under- lying ranking of documents does not necessarily result in an improved candidate ranking.",Expert Search Evaluation by Supporting Documents
"['Bernie C. Till', 'Manjinder Singh Benning', 'N. J. Livingston']","The WISP is a novel wireless sensor that uses 3 axis magnetometers, accelerometers, and rate gyroscopes to provide a real-time measurement of its own orientation in space. Orientation data are transmitted via the Open Sound Control protocol (OSC) to a synthesis engine for interactive live dance performance.",Wireless inertial sensor package (WISP)
['Shuangqing Wei'],"Synchronization of relay nodes is an important and critical issue in exploiting cooperative diversity in wireless networks. In this paper, two asynchronous cooperative diversity schemes are proposed, namely, distributed delay diversity and asynchronous space-time coded cooperative diversity schemes. In terms of the overall diversity-multiplexing (DM) tradeoff function, we show that the proposed independent coding based distributed delay diversity and asynchronous space-time coded cooperative diversity schemes achieve the same performance as the synchronous space-time coded approach which requires an accurate symbol-level timing synchronization to ensure signals arriving at the destination from different relay nodes are perfectly synchronized. This demonstrates diversity order is maintained even at the presence of asynchronism between relay node. Moreover, when all relay nodes succeed in decoding the source information, the asynchronous space-time coded approach is capable of achieving better DM tradeoff than synchronous schemes and performs equivalently to transmitting information through a parallel fading channel as far as the DM tradeoff is concerned. Our results suggest the benefits of fully exploiting the space-time degrees of freedom in multiple antenna systems by employing asynchronous space-time codes even in a frequency-flat-fading channel. In addition, it is shown asynchronous space-time coded systems are able to achieve higher mutual information than synchronous space-time coded systems for any finite signal-to-noise ratio (SNR) when properly selected baseband waveforms are employed.",DiversityÉ??Multiplexing Tradeoff of Asynchronous Cooperative Diversity in Wireless Networks
"['Olivier Fourdrinoy', '??ric Gr??goire', 'Bertrand Mazure', 'Lakhdar Sais']","This last decade, propositional reasoning and search has been one of the hottest topics of research in the A.I. community, as the Boolean framework has been recognized as a powerful setting for many reasoning paradigms thanks to dramatic improvements of the efficiency of satisfiability checking procedures. SAT, namely checking whether a set of propositional clauses is satisfiable or not, is the technical core of this framework. In the paper, a new linear-time pre-treatment of SAT instances is introduced. Interestingly, it allows us to discover a new polynomial-time fragment of SAT that can be recognized in linear-time, and show that some benchmarks from international SAT competitions that were believed to be difficult ones, are actually polynomial-time and thus easy-to-solve ones.",Reducing hard SAT instances to polynomial ones
"['K. Fujimura', 'Toshihiro Konoma', 'Kamijo Shunsuke', 'Tatsuru Daimon']","This paper describes the evaluation of driver assistance system by a driving simulator that authors have developed that provides information on congestion to the driver in the expressway. In the evaluation, driver behavior measurement and subjective assessment by the questionnaire are done, when the system correctly offers drivers warning of congestion (true warning), and when the warning cannot be offered (lack of warning). As a result of these evaluations, the influence that both the true warning by the system and lack of warning by the system give to the driver behavior is clarified.",Evaluation of VII system using vision sensors in expressway by driving simulator
"['Aidan Burton', 'Vassilis Glenis', 'Mari R. Jones', 'Chris Kilsby']","Automated easy-to-use tools capable of generating spatial-temporal weather scenarios for the present day or downscaled future climate projections are highly desirable. Such tools would greatly support the analysis of hazard, risk and reliability of systems such as urban infrastructure, river catchments and water resources. However, the automatic parameterization of such models to the properties of a selected scenario requires the characterization of both point and spatial statistics. Whilst point statistics, such as the mean daily rainfall, may be described by a map, spatial properties such as cross-correlation vary according to a pair of sample points, and should ideally be available for every possible pair of locations. For such properties simple automatic representations are needed for any pair of locations.To address this need simple empirical models are developed of the lag-zero cross-correlation-distance (XCD) properties of United Kingdom daily rainfall. Following error and consistency checking, daily rainfall timeseries for the period 1961-1990 from 143 raingauges are used to calculate observed XCD properties. A three parameter double exponential expression is then fitted to appropriate data partitions assuming isotropic and piecewise-homogeneous XCD properties. Three models are developed: 1) a national aseasonal model; 2) a national model partitioned by calendar month; and 3) a regional model partitioned by nine UK climatic regions and by calendar month. These models provide estimates of lag-zero cross-correlation properties of any two locations in the UK.These cross-correlation models can facilitate the development of automated spatial rainfall modelling tools. This is demonstrated through implementation of the regional model into a spatial modelling framework and by application to two simulation domains (both ~10,000?km2), one in north-west England and one in south-east England. The required point statistics are generally well simulated and a good match is found between simulated and observed XCD properties.The models developed here are straightforward to implement, incorporate correction of data errors, are pre-calculated for computational efficiency, provide smoothing of sample variability arising from sporadic coverage of observations and are repeatable. They may be used to parameterise spatial rainfall models in the UK and the methodology is likely to be easily adaptable to other regions of the world. Spatial properties of UK daily rainfall are described with simple models.An aseasonal national model estimates lag-0 cross-correlation using only distance.More tailored models specialize by calendar month and for nine climatic regions.Such properties allow spatial rainfall models to be parameterized easily.A simple tool for spatial-temporal rainfall modelling is demonstrated.",Models of daily rainfall cross-correlation for the United Kingdom
"['Xin Fan', 'Zhongxuan Luo', 'Jielin Zhang', 'Xinchen Zhou', 'Qi Jia', 'Daiyun Luo']","Geometric invariants are important for shape recognition and matching. Existing invariants in projective geometry are typically defined on the limited number (e.g., five for the classical cross-ratio) of collinear planar points and also lack the ability to characterize the curve or surface underlying the given points. In this paper, we present a projective invariant named after the characteristic number of planar algebraic curves. The characteristic number in this work reveals an intrinsic property of an algebraic hypersurface or curve, which relies no more on the existence of the surface or curve as its planar version. The new definition also generalizes the cross-ratio by relaxing the collinearity and number of points for the cross-ratio. We employ the characteristic number to construct more informative shape descriptors that improve the performance of shape recognition, especially when severe affine and perspective deformations occur. In addition to the application to shape recognition, we incorporate the geometric constraints on facial feature points derived from the characteristic number into facial feature matching. The experiments show the improvements on accuracy and robustness to pose and view changes over the method with the collinearity and cross-ratio constraints.",Characteristic Number: Theory and Its Application to Shape Analysis
"['Suchendra M. Bhandarkar', 'Tongzhang Jiang', 'Kunal Verma', 'Nan Li']",The design and implementation of a computer vision system called DNAScan for the automated analysis of DNA hybridization images is presented. The hybridization of a DNA clone with a radioactively tagged probe manifests itself as a spot on the hybridization membrane. The imaging of the hybridization membranes and the automated analysis of the resulting images are imperative for high-throughput genomics experiments. A recursive segmentation procedure is designed and implemented to extract spotlike features in the hybridization images in the presence of a highly inhomogeneous background. Positive hybridization signals (hits) are extracted from the spotlike features using grouping and decomposition algorithms based on computational geometry. A mathematical model for the positive hybridization patterns and a Bayesian pattern classifier based on shape-based moments are proposed and implemented to distinguish between the clone-probe hybridization signals. Experimental results on real hybridization membrane images are presented.,Automated analysis of DNA hybridization images for high-throughput genomics
"['Dequan Li', 'Xiaofan Wang']","The paper concerns with the problem of asynchronous gossip-based dynamic averaging estimation with communication constraints, where each agent estimates the local time-varying parameters individually, then random pairs of connected agents iteratively and locally perform a pairwise average of their estimations through quantized information communication. How quantization affects the evolution of the gossip-based dynamic averaging estimation algorithm is investigated. We prove that the agents' states converge to a random variable that deviates from the average of the estimated parameters. We derive a strong result about the upper bound for the asymptotic mean square error of the states, which just captures effect of the quantized precision and is independent of the network parameters.",Asynchronously dynamic averaging estimation with communication constraints
"['Fabio Bellifemine', 'Romualdo Picco']","An image coding scheme using the discrete cosine transform is analyzed when the transform coefficients are vector quantized. The coding method is based on the known scheme proposed by Chen and Smith (1977) which sorts the picture blocks into classes according to the level of image activity. The coding scheme is modified to allow for vector quantization of the ac coefficients, in particular a pyramid vector quantizer (PVQ) is used. This is based on the statistical and geometric properties of a Laplacian source which, in fact, is the best model for the ac coefficients of the two-dimensional discrete cosine transform (2D-DCT) of an image. A method for forming almost statistically independent vectors is also suggested and improves quantization performance. Images are encoded with both the PVQ and standard scalar quantizer transform coders, demonstrating that the PVQ coder reduces the mean square encoding error and improves image quality. In particular, emphasis is given to how the use of fractional bit rates affects the objective and subjective gains obtained. The results presented (i.e. mean square error values and printed images) have been obtained experimentally, working with a statistical criterion in a group of images whose size was in accordance with the 50 Hz CCIR Recommendation 601 Standard. >",Video signal coding with DCT and vector quantization
"['Hyung Chan Kim', 'Angelos D. Keromytis', 'Michael J. Covington', 'Ravi Sahita']","Dynamic taint analysis (DTA) is a technique used for tracking information flow by propagating taint propagation across memory locations during program execution. Most implementations of DTA are based on dynamic binary instrumentation (DBI) frameworks or whole-system emulators/virtual machine monitors. The boundary of information tracking with DBI frameworks is a single process, while system emulators can cover a host, including the OS. Using system emulators, it may be possible to consider taint propagation across multiple processes executing locally, within the emulator. However, there is an increasing need for tracking information flow across single-system boundaries and across the whole enterprise. We describe a proof-of-concept architecture for tracking multiple mixed-information flows among several processes across a distributed enterprise. Our DTA tool is based on PIN, a DBI framework by Intel, and the concatenated DTA processing is realized with per-host flow managers. We have tested our prototype with typical enterprise applications. As a motivating example, we track information leakage due to a SQL injection attack from a web-based database server query. Our work is of an exploratory nature, aiming to expose our early findings and identify areas where additional research is needed in improving usability and performance.",Capturing Information Flow with Concatenated Dynamic Taint Analysis
"['Sam Jebeile', 'Robert Reeve']","This study examines the issue of technology acceptance in a multi-campus secondary college in Sydney, Australia. Seventy-five teachers across two campuses were surveyed as to their perceptions regarding technology acceptance. Regression analysis was used to compare the explanatory power of the perceived characteristics of innovating model (PCIM),  and the technology acceptance model (TAM).  Both models explained a substantial amount of variation in technology acceptance. However, our findings suggest that it is preferable to use the PCIM, rather than the TAM, to explain intention to use an information technology innovation. Implications for both future research and practice are discussed.",Explaining intention to use an information technology innovation: an empirical comparison of the perceived characteristics of innovating and technology acceptance models
"['Morris A. Swertz', 'Martijn Dijkstra', 'Tomasz Adamusiak', 'K. Joeri van der Velde', 'Alexandros Kanterakis', 'Erik Roos', 'Joris Lops', 'Gudmundur A. Thorisson', 'Danny Arends', 'George Byelas', 'Juha Muilu', 'Anthony J. Brookes', 'Engbert O. de Brock', 'Ritsert C. Jansen', 'Helen Parkinson']","Background: There is a huge demand on bioinformaticians to provide their biologists with user friendly and scalable software infrastructures to capture, exchange, and exploit the unprecedented amounts of new *omics data. We here present MOLGENIS, a generic, open source, software toolkit to quickly produce the bespoke MOLecular GENetics Information Systems needed. Methods: The MOLGENIS toolkit provides bioinformaticians with a simple language to model biological data structures and user interfaces. At the push of a button, MOLGENISÉ?? generator suite automatically translates these models into a feature-rich, ready-to-use web application including database, user interfaces, exchange formats, and scriptable interfaces. Each generator is a template of SQL, JAVA, R, or HTML code that would require much effort to write by hand. This É??model-drivenÉ?? method ensures reuse of best practices and improves quality because the modeling language and generators are shared between all MOLGENIS applications, so that errors are found quickly and improvements are shared easily by a re-generation. A plug-in mechanism ensures that both the generator suite and generated product can be customized just as much as hand-written software. Results: In recent years we have successfully evaluated the MOLGENIS toolkit for the rapid prototyping of many types of biomedical applications, including next-generation sequencing, GWAS, QTL, proteomics and biobanking. Writing 500 lines of model XML typically replaces 15,000 lines of hand-written programming code, which allows for quick adaptation if the information system is not yet to the biologistÉ??s satisfaction. Each application generated with MOLGENIS comes with an optimized database back-end, user interfaces for biologists to manage and exploit their data, programming interfaces for bioinformaticians to script analysis tools in R, Java, SOAP, REST/JSON and RDF, a tab-delimited file format to ease upload and exchange of data, and detailed technical documentation. Existing databases can be quickly enhanced with MOLGENIS generated interfaces using the É??ExtractModelÉ?? procedure. Conclusions: The MOLGENIS toolkit provides bioinformaticians with a simple model to quickly generate flexible web platforms for all possible genomic, molecular and phenotypic experiments with a richness of interfaces not provided by other tools. All the software and manuals are available free as LGPLv3 open source at http://www. molgenis.org.",The MOLGENIS toolkit: rapid prototyping of biosoftware at the push of a button
"['Sanjoy Dasgupta', 'Samory Kpotufe']","We present two related contributions of independent interest: (1) high-probability finite sample rates for k-NN density estimation, and (2) practical mode estimators - based on k-NN - which attain minimax-optimal rates under surprisingly general distributional conditions.",Optimal rates for k-NN density and mode estimation
"['Jing Ning', 'Klaus Hofmann']","This paper presents a integrated high voltage digital-to-analog converter array, which is designed by using a 0.35?Êm high voltage CMOS technology(AMS H35), and can be applied in high voltage applications up to 120V. The DAC array has 16 high voltage DACs controlled by a digital controller on chip. To fulfil the requirement of communication system with reconfigurable antenna implemented using materials which have voltage dependent capacitance, the DACs are designed to have 8 bits of resolution. In order to improve the accuracy and decrease the required area, each independent DAC is implemented by a low voltage DAC and a high voltage amplifier for boosting the controllable output voltage. Since the current consumption from the high voltage power supply is only 1.28mA, it is possible to be powered by a charge pump which generates high voltage power supply from a battery. The proposed HV DAC array can drive up to 16 individual channels of antenna array with different voltages from 0V to 120V. It will greatly reduce the complexity and cost of mobile applications required high voltage. The feasibility is proved by post-simulation result.",A 120V high voltage DAC array for a tunable antenna in communication system
"['Jason Bellorado', 'Aleksandar Kavcic', 'Marcus Marrow', 'Li Ping']","In this paper, we present a practical approach to the iterative decoding of Reed-Solomon (RS) codes. The presented methodology utilizes an architecture in which the output produced by steps of belief-propagation (BP) is successively applied to a legacy decoding algorithm. Due to the suboptimal performance of BP conducted on the inherently dense RS parity-check matrix, a method is first provided for the construction of reduced-density, binary, parity-check equations. Iterative decoding is then conducted utilizing a subset of a redundant set of parity-check equations to minimize the number of connections into the least-reliable bits. Simulation results show that performance comparable to (and exceeding) the best known practical RS decoding techniques is achievable with the presented methodology. The complexity of the proposed algorithm is significantly lower than these existing procedures and permits a practical implementation in hardware.",Low-Complexity Soft-Decoding Algorithms for ReedÉ??Solomon CodesÉ??Part II: Soft-Input Soft-Output Iterative Decoding
"['Abhijit Sarma', 'Sandip Chakraborty', 'Sukumar Nandi', 'Anchal Choubey']","IEEE 802.11 has become very popular wireless technology to offer high speed Internet access at public places called the `Hot-Spots'. This has enabled users to access multimedia and other real time applications using wireless local area networks (WLAN). In IEEE 802.11 WLAN technology, associations between a mobile station and an access point (AP) is controlled by the mobile station, allowing the station to select an AP with the strongest signal in terms of either `Received Signal Strength Identifier' or `Signal to Interference and Noise Ratio'. In real time scenarios, the traffic patterns of mobile users are dynamic in nature. This leads to a situation where the traffic loads on the APs are unevenly distributed in the WLAN. Such imbalance in traffic load causes severe degradation in performance of the applications running on the mobile stations associated with the overloaded APs. In this paper, we propose a scheme which dynamically improves the performance of the overloaded APs by handing off some of its associated stations to nearby APs. This handoff decision is taken by an AP in assistance with the mobile stations. The effectiveness of the load distribution through dynamic hand-over in a WLAN is analyzed through theoretical analysis. Simulation results show the overall improvements in terms of delay, throughput and number of stations that an AP can support. The performance improvement in the proposed scheme is also justified through the results obtained from a IEEE 802.11 WLAN testbed.",Context Aware Inter-BSS Handoff in IEEE 802.11 Networks: Efficient Resource Utilization and Performance Improvement
"['Ho-Taek Lee', 'Dong-bok Lee', 'Byungju Lee', 'Byung Cheol Song']","This paper proposes an accurate deinterlacing algorithm using a maximum a posteriori (MAP) esti- mator. First, we produce accurate motion vector fields between the current field and adjacent fields by employing an advanced motion compensation scheme that is suitable for an interlaced format. Next, the progressive frame corresponding to the current field is found via the MAP estimator based on the derived motion vector fields. Here, in order to obtain a stable solution, well-known bilateral total variation-based regularization is applied. Then, at a specific mode decision step, it is decided whether the result from the aforementioned temporal deinterlacing is acceptable or not. Finally, if the temporal deinterlacing is determined to be inappropriate by the mode decision, a typical spatial deinterlacing is applied instead of the MAP estimator-based temporal deinterlacing. Experimental results show that the proposed algorithm provides at maximum 2 dB higher PSNR than a cutting-edge dein- terlacing algorithm, while providing better visual quality than the latter. ?? The Authors. Published by SPIE under a Creative Commons Attribution 3.0 Unported License. Distribution or reproduction of this work in whole or in part requires full attribution of the original publication, including its DOI. (DOI: 10.1117/1.JEI.22.4.043038)",Spatiotemporal deinterlacing using a maximum a posteriori estimator based on multiple-field registration
"['Monique Laurent', 'Teresa Piovesan']","We investigate the completely positive semidefinite cone ${\mathcal{CS}_{+}^n}$, a new matrix cone consisting of all $n\times n$ matrices that admit a Gram representation by positive semidefinite matrices (of any size). In particular, we study relationships between this cone and the completely positive and the doubly nonnegative cone, and between its dual cone and trace positive noncommutative polynomials. We use this new cone to model quantum analogues of the classical independence and chromatic graph parameters $\alpha(G)$ and $\chi(G)$, which are roughly obtained by allowing variables to be positive semidefinite matrices instead of $0/1$ scalars in the programs defining the classical parameters. We can formulate these quantum parameters as conic linear programs over the cone ${\mathcal{CS}_{+}^n}$. Using this conic approach we can recover the bounds in terms of the theta number and define further approximations by exploiting the link to trace positive polynomials.",Conic approach to quantum graph parameters using linear optimization over the completely positive semidefinite cone
"['Uwe J??nen', 'J??rg H??hner', 'Christian M?¨ller-Schloer']","This paper describes an approach for object-tracking in large smart camera networks, which is based on software-agents with conflicting goals. The focus is on the system architecture. These software-agents are specialized on concrete functions, e.g. maximizing sensor coverage of a surveillance area by aligning the cameras' fields of view. To reach collaborative behavior, an approach inspired by Multi-Criteria Decision Making (MCDM) of the operations research is used.",PhD forum: Competing agents for distributed object-tracking in smart camera networks
['Ronald A. Iltis'],The problem of delay estimation in the presence of multipath is considered. It is shown that the extended Kalman filter (EKF) can be used to obtain joint estimates of time-of-arrival and multipath coefficients for deterministic signals when the channel can be modeled as a tapped-delay line. Simulation results are presented for the EKF joint estimator used for synchronization in a direct-sequence spread-spectrum system operating over a frequency-selective fading channel. A simplified model of the EKF joint estimator is considered for analysis purposes. The evolution in time of the tracking error probability density function and the nonlinear tracking error variance are examined through numerical solution of the Chapman-Kolmogorov equation. The nonlinear tracking error variance is compared to both the linear error variance estimate directly provided by the EKF and the Cramer-Rao lower bound. >,Joint estimation of PN code delay and multipath using the extended Kalman filter
"['Zhaobin Liu', 'Huihui Zhang', 'Wenyu Qu', 'Tianquan Li']","Storage QoS methodology has been extensively mentioned in large-scale storage systems. Since storage resources tend to be heterogeneous and dynamic changed, efficient storage states capturing becomes essential. In this paper we propose a novel capturing and preserving mechanism, called Change State Capture (CSC). CSC can capture the changed state when storage provider QoS modified. The changed event can trigger the Broker module to reorganize the storage resources to meet this change. The experimental results show that CSC can achieve better performance than existing periodic scanning algorithm in terms of efficiency.",Change State Capture in Service Aware Storage
"['Bill Keller', 'Tim Owen', 'Ian Wakeman', 'Julie Weeds', 'David J. Weir']","In this paper, we describe the middleware that has evolved from our attempt to capture user descriptions of policies controlling devices and services from natural language. Description logic (DL) provides a formal link between the natural language processing, the ontology and the middleware. We show that the use of a formalism such as DL opens useful avenues to detecting and resolving conflicts in policies, both in formulation and when resolving them against incoming events and requests. We finish by arguing that pervasive middleware needs to move closer to the users' abstractions to provide a service for what will be a highly dynamic environment.",Middleware for user controlled environments
"['Bac Le', 'Huy Nguyen', 'Bay Vo']","Methods for mining high utility itemsets from databases have been discussed widely in recent years. They mine itemsets having high utility from databases. Pruning candidates based on transaction-weighted utilisation value is a good method at all. In this paper, we develop a tree structure called WIT-tree, and use it in the proposed TWU-mining algorithm, an algorithm for improving the mining time and the search space. Using Diffset for fast computing transaction-weighted utilisation values and saving memory will be discussed. Experimental results show that proposed algorithms are more efficient.",An efficient strategy for mining high utility itemsets
"['Shailender Chaudhry', 'Alok N. Choudhary']","With the advances in server technology, and the advent of fast gigabit networks, it has become possible to support multimedia applications. To support the requirements for the transmission of isochronous data, the network must provide service guarantees to connections, including minimum bandwidth, packet delay, delay jitter, and loss. Three factors determine the utilization of the network when providing these services. These are the scheduling algorithm employed at each switch; the accuracy (tightness) of the admission control (schedulability condition) that detects violations to the service guarantees; accuracy of the input traffic characterization. In this paper we present a scheduling algorithm, its schedulability condition and implementation. The schedulability condition is free of input traffic characterization and thus any input traffic model can be used. Further, the algorithm is capable of achieving up to maximum efficiency possible at each switch.",Time dependent priority scheduling for guaranteed QoS systems
"['Ian P. Gent', 'Toby Walsh']",,Satisfiability in the Year 2000
"['Jeng Ji Huang', 'Wei Ting Wang', 'Yi Hsuan Chen', 'Huei Wen Ferng', 'David Shiung']","In a cellular code division multiple access (CDMA) system, uplink capacity is limited by interference. For a ground-based CDMA system or a high altitude platform station (HAPS) CDMA system, the interference is contributed by users in the same cell and in other cells. In this paper, a novel scheme is proposed to reduce both the same-cell and other-cell interferences in a cellular CDMA system. The proposed scheme attempts to lower the required transmitting power for each user by integrating terrestrial base stations (BSs) with a HAPS to provide two separate paths for signal reception. Numerical results show that the total interference is significantly reduced and the uplink capacity is improved under the proposed scheme, compared to either a HAPS CDMA system or a ground-based CDMA system.",Interference Reduction for Terrestrial Cellular CDMA Systems via High Altitude Platform Station
"['Aur??lie Bouzerda-Wahlen', 'Louis Nahum', 'Maria Chiara Liverani', 'Adrian G. Guggisberg', 'Armin Schnider']","Memory influences behavior in multiple ways. One important aspect is to remember in what precise context in the past a piece of information was acquired context source monitoring. Another important aspect is to sense whether an upcoming thought, composed of fragments of memories, refers to present reality and can be acted upon orbitofrontal reality filtering. Whether these memory control processes share common underlying mechanisms is unknown. Failures of both have been held accountable for false memories, including confabulation. Electrophysiological and imaging studies suggest a dissociation but used very different paradigms. In this study, we juxtaposed the requirements of context source monitoring and reality filtering within a unique continuous recognition task, which healthy participants performed while high-resolution evoked potentials were recorded. The mechanisms dissociated both behaviorally and electrophysiologically: Reality filtering induced a frontal positivity, absence of a specific electrocortical configuration, and posterior medial orbitofrontal activity at 200-300 msec. Context source monitoring had no electrophysiological expression in this early period. It was slower and less accurate than reality filtering and induced a prolonged positive potential over frontal leads starting at 400 msec. The study demonstrates a hitherto unrecognized separation between orbitofrontal reality filtering and source monitoring. Whereas deficient orbitofrontal reality filtering is associated with reality confusion in thinking, the behavioral correlates of deficient source monitoring should be verified with controlled experimental exploration.",An electrophysiological dissociation between orbitofrontal reality filtering and context source monitoring
"['Robert B. France', 'Bernhard Rumpe']",,"Model-based lifecycle management of software-intensive systems, applications, and services"
"['Randal C. Burns', 'Darrell D. E. Long', 'Robert M. Rees']","Distributed file systems are often used to replicate a Web site's content among its many servers. However, for content that needs to be dynamically updated and distributed to many servers, file system locking protocols exhibit high latency and heavy network usage. Poor performance arises because the Web-serving workload differs from the assumed workload. To address the shortcomings of file systems, we introduce the  publish consistency  model well suited to the Web-serving workload and implement it in the  producer-consumer locking protocol. A comparison of this protocol against other file system protocols by simulation shows that producer-consumer locking removes almost all latency due to protocol overhead and significantly reduces network load.",Consistency and locking for distributing updates to web servers using a file system
"['Behnoush Abdollahi', 'Olfa Nasraoui']","We present a cross-modal recommendation engine that leverages multiple domains of data while performing matrix factorization. We show how our approach has the potential to alleviate the cold-start problem for new items, one of the notorious limitations of Collaborative Filtering (CF) techniques.",A cross-modal warm-up solution for the cold-start problem in collaborative filtering recommender systems
"['E. van Heck', 'E. van Damme', 'Jack P. C. Kleijnen', 'P.M.A. Ribbers']","This article discusses the successful entrance of Tele Flower Auction (TFA) into the Dutch flower industry, enabled by information technology (IT). Indeed, the development and introduction of TFA is one of the initiatives in response to import restrictions by the traditional Dutch flower auctions. TFA is an electronic alternative that enables buyers to trade at a distance; this alternative is currently exploited by an import organization called East African Flowers (EAF). This article aims to provide a better understanding of the success of TFA. It provides a descriptive framework for analyzing the merits of electronic auctions. It uses that framework to evaluate the TFA case. The results of the analysis and the framework itself illustrate the various complex issues that arise in the design and implementation of electronic markets.",New entrants and the role of information technology case-study: the Tele Flower Auction in the Netherlands
"['Twan van Laarhoven', 'Elena Marchiori']","We investigate axioms that intuitively ought to be satised by graph clustering objective functions. Two tailored for graph clustering objectives are introduced, and the four axioms introduced in previous work on distance based clustering are reformulated and generalized for the graph setting. We show that modularity, a standard objective for graph clustering, does not satisfy all these axioms. This leads us to consider adaptive scale modularity, a variant of modularity, that does satisfy the axioms. Adaptive scale modularity has two parameters, which give greater control over the clustering. Standard graph clustering objectives, such as normalized cut and unnormalized cut, are obtained as special cases of adaptive scale modularity. We furthermore show that adaptive scale modularity does not have a resolution limit. In general, the results of our investigation indicate that the considered axioms cover existing É??goodÉ?? objective functions for graph clustering, and can be used to derive an interesting new family of objectives.",An axiomatic study of objective functions for graph clustering
"['Lutz Leistritz', 'Miroslaw Galicki', 'Herbert Witte', 'Eberhard F. Kochs']","The problem of learning multiple continuous trajectories by means of recurrent neural networks with (in general) time-varying weights is addressed. The learning process is transformed into an optimal control framework where both the weights and the initial network state to be found are treated as controls. For such a task, a learning algorithm is proposed which is based on a variational formulation of Pontryagin's maximum principle. The convergence of this algorithm, under reasonable assumptions, is also investigated. Numerical examples of learning nontrivial two-class problems are presented which demonstrate the efficiency of the approach proposed.",Initial state training procedure improves dynamic recurrent networks with time-dependent weights
"['Navid Shahriari', 'Edsko E. G. Hekman', 'Matthijs Oudkerk', 'Sarthak Misra']","Purpose#R##N#Percutaneous needle insertion procedures are commonly used for diagnostic and therapeutic purposes. Although current technology allows accurate localization of lesions, they cannot yet be precisely targeted. Lung cancer is the most common cause of cancer-related death, and early detection reduces the mortality rate. Therefore, suspicious lesions are tested for diagnosis by performing needle biopsy.",Design and evaluation of a computed tomography (CT)-compatible needle insertion device using an electromagnetic tracking system and CT images
"['Yang Hu', 'David Astely', 'Robert Baldemair', 'Sorour Falahati']","In Long Term Evolution (LTE) specification, physical uplink control channel (PUCCH) consists of two basic transmission formats, i.e. formats 1a/1b and format 2, carrying some control signals fed back to eNodeB, e.g. ACK/NACK related to downlink data transmission blocks and downlink channel quality indicator (CQI). At eNodeB, conventional receiver detection algorithm is to do coherent data detection by utilizing reference signals (RS) based channel estimation, also called non-blind algorithm, whose detection performance is greatly limited by RS density and data coding rate. In this paper, a semi-blind algorithm is introduced to improve channel estimation accuracy by also using data energy besides RS energy at a cost of a higher computational complexity. The analyzes is done for both formats 1a/1b and format 2. Simulation results show that semi-blind algorithms efficiently improve the performance of format 2 because format 2 has lower RS density and higher coding rate than formats 1a/1b.",Semi-Blind Multi-User Detection for LTE PUCCH
"['Mudasser Iqbal', 'Iqbal Gondal', 'Laurence S. Dooley']","This paper proposes a cross-layer protocol for energy-aware routing in wireless sensor networks. The protocol combines the energy depreciation rate, node distance and neighbourhood information from physical layer together with TDMA schedules from the MAC layer and also network life requirements from the application layer, to effectively determine the most efficient routes to the base station. This cross layer efficiency measure is then used to form dynamic clusters that adapt to changing traffic and energy conditions so assisting to both control the transmission power and schedule the sleep-wake cycles of nodes for better energy utilisation. The proposed protocol is a recursive aggregation scheme that transforms a network-wide routing dissemination problem into a single-hop query protocol that aids nodes in making multi-hop routing decisions based solely upon the information provided by their single-hop neighbours. Results confirm that the proposed technique balances the load on forwarding nodes, adapts the MAC layer precisely to the routing layer and minimizes data delivery time for increased traffic and large scale networks.",A Cross-Layer Data Dissemination Protocol for Energy Efficient Sink Discovery in Wireless Sensor Networks
"['Chun-Hao Chen', 'Tzung-Pei Hong', 'Vincent S. Tseng']","Since items may have their own characteristics, different minimum support values and membership functions may be specified for different items. In this paper, an enhanced approach is proposed, which processes the items in a divide-and-conquer strategy. The approach is designed for finding minimum support values, membership functions, and fuzzy association rules. Possible solutions are evaluated by their requirement satisfaction divided by their suitability of derived membership functions. The proposed GA framework maintains multiple populations, each for one itempsilas minimum support value and membership functions. The final best minimum support values and membership functions in all the populations are then gathered together to be used for mining fuzzy association rules. Experimental results also show the effectiveness of the proposed approach.",A divide-and-conquer genetic-fuzzy mining approach for items with multiple minimum supports
"['Jayasree Chakraborty', 'Sudipta Mukhopadhyay', 'Veenu Singla', 'Niranjan Khandelwal', 'Rangaraj M. Rangayyan']","Mammographic masses are important signs of breast cancer. However, due to its irregular and obscured margin, variability in size, and occlusion within dense breast tissue, a mass is often difficult to detect. In this paper, a multilevel thresholding approach controlled by gradient and intensity is proposed, where an image is considered as a 3D topographic map with intensity as the third dimension. A multilevel high-to-low intensity thresholding approach is used for the detection of the focal region of a mass. After each step of thresholding, a region growing technique is applied on each focal region to detect potential sites of masses using gradient and intensity information. The performance of the proposed method is tested on 107 scanned-film images consisting of 52 images with masses and 55 normal images from the mini-MIAS database, and 158 digital radiography (DR) images containing 78 images with masses and 80 normal images from a local database. A sensitivity of 95% with 5.2 false positives (FPs)/image is obtained with the mini-MIAS images and a sensitivity of 98.8% with 2.1 FPs/image is obtained with the DR images.",Detection of masses in mammograms using region growing controlled by multilevel thresholding
"['F. Amoroso', 'Robert A. Monzingo']","The regrowth of digital data power spectra due to amplifier nonlinearity is analyzed. The frequency-domain expression for the amplifier output shows how spectral regrowth will depend on the cubic coefficient of the Taylor's series of the amplifier nonlinearity as well as the combination of input phase modulation and amplitude ripple. A new, canonical ""para-spectrum"" is introduced that depends only on the signal and figures critically in the sidelobe regrowth calculation.",Analysis of data spectral regrowth from nonlinear amplification
"['Radim Belohlavek', 'Bernard De Baets', 'Jan Outrata', 'Vilem Vychodil']","We present a fast bottom-up algorithm to compute all fixpoints of a fuzzy closure operator in a finite set over a finite chain of truth degrees, along with the partial order on the set of all fixpoints. Fuzzy closure operators appear in several areas of fuzzy logic and its applications, including formal concept analysis (FCA) that we use as a reference area of application in this paper. Several problems in FCA, such as computing all formal concepts from data with graded attributes or computing non-redundant bases of all attribute dependencies, can be reduced to the problem of computing fixpoints of particular fuzzy closure operators associated with the input data. The development of a general algorithm that is applicable, in particular, to these problems is the ultimate purpose of this paper. We present the algorithm, its theoretical foundations, and experimental evaluation.",Computing the Lattice of All Fixpoints of a Fuzzy Closure Operator
"['Lars-??ke Fredlund', 'Jan Friso Groote', 'Henri Korver']","Abstract   In 1982 Dolev, et al. [10] presented an  O ( n log n ) unidirectional distributed algorithm for the  circular extrema-finding (or leader-election) problem . At the same time Peterson came up with a nearly identical solution. In this paper, we bring the correctness of this algorithm to a completely formal level. This relatively small protocol, which can be described on half a page, requires a rather involved proof for guaranteeing that it behaves well in all possible circumstances. To our knowledge, this is one of the more advanced case-studies in formal verification based on process algebra.",Formal verification of a leader election protocol in process algebra
"['Alf Inge Wang', 'Erik Arisholm', 'Letizia Jaccheri']","This paper reports experiences from an experiment in a software architecture course where the focus was both on giving students valuable education as well as getting important empirical results. The paper describes how the experiment was integrated in the course, and presents an evaluation of the experiment from an educational point of view. Further, the paper reflects on the costs and the benefits of carrying out an experiment in the context of a software architecture course for the involved stakeholders namely the researchers, the students, and the instructors. We also describe some guidelines for planning and executing experiments as a part of a software engineering course.",Educational Approach to an Experiment in a Software Architecture Course
"['Colin Doutre', 'Panos Nasiopoulos', 'Konstantinos N. Plataniotis']","Consumer digital cameras usually use a single light sensor together with a color filter array (CFA) for capturing color images. This results in a mosaic image being captured, where only one color sample (red, green or blue) is obtained at each pixel location. Demosaicking is the process of interpolating the two missing color samples at each location to generate a full color image. Virtually all current demosaicking methods produce an RGB output image. If the image is to be compressed afterwards, it will typically be converted from RGB to YCbCr 4:2:0 formats. In this paper we propose a new demosaicking method that directly produces an YCbCr 4:2:0 images, so the output can be directly compressed. Simulation results show that the proposed method provides higher image quality than fast RGB based demosaicking methods and has lower computational complexity 1 .",A Fast Demosaicking Method Directly Producing YCbCr 4:2:0 Output
"['Jinwook Seo', 'Ben Shneiderman']","Exploratory analysis of multidimensional data sets is challenging because of the difficulty in comprehending more than three dimensions. Two fundamental statistical principles for the exploratory analysis are (1) to examine each dimension first and then find relationships among dimensions, and (2) to try graphical displays first and then find numerical summaries (D.S. Moore, (1999). We implement these principles in a novel conceptual framework called the rank-by-feature framework. In the framework, users can choose a ranking criterion interesting to them and sort 1D or 2D axis-parallel projections according to the criterion. We introduce the rank-by-feature prism that is a color-coded lower-triangular matrix that guides users to desired features. Statistical graphs (histogram, boxplot, and scatterplot) and information visualization techniques (overview, coordination, and dynamic query) are combined to help users effectively traverse 1D and 2D axis-parallel projections, and finally to help them interactively find interesting features",A Rank-by-Feature Framework for Unsupervised Multidimensional Data Exploration Using Low Dimensional Projections
"['Wei-Chen Chen', 'Ranjan Maitra']","We propose a model-based approach for clustering time series regression data in an unsupervised machine learning framework to identify groups under the assumption that each mixture component follows a Gaussian autoregressive regression model of order p. Given the number of groups, the traditional maximum likelihood approach of estimating the parameters using the expectation-maximization (EM) algorithm can be employed, although it is computationally demanding. The somewhat fast tune to the EM folk song provided by the Alternating Expectation Conditional Maximization (AECM) algorithm can alleviate the problem to some extent. In this article, we develop an alternative partial expectation conditional maximization algorithm (APECM) that uses an additional data augmentation storage step to efficiently implement AECM for finite mixture models. Results on our simulation experiments show improved performance in both fewer numbers of iterations and computation time. The methodology is applied to the problem of clustering mutual funds data on the basis of their average annual per cent returns and in the presence of economic indicators.",ModelÉ?êbased clustering of regression time series data via APECMÉ??an AECM algorithm sung to an even faster beat
"['Enzo Baccarelli', 'Mauro Biagi']","In this letter, we introduce a simple co-decoding scheme for ultra wideband (UWB) impulse radio systems impaired by multiuser interference (MUI). The proposed scheme exhibits an adaptive capability with respect to MUI degrading effects and, depending on the MUI level, it may switch from a hard-detection operating mode to a soft-detection one. So doing, the presented scheme is able to gain resistance against near-far effects. Several numerical tests support the conclusion that the presented scheme outperforms conventional ones currently planned for noise-limited UWB systems up to orders of magnitude in MUI-limited application scenarios.",A simple adaptive coding scheme for multiuser interference suppression in ultra-wideband radio transmissions
"['Noah C. Benson', 'Valerie Daggett']","As high-throughput molecular dynamics simulations of proteins become more common and the databases housing the results become larger and more prevalent, more sophisticated methods to quickly and accurately mine large numbers of trajectories for relevant information will have to be developed. One such method, which is only recently gaining popularity in molecular biology, is the continuous wavelet transform, which is especially well-suited for time course data such as molecular dynamics simulations. We describe techniques for the calculation and analysis of wavelet transforms of molecular dynamics trajectories in detail and present examples of how these techniques can be useful in data mining. We demonstrate that wavelets are sensitive to structural rearrangements in proteins and that they can be used to quickly detect physically relevant events. Finally, as an example of the use of this approach, we show how wavelet data mining has led to a novel hypothesis related to the mechanism of the protein ???? resolvase.",Wavelet Analysis of Protein Motion.
"['Yuhang Wang', 'Fillia Makedon']","Numerous recent studies have shown that microarray gene expression data is useful for cancer classification. Classification based on microarray data is very different from previous classification problems in that the number of features (genes) greatly exceeds the number of instances (tissue samples). It has been shown that selecting a small set of informative genes can lead to improved classification accuracy. It is thus important to first apply feature selection methods prior to classification. In the machine learning field, one of the most successful feature filtering algorithms is the Relief-F algorithm. In this work, we empirically evaluate its performance on three published cancer classification data sets. We use the linear SVM and the k-NN as classifiers in the experiments, and compare the performance of Relief-F with other feature filtering methods, including Information Gain, Gain Ratio, and /spl chi//sup 2/-statistic. Using the leave-one-out cross validation, experimental results show that the performance of Relief-F is comparable with other methods.",Application of Relief-F feature filtering algorithm to selecting informative genes for cancer classification using microarray data
"['Haijun Niu', 'Lifeng Li', 'Fengling Ma', 'Fang Pu', 'Deyu Li', 'Yubo Fan']","It is significant to monitor bladder volume continuously and accurately in many specific clinical conditions or during treatment or management of urological disorders. This study try to explore the dynamic bladder storing process and understand the change of bladder shape and intra-vesical volume under the specific conditions of water-drinking using reconstructed three-dimension (3D) bladder models based on magnetic resonance imaging (MRI). A 3T GE MRI system was used to acquire bladder images while storing in the transverse section. Segmentation of the bladder MRI Image, 3D model reconstruction, and volume measurement in different scan phases were performed with Mimics software. The results showed that the bladder varied from small bowl-like shape to irregular ellipsoid during urinary store. The increase in bladder volume was nonlinear under the specific conditions of water-drinking. The diameter change of the bladder in different directions was inconsistent during urinary store.",The Study of Nonlinear Change of Bladder During Urinary Store Based on MRI
"['Konstantinos Drossos', 'Stylianos Ioannis Mimilakis', 'Andreas Floros', 'Nikolaos Kanellopoulos']","Modern mobile, hand-held devices offer enhanced capabilities for video and sound reproduction. Nevertheless, major restrictions imposed by their limited size render them inconvenient for headset-free stereo sound reproduction, since the corresponding short-distant loudspeakers placement physically narrows the perceived stereo sound localization potential. In this work, we aim at evaluating a spatial enhancement technique for small-size mobile devices. This technique extracts the original panning information from an original stereo recording and spatially extends it using appropriate binaural rendering. A sequence of subjective tests performed shows that the derived spatial perceptual impression is significantly improved in all test cases considered, rendering the proposed technique an attractive approach towards headset-free mobile audio reproduction.",Stereo Goes Mobile: Spatial Enhancement for Short-distance Loudspeaker Setups
"['Simon Forbes', 'David Beare', 'Prasad Gunasekaran', 'Kenric Leung', 'Nidhi Bindal', 'Harry Boutselakis', 'Minjie Ding', 'Sally Bamford', 'Charlotte G. Cole', 'Sari Ward', 'Chai Yin Kok', 'Mingming Jia', 'Tisham De', 'Jon Teague', 'Michael R. Stratton', 'Ultan McDermott', 'Peter J. Campbell']","COSMIC, the Catalogue Of Somatic Mutations In Cancer (http://cancer.sanger.ac.uk) is the world's largest and most comprehensive resource for exploring the impact of somatic mutations in human cancer. Our latest release (v70; Aug 2014) describes 2 002 811 coding point mutations in over one million tumor samples and across most human genes. To emphasize depth of knowledge on known cancer genes, mutation information is curated manually from the scientific literature, allowing very precise definitions of disease types and patient details. Combination of almost 20 000 published studies gives substantial resolution of how mutations and phenotypes relate in human cancer, providing insights into the stratification of mutations and biomarkers across cancer patient populations. Conversely, our curation of cancer genomes (over 12 000) emphasizes knowledge breadth, driving discovery of unrecognized cancer-driving hotspots and molecular targets. Our high-resolution curation approach is globally unique, giving substantial insight into molecular biomarkers in human oncology. In addition, COSMIC also details more than six million noncoding mutations, 10 534 gene fusions, 61 299 genome rearrangements, 695 504 abnormal copy number segments and 60 119 787 abnormal expression variants. All these types of somatic mutation are annotated to both the human genome and each affected coding gene, then correlated across disease and mutation types.",COSMIC: exploring the world's knowledge of somatic mutations in human cancer
"['Sokratis Makrogiannis', 'Ragini Verma', 'Bilge Kara??ali', 'Christos Davatzikos']","Existing computational anatomy methodologies for morphometric analysis of medical images are often based solely on the shape transformation, typically being a diffeomorphism, that warps these images to a common template or vice versa. However, anatomical differences as well as changes induced by pathology, prevent the warping transformation from producing an exact correspondence. The residual image captures information that is not reflected by the diffeomorphism, and therefore allows us to maintain the entire morphological profile for analysis. In this paper we present a morphological descriptor which combines the warping transformation with the residual image in an equivalence class formulation, to characterize morphology of anatomical structures. Equivalence classes are formed by pairs of transformation and residual, for different levels of smoothness of the warping transformation. These pairs belong to the same equivalence class, since they jointly reconstruct the exact same morphology. Moreover, pattern classification methods are trained on the entire equivalence class, instead of a single pair, in order to become more robust to a variety of factors that affect the warping transformation, including the anatomy being measured. This joint descriptor is evaluated by statistical testing and estimation of class separation by classification, initially for 2-D synthetic images with simulated atrophy and subsequently for a volumetric dataset consisting of schizophrenia patients and healthy controls. Results of class separation indicate that this joint descriptor produces generally better and more robust class separation than using each of the components separately.",A Joint Transformation and Residual Image Descriptor for Morphometric Image Analysis using an Equivalence Class Formulation
"['Kshitij Limaye', 'Box Leangsuksun', 'Venkata K. Munganuru', 'Z. D. Greenwood', 'Stephen L. Scott', 'Richard Libby', 'Kasidit Chanchio']","Physicists today have employed grid technology to overcome various resource level hurdles. The collective resource utilization achieved through grid computing is critical to the overall computing capacity of the community and should be guaranteed. In an environment where job sites are cluster systems, a service node failure renders a whole system outage. Our grid-aware HA-OSCAR effort was motivated by the popularity of the cluster architecture in the grid environment. We propose the high-availability architecture, HA-OSCAR, for cluster-based job sites in the grid environment. This architecture deals with fault tolerance at the service level complementing task-based solutions such as checkpoint/restart. We discuss various service availability issues related to the grid, some issues and preliminary results obtained while implementing the smart failover feature and the automated grid installation package. Our report entails the performance benefits achieved after applying the HA-OSCAR solution to the cluster components of the grid compared to regular Beowulf style cluster solutions.",Grid aware HA-OSCAR
"['John Y. Fong', 'Randy Acklin', 'John Roscher', 'Feng Li', 'Cindy Laird', 'Cezary Pietrzyk']","Nonvolatile repair caches require less area than traditional row, column, or block redundancy schemes, to repair random defective memory cells in deep submicron embedded SRAMs and new nonvolatile memories such as FeRAM, MRAM, and OUM. Small memories with few defects can be repaired efficiently in real time by the direct mapped nonvolatile repair cache whereas large memories with many defects can be repaired more effectively and in real time by the N way set associative repair cache. An 8 way set associative repair cache was implemented in the Texas Instruments-Agilent Technologies 64 Mbit FeRAM chip.",Nonvolatile repair caches repair embedded SRAM and new nonvolatile memories
"['Bowei Song', 'Lin Gui', 'Yunfeng Guan', 'Wenjun Zhang']","In TDS-OFDM (time-domain synchronous orthogonal frequency division multiplexing) systems, pseudonoise (PN) sequences rather than cyclic prefixes are inserted as guard interval, between consecutive inverse discrete Fourier transformed (IDFT) symbol blocks. Since the PN sequences can also be used as training symbols, such system can provide higher spectrum efficiency. However, due to non-cyclic property of the signal, the simple channel estimation and equalization techniques for conventional cyclic prefixed OFDM (CP-OFDM) can not be applied to TDS-OFDM. In this paper, we propose a channel estimation and equalization method for TDS-OFDM. Channel estimation depends on time domain correlation and iterative interference cancellation techniques, while equalization is based on tail cancellation and cyclic restoration algorithm (TCCR). It is shown that our proposed method can provide satisfactory performance in TDS-OFDM based terrestrial high-definition television (HDTV) broadcasting system.",On channel estimation and equalization in TDS-OFDM based terrestrial HDTV broadcasting system
"['Damien Castaignet', 'Ian Couchman', 'Niels Kj??lstad Poulsen', 'Thomas Buhl', 'Jens Jakob Wedel-Heinen']","This paper presents the load reduction achieved with trailing edge flaps during a full-scale test on a Vestas V27 wind turbine. The trailing edge flap controller is a frequency-weighted linear model predictive control (MPC) where the quadratic cost consists of costs on the zero-phase filtered flapwise blade root moment and trailing edge flap deflection. Frequency-weighted MPC is chosen for its ability to handle constraints on the trailing edge flaps deflection, and to target at loads with given frequencies only. The controller is first tested in servo-aeroelastic simulations, before being implemented on a Vestas V27 wind turbine. Consistent load reduction is achieved during the full-scale test. An average of 13.8% flapwise blade root fatigue load reduction is measured.",Frequency-Weighted Model Predictive Control of Trailing Edge Flaps on a Wind Turbine Blade
"['Vratislav Harabis', 'Radim Kolar', 'Radovan Jirik']","This paper describes a method for registration of images in ultrasound sequences when contrast agent is administrated in blood stream. The proposed method is based on mutual information, image enhancement and automatic selection of multi-reference images. The registration process and the properties of the normalized mutual information are investigated and tested on real ultrasound contrast sequences from myocardial perfusion application.",Registration of ultrasound image sequences for perfusion analysis
"['Greta L. Polites', 'Elena Karahanna']","Despite recent interest in studying information system habits, our understanding of how these habits develop and operate in an organizational context is still limited. Within organizations, IS habits may develop over long periods of time and are typically embedded within larger, frequently practiced, higher-level work routines or task sequences. When new systems are introduced for the purpose of at least partially replacing incumbent systems, existing IS habits embedded in these routines may inhibit adoption and use of the new systems. Therefore, understanding how work-related IS habits form, how they enable and inhibit behavior, and how they can be disrupted or encouraged requires that we examine them within the context of organizational and individual level work routines. The current study integrates psychology and organizational behavior literature on cognitive scripts and work routines to examine IS habits as they occur embedded within larger, more complex task sequences. The objective of the paper is to contribute to the IS habit literature by (1) situating IS habits within the context of their associated work routines and task sequences, and (2) providing a theoretical understanding of how incumbent system habits can be disrupted, and how development of new system habits can be encouraged, within this context. We draw from extant research in psychology, organizational behavior, and consumer behavior to offer propositions about context-focused organizational interventions to break, or otherwise discourage, the continued performance of incumbent system habits and to encourage the development of new system habits. Specifically, our theoretical development includes script disruption techniques, training-in-context, and performance goal suspension as organizational interventions that disrupt incumbent system habits. We further theorize how stabilizing contextual variables associated with modified work routines can facilitate the development of new system habits. The paper concludes by discussing the importance of combining intervention strategies to successfully disrupt incumbent system habits and encourage development of new system habits and thus facilitate adoption of new systems.",The embeddedness of information systems habits in organizational and individual level routines: development and disruption
"['Andrea Giuseppe Bottino', 'Aldo Laurentini', 'Luisa Rosano']","Locating sensors in 2D can be often modelled as an art gallery problem. Tasks such as surveillance require observing or ""covering"" the interior of a polygon with a minimum number of sensors or ""guards"". Observing the boundaries of a polygonal environment is sufficient for tasks such as inspection and image based rendering. As interior covering, also edge covering (EC) is NP-hard, and no finite algorithm is known for its exact solution. A number of heuristics have been proposed for the approximate solution of this important problem, but their performances with respect to optimality is unknown. Therefore, a polygon specific tight lower bound for the number of sensors is very useful for assessing the performances of these algorithms. In this paper, we propose a new lower bound for the EC problem. It can be computed in reasonable time for environments with up to a few hundreds of edges. To evaluate its closeness to optimality, we compare it with a previously developed lower bound and with the solution provided by a recent incremental EC algorithm. Tests over hundreds of polygons with different number of edges show that the new lower bound is tight and outperforms the previous one.",A tight lower bound for art gallery sensor location algorithms
"['Himal A. Suraweera', 'Raymond Hall Yip Louie', 'Yonghui Li', 'George K. Karagiannidis', 'Branka Vucetic']","The performance of a two hop amplify-and-forward relay system, where the source-relay and the relay-destination channels experience Rayleigh and Rician fading respectively, is investigated. We derive exact and lower bound expressions for the outage probability and average bit error probability, where the bounds become tight at high signal-to-noise ratios (SNR). Our results are verified through comparison with Monte Carlo simulations, where we also illustrate the positive impact of the Rician factor on the system performance.",Two hop amplify-and-forward transmission in mixed rayleigh and rician fading channels
"['James Kariuki', 'Emily M. Ervin', 'Carly Olafson']","The development of portable sensors that can be used outside the lab is an active area of research in the electroanalytical field. A major focus of such research is the development of low-cost electrodes for use in these sensors. Current electrodes, such as glassy-carbon electrodes (GCEs), are costly and require time-consuming preparation. Alternatives have been proposed, including mechanical pencil-lead electrodes (MPEs). However, MPEs themselves possess numerous drawbacks, particularly structural fragility. In this paper, we present a novel pencil-graphite electrode (PGE) fabricated from a regular HB#2 pencil. This PGE is a simple, disposable, extremely low-cost alternative to GCEs ($0.30 per PGE, vs. $190 + per GCE), and possesses the structural stability that MPEs lack. PGEs were characterized by square-wave voltammetry of ferricyanide, gallic acid, uric acid, dopamine, and several foodstuffs. In all cases, PGEs demonstrated sensitivities comparable or superior to those of the GCE and MPE (LOD = 5.62 ?? 10É??4 M PGE, 4.80 ?? 10É??4 M GCE, 2.93 ?? 10É??4 M MPE). Signal areas and peak heights were typically four to ten times larger for the PGE relative to the GCE.","Development of a Novel, Low-Cost, Disposable Wooden Pencil Graphite Electrode for Use in the Determination of Antioxidants and Other Biological Compounds"
"['Consolatina Liguori', 'Alfredo Paolillo', 'Antonio Pietrosanto']","This paper presents an automatic system for the measurement of carotid intima-media thickness (IMT) that is based on the digital processing of ultrasound images. The measurement technique is described in detail, highlighting the advantages compared to other methods and, reporting some experimental results. Finally, an analytical approach is used to estimate the intrinsic accuracy of the system.",An automatic measurement system for the evaluation of carotid intima-media thickness
"['David Fuin', 'Eric Garcia', 'Herv?? Guyennet', 'Jean-Christophe Lapayre']","The Network and Distributed Systems Group within the University of Franche-Comte's computer research lab (LIFC) gained solid expertise on medical e-diagnosis in the area of remote collaboration through continued research and findings. TeNeCi (Cooperative Teleneurology) is a European remote diagnosis project applied to neurology developed under the aegis of INTERREGIII. INTERREGIII is a European Community Initiative program aiming at supporting cross-border, transnational and interregional co-operation in both social and economic perspectives. This paper has a dual objective: it first presents the improvements and contributions made to advance the TeNeCi project which is a research and development tool, and then it synthesises our research work in collaborative medical e-diagnosis. The TeNeCi tool originality is to allow practitioners to act as if they were at the same diagnosis table, using a great panel of medical tools (images, software,...). Collaboration and awareness features are used to make TeNeCi more efficient than classical telemedicine software in terms of collaboration level.",Collaborative interactions for medical e-diagnosis
"['Hanjiang Lai', 'Yan Pan', 'Cong Liu', 'Liang Lin', 'Jie Wu']","Learning-to-rank for information retrieval has gained increasing interest in recent years. Inspired by the success of sparse models, we consider the problem of sparse learning-to-rank, where the learned ranking models are constrained to be with only a few nonzero coefficients. We begin by formulating the sparse learning-to-rank problem as a convex optimization problem with a sparse-inducing $(\ell_1)$ constraint. Since the $(\ell_1)$ constraint is nondifferentiable, the critical issue arising here is how to efficiently solve the optimization problem. To address this issue, we propose a learning algorithm from the primal dual perspective. Furthermore, we prove that, after at most $(O({1\over \epsilon } ))$ iterations, the proposed algorithm can guarantee the obtainment of an $(\epsilon)$-accurate solution. This convergence rate is better than that of the popular subgradient descent algorithm. i.e., $(O({1\over \epsilon^2} ))$. Empirical evaluation on several public benchmark data sets demonstrates the effectiveness of the proposed algorithm: 1) Compared to the methods that learn dense models, learning a ranking model with sparsity constraints significantly improves the ranking accuracies. 2) Compared to other methods for sparse learning-to-rank, the proposed algorithm tends to obtain sparser models and has superior performance gain on both ranking accuracies and training time. 3) Compared to several state-of-the-art algorithms, the ranking accuracies of the proposed algorithm are very competitive and stable.",Sparse Learning-to-Rank via an Efficient Primal-Dual Algorithm
"['W. Sabrina Lin', 'H. Vicky Zhao', 'K.J.R. Liu']","Multimedia social network analysis is a research area with growing importance, in which the social network members share multimedia contents with all different purposes and analyzing their behavior help design more secured and efficient multimedia and networking systems. In this paper, we focus on multimedia fingerprinting social network, in which multi-user collusion being a powerful attack, where a group of attackers collectively undermine the traitor tracing capability. During collusion, different colluders have different objective thus, the colluders form a social network and an how to achieve agreement on distributing the risk/profit among colluders and ensure fairness of the attack is a crucial question. This paper models the dynamics among colluders as a non-cooperative game, propose a general model of utility functions and study four different bargaining solutions of this game.",Fairness dynamics in multimedia colludersÉ?? social networks
"['Daniel Glozman', 'Moshe Shoham']","This paper presents a robotic system for steering under real-time fluoroscopic guidance a flexible needle in soft tissue. Given a target and possible obstacle locations, the computer calculates the flexible needle-tip trajectory that avoids the obstacle and hits the target. Using an inverse kinematics algorithm, the needle base maneuvers required for a tip to follow this trajectory are calculated, enabling a robot to perform controlled needle insertion. Assuming small displacements, the flexible needle is modeled as a linear beam supported by virtual springs, where the stiffness coefficients of the springs can vary along the needle. Using this simplified model, the forward and inverse kinematics of the needle are solved analytically, enabling both path planning and path correction in real time. The needle shape is detected in real time from fluoroscopic images, and the controller commands the needle base motion that minimizes the needle tip error. This approach was verified experimentally using a robot to maneuver the base of a flexible needle inserted into a muscle tissue. Along the 40-mm trajectory that avoids the obstacle and hits the target, the error stayed below the 0.5-mm level. This study demonstrates the ability to perform closed-loop control and steering of a flexible needle by maneuvering the needle base so that its tip achieves a planned trajectory.",Image-Guided Robotic Flexible Needle Steering
"['Sungjin Lee', 'Gary Geunbae Lee']","The development of Dialog-Based Computer-Assisted Language Learning (DB-CALL) systems requires research on the simulation of language learners. This paper presents a new method for generation of grammar errors, an important part of the language learner simulator. Realistic errors are generated via Markov Logic, which provides an effective way to merge a statistical approach with expert knowledge about the grammar error characteristics of language learners. Results suggest that the distribution of simulated grammar errors generated by the proposed model is similar to that of real learners. Human judges also gave consistently close judgments on the quality of the real and simulated grammar errors.",Realistic Grammar Error Simulation using Markov Logic
"['Axel J. Soto', 'Marc Strickert', 'Gustavo E. Vazquez']","QSPR methods represent a useful approach in the drug discovery process, since they allow predicting in advance biological or physicochemical properties of a candidate drug. For this goal, it is necessary that the QSPR method be as accurate as possible to provide reliable predictions. Moreover, the selection of the molecular descriptors is an important task to create QSPR prediction models of low complexity which, at the same time, provide accurate predictions.#R##N##R##N#In this work, a matrix-based method [1] is used to transform the original data space of chemical compounds into an alternative space where compounds with different target properties can be better separated. For using this approach, QSPR is considered as a classification problem. The advantage of using adaptive matrix metrics is twofold: it can be used to identify important molecular descriptors and at the same time it allows improving the classification accuracy.#R##N##R##N#A recently proposed method making use of this concept [2] is extended to multi-class data. The new method is related to linear discriminant analysis and shows better results at yet higher computational costs. An application for relating chemical descriptors to hydrophobicity property [3] shows promising results.",Adaptive matrix metrics for molecular descriptor assessment in QSPR classification
"['Wonkyu Han', 'Mike Mabey', 'Gail Joon Ahn']","Large and complex systems, such as the Smart Grid, are often best understood through the use of modeling and simulation. In particular, the task of assessing a complex system's risks and testing its tolerance and recovery under various attacks has received considerable attention. However, such tedious tasks still demand a systematic approach to model and evaluate each component in complex systems. In other words, supporting a formal validation and verification without needing to implement the entire system or accessing the existing physical infrastructure is critical since many elements of the Smart Grid are still in the process of becoming standardized for widespread use. In this paper, we describe our simulation-based approach to understanding and examining the behavior of various components of the Smart Grid in the context of verification and validation. To achieve this goal, we adopt the discrete event system specification (DEVS) modeling methodology, which allows generalization and specialization of the entities in the model for a customized simulation with specific scenarios. In addition, we articulate metrics for supporting our simulation-based verification and validation and demonstrate the feasibility and effectiveness of our approach with a real-world use case.",Simulation-based validation for smart grid environments
"['Muhammet Nuri Seyman', 'Necmi Taspinar']","Channel estimation is an essential task in MIMO-OFDM systems for coherent demodulation and data detection. Also designing pilot tones that affect the channel estimation performance is an important issue for these systems. For this reason, in this article we propose particle swarm optimization (PSO) to optimize placement and power of the comb-type pilot tones that are used for least square (LS) channel estimation in MIMO-OFDM systems. To optimize the pilot tones, upper bound of MSE is used as the objective function of PSO. The effects of Doppler shifts on designing pilot tones are also investigated. According to the simulation results, PSO is an effective solution for designing pilot tones.",Particle swarm optimization for pilot tones design in MIMO-OFDM systems
"['Long Han', 'Mark J. Embrechts', 'Yunqing Chen', 'Xi-Cheng Zhang']","This paper introduces kernel partial least squares (K-PLS) for the identification of mixture content from terahertz spectra. Kernel partial least squares is a nonlinear extension of the partial least squares (PLS) method, commonly used in chemometrics. K-PLS and PLS are considered superior to peak matching methods for mixture spectra of multiple compounds because it avoids having to address the problem of overlapping peaks explicitly. Terahertz (THz) radiation is capable of transmitting easily through most dielectric materials and is used as a new tool to collect the original spectral readings from transmission, diffusion and reflection. A multi-output kernel partial least squares method is presented to model mixture composition based on pure substance training patterns, under the assumption of linear spectral mixture behavior. Preprocessing consists of a wavelet transform of the THz spectra and an independent component analysis (ICA) transform. Preliminary results show that the ICA+K-PLS approach is able to classify pure spectra accurately and allows for an accurate estimate of the composition from THz mixed spectra even where there are severe overlapped peaks in these spectra.",Kernel Partial Least Squares for the Identification of Mixture Content from TeraHertz Spectra
"['Paul Andr??', 'Robert E. Kraut', 'Aniket Kittur']","Distributed online groups have great potential for generating interdependent and complex products like encyclopedia articles or product design. However, coordinating multiple group members to work together effectively while minimizing process losses remains an open challenge. We conducted an experiment comparing the effectiveness of two coordination strategies (simultaneous vs. sequential work) on a complex creative task as the number of group members increased. Our results indicate that, contrary to prior work, a sequential work structure was more effective than a simultaneous work structure as the size of the group increased. A mediation analysis suggests that social processes such as territoriality partially accounts for these results. A follow up experiment giving workers specific roles mitigated the detrimental effects of the simultaneous work structure. These results have implications for small group theory and crowdsourcing research.",Effects of simultaneous and sequential work structures on distributed collaborative interdependent tasks
"['Weiming Hu', 'D. Z. Xie', 'Tieniu Tan']","The understanding and description of object behaviors is a hot topic in computer vision. Trajectory analysis is one of the basic problems in behavior understanding, and the learning of trajectory patterns that can be used to detect anomalies and predict object trajectories is an interesting and important problem in trajectory analysis. In this paper, we present a hierarchical self-organizing neural network model and its application to the learning of trajectory distribution patterns for event recognition. The distribution patterns of trajectories are learnt using a hierarchical self-organizing neural network. Using the learned patterns, we consider anomaly detection as well as object behavior prediction. Compared with the existing neural network structures that are used to learn patterns of trajectories, our network structure has smaller scale and faster learning speed, and is thus more effective. Experimental results using two different sets of data demonstrate the accuracy and speed of our hierarchical self-organizing neural network in learning the distribution patterns of object trajectories.",A hierarchical self-organizing approach for learning the patterns of motion trajectories
"['E.R. van Dam', 'D. Fon-der-Flaass']","We consider functions on binary vector spaces which are far from linear functions in different senses. We compare three existing notions: almost perfect nonlinear functions, almost bent (AB) functions, and crooked (CR) functions. Such functions are of importance in cryptography because of their resistance to linear and differential attacks on certain cryptosystems. We give a new combinatorial characterization of AB functions in terms of the number of solutions to a certain system of equations, and a characterization of CF in terms of the Fourier transform. We also show how these functions can be used to construct several combinatorial structures; such as semi-biplanes, difference sets, distance regular graphs, symmetric association schemes, and uniformly packed (BCH and Preparata) codes.","Codes, graphs, and schemes from nonlinear functions"
"['Siti Zura A. Jalil', 'Mohd Nasir Taib', 'Hasnain Abdullah Idris', 'Megawati Mohd Yunus']","This paper describes an analysis of body radiation frequency for the purpose of gender classification. The human radiation frequency is experimentally studied from 33 healthy human subjects of 17 males and 16 females. KNN classifier is employed for gender classification. The number of training to testing ratio was evaluated at 50 to 50, 60 to 40 and 70 to 30, to determine best classification accuracy. The data was analyzed separately of raw dataset and post-processing dataset to compare the classification results. At first, the data was classified using raw dataset and yields the classification accuracy of 93.8%. Then, the post-processing data was applied to the classifier, and it was found that the classification accuracy was improved to perfect classification on k = 5, 7, 11 and 13 to 15.",Gender Classification Based on Human Radiation Wave Analysis
"['Kenneth P. Birman', 'Thomas A. Joseph']","The design and correctness of a communication facility for a distributed computer system are reported on. The facility provides support for  fault-tolerant process groups  in the form of a family of reliable multicast protocols that can be used in both local- and wide-area networks. These protocols attain high levels of concurrency, while respecting application-specific delivery ordering constraints, and have varying cost and performance that depend on the degree of ordering desired. In particular, a protocol that enforces causal delivery orderings is introduced and shown to be a valuable alternative to conventional asynchronous communication protocols. The facility also ensures that the processes belonging to a fault-tolerant process group will observe consistent orderings of events affecting the group as a whole, including process failures, recoveries, migration, and dynamic changes to group properties like member rankings. A review of several uses for the protocols in the ISIS system, which supports fault-tolerant resilient objects and bulletin boards, illustrates the significant simplification of higher level algorithms made possible by our approach.",Reliable communication in the presence of failures
"['Jonathan W. Hui', 'David E. Culler']","Extending IP to low-power, wireless personal area networks (LoWPANs) was once considered impractical because these networks are highly constrained and must operate unattended for multiyear lifetimes on modest batteries. Many vendors embraced proprietary protocols, assuming that IP was too resource-intensive to be scaled down to operate on the microcontrollers and low-power wireless links used in LoWPAN settings. However, 6LoWPAN radically alters the calculation by introducing an adaptation layer that enables efficient IPv6 communication over IEEE 802.15.4 LoWPAN links.","Extending IP to Low-Power, Wireless Personal Area Networks"
"['Michal Pec', 'Michal Bujacz', 'Pawel Strumillo']","Filtering of sounds through head related transfer functions (HRTFs) is a common method for obtaining audio spatialization. HRTFs depend highly on an individual's anatomy, especially head dimensions and outer ear shape. The paper describes a system designed for efficient measurement of personalized HRTFs and verification of the collected data on a group of volunteers. The main goal of utilizing personalized HRTFs was to obtain a high level of externalization, i.e. the illusion that a sound source is located outside one's head, as well as a high resolution of sound localization. Measurement of the HRTFs, using the constructed equipment in an anechoic chamber, was performed for 15 volunteers (9 of them blind, as our current research concerns electronic travel aids for visually impaired). A series of trials were conducted, which verified the personalized HRTFs in externalization and localization quality. Average precision of localization of moving virtual sound sources reached 6.7?¯ in azimuth and 10.6?¯ in elevation. At the same time influence of other factors, such as the source type or movement, on sound localization was also tested.",Personalized head related transfer function measurement and verification through sound localization resolution
"['Mariano G. Beir??', 'Sebastian P. Grynberg', 'J. Ignacio Alvarez-Hamelin']","The Internet is composed of routing devices connected between them and organized into independent administrative entities: the Autonomous Systems. The existence of different types of Autonomous Systems (like large connectivity providers, Internet Service Providers or universities) together with geographical and economical constraints, turns the Internet into a complex modular and hierarchical network. This organization is reflected in many properties of the Internet topology, like its high degree of clustering and its robustness.",Router-level community structure of the Internet Autonomous Systems
"['Alaa E. Abdel-Hakim', 'Aly A. Farag']","SIFT has been proven to be the most robust local invariant feature descriptor. SIFT is designed mainly for gray images. However, color provides valuable information in object description and matching tasks. Many objects can be misclassified if their color contents are ignored. This paper addresses this problem and proposes a novel colored local invariant feature descriptor. Instead of using the gray space to represent the input image, the proposed approach builds the SIFT descriptors in a color invariant space. The built Colored SIFT (CSIFT) is more robust than the conventional SIFT with respect to color and photometrical variations. The evaluation results support the potential of the proposed approach.",CSIFT: A SIFT Descriptor with Color Invariant Characteristics
"['Gyan Bhanot', 'Alan Gara', 'Philip Heidelberger', 'Eoin M. Lawless', 'James C. Sexton', 'R. E. Walkup']","A general method for optimizing problem layout on the Blue Gene??/L (BG/L) supercomputer is described. The method takes as input the communication matrix of an arbitrary problem as an array with entries C(i, j), which represents the data communicated from domain i to domain j. Given C(i, j), we implement a heuristic map that attempts to sequentially map a domain and its communication neighbors either to the same BG/L node or to near-neighbor nodes on the BG/L torus, while keeping the number of domains mapped to a BG/L node constant. We then generate a Markov chain of maps using Monte Carlo simulation with free energy F =?úi,j C(i, j)H(i, j), where H(i, j) is the smallest number of hops on the BG/L torus between domain i and domain j. For two large parallel applications, SAGE and UMT2000, the method was tested against the default Message Passing Interface rank order layout on up to 2,048 BG/L nodes. It produced maps that improved communication efficiency by up to 45%.",Optimizing task layout on the Blue Gene/L supercomputer
"['Javier Carb??', 'Jos?? M. Molina']","Trust and Reputation management play an important role in agent-based Recommender Systems. Although several protocols and ontologies of agents using trust and reputation has been proposed, none of them has been so extensively used and implicitly accepted by research community as those from Agent Reputation and Trust (ART in advane) testbed. The motivation of this adaptation is to facilitate the use of ART principles in real distributed applications instead of a centralized testbed for experimentation. This paper presents an adaptation of the protocols proposed by ART testbed to a codification for the most popular Agent platform: JADE. This implementation follows a coherent API with the FIPA protocols included in JADE distribution for an easy use. We also complement the behaviours of corresponding initiators and responders of the protocols with an ontology formed by a collection of concepts, predicates and agent actions that may represent as the ART application domain as any other service-oriented domain. The proposal has been designed to be applied in domains where multi-agent e-commerce solutions are needed. Future work includes the integration of this ontology and protocols in context-aware scenarios such as an airport.",A JADE-Based ART-Inspired Ontology and Protocols for Handling Trust and Reputation
"['Lionel Delphin-Poulat', 'Chafic Mokbel', 'J??r??me Idier']","An acoustic mismatch between a given utterance and a model degrades the performance of the speech recognition process. We choose to model speech by hidden Markov models (HMMs) in the cepstrum domain and the mismatch by a parametric function. In order to reduce the mismatch, one has to estimate the parameters of this function. We present a frame synchronous estimation of these parameters. We show that the parameters can be computed recursively. Thanks to such methods, parameters variations can be tracked. We give general equations and study the particular case of an affine transform. Finally, we report recognition experiments carried out over both PSTN and cellular telephone network to show the efficiency of the method in a real context.",Frame-synchronous stochastic matching based on the Kullback-Leibler information
"['Toru Nakashika', 'Tetsuya Takiguchi', 'Yasuo Ariki']","This paper presents a voice conversion (VC) method that utilizes conditional restricted Boltzmann machines (CRBMs) for each speaker to obtain high-order speaker-independent spaces where voice features are converted more easily than those in an original acoustic feature space. The CRBM is expected to automatically discover common features lurking in time-series data. When we train two CRBMs for a source and target speaker independently using only speaker-dependent training data, it can be considered that each CRBM tries to construct subspaces where there are fewer phonemes and relatively more speaker individuality than the original acoustic space because the training data include various phonemes while keeping the speaker individuality unchanged. Each obtained high-order feature is then concatenated using a neural network (NN) from the source to the target. The entire network (the two CRBMs and the NN) can be also fine-tuned as a recurrent neural network (RNN) using the acoustic parallel data since both the CRBMs and the concatenating NN have network-based representation with time dependencies. Through voice-conversion experiments, we confirmed the high performance of our method especially in terms of objective evaluation, comparing it with conventional GMM, NN, RNN, and our previous work, speaker-dependent DBN approaches.",Voice conversion using speaker-dependent conditional restricted Boltzmann machine
"['Randal E. Bryant', 'Steven M. German', 'Miroslav N. Velev']","The logic of Equality with Uninterpreted Functions (EUF) provides a means of abstracting the manipulation of data by a processor when verifying the correctness of its control logic. By reducing formulas in this logic to propositional formulas, we can apply Boolean methods such as ordered Binary Decision Diagrams (BDDs) and Boolean satisfiability checkers to perform the verification. We can exploit characteristics of the formulas describing the verification conditions to greatly simplfy the propostional formulas generated. We identify a class of terms we call É??p-termsÉ?ù for which equality comparisons can only be used in monotonically positive formulas. By applying suitable abstractions to the hardware model, we can express the functionality of data values and instruction    addresses flowing through an instruction pipeline with p-terms. A decision procedure can exploit the restricted uses of p-terms by considering only É??maximally diverseÉ?ù interpretations of the associated function symbols, where every function application yields a different value execept when constrainted by functional consistency. We present two methods to translate formulas in EUF into propositional logic. The first interprets the formula over a domain of fixed-length bit vectors and uses vectors of propositional variables to encode domain variables. The second generates formulas encoding the conditions under which pairs of terms have equal valuations, introducing propostional variables to encode the equality relations between pairs of terms. Both of these approaches can exploit  maximal diversity to greatly reduce the number of propositional variables that need to be introduced and to reduce the overall formula sizes. We present experimental results demonstrating the efficiency of this approach when verifying pipelined processors using the method proposed by Burch and Dill. Exploiting positive equality allows us to overcome the experimental blow-up experienced previously when verifying microprocessors with load, store, and branch instructions.",Processor verification using efficient reductions of the logic of uninterpreted functions to propositional logic
['Johnnie R. Charnetski'],"Conventional Bayesian decision-making models require decision makers to stipulate numerically, either directly or indirectly, a specific set of conditional densities posterior distributions on the state/ signal set. The method presented permits decision makers to use a Bayesian approach when only ordinal information, in the form of partial orders possibly incomplete, exists concerning the likelihood of individual and/or joint state/signal occurrence. From the partial orders provided, the mean values of the reward functions may be statistically approximated and the choice of a best action selected. We present a variation of a well-known Bayesian decision problem.",Technical Note-Bayesian Decision Making with Ordinal Information
"['Hyun Mun Kim', 'Jerry M. Mendel']","Fuzzy basis functions (FBF's) which have the capability of combining both numerical data and linguistic information, are compared with other basis functions. Because a FBF network is different from other networks in that it is the only one that can combine numerical and linguistic information, comparisons are made when only numerical data is available. In particular, a FBF network is compared with a radial basis function (RBF) network from the viewpoint of function approximation. Their architectural interrelationships are discussed. Additionally, a RBF network, which is implemented using a regularization technique, is compared with a FBF network from the viewpoint of overcoming ill-posed problems. A FBF network is also compared with Specht's probabilistic neural network and his general regression neural network (GRNN) from an architectural point of view. A FBF network is also compared with a Gaussian sum approximation in which Gaussian functions play a central role. Finally, we summarize the architectural relationships between all the networks discussed in this paper. >",Fuzzy basis functions: comparisons with other basis functions
"['Yin Zhu', 'Yu Zheng', 'Liuhang Zhang', 'Darshan Santani', 'Xing Xie', 'Qiang Yang']","In this paper, we infer the statuses of a taxi, consisting of occupied, non-occupied and parked, in terms of its GPS trajectory. The status information can enable urban computing for improving a cityÉ??s transportation systems and land use planning. In our solution, we first identify and extract a set of effective features incorporating the knowledge of a single trajectory, historical trajectories and geographic data like road network. Second, a parking status detection algorithm is devised to find parking places (from a given trajectory), dividing a trajectory into segments (i.e., sub-trajectories). Third, we propose a two-phase inference model to learn the status (occupied or non-occupied) of each point from a taxi segment. This model first uses the identified features to train a local probabilistic classifier and then carries out a Hidden SemiMarkov Model (HSMM) for globally considering long term travel patterns. We evaluated our method with a large-scale real-world trajectory dataset generated by 600 taxis, showing the advantages of our method over baselines.",Inferring Taxi Status Using GPS Trajectories
"['Ioannis Sarris', 'Andrew R Nix']","The high demand for unlicensed spectrum mandates the use of higher frequencies, such as the 60 GHz band, where large amounts unlicensed bandwidth exists. In this paper, the results from Power Azimuth Spectrum (PAS) measurements in a home and an office environment are presented with a view of revealing the amount of scattering power at that frequency and its relative power to the Line-of-Sight (LoS) signal. Moreover, a tractable scheme is proposed for the estimation of the Power Azimuth Spectrum from absolute power measurement data which produces a very close fit with the measured PAS. A discussion is made on the accuracy of this method which is dependent on the directionality of the antenna elements, the fading in the channel and the presence of noise.",Power Azimuth Spectrum Measurements in Home and Office Environments at 62.4 GHz
['Luiz Marcio Cysneiros'],"There are many different approaches to elicit requirements, each having strengths and weaknesses. Hence, some approaches may be more suitable to one domain than another. Moreover, some domains may require these approaches to be carefully applied or even adapted to work efficiently. Health care domain is one of these domains. It is a complex domain with many subtleties, such as political and legal issues that have to be taken into account. This work brings some of the lessons learned in more than six years working with several hospitals and laboratories. Particularly, this paper presents some elicitation techniques that had to be adapted in order to comply with the constraints imposed by several peculiarities intrinsic to this domain. It also points out some special considerations that must be taken into account regardless the method one chooses to elicit requirements.",Requirements engineering in the health care domain
"['Jo?úo Paulo Costeira', 'Takeo Kanade']","The structure-from-motion problem has been extensively studied in the field of computer vision. Yet, the bulk of the existing work assumes that the scene contains only a single moving object. The more realistic case where an unknown number of objects move in the scene has received little attention, especially for its theoretical treatment. In this paper we present a new method for separating and recovering the motion and shape of multiple independently moving objects in a sequence of images. The method does not require prior knowledge of the number of objects, nor is dependent on any grouping of features into an object at the image level. For this purpose, we introduce a mathematical construct of object shapes, called the shape interaction matrix, which is invariant to both the object motions and the selection of coordinate systems. This invariant structure is computable solely from the observed trajectories of image features without grouping them into individual objects. Once the matrix is computed, it allows for segmenting features into objects by the process of transforming it into a canonical form, as well as recovering the shape and motion of each object. The theory works under a broad set of projection models (scaled orthography, paraperspective and affine) but they must be linear, so it excludes projective É??camerasÉ?ù.",A Multibody Factorization Method for Independently Moving Objects
"['Enzo Baccarelli', 'Antonio Fasano']","In this article, quickly computable upper and lower bounds are presented on the symmetric capacity of flat-faded Rice and Nakagami channels with side information (SI) for data-transmissions via finite-size quadrature amplitude modulation (QAM) constellations. The proposed bounds exhibit the appealing feature to be tight and asymptotically exact both for high and low signal-to-noise ratios (SNRs). Furthermore, exponentially tight Chernoff-like formulas are also presented for an analytical evaluation of the resulting system outage probabilities when interleaved packet transmissions are carried out.",Some simple bounds on the symmetric capacity and outage probability for QAM wireless channels with Rice and Nakagami fadings
"['Guohua Liu', 'Hui-Dong Ma', 'Xu Li', 'Peng Liang']","It is an important problem to extract features from Chi- nese documents for protecting intellectual property. The ex- isting approaches are major oriented to words frequency or semantic, they can't extract features efficiently. By mapping Chinese documents into an ordered set of integers, we find that a Chinese document can be corresponded to a unique ordered set of integers and the set is an isomorphism of the document. So, we propose an algorithm which can hash the set to three kinds of hash value sequences: paragraph se- quence, sentence sequence and chunk sequence, which can represent the features of the document completely. In or- der to reduce the numbers of the features defined as digital fingerprints in this paper, we present an optimal strategy to select some hash values from the sequences. The experiment results show that the algorithms proposed are efficient.",Extracting Digital Fingerprints from Chinese Documents
"['Cristiana Bolchini', 'Antonio Miele']","This paper proposes a design methodology that enhances the classical system-level design flow for embedded systems to introduce reliability-awareness. The mapping and scheduling step is extended to support the application of hardening techniques to fulfill the required fault management properties that the final system must exhibit; moreover, the methodology allows the designer to specify that only some parts of the systems need to be hardened against faults. The reference architecture is a complex distributed one, constituted by resources with different characteristics in terms of performance and available fault detection/tolerance mechanisms. The approach is evaluated and compared against the most recent and relevant work, with an in-depth analysis on a large set of benchmarks.",Reliability-Driven System-Level Synthesis for Mixed-Critical Embedded Systems
"['Chih-Hao Liu', 'P. P. Vaidyanathan']","This paper considers the DFE transceiver optimization for time-varying memoryless MIMO channels under zero-forcing (ZF) constraint. For time-varying channels, the uncoded average BER of the conventional geometric mean decomposition (GMD) based systems is not minimized because of the diverse arithmetic MSEs at different block times. To minimize the BER, a new GMD transceiver is proposed, in which the data vectors are grouped into space-time blocks (ST-blocks). A channel independent-temporal precoder is superimposed on the conventional blockwise GMD for the equalization of arithmetic MSEs across different blocks. So, the proposed system can take advantage of both the temporal and spatial diversity offered by time-varying MIMO channels. At moderate high SNR, corresponding to reasonable BER, there exists a class of optimal channel-independent temporal precoders, e.g., DFT and Hadamard matrices, for the minimization of average BER. Furthermore, simulation results show that the average BER performance of the proposed system improves with ST-block size and converges at moderate ST-block size. 1",ZF-DFE transceiver for time-varying MIMO channels with channel-independent temporal precoder
"['Ioanis Nikolaidis', 'Ian F. Akyildiz']","A serious fairness problem occurs whenever homogeneous, variable bit-rate, video traffic streams are multiplexed at a finite buffer statistical multiplexer. Namely, the cell loss ratio performance varies widely across individual traffic streams. The cause of the problem is the particular random relation of the periodic frame transmission epochs of the video sources that feed the multiplexer. In this paper a protocol is presented which controls the frame transmission epochs, in order to achieve fair distribution of losses among the admitted connections. The protocol operates with the participation of the sources by imposing an additional per-source delay on the traffic entering the network. These additional delays are calculated whenever a connection is accepted or terminated. The overall additional delay imposed by the protocol is guaranteed to be minimal through the use of an appropriate optimization algorithm. The performance of the proposed protocol, operating in conjunction with the optimization algorithm, is simulated and evaluated. A basic framework is given for its implementation on an ATM network. Finally, the issues of buffer overhead and the impact of the protocol on the delay experienced at the multiplexer are also discussed.",A cell loss equalization protocol for video multiplexers
"['Jianhua Qu', 'Xiyu Liu']","Clustering of spatial data in the presence of obstacles has a wide application. It is an important research topic in the spatial data mining. This paper discusses the problem of spatial clustering with obstacles constraints and presents a revised method named ant clustering algorithm with obstacle constraints(ACAOC) based on the basic ant model. This algorithm avoids some defects of other spatial clustering algorithms. These defects make algorithm not iterate when it has arrived at the stagnating state of the iteration or local optimum solution. ACAOC algorithm proposed in this paper can not only give attention to local converging and the whole converging, but also consider the obstacles that exit in the real world and make the clustering result more practical. Because of the use of Approximate Nearest Neighbor (ANN), the computing speed is increased greatly. The last experimental results conducted on synthetic data sets demonstrate that this method could extract the correct number of clusters with good clustering quality and high whole converging speed compared to the results obtained from clustering algorithm ignoring considering obstacles constraints.",A Revised Ant Clustering Algorithm with Obstacle Constraints
"['A. Reches', 'I. Laufer', 'Keren Ziv', 'G. Cukierman', 'Kevin McEvoy', 'M. Ettinger', 'Robert T. Knight', 'Adam Gazzaley', 'A. B. Geva']","article i nfo Attentional selection in the context of goal-directed behavior involves top-down modulation to enhance the contrast between relevant and irrelevant stimuli via enhancement and suppression of sensory cortical activity. Acetylcholine (ACh) is believed to be involved mechanistically in such attention processes. The objective of the current study was to examine the effects of donepezil, a cholinesterase inhibitor that increases synaptic levels of ACh, on the relationship between performance and network dynamics during a visual working memory (WM) task involving relevant and irrelevant stimuli. Electroencephalogram (EEG) activity was recorded in 14 healthy young adults while they performed a selective face/scene working memory task. Each participant re- ceived either placebo or donepezil (5 mg, orally) on two different visits in a double-blinded study. To investigate the effects of donepezil on brain network dynamics we utilized a novel EEG-based Brain Network Activation (BNA) analysis method that isolates location-time-frequency interrelations among event-related potential (ERP) peaks and extracts condition-specific networks. The activation level of the network modulated by donepezil, reflected in terms of the degree of its dynamical organization, was positively correlated with WM performance. Further analyses revealed that the frontal-posterior theta-alpha sub-network comprised the critical regions whose activation level correlated with beneficial effects on cognitive performance. These results indicate that condition-specific EEG network analysis could potentially serve to predict beneficial effects of therapeutic treatment in working memory.",Network dynamics predict improvement in working memory performance following donepezil administration in healthy young adults
['Burkhard C. Schipper'],"It is known that there are uncoupled learning heuristics leading to Nash equilibrium in all finite games. Why should players use such learning heuristics and where could they come from? We show that there is no uncoupled learning heuristic leading to Nash equilibrium in all finite games that a player has an incentive to adopt, that would be ""evolutionary stable"" or that ""could learn itself"". Rather, a player has an incentive to strategically teach such a learning opponent in order secure at least the Stackelberg leader payoff. The impossibility result remains intact when restricted to the classes of generic games, two-player games, potential games, games with strategic complements or 2x2 games, in which learning is known to be ""nice"". More generally, it also applies to uncoupled learning heuristics leading to correlated equilibria, rationalizable outcomes, iterated admissible outcomes, or minimal curb sets. A possibility result restricted to ""strategically trivial"" games fails if some generic games outside this class are considered as well.",Strategic Teaching and Learning in Games
"['Rong Li', 'Pooi Yuen Kam']","We propose the generalized quadratic receivers (GQRs) for unitary space--time modulation over flat Rayleigh fading channels. The GQRs realize the performance improvement potential, known to be approximately 2--4 dB, between the quadratic receiver (QR) and the coherent receiver (CR), by performing channel estimation without the help of additional training signals that consume additional bandwidth. They are designed for various unitary space--time constellations (USTC) in which signal matrices may or may not contain explicit inherent training blocks, and may be orthogonal or nonorthogonal to one another. As the channel memory span exploited for channel estimation increases, the error probability of the GQRs reduces from that of the QR to that of the CR. The GQRs work well for both slow and fast fading channels, and the performance improvement increases as the channel fade rate decreases. For a class of USTC with the orthogonal design structure, the GQR is simplified to a form whose complexity can be less than the complexity of the QR or even that of the simplified form of the QR.",Generalized Quadratic Receivers for Unitary Space--Time Modulation Over Rayleigh Fading Channels
"['Alexandre Rok', 'Bartosz Walczak']","An outerstring graph is an intersection graph of curves lying in a halfplane with one endpoint on the boundary of the halfplane. It is proved that the outerstring graphs are ??-bounded, that is, their chromatic number is bounded by a function of their clique number. This generalizes a series of previous results on ??-boundedness of outerstring graphs with various restrictions of the shape of the curves or the number of times the pairs of curves can intersect. This also implies that the intersection graphs of x-monotone curves with bounded clique number have chromatic number O(log n), improving the previous polylogarithmic upper bound. The assumption that each curve has an endpoint on the boundary of the halfplane is justified by the known fact that triangle-free intersection graphs of straight-line segments can have arbitrarily large chromatic number.",Outerstring graphs are ??-bounded
"['Michihiro Mikamo', 'Marcos Slomp', 'Shungo Yanase', 'Bisser Raytchev', 'Toru Tamaki', 'Kazufumi Kaneda']","Non-photo realistic rendering (NPR) is an appealing subject in computer graphics with a wide array of applications. As opposed to photo realistic rendering, NPR focuses on highlighting features and artistic traits instead of physical accuracy. Photo mosaic generation is one of the most popular NPR techniques, where a single image is assembled from several smaller ones. Visual responses change depending on the proximity to the photo mosaic, leading to many creative prospects for publicity. Synthesizing photo mosaics typically requires very large image databases in order to produce pleasing results. Moreover, repetitions are allowed to occur which may locally bias the mosaic. This paper provides alternatives to prevent repetitions while still being robust enough to work with coarse image subsets. Three approaches were devised for the matching stage of photo mosaics: a greedy-based procedural algorithm, simulated annealing and Soft Assign. We found that the latter two approaches deliver adequate arrangements in cases where only a restricted number of images is available.",Maximizing Image Utilization in Photomosaics
"['Tobias Graf', 'Toshio Koike', 'Thomas Pfaff', 'Ken-ichiro Muramoto', 'Kazumasa Aonashi']","Snowfall is an important geophysical parameter and the observation of the spatial and temporal distribution of snowfall can provide valuable information for a wide range of applications including climate change studies and atmospheric modelling. This paper investigates the feasibility to estimates the amount of solid precipitation and the cloud liquid water content over the ocean using AMSR-E passive microwave brightness temperature observations. The parameters are retrieved by minimizing the difference between the observed and modeled brightness temperature. The radiative transfer in the atmosphere is solved using the discrete ordinate method (4 streams) and the Henyey-Greenstein phase function. The scattering effect of the snow particles is calculated using Mie theory and the liquid-equivalent size of the ice particle. Except for the snowfall and the cloud liquid water content, most parameters, which influence the observation are derived from other data sources. The Newton-Raphson method is used to solve the iteration process using observed brightness temperatures at 89 GHz vertical polarization and 36.5 GHz horizontal polarization. The algorithm was applied using data from the Wakasa Bay Experiment 2003 in Japan and the results are compared to snowfall observation derived using a Z-R relationship and data from the Mikuni Doppler radar. Good agreement was achieved for different atmospheric conditions",Estimation of snowfall over the sea of Japan using AMSR-E passive microwave remote sensing observation
"['Stina Svensson', 'Ida-Maria Sintorn']","A modification of the hierarchical chamfer matching algorithm (HCMA) with the effect that no binarisation of the edge information is performed is investigated HCMA is a template matching algorithm used in many applications A distance transform (DT) from binarised edges in the search image is used to guide the template to good positions Local minima of a function using the distance values hit by the template correspond to potential matches We propose to use distance weighted propagation of gradient magnitude information as a cost image instead of a DT from the edges By this we keep as much information as possible until later in the matching process and, hence, do not risk to discard good matches in the edge detection and binarisation process.",Hierarchical chamfer matching based on propagation of gradient strengths
"['Ahmed Sharaf Eldin', 'Alaa H. Elnahry', 'Adel Elsayed', 'Rania Elsayed Ibrahim']","The current study seeks to introduce a new pedagogical design for geo-informatics courses using an e-training support system. Laurillard's conversational approach based on conceptual representation for both instructor and learner was used to form the framework. As the current study specifically interested in training as a special form for learning, so, we sought methods and strategies to integrate requirements of both company and employee into the design of training programs. Therefore, a competency perspective was adopted into the conversational framework to use learning design that leads to learning activities tightly related to the needs of the company and employee. The above framework has been developed with special consideration to the underpinning pedagogical principles and the needs of lifelong learning that continues after the training has been completed. The implementation of the developed framework needs a special computerized system, so an e-training support system (ETSS) was developed to realize the framework. ETSS is an open source and standard-based infrastructure to enable and foster competence development and exchange of learning activities and learning units. Although the domain of the current study focuses on geo-informatics, ETSS is applicable to any other domain. The developed framework through its ETSS implementation were evaluated in a typical training environment. The results indicated that the best method in training was the training with the developed system with 91.5 % in comparison with the traditional training method with 81.4 %.",A new pedagogical design for geo-informatics courses using an e-training support system
"['Milos Gligoric', 'Tihomir Gvero', 'Vilas Jagannath', 'Sarfraz Khurshid', 'Viktor Kuncak', 'Darko Marinov']","We present an approach for describing tests using non-deterministic  test generation programs . To write such programs, we introduce UDITA, a Java-based language with non-deterministic choice operators and an interface for generating linked structures. We also describe new algorithms that generate concrete tests by efficiently exploring the space of all executions of non-deterministic UDITA programs.   We implemented our approach and incorporated it into the official, publicly available repository of Java PathFinder (JPF), a popular tool for verifying Java programs. We evaluate our technique by generating tests for data structures, refactoring engines, and JPF itself. Our experiments show that test generation using UDITA is faster and leads to test descriptions that are easier to write than in previous frameworks. Moreover, the novel execution mechanism of UDITA is essential for making test generation feasible. Using UDITA, we have discovered a number of bugs in Eclipse, NetBeans, Sun javac, and JPF.",Test generation through programming in UDITA
"['Sunho Lim', 'Guohong Cao', 'Chita R. Das']","Summary We propose a unified framework consisting of a differential bandwidth reservation (DBR) algorithm and a Quality of Service (QoS)-aware admission control scheme to provide QoS guarantees to on-going connections in cellular networks. The differential bandwidth reservation policy uses a sector of cells in making the bandwidth reservation for accepting a new call. Based on the distance of the target cells in the sectors, two different bandwidth reservation policies are applied to optimize the connection dropping rate (CDR), while maintaining a competitive connection blocking rate (CBR). In addition, two possible mobile terminal (MT) movements are analyzed using the DBR mechanism. In the first case, no knowledge of an MTÉ??s moving path is assumed to be known, while in the second case, prior knowledge of an user profile is used in bandwidth reservation, and it is called user profile-based DBR (UPDBR) algorithm. Using the DBR scheme, we propose an admission control algorithm that uses varying number of cells in a sector to meet admission decisions. Extensive simulation is performed to evaluate our methodology. Comparison of the proposed scheme with two prior schemes shows that our approach is not only capable of providing better QoS guarantees, but is also flexible in terms of using varying number of cells in satisfying the high-level QoS requirements. Copyright # 2004 John Wiley & Sons, Ltd.",A unified bandwidth reservation and admission control mechanism for QoS provisioning in cellular networks
"['Pei-Chi Tu', 'Jen-Chuen Hsieh', 'Cheng-Ta Li', 'Ya-Mai Bai', 'Tung-Ping Su']","The cingulo-opercular network (CON) is a newly defined control network responsible for various cognitive processes that have been consistently found to be impaired in schizophrenia. The aim of this study was to use functional connectivity magnetic resonance imaging (fcMRI) to test the hypothesis that schizophrenia is associated with functional disconnection within the CON. Thirty subjects with schizophrenia and thirty healthy controls were enrolled in the study. Each subject received resting fMRI scanning, clinical evaluations and cognitive examinations. The CON of each subject was derived by calculating the functional connectivity map of a seed in the dorsal anterior cingulate (dACC). A between-group comparison was performed using a random effect analysis. Further network analyses with multiple regions of interest (ROIs) were performed to characterize the pattern of functional disconnection within the entire CON. Using the dACC seed in healthy controls, we derived the CON, which includes the following anatomical structures: the dACC; the bilateral anterior prefrontal, inferior parietal and anterior insular cortices; the putamen; the thalamus; and the cerebellum. Compared with healthy controls, schizophrenic patients showed significantly reduced functional connectivity in the bilateral putamens. Further network analysis demonstrated widespread cortico-striatal disconnection within the CON of schizophrenic patients. The disconnections correlated with negative symptom severity. Behavioral regression revealed that cortico-striatal functional connectivity predicted 2-back working memory performance in healthy controls, but not in schizophrenic patients. Our findings suggest that schizophrenia is associated with corticalÉ??striatal disconnection within the CON. The result provides a network basis for the cortico-striatal disconnection hypothesis of schizophrenia.",Cortico-striatal disconnection within the cingulo-opercular network in schizophrenia revealed by intrinsic functional connectivity analysis: A resting fMRI study
"['Hussam Mousa', 'Chandra Krintz', 'Lamia Youseff', 'Richard Wolski']","In this paper, we present VIProf, a full-system, performance sampling system capable of extracting runtime behavior across an entire software stack. Our long-term goal is to employ VIProf profiles to guide online optimization of programs and their execution environments according to the dynamically changing execution behavior and resource availability. VIProf thus, must be transparent while producing accurate and useful performance profiles. We overview the design and implementation of VIProf and empirically evaluate the system using a popular software stack - one that includes a Linux operating system, a Java virtual machine, and a set of applications. This composition is commonly employed and important for high-end systems such as application and Web servers as well as computational grid services. We show that VIProf introduces little overhead and is able to capture accurate (function-level) full-system performance data that previously required multiple profiles and extensive, manual, and offline post-processing of profile data.",VIProf: Vertically Integrated Full-System Performance Profiler
"['Luca Sanguinetti', ""Antonio A. D'Amico"", 'Ivan Cosovic']","An efficient scheme for limiting the inherent high out-of-band radiation of the orthogonal frequency-division multiplexing signal has been studied by Brandes et al.. The scheme achieves sidelobe suppression by using few subcarriers, referred to as cancellation carriers (CCs), positioned at the edges of the spectrum and modulated by data-dependent complex-valued symbols that are designed so as to minimize the out-of-band radiation. Unfortunately, its performance has been investigated at the input of the power amplifier (PA) without considering the spectral re-growth induced by PA's non-linear region. Motivated by the above consideration, in this work we return to the aforementioned scheme and investigate its performance at the output of the PA. In addition, since reducing the time-domain signal peaks may alleviate the problem of spectral re-growth, we also adopt two well-known tone reservation (TR)-based peak reduction algorithms. Computer simulations are used to assess the performance of the investigated schemes under different operating conditions. It turns out that the CC-based solution may provide some performance gain at the PA output only for input power backoffs larger than 6 dB. Additional benefits can be achieved when used in conjunction with TR and some linearization of the PA characteristic.",On the Performance of Cancellation Carrier-Based Schemes for Sidelobe Suppression in OFDM Networks
"['Willem Zuidema', 'Bart de Boer']","A fundamental, universal property of human language is that its phonology is combinatorial. That is, one can identify a set of basic, distinct units (phonemes, syllables) that can be productively combined in many different ways. In this paper, we develop a methodological framework based on evolutionary game theory for studying the evolutionary transition from holistic to combinatorial signal systems, and use it to evaluate a number of existing models and theories. We find that in all problematic linguistic assumptions are made or crucial components of evolutionary explanations are omitted. We present a novel model to investigate the hypothesis that combinatorial phonology results from optimizing signal systems for perceptual distinctiveness. Our model differs from previous models in three important respects. First, signals are modeled as trajectories through acoustic space; hence, both holistic and combinatorial signals have a temporal structure. Second, acoustic distinctiveness is defined in terms of the probability of confusion. Third, we show a path of ever increasing fitness from unstructured, holistic signals to structured signals that can be analyzed as combinatorial. On this path, every innovation represents an advantage even if no-one else in a population has yet obtained it.",The evolution of combinatorial phonology
"['Peter Larsson', 'Besma Smida', 'Toshiaki Koike-Akino', 'Vahid Tarokh']","In this paper, we consider network coded (NCed) Hybrid-ARQ (HARQ) for multiple unicast flows. The main contribution of the paper is the derivation of throughput expressions for NCed HARQ with arbitrary number of users in identical i.i.d. channels amid packets for all users. We apply the result to Rayleigh fading channels and two packet combining schemes: incremental redundancy (IR) and chase combining (CC). We verify the analytical results with simulations and observe substantial SNR improvements over NCed ARQ and HARQ. The SNR gains in the moderate/high and low throughput regimes are mainly due to network coding and packet combining, respectively. For low/moderate SNRs, NCed HARQ with IR surpasses the CC performance. In addition, we introduce a novel re-transmission strategy that makes the network coding more efficient at low SNR.",Analysis of Network Coded HARQ for Multiple Unicast Flows
"['Yan Zhou', 'Zach Jorgensen', 'W. Meador Inge']","Statistical spam filters are known to be vulnerable to adversarial attacks. One such adversarial attack, known as the good word attack, thwarts spam filters by appending to spam messages sets of ""good"" words, which are common in legitimate e-mail but rare in spam. We present a counter attack strategy that first attempts to differentiate spam from legitimate e-mail in the input space, by transforming each e- mail into a bag of multiple segments, and subsequently applies multiple instance logistic regression on the bags. We treat each segment in the bag as an instance. An e-mail is classified as spam if at least one instance in the corresponding bag is spam, and as legitimate if all the instances in it are legitimate. We show that a spam filter using our multiple instance counter-attack strategy stands up better to good word attacks than its single instance counterpart and the commonly practiced Bayesian filters.",Combating Good Word Attacks on Statistical Spam Filters with Multiple Instance Learning
"['Zhendong Luo', 'Hong Li', 'Ping Sun', 'Jing An', 'Ionel Michael Navon']","Proper orthogonal decomposition (POD) method has been successfully used in the reduced-order modeling of complex systems. In this paper, we extend the applications of POD method, i.e., combine the classical finite volume element (FVE) method with the POD method to obtain a reduced-order FVE formulation with lower dimensions and sufficiently high accuracy for two-dimensional solute transport problems, which have real life practical applications. We then provide error estimates between the reduced-order POD FVE solutions and classical FVE solutions and we provide implementation of an extrapolation algorithm for solving the reduced-order FVE formulation. Thus, we provide the theoretical basis for practical applications. A numerical example is then used to ascertain that the results of numerical computation are consistent with the theoretical derivations. Moreover, it is shown that the reduced-order FVE formulation based on POD method is both feasible and efficient for solving two-dimensional solute transport problems.",A reduced-order finite volume element formulation based on POD method and numerical simulation for two-dimensional solute transport problems
"['Z. Jarvis-Wloszek', 'D. Philbrick', 'M.A. Kaya', 'Andrew Packard', 'G.J. Balas']","We present a intuitive and self-contained formulation of a stability preserving receding horizon control strategy for a system where limited preview information is available for the disturbances. The simplicity of the derivation is due to (and its benefits somewhat offset by) a set of stringent and highly structured assumptions. The formulation uses a suboptimal value function for terminal cost, and relies on optimization strategies that only require a trivial improvement property, allowing implementation as an ""anytime"" algorithm. The nature of this strategy's performance is clarified with linear examples.",Control with disturbance preview and online optimization
"['Sahid Rahman', 'Walter Alexandre Carnielli']","Being a pragmatic and not a referential approach tosemantics, the dialogical formulation ofparaconsistency allows the following semantic idea tobe expressed within a semi-formal system: In anargumentation it sometimes makes sense to distinguishbetween the contradiction of one of the argumentationpartners with himself (internal contradiction) and thecontradiction between the partners (externalcontradiction). The idea is that externalcontradiction may involve different semantic contextsin which, say A and ?™A have been asserted.The dialogical approach suggests a way of studying thedynamic process of contradictions through which thetwo contexts evolve for the sake of argumentation intoone system containing both contexts.More technically, we show a new, dialogical, way tobuild paraconsistent systems for propositional andfirst-order logic with classical and intuitionisticfeatures (i.e. paraconsistency both with and withouttertium non-datur) and present theircorresponding tableaux.",The Dialogical Approach to Paraconsistency
['Juraj Hromkovic'],"It is proved that each Boolean circuit with O(na)-separator for an a < 1 must have ??(n1a) processors to compute some specific one-output Boolean functions. The above stated result holds also for unbounded fan-in, fan-out Boolean circuits. A nonlinear lower bound on the number of processors is achieved also for planar VLSI circuits computing some one-output Boolean functions in time O(nb) for b<12.",Nonlinear lower bounds on the number of processors of circuits with sublinear separators
"['Xin Fan', 'Xing Xie', 'Wei-Ying Ma', 'Hong-Jiang Zhang', 'He-Qin Zhou']","Images have become more and more common in mobile communications. People now can easily take and exchange pictures on the move using their mobile devices and digital cameras. However, a crucial challenge is to provide a better user experience for browsing large images on limited and heterogeneous screen sizes of mobile devices. In this paper, we propose a novel image viewing technique based on an adaptive attention shifting model. A presentation technique named rapid serial visual presentation (RSVP), borrowed from the UI community, is used to simulate the attention shifting process. We show a prototype image viewer developed for pocket PC and conduct some evaluations to demonstrate the effectiveness of our approach.",Visual attention based image browsing on mobile devices
"['Jeffrey M. Rzeszotarski', 'Aniket Kittur']","Wikipedia's remarkable success in aggregating millions of contributions can pose a challenge for current editors, whose hard work may be reverted unless they understand and follow established norms, policies, and decisions and avoid contentious or proscribed terms. We present a machine learning model for predicting whether a contribution will be reverted based on word level features. Unlike previous models relying on editor-level characteristics, our model can make accurate predictions based only on the words a contribution changes. A key advantage of the model is that it can provide feedback on not only whether a contribution is likely to be rejected, but also the particular words that are likely to be controversial, enabling new forms of intelligent interfaces and visualizations. We examine the performance of the model across a variety of Wikipedia articles.",Learning from history: predicting reverted work at the word level in wikipedia
"['Stephen Williams', 'Ayanna M. Howard']","Arctic regions present one of the harshest environments on earth for people or mobile robots, yet many important scientific studies, particularly those involving climate change, require measurements from these areas. For the successful deployment of mobile sensors in the arctic, a reliable, fault tolerant, low-cost method of navigating must be developed. One aspect of an autonomous navigation system must be an assessment of the local terrain, including the slope of nearby regions. Presented here is a method of estimating the slope of the terrain in the robot's coordinate frame using only a single camera, which has been applied to both simulated arctic terrain and real images. The slope estimates are then converted into the global coordinate frame using information from a roll sensor, used as an input to a fuzzy logic navigation scheme, and tested in a simulated arctic environment.",A single camera terrain slope estimation technique for natural arctic environments
"['Leonardo Angelini', 'Maurizio Caon', 'Stefano Carrino', 'Luc Bergeron', 'Nathalie Nyffeler', 'M??lanie Jean-Mairet', 'Elena Mugellini']","In this paper, we present the design process of a smart bracelet that aims at enhancing the life of elderly people. The bracelet acts as a personal assistant during the user's everyday life, monitoring the health status and alerting him or her about abnormal conditions, reminding medications and facilitating the everyday life in many outdoor and indoor activities.",Designing a desirable smart bracelet for older adults
"['R. de Lemos', 'Amer Saeed', 'Tom Anderson']",,A train set as a case study for the requirements analysis of safety-critical systems
"['Ioannis Ch. Paschalidis', 'Spyridon Vassilaras']","We propose estimators of the buffer overflow probability in queues fed by a Markov-modulated input process and serviced by an autocorrelated service process. These estimators are based on large-deviations asymptotics for the overflow probability. We demonstrate that the proposed estimators are less likely to underestimate the overflow probability than the estimator obtained by certainty equivalence. As such, they are appropriate in situations where the overflow probability is associated with quality of service (QoS) and we need to provide firm QoS guarantees. We also show that as the number of observations increases to infinity the proposed estimators converge with probability one to the appropriate target, and thus, do not lead to underutilization of the system in this limit.",On the estimation of buffer overflow probabilities from measurements
"['Xiping Luo', 'Li-Xin Zhen', 'Gang Peng', 'Jun Li', 'Bai-Hua Xiao']","With the availability of high-resolution cameras and increased computation power, it becomes possible to implement OCR applications such as business card reader in the mobile device. In this paper we introduced the design and implementation of a mixed-lingual business card reader based on built-in camera. It has the capability to recognize business cards with Chinese or English characters. In order to deal with the challenge of limited resource in mobile device, we proposed some new methods to reduce the resource requirement of the image processing and the Chinese OCR engine. Our experiment gives satisfactory result.",Camera based mixed-lingual card reader for mobile device
"['Hyoung Seok Hong', 'Yong Rae Kwon', 'Sung Deok Cha']","In object-oriented testing literature, a class is considered to be a basic unit of testing. A major characteristic of classes is the interaction between data members and member functions. This interaction is represented as definitions and uses of data members in member functions and can be properly modeled with finite state machines (FSM). We discuss how FSMs can be effectively used for class testing. We demonstrate how to specify the behavior of classes using FSMs and present a test case generation technique based on FSMs. In our technique, FSMs are transformed into a flow of the graph from which we can explicitly identify data flows of the FSM. Then we generate test cases using conventional data flow testing techniques upon the flow graph.",Testing of object-oriented programs based on finite state machines
"['Allen Foster', 'David Ellis']","Purpose É?? The purpose of this paper is to explore the concept of serendipity and approaches to its study particularly in relation to information studies. Design/methodology/approach É?? The origins of the term serendipity are described and its elaboration as an exploratory and explanatory concept in science and the social sciences are outlined. The distinction between serendipity and serendipity pattern is explained and theoretical and empirical studies of both serendipity and the serendipity patterns are explored. The relationship between information encountering is described. Empirical studies of serendipity using Citation Classics and other research approaches in information studies are described. Findings É?? The discrepancy between occurrences of serendipity in studies using Citation Classics and reported serendipity in philosophy of science, research anecdotes, information encountering and information seeking by inter-disciplinary researchers is highlighted. A comparison between a process model of seren...",Serendipity and its study
['Ali Khaleghi'],"New design of a single feed patch antenna with polarization pattern diversity is presented. The radiating circular patch, printed on a thin substrate, is supported by non conductive posts on a conducting ground plane and is excited with a capacitively coupled feed. Wideband impedance matching is achieved. The pattern diversity is generated using two switch parasitic pins between the circular patch and the ground plane. By optimizing the position of the switching elements, two orthogonal patterns with right-hand circular polarization (RHCP) and left-hand circular polarization (LHCP) are generated. These patterns are used for diversity reception in indoor multipath fading channel for wireless LAN applications. Design of the antenna is described. Experimental results for return loss, radiation patterns, and diversity performance in Rayleigh fading channel are presented.",Single-Port Circular-Patch Polarization Diversity Antenna
"['Markus Mueck', 'C. Rom', 'Wen Xu', 'A. Polydoros', 'N. Dimitriou', 'A.S. Diaz', 'Hanna Bogucka', 'Sven Zeisberg', 'Tobias Renk', 'Friedrich K. Jondral', 'P. Jung']","This paper presents an extension of the Cognitive Pilot Channel (CPC) concept to the case of a distributed version thereof (namely, DCPC), and demonstrates a potential instantiation of this concept in the design of Smart Femto-Cell Controllers (SFC-C) for handling the management of co-existing homogeneous or heterogeneous networks. Instead of covering the various networks by a single CPC, the novel idea consists in deploying the CPC transmission in a distributed way within each of the individual Smart Femto Cell (SFC) controlled Composite Networks (CNs), an example of which can be a heterogeneous home or business deployment including the collection of a Femto- Cell Base Station (FC-BS), a Wireless LAN (WLAN) Access Point (AP), a ZigBee network controller, a Bluetooth network controller, etc. A typical implementation of this concept leads to an integration of the DCPC transmitter into a SFC which consists of the FC-BS for provision of cellular access and the SFC-C for network management and DCPC provision. In this case, the SFC owner is able to manage the attached composite network by providing suitable context and policy information within the DCPC. The paper furthermore introduces intra-network and inter-network interference scenarios and explains how the DCPC concept allows the introduction of corresponding interference management mechanisms.",Smart femto-cell controller based distributed cognitive pilot channel
"['H. H. Tsai', 'Long-Wen Chang']","A novel reversible visible watermarking algorithm is proposed. It can fully remove the watermark from the visible watermarked image such that the original image can be restored. Pixel values of original image beneath the watermark are mapped to a small range [alpha, alpha + 127] to generate a visible watermarked image. Since the mapping is many-to-one, taking inverse mapping can only approximate the original image. To restore the original image, the difference image of subtracting the approximated image from the original image and other side information are losslessly compressed to be embedded in the visible watermarked image by a reversible data embedding algorithm. We proposed a key-based scheme for the compromise between transparency and robustness. The key is a random variable with discrete normal distribution. In addition, only users with correct key can restore the original image. In the experimental results, we show the transparent degree of watermark can be controlled by the variance of the key. Users with wrong key can not restore the original image from the visible watermarked image.",A High Secure Reversible Visible Watermarking Scheme
"['Yun Ye', 'Song Ci', 'Aggelos K. Katsaggelos', 'Yanwei Liu']","This paper presents a multi-camera motion capture system aiming to provide caregivers with timely access to the patient's health status through mobile communication devices. The major components include video capture, object detection, video coding and transmission, error concealment, and video analysis. Our contribution is twofold. First, several novel ideas are developed, including fast object detection, and content-aware and adaptive video coding and transmission. Second, all components are seamlessly integrated in a unified optimization framework dedicated for online data transmission. In the scenario, the subject walked on a treadmill with four tripod cameras capturing the video from different viewpoints. After video compression and transmission over a wireless sensor network, the remote receiver recovered the videos and performed multi-view motion capture for gait analysis. Experimental results show that the presented system design achieves better video quality than traditional video coding and transmission scheme, while the requirement for a low-cost, noninvasive and real-time healthcare monitoring system is accommodated.",A multi-camera motion capture system for remote healthcare monitoring
"['Panagiotis K. Linos', 'Esther T. Ososanya', 'H. Natarajan']","This paper reports on results gathered during a case study using a Web maintenance tool called Perlbot. The main objective of the study is to observe the history of modifications made on various Web documents. Based on selected metrics, we collect data with respect to how such sites are maintained and evolve. The data gathered support our hypothesis that there is a worldwide tendency to accumulate large numbers of stale documents over long periods of time. Moreover, our quantitative results reveal inconsistent Web maintenance activities spanning over various international sites.",Maintenance support for web sites: a case study
"['Sandeep Singh Rawat', 'Lakshmi Rajamani']","When large data repositories are coupled with geographic distribution of data, users and systems, it is necessary to combine different technologies for implementing high-performance distributed knowledge discovery systems. On the other hand, computational grid is emerging as a very promising infrastructure for high-performance distributed computing. Grid applications such as astronomy, chemistry, engineering, climate studies, geology, oceanography, ecology, physics, biology, health sciences and computer science often involve large amounts of computing and/or data. For these reasons, we think grids can offer an effective support to the implementation and use of parallel and distributed data mining systems. This paper describes development of parallel and distributed prior algorithm on grid environment. Apriori algorithm along with FP-growth (Frequent Pattern Growth) is implemented on grid network in each grid node, which finds the local support counts and prunes all infrequent item sets. After completing local pruning, each grid node broadcasts messages containing all the remaining frequent patterns to the coordinator. We have compared the output of conventional method of apriori algorithm with FP-tree in both homogenous and heterogeneous environments. Practical datasets are large in nature and taken from the UCI machine repository and are related to adult, mushroom, and letter recognition, are used to measure the system performance. The detailed experiment procedure and result analysis are also discussed in this paper. In future the security issue among different local datasets and the huge communication cost in data migration can be considered.",Performance of distributed apriori algorithms on a computational grid
"['Torsten Wilhelm', 'Hans-Joachim B??hme', 'Horst-Michael Gross']","This paper describes a user detection system which employs a saliency system working on an omnidirectional camera delivering a rough and fast estimate of the position of a potential user. It consists of a vision (skin color) and a sonar based component, which are combined to make the estimate more reliable. To make the skin color detection robust under varying illumination conditions, it is supplied with an automatic white balance algorithm. The active vision head looks continuously in the direction of the salient region. Thus, a high resolution image can be grabbed and analyzed with a face detector.",A Multi-Modal System for Tracking and Analyzing Faces on a Mobile Robot
['Stan Z. Li'],The combinatorial optimization problem of MAP estimation is converted to one of constrained real optimization and then solved by using the augmented Lagrange-Hopfield (ALH) method proposed in 1984. The ALH effectively overcomes instabilities that are inherent in the penalty method or the Lagrange multiplier method in constrained optimization. It produces good solutions with reasonable costs.,MAP image restoration and segmentation by constrained optimization
"['Walter Daems', 'Georges Gielen', 'Willy Sansen']",This paper presents an new direct--fitting method to generate posynomial response surface models with arbitrary constant exponents for linear and nonlinear performance parameters of analog integrated circuits. Posynomial models enable the use of efficient geometric programming techniques for circuit sizing and optimization. The automatic generation avoids the time--consuming nature and inaccuracies of handcrafted analytic model generation. The technique is based on the fitting of posynomial model templates to numerical data from SPICE simulations. Attention is paid to estimating the relative `goodness--of--fit' of the generated models. Experimental results illustrate the significantly better accuracy of the new approach.,An efficient optimization-based technique to generate posynomial performance models for analog integrated circuits
"['Kota Tsubouchi', 'Kazuo Hiekata', 'Hiroyuki Yamato']",This paper proposes predicting the operation function (POF) for the workflow-based knowledge management software. POF is a function of the system that can estimate the documents which software users want to see and provides the users not only with proper documents but also unexpected knowledge which another users operation reflected. The system stores the log data of operation and judge which documents are needed from the viewpoint of the present user's characteristics and everyone's behavior. The simulation experiment with real data concerning the On-Demand Bus project shows that users' behavior is strongly related with the previous actions and the POF can improve the usability of the knowledge management software.,A Research on Predicting the Operation Function for the Workflow-Based Knowledge Management Software
"['Ricardo Bezerra de Andrade e Silva', 'Wei Chu', 'Zoubin Ghahramani']","When predicting class labels for objects within a relational database, it is often helpful to consider a model for relationships: this allows for information between class labels to be shared and to improve prediction performance. However, there are different ways by which objects can be related within a relational database. One traditional way corresponds to a Markov network structure: each existing relation is represented by an undirected edge. This encodes that, conditioned on input features, each object label is independent of other object labels given its neighbors in the graph. However, there is no reason why Markov networks should be the only representation of choice for symmetric dependence structures. Here we discuss the case when relationships are postulated to exist due to hidden common causes. We discuss how the resulting graphical model differs from Markov networks, and how it describes different types of real-world relational processes. A Bayesian nonparametric classification model is built upon this graphical representation and evaluated with several empirical studies.",Hidden Common Cause Relations in Relational Learning
"['Tony S. Verma', 'Teresa H. Meng']","We present a flexible analysis/synthesis tool for transient signals that extends current sinusoidal and sines+noise models for audio to sines+transients+noise. The explicit handling of transients provides a more realistic and robust signal model. Because the transient model presented is the frequency domain dual to sinusoidal modeling, it has similar flexibility and allows for a wide range of transformations on the parameterized signal. In addition, due to this duality, a major portion of the transient model is sinusoidal modeling performed in the frequency domain. In order to make the transient and sinusoidal models work more effectively together, we present a formulation of sinusoidal modeling (and therefore transient modeling) in terms of matching pursuits and overlap-add synthesis. This formulation provides a tight coupling between the sines+transients+noise model because it allows a simple heuristic, based on tonality, as to when an audio signal should be modeled as sines and/or transients and/or noise.",An analysis/synthesis tool for transient signals that allows a flexible sines+transients+noise model for audio
"['Karel L. Sterckx', 'Jaafar M. H. Elmirghani', 'Robert A. Cryan']",This paper analyses the merits of adaptive threshold detection (ATD) over fixed threshold detection (FTD) as a means to reduce the impact of ambient shot noise on wireless infrared communication links. Original analytic details are provided to allow for an accurate sensitivity account of an optical wireless receiver that employs either ATD or FTD. Proposals towards the implementation of adaptive threshold detection are outlined with considerations on their respective benefits and drawbacks.,On the use of adaptive threshold detection in optical wireless communication systems
"['Lisheng Fan', 'Xianfu Lei', 'Trung Q. Duong', 'Rose Qingyang Hu', 'Maged Elkashlan']","In this paper, we propose a multiuser cognitive relay network, where multiple secondary sources communicate with a secondary destination through the assistance of a secondary relay in the presence of secondary direct links and multiple primary receivers. We consider the two relaying protocols of amplify-and- forward (AF) and decode-and-forward (DF), and take into account the availability of direct links from the secondary sources to the secondary destination. With this in mind, we propose an optimal solution for cognitive multiuser scheduling by selecting the optimal secondary source, which maximizes the received signal-to-noise ratio (SNR) at the secondary destination using maximal ratio combining. This is done by taking into account both the direct link and the relay link in the multiuser selection criterion. For both AF and DF relaying protocols, we first derive closed-form expressions for the outage probability and then provide the asymptotic outage probability, which determines the diversity behavior of the mul- tiuser cognitive relay network. Finally, this paper is corroborated by representative numerical examples.",Multiuser Cognitive Relay Networks: Joint Impact of Direct and Relay Communications
['Kevin Grant'],"In this paper, we consider efficient indexing methods for recursive conditioning algorithms. We compare two wellknown methods for indexing, a top-down method and a bottom-up method, and discuss the redundancy that each of these suffer from. We also present a new method for indexing that is a hybrid of these models. Using this new approach shows an improvement in the amount of indexing operations by 55% or more in our test networks, as well as a reduction in the cumulative time for inference by 33% or more.",Efficient Indexing for Recursive Conditioning Algorithms
"['Kamil Szczesny', 'Matthias K. Hamm', 'Markus K??nig']","An efficient execution of complex construction projects requires a comprehensive scheduling of all construction activities. For this, it is necessary to consider various restrictions and the availability of required resources. The generation of efficient schedules is a very challenging task, in fact an NP-hard optimization problem. An appropriate approach is the application of discrete-event simulation for the generation of valid schedules. Additionally, simulation is combined with optimization methods to determine efficient schedules regarding given objectives. The applied optimization techniques are evolutionary algorithms. Thus, operators have to be implemented that define the way of generating new schedules in the recombination step of the algorithm. In this paper an improved operator is presented that outperforms common operators for scheduling problems by considering technological dependencies between activities, so that it will be possible to determine efficient schedules for complex construction scheduling problems. An example of implementation is presented to validate the developed operator.",Adjusted recombination operator for simulation-based construction schedule optimization
"['Hui-Ling Lou', 'Deepen Sinha', 'Carl-Erik W. Sundberg']","Hybrid in band on channel (IBOC) digital audio broadcasting simultaneously with analog amplitude modulation (AM) has been proposed as a solution to DAB in the AM band. Since the AM band is crowded and the available bandwidth per program is limited, adding digital transmission presents a challenge. To achieve FM-like audio quality, an audio coder rate of 32-64 kb/sec may be required. One currently proposed hybrid IBOC-AM system is 30 kHz wide. Severe second adjacent interference may occur in certain areas, leading to a possible 40% loss of the effective audio bit rate. To cope with such harsh transmission conditions, we present a solution based on embedded/multidescriptive audio coding with matched multistream transmission in separate frequency bands. With the loss of one frequency band, the embedded system blends to a lower audio coder rate with a much better quality than analog AM. The nonembedded system, without multistream transmission, fails catastrophically when a little more than one sideband is severely interfered with, causing a severe discontinuity in quality while blending directly to analog AM. Some detailed robust embedded systems are outlined. We also show how multistream transmission schemes can be used with nonembedded audio coders. Both daytime and nighttime scenarios are included. This paper contains a catalog of possible systems for different audio quality levels and interference scenarios, including systems with 20 kHz bandwidth rather than 30 kHz.",Multistream transmission for hybrid IBOC-AM with embedded/multidescriptive audio coding
"['Manoj Kumar Jain', 'M. Balakrishnan', 'Anshul Kumar']","Interest in synthesis of Application Specific Instruction Processors or ASIPs has increased considerably and a number of methodologies have been proposed in the last decade. This paper attempts to survey the state of the art in this area and identifies some issues which need to be addressed. We have identified the five key steps in ASIP design as application analysis, architectural design space exploration, instruction set generation, code synthesis and hardware synthesis. A broad classification of the approaches reported in the literature is done. The paper notes the need to broaden the architectural space being explored and to tightly couple the various subtasks in ASIP synthesis.",ASIP design methodologies: survey and issues
"['Nizar Sakr', 'Jilin Zhou', 'Nicolas D. Georganas', 'Jiying Zhao', 'Emil M. Petriu']","In this paper, two robust perception-based haptic data reduction and transmission techniques are presented to reduce data traffic in telehaptic systems. A prediction approach that relies on the least-squares method and median filtering is exploited in order to reduce the number of packets transmitted, and efficiently reconstruct unsuccessfully received data samples. Knowledge from human haptic perception is also used and incorporated into the general data reduction architecture. The techniques are initially evaluated in a basic experimental setting in order to validate their performance. Their application in a haptic-enabled telementoring surgery simulation is also demonstrated. The experimental results prove the proposed approach's effectiveness as haptic data packets can be reduced by as much as 96% in normal network conditions and up to 93% in the presence of significant communication delay and packet loss, while preserving the overall quality of the telehaptic environment.",Robust perception-based data reduction and transmission in telehaptic systems
"['Chia-Te Chou', 'Sheng-Wen Shih', 'Duan-Yu Chen']","In this paper, we propose a systematic approach to design Gabor filter banks for iris feature extraction. The spectra of normalized iris images were analyzed and the most informative band (MIB) was located. When tuning the filter parameters, the pass bands of the filter banks are confined to be within the MIB. It turns out that with the MIB constraint the number of orientation parameters of the Gabor filters can be reduced to one. Furthermore, we show that the spatial locations of the Gabor filters can be arbitrarily and uniformly selected provided that the conditions of the Nyquist sampling theorem are satisfied. The main difficulty of designing filter banks for iris feature extraction is that there are too many free parameters to be determined, e.g., there will be 6N parameters to be designed for N filters. However, after analyzing the possible factors influencing the iris feature extraction process, we found that the free parameters of the Gabor filter banks can be reduced from 6N to three. By using the genetic algorithm to tune the filter parameters, the performance of our recognition system has been gradually improved. The experimental results show that the best equal error rate of our system is about 0.05% computed with the CASIA database.",Design of Gabor Filter Banks for Iris Recognition
"['Adriana Fodor', 'Eddy Karnieli']","The main purpose of this chapter is to discuss the place of telemedicine in the modern medicine, its present and future application in the clinical medicine. It covers aspects of clinical telemedicine practice, technical advances, principles and practices, health policy and regulation, and health services research dealing with clinical effectiveness, efficacy, and safety of telemedicine and its effects on quality, cost, and accessibility of care. The diabetic foot problem was chosen as a suitable model to examine whether the use of telemedicine technology will improve the quality of medicine and reduce medical costs. According to the American Telemedicine Association, telemedicine is the exchange of medical information from one site to another using electronic communication, such as telephone, Internet or videoconference to improve patients' health status (1). Related with telemedicine is the term ""telehealth,"" which covers a quite broader definition of remote healthcare, being more focused on other health-related services that do not always involve direct patient clinical services. Telemedicine practices allow for specialist consultation, direct patient consultation, patient monitoring, and medical education. Although the term telemedicine is a relatively recent one, since 1970s, medicine has long made use of various communication technologies dating back to 1906. Wilhelm Einthoven, inventor of the electrocardiograph, created the ""telecardio- gram,"" which transmitted electrocardiograms via telephone from the clinic to his office, enabling him to monitor his patients' condition at a distance (2). In the early 1990s, telemedicine experienced a considerable progress due to rapid advancements in information and telecommunications technologies and digital data transmission. Since then, the interest in the use of telemedicine procedures and the number of related publications had rapidly increased. A search of MEDLINE in 1990 found six publications on telemedicine; while by February 2009 there were more than 10,700 entries under the search term ""telemedicine.""","Telemedicine for the Diabetic Foot: A Model for Improving Medical Care, Developing Decision Support Systems, and Reducing Medical Cost"
"['Di Miao', 'Shuoyu Wang']","The human auditory system possesses intelligence characteristics such as the ability to extract information from multiple voice signals, and these characteristics correlate with brain function. If the relation between auditory intelligence and brain status can be identified, it will be possible to evaluate brain status according to auditory intelligence. At present there are no integrated measurement and evaluation standards for auditory intelligence. We present a method for the measurement of auditory intelligence based on discontinuous audio signals.",Measurement of Functional Auditory Intelligence in Humans
"['Daniel Ayala', 'Ouri Wolfson', 'Bo Xu', 'Bhaskar DasGupta', 'Jie Lin']","The proliferation of mobile devices, location-based services and embedded wireless sensors has given rise to applications that seek to improve the efficiency of the transportation system. In particular, new applications are already available that help travelers to find parking in urban settings by conveying the parking slot availability near the desired destinations of travelers on their mobile devices.   In this paper we present two notions of parking choice: the optimal and the equilibrium. The equilibrium describes the behavior of individual, selfish agents in a system. We will show how a pricing authority can use the parking availability information to set prices that entice drivers to choose parking in the optimal way, the way that minimizes total driving distance by the vehicles and is then better for the transportation system (by reducing congestion) and for the environment. We will present two pricing schemes that perform this task. Furthermore, through simulations we show the potential congestion improvements that can be obtained through the use of these schemes.",Pricing of parking for congestion reduction
"['Julio C. Sosa', 'Roc??o G??mez-Fabela', 'Jose Antonio Boluda', 'Fernando Pardo']","Optical flow computation has been extensively used for object motion estimation in image sequences. However, the results obtained by most optical flow techniques are as accurate as computationally intensive due to the large amount of data involved. A new strategy for image sequence processing has been developed; pixels of the image sequence that significantly change fire the execution of the operations related to the image processing algorithm. The data reduction achieved with this strategy allows a significant optical flow computation speed up. Furthermore, FPGAs allow the implementation of a custom data-flow architecture specially suited for this strategy. The bases of the change-driven image processing are presented, as well as the hardware custom implementation.",FPGA Implementation of a Change-Driven Image Processing Architecture for Optical Flow Computation
"['Siu-Long Lei', 'Hai-Wei Sun']","The implicit finite difference scheme with the shifted Grunwald formula, which is unconditionally stable, is employed to discretize fractional diffusion equations. The resulting systems are Toeplitz-like and then the fast Fourier transform can be used to reduce the computational cost of the matrix-vector multiplication. The preconditioned conjugate gradient normal residual method with a circulant preconditioner is proposed to solve the discretized linear systems. The spectrum of the preconditioned matrix is proven to be clustered around 1 if diffusion coefficients are constant; hence the convergence rate of the proposed iterative algorithm is superlinear. Numerical experiments are carried out to demonstrate that our circulant preconditioner works very well, even though for cases of variable diffusion coefficients.",A circulant preconditioner for fractional diffusion equations
"['Bruno Ot?≠vio Soares Teixeira', 'Jaganath Chandrasekar', 'L. Torres', 'Luis Antonio Aguirre', 'Dennis S. Bernstein']","This article addresses the state-estimation problem for linear and non-linear systems for the case in which prior knowledge is available in the form of an equality constraint. The equality-constrained Kalman filter (KF) is derived as the maximum-a-posteriori solution to the equality-constrained state-estimation problem for linear and Gaussian systems and is compared to alternative algorithms. Then, four novel algorithms for non-linear equality-constrained state estimation based on the unscented KF are presented, namely, the equality-constrained unscented KF, the projected unscented KF, the measurement-augmentation unscented KF, and the constrained unscented KF. Finally, these methods are compared on linear and non-linear examples.",State estimation for linear and non-linear equality-constrained systems
['Adriaan van Oosterom'],"This invited paper presents a personal view on the current status of the solution to the inverse problem of bioelectricity. Its focus lies on applications in the field of electrocardiography. The topic discussed is also relevant in other medical domains, such as electroencephalography, electroneurography and electromyography. In such domains the methodology involved rests on the same basic principles of physics and electrophysiology as well as on the applied techniques of signal analysis and numerical analysis.",The inverse problem of bioelectricity: an evaluation.
['Sarah-Jayne Blakemore'],"article i nfo Article history: Accepted 25 November 2011 Available online 8 December 2011 The past 15 years has seen a rapid expansion in the number of studies using neuroimaging techniques to investigate maturational changes in the human brain. In this paper, I review MRI studies on structural changes in the developing brain, and fMRI studies on functional changes in the social brain during adolescence. Both MRI and fMRI studies point to adolescence as a period of continued neural development. In the final section, I discuss a number of areas of research that are just beginning and may be the subject of developmental neuroimaging in the next twenty years. Future studies might focus on complex questions including the development of functional connectivity; how gender and puberty influence adolescent brain development; the effects of genes, environment and culture on the adolescent brain; development of the atypical adolescent brain; and implications for policy of the study of the adolescent brain.",Imaging brain development: the adolescent brain.
"['Jonathan Ragan-Kelley', 'Charlie Kilpatrick', 'Brian W. Smith', 'Doug Epps', 'Paul Green', 'Christophe Hery', 'Fr??do Durand']","We present an automated approach for high-quality preview of feature-film rendering during lighting design. Similar to previous work, we use a deep-framebuffer shaded on the GPU to achieve interactive performance. Our first contribution is to generate the deep-framebuffer and corresponding shaders automatically through data-flow analysis and compilation of the original scene. Cache compression reduces automatically-generated deep-framebuffers to reasonable size for complex production scenes and shaders. We also propose a new structure, the  indirect framebuffer , that decouples shading samples from final pixels and allows a deep-framebuffer to handle antialiasing, motion blur and transparency efficiently. Progressive refinement enables fast feedback at coarser resolution. We demonstrate our approach in real-world production.",The lightspeed automatic interactive lighting preview system
"['Tajudeen A. Atolagbe', 'Vlatka Hlupic']","SimTutor is a multimedia intelligent tutoring system (ITS) for simulation modeling. Multimedia systems are now de facto standard on personal computers and increasing number of intelligent tutoring systems incorporate multimedia systems to enhance interaction with students. SimTutor provides a graphical environment in which the student can practice conceptual model development and interact direct with different simulation modeling software. We used multimedia systems to enhance the pedagogy and for incorporating different strategy into the courseware design. ITS components are accessed through the graphical user interface, allowing them to be developed independently. The modular architecture allows for interoperability of applications with the same event changing protocol. An object-oriented approach is adapted to allow the system to evolve and to flexibly change the data.",SimTutor: a multimedia intelligent tutoring system for simulation modeling
"['Yu-lung Lo', 'Chun-yu Chen']","A non-trivial repeating pattern is commonly used in analyzing the repeated part of a music object and looking for the theme. Non-trivial repeating patterns exclude those patterns included in other longer patterns such that they can reduce the redundancy and speedup music search. So far, existing approaches discover a repeating pattern in such a way that the sequence of notes in a music object appears more than once in exactly matching. If we allow the similar sequences with partial different notes also being a repeating pattern, it can reduce the number of repeating patterns and construct more efficient music indexes. The more accurate music theme also could be analyzed. Therefore, in this paper, we propose a fault-tolerant non-trivial repeating pattern discovering technique. The experimental results show that our approach can not only reduce the number of non-trivial repeating patterns but also improve the hit ratios of queries for music databases",Fault Tolerant Non-trivial Repeating Pattern Discovering for Music Data
"['Ruihong Huang', 'Ellen Riloff']","The goal of our research is to improve event extraction by learning to identify secondary role filler contexts in the absence of event keywords. We propose a multi-layered event extraction architecture that progressively ""zooms in"" on relevant information. Our extraction model includes a document genre classifier to recognize event narratives, two types of sentence classifiers, and noun phrase classifiers to extract role fillers. These modules are organized as a pipeline to gradually zero in on event-related information. We present results on the MUC-4 event extraction data set and show that this model performs better than previous systems.",Peeling Back the Layers: Detecting Event Role Fillers in Secondary Contexts
"['Istv?≠n A. Bogd?≠n', 'Daniel Coca', 'Robert J. Beynon']","The reconfigurable computing paradigm, which exploits the flexibility and versatility of field-programmable gate arrays (FPGAs), has emerged as a powerful solution for speeding up time-critical algorithms. This paper describes a reconfigurable computing solution for processing raw mass spectrometric data generated by MALDI-TOF instruments. The hardware-implemented algorithms for denoising, baseline correction, peak identification, and deisotoping, running on a Xilinx Virtex-2 FPGA at 180 MHz, generate a mass fingerprint that is over 100 times faster than an equivalent algorithm written in C, running on a Dual 3-GHz Xeon server. The results obtained using the FPGA implementation are virtually identical to those generated by a commercial software package MassLynx.",Peptide Mass Fingerprinting Using Field-Programmable Gate Arrays
"['Katrin Amunts', 'Michael Hawrylycz', 'D. C. Van Essen', 'J.D. Van Horn', 'Noam Harel', 'Jean Baptiste Poline', 'F. De Martino', 'Jan G. Bjaalie', 'Ghislaine Dehaene-Lambertz', 'S. Dehaene', 'P. Valdes-Sosa', 'Bertrand Thirion', 'Karl Zilles', 'S.L. Hill', 'M.B. Abrams', 'Peter A. Tass', 'Wim Vanduffel', 'Alan C. Evans', 'Simon B. Eickhoff']","The last two decades have seen an unprecedented development of human brain mapping approaches at various spatial and temporal scales. Together, these have provided a large fundus of information on many different aspects of the human brain including micro- and macrostructural segregation, regional specialization of function, connectivity, and temporal dynamics. Atlases are central in order to integrate such diverse information in a topographically meaningful way. It is noteworthy, that the brain mapping field has been developed along several major lines such as structure vs. function, postmortem vs. in vivo, individual features of the brain vs. population-based aspects, or slow vs. fast dynamics. In order to understand human brain organization, however, it seems inevitable that these different lines are integrated and combined into a multimodal human brain model. To this aim, we held a workshop to determine the constraints of a multi-modal human brain model that are needed to enable (i) an integration of different spatial and temporal scales and data modalities into a common reference system, and (ii) efficient data exchange and analysis. As detailed in this report, to arrive at fully interoperable atlases of the human brain will still require much work at the frontiers of data acquisition, analysis, and representation. Among them, the latter may provide the most challenging task, in particular when it comes to representing features of vastly different scales of space, time and abstraction. The potential benefits of such endeavor, however, clearly outweigh the problems, as only such kind of multi-modal human brain atlas may provide a starting point from which the complex relationships between structure, function, and connectivity may be explored.",Interoperable atlases of the human brain.
"['Brian Palmer', 'Joseph F. Danzer', 'Kevin Hambly', 'Derek A. Debe']","Advances in protein crystallography and homology modeling techniques are producing vast amounts of high resolution protein structure data at ever increasing rates. As such, the ability to quickly and easily extract structural similarities is a key tool in discovering important functional relationships. We report on an approach for creating and maintaining a database of pairwise structure alignments for a comprehensive database comprising the PDB and homology models for the human and select pathogen genomes. Our approach consists of a novel, multistage method for determining pairwise structural similarity coupled with an efficient clustering protocol that approximates a full NN assessment in a fraction of the time. Since biologists are commonly interested in recently released structures, and the homology models built from them, an automatically updating database of structural alignments has great value. Our approach yields a querying system that allows scientists to retrieve databank-wide protein structure similarities as easily as retrieving protein sequence similarities via BLAST or PSI-BLAST. Basic, noncommercial access to the database can be requested at https://tip.eidogen-sertanty.com/.",StructSorter : A method for continuously updating a comprehensive protein structure alignment database
['Olivera Marjanovic'],"Supported by technology, organizations are becoming more dynamic, adaptive and networked. A term ambient organizations is used to describe evolving organisational forms, enabled by integrated information systems that are designed to support customercentered business processes and to enhance flexibility and knowledge sharing across functional and organisational boundaries. In ambient organizations, customers become the central focus and various strategies are implemented to involve them as É??business partnersÉ?ù or virtual resources. This paper investigates ambient organizations within the context of SMEs (SmallÉ??to-Medium Enterprises) with the special emphasis on enabling IT solutions. It then illustrates the concept by an example of a service-oriented SME.",Using IT to Enable Ambient-to-Be SMEs
"['Gamal M. Mahmoud', 'Tassos Bountis', 'Emad E. Mahmoud']","Chaos synchronization is a very important nonlinear phenomenon, which has been studied to date extensively on dynamical systems described by real variables. There also exist, however, interesting cases of dynamical systems, where the main variables participating in the dynamics are complex, for example, when amplitudes of electromagnetic fields are involved. Another example is when chaos synchronization is used for communications, where doubling the number of variables may be used to increase the content and security of the transmitted information. It is also well-known that similar generalization of the Lorenz system to one with complex ODEs has been introduced to describe and simulate the physics of a detuned laser and thermal convection of liquid flows. In this paper, we study chaos synchronization by applying active control and Lyapunov function analysis to two such systems introduced by Chen and Lu. First we show that, written in terms of complex variables, these systems can have chaotic dynamics and exhibit strange attractors. We calculate numerically the values of the parameters at which these attractors exist. Active control and global synchronization techniques are then applied to study the phenomenon of chaos synchronization. Analytical criteria concerning the stability of these techniques are implemented and excellent agreement is found upon comparison with numerical experiments. In particular, studying the time evolution of ""errors"" (or differences between drive and control dynamics), we show that both techniques are very effective for controlling the behavior of these systems, even in regimes of very strong chaos.",Active control and global synchronization of the complex chen and L?¨ systems
"['Luis Guanter', 'Luis Alonso', 'Jos?? F. Moreno']","The Compact High Resolution Imaging Spectrometer (CHRIS) onboard the Project for On-Board Autonomy (PROBA) platform system provides the first high spatial resolution hyperspectral/multiangular remote sensing data from a satellite system, what represents a new source of information for Earth Observation purposes. A fully consistent radiative transfer approach is always preferred when dealing with the retrieval of surface reflectance from hyperspectral/multiangular data. However, due to the reported calibration anomalies for CHRIS data, a direct atmospheric correction based on physical radiative transfer modeling is not possible, and the method must somehow compensate for such calibration problems in specific wavelength ranges. A dedicated atmospheric correction algorithm for PROBA/CHRIS data over land is presented in this work. It consists in the combination of radiative transfer and empirical line approaches to atmospheric correction, in order to retrieve surface reflectance images free from both the atmospheric distortion and artifacts due to miscalibration. The atmospheric optical parameters and the updated set of calibration coefficients are obtained jointly in an autonomous process, without the need for any ancillary data. Results from the application of the algorithm to PROBA/CHRIS data from the two European Space Agency SPectra bARrax Campaign (SPARC) held at the Barrax study site (La Mancha, Spain) in 2003 and 2004 are presented in this work, focusing on the validation of the final surface reflectance using in situ measurements acquired simultaneously to PROBA overpasses.",A method for the surface reflectance retrieval from PROBA/CHRIS data over land: application to ESA SPARC campaigns
['Nir Kshetri'],"This paper attempts to gain an understanding of the diffusion dynamics of Linux by assessing it on RogersÉ?? technology dimensions É?? relative advantage, compatibility, complexity, observability, and trialability. The analysis makes clear that Linux possesses greater relative advantage than its proprietary competitors because of its low cost, lower susceptibility to bugs and crashes, resilience to obsolescence, ability to run on older machines and higher perceived security. Linux is facing compatibility problems with applications, hardware and other corporate resources; suppliersÉ?? and customersÉ?? technologies; and, skills of current and potential employees. Extreme configurability and user unfriendly interface; limited support and staff knowledge; and, potential hazard of forking into competing versions have been some major sources of LinuxÉ??s complexity. Linux seems to have a reasonably good performance on observability and trialability dimensions. The paper concludes by offering some suggestions on how to accelerate the diffusion of Linux among software developers, national governments and international agencies.",Diffusion pattern of Linux: An assessment on major technology dimensions
"['Fahimeh Jafari', 'Zhonghai Lu', 'Axel Jantsch', 'Mohammad Hossein Yaghmaee']","For network-on-chip (NoC) designs, optimizing buffers is an essential task since buffers are a major source of cost and power consumption. This paper proposes flow regulation and has defined a regulation spectrum as a means for system-on-chip architects to control delay and backlog bounds. The regulation is performed per flow for its peak rate and burstiness. However, many flows may have conflicting regulation requirements due to interferences with each other. Based on the regulation spectrum, this paper optimizes the regulation parameters aiming for buffer optimization. Three timing-constrained buffer optimization problems are formulated, namely, buffer size minimization, buffer variance minimization, and multiobjective optimization, which has both buffer size and variance as minimization objectives. Minimizing buffer variance is also important because it affects the modularity of routers and network interfaces. A realistic case study exhibits 62.8% reduction of total buffers, 84.3% reduction of total latency, and 94.4% reduction on the sum of variances of buffers. Likewise, the experimental results demonstrate similar improvements in the case of synthetic traffic patterns. The optimization algorithm has low run-time complexity, enabling quick exploration of large design spaces. This paper concludes that optimal flow regulation can be a highly valuable instrument for buffer optimization in NoC designs.",Buffer Optimization in Network-on-Chip Through Flow Regulation
"['Koustav Bhattacharya', 'Nagarajan Ranganathan']","Differential power analysis (DPA) has been shown to be the dominant type of side-channel attacks that significantly jeopardize the security in integrated circuits. It has been shown that the data, the functional unit operations as well as the internal micro-architectures can be detected through current and power analysis. Subsequently, different CMOS logic styles have been proposed in the literature for performing computations in such a manner that the current and power signatures can be concealed through reduction of the variance in transient power dissipation. In this work, we propose a gate sizing formulation based on traditional static CMOS standard cells that improves the security of the circuits while maintaining low overheads in terms of area, power and delay. The proposed algorithm considers all disjoint paths from primary inputs to the primary outputs, performing gate sizing with the objective of balancing the switched path capacitances among the various paths making it difficult to extract power or current signatures through current or power profiling. Further, we show that the path based security aware gate sizing formulation is NP-complete and propose a greedy approximation algorithm based on linear programming. The proposed algorithm has been implemented and validated on the ISCAS85 benchmarks and the experimental results indicate a reduction of the variance of transient dynamic power by about 40% with very low overhead in terms of delay, area and power.",A linear programming formulation for security-aware gate sizing
['Ping Wah Wong'],"Two different approaches in the inverse halftoning of error-diffused images are considered. The first approach uses linear filtering and statistical smoothing that reconstructs a gray-scale image from a given error-diffused image. The second approach can be viewed as a projection operation, where one assumes the error diffusion kernel is known, and finds a gray-scale image that will be halftoned into the same binary image. Two projection algorithms, viz., minimum mean square error (MMSE) projection and maximum a posteriori probability (MAP) projection, that differ on the way an inverse quantization step is performed, are developed. Among the filtering and the two projection algorithms, MAP projection provides the best performance for inverse halftoning. Using techniques from adaptive signal processing, we suggest a method for estimating the error diffusion kernel from the given halftone. This means that the projection algorithms can be applied in the inverse halftoning of any error-diffused image without requiring any a priori information on the error diffusion kernel. It is shown that the kernel estimation algorithm combined with MAP projection provide the same performance in inverse halftoning compared to the case where the error diffusion kernel is known. >",Inverse halftoning and kernel estimation for error diffusion
"['Mark W. Schmidt', 'Ewout van den Berg', 'Michael P. Friedlander', 'Kevin P. Murphy']","An optimization algorithm for minimizing a smooth function over a convex set is described. Each iteration of the method computes a descent direction by minimizing, over the original constraints, a diagonal plus lowrank quadratic approximation to the function. The quadratic approximation is constructed using a limited-memory quasi-Newton update. The method is suitable for large-scale problems where evaluation of the function is substantially more expensive than projection onto the constraint set. Numerical experiments on one-norm regularized test problems indicate that the proposed method is competitive with state-of-the-art methods such as boundconstrained L-BFGS and orthant-wise descent. We further show that the method generalizes to a wide class of problems, and substantially improves on state-of-the-art methods for problems such as learning the structure of Gaussian graphical models and Markov random elds.",Optimizing Costly Functions with Simple Constraints: A Limited-Memory Projected Quasi-Newton Algorithm
"['Bo Zhang', 'Fangguo Zhang']","Similarity measures play an important role in classification problems, cluster analysis, and identification issues. This paper studies the secure similarity coefficients computation in the two-party setting. Recently, a privacy-preserving similarity coefficients protocol for binary data was proposed by Wong and Kim Computers and Mathematics with Application 2012. We point out that their protocol is not secure, even in the semi-honest model. In their protocol, the client can retrieve the inputs of the server without deviating from the protocol. Next, we propose a secure similarity coefficients computation protocol in the presence of malicious adversaries, which solves the same similarity coefficients functionality as that proposed by Wong and Kim. Meanwhile, we prove the protocol secure against the malicious adversaries by using the standard simulation-based security definitions for secure two-party computation. Also several extensions of our protocol for settling other specific problems are discussed. At last, we present a protocol computing the similarity coefficients with better privacy by using the secure integer division on ciphertexts. Copyright ?? 2013 John Wiley & Sons, Ltd.",Secure similarity coefficients computation for binary data and its extensions
"['O. Gottlieb', 'Alexander Oron']","We investigate the stability and bifurcations of parametrically excited thin liquid films. A recently derived nonlinear evolution equation for the two-dimensional spatio-temporal dynamics of falling liquid films on an oscillating vertical wall is expanded to low order Fourier modes. A fourth-order modal dynamical system is validated to yield the primary bifurcation structure of the fundamental falling film dynamics described by the Benney equation, and accurately predicts the quasi-periodic structure of the temporally modulated Benney equation (TMBE). The stability of fundamental steady and periodic solutions is analytically and numerically investigated so as to reveal the threshold for nonstationary and chaotic solutions corresponding to aperiodic modulated traveling waves. The reduced modal dynamical system enables construction of a comprehensive bifurcation structure, which is verified by numerical simulation of the evolution equation.",STABILITY AND BIFURCATIONS OF PARAMETRICALLY EXCITED THIN LIQUID FILMS
"['Stefan Rass', 'Peter Schartner', 'Michaela Greiler']","Quantum cryptographic key distribution (QKD) is a promising candidate for achieving unconditional security, making the renowned one-time pad encryption technically feasible for building computer networks. However, although well-developed theoretical foundations perfectly ensure protection against eavesdropping, no natural mechanism is yet able to successfully repel an adversary sitting between Alice and Bob, performing QKD with both and re-encrypting each message after heaving read it in plain text. Authentication is hence of crucial importance, and normally applied to all messages that are related to the public discussion part of the QKD protocol. We present an analysis of a scenario, in which authentication is postponed until the end of the QKD protocol. This yields to reduced computational effort, as well as simple and tight bounds on the amount of preshared key material. Our solution relies on a combination of quantum key distribution and quantum coin-flipping, which ensures noncontrollability of the QKD key. Based on this assumption, we can apply a standard fingerprint comparison for authentication, to guard the protocol against a person-in-the-middle attack.",Quantum Coin-Flipping-Based Authentication
"['Mongkol Ekpanyapong', 'Jacob Rajkumar Minz', 'Thaisiri Watewai', 'Hsien-Hsin S. Lee', 'Sung Kyu Lim']","As very large scale integration (VLSI) process technology migrates to nanoscale with a feature size of less than 100 nm, global wire delay is becoming a major hindrance in keeping the latency of intrachip communication within a single cycle, thus substantially decaying performance scalability. In addition, an effective microarchitectural floor planning algorithm can no longer ignore the dynamic communication patterns of applications. This article, using the profile information acquired at the microarchitecture level, proposes a ""profile-guided microarchitectural floor planner"" that considers both the impact of wire delay and the architectural behavior, namely, the intermodule communication, to reduce the latency of frequent routes inside a processor and to maintain performance scalability. Based on the simulation results here, the proposed profile-guided method shows a 5%-40% average instructions per cycle (IPC) improvement when the clock frequency is fixed. From the perspective of instruction throughput in billion instructions per second (BIPS), the floor planner is much more scalable than the conventional wirelength-based floor planner",Profile-guided microarchitectural floor planning for deep submicron processor design
"['Myungsik Kim', 'Nak Young Chong', 'Wonpil Yu']","Finding and moving to a target is a key element toward enhancing functionality and autonomy of mobile robots in a variety of applications. For the purpose, the location sensing radio frequency identification (RFID) system has been proposed by the authors. Real time tracking of the target transponder became available by employing the dual-directional antenna. However, since the system depended on the accuracy of the estimation for direction of arrival (DOA) of transponder signals, the systempsilas performance may deteriorate in electromagnetically noisy or cluttered environments. In this paper, the features of the system are improved to accommodate such situations. The error correction algorithm is incorporated to provide a robust estimation of DOA, and sonar data are fused to characterize the environment. To verify the validity of the proposed system, we perform simulations and experiments of mobile robot docking in a real environment populated with stationary and movable obstacles.",Fusion of direction sensing RFID and sonar for mobile robot docking
"['Tuncer I. ??ren', 'Bernard P. Zeigler']","AW Wymore, the founder of the world's first systems engineering department at the University of Arizona, has been at the origin of the system theoretic foundations of modeling and simulation. Wymore's intellectual family tree, which goes back to Gauss and Weierstrass, is given. How the authors met, cooperated, and advocated system theory for the advancement of modeling and simulation are explained. The concept of model-based simulation was also one of the outcomes of this cooperation. This article reviews the emergence of systems-theory-based modeling and simulation languages and environments, such as the General System Theory implementor and Discrete Event System Specification, and their relation to Wymore's concepts. We also discuss the application of powerful software development frameworks to support user-friendly access to systems concepts and to increase the power to support systems design and engineering.",System theoretic foundations of modeling and simulation: a historic perspective and the legacy of A Wayne Wymore
"['Alejandro Jaimes', 'John R. Smith']","In this paper we investigate semi-automatic construction of multimedia ontologies using a data-driven approach. We start with a collection of videos for which we wish to build an ontology (an explicit specification of a domain). Each video is pre-processed: scene cut detection, automatic speech recognition (ASR), and metadata extraction are performed. In addition we automatically index the videos based on visual content by extracting syntactic (e.g., color, texture, etc.) and semantic features (e.g., face, landscape, etc.). We then combine standard tools for ontology engineering and tools in content-based retrieval to semi-automatically build ontologies. In the first stage we process the text information available with the videos (ASR, metadata, and annotations, if any). Stop words (e.g., a, on, the) are eliminated and statistics (e.g., frequency, TFIDF, and entropy) are computed for all terms. Based on this data we manually select concepts and relationships to include in the ontology. Then we use content-based retrieval tools to assign multimedia entities (e.g., shots, videos, collections of videos) to concepts, properties, or relationships in the ontology, and to select multimedia entities as concepts, relationships, or properties in the ontology. We explore this methodology to construct multimedia ontologies from 24 hours of educational films from the 1940s-1960s used in the TREC video retrieval benchmark and discuss the problems encountered and future directions.","Semi-automatic, data-driven construction of multimedia ontologies"
"['Caaliph Andriamisaina', 'Emmanuel Casseau', 'Philippe Coussy']","In this paper, we propose a design methodology for implementing a multimode (or multi-configuration) and multi-throughput system into a single hardware architecture. The inputs of the design flow are the data flow graphs (DFGs), representing the different modes (i.e. the different applications to be implemented), with their respective throughput constraints. While traditional approaches merge DFGs together before the synthesis process, we propose to use ad-hoc scheduling and binding steps during the synthesis of each DFG. The scheduling, which assigns operations to specific time steps, maximizes the similarity between the control steps and thus decreases the controller complexity. The binding process, which assigns operations to specific functional units and data to specific storage elements, maximizes the similarity between datapaths and thus minimizes steering logic and register overhead. First results show the interest of the proposed synthesis flow.",Synthesis of Multimode digital signal processing systems
"['Jason Westmacott', 'Peter Lozo', 'Lakhmi C. Jain']",This paper presents an extension to the SAART (selective-attention adaptive resonance theory) neural network to allow distortion invariance. This extended SAART model is now capable of recognising distorted 2D shapes in cluttered environments. This new capability is demonstrated using simulations.,Distortion invariant selective attention adaptive resonance theory neural network
"['Hang Long', 'Meiying Wei', 'Ruchen Duan', 'Kan Zheng', 'Wenbo Wang']","The unicasting mode is a natural choice in multi-relay systems to utilize the spatial potential of source and destination nodes. In the unicasting phase, the source node transmits different data streams to several relay nodes with the downlink multi-user precoding; in the multi-accessing phase, relay nodes transmit the received data streams to the destination node simultaneously. In this paper, the received signal-to-noise-ratio (SNR) at the destination node of the unicasting mode is analyzed and derived, which can be calculated with two SNRs related to the two phases as an approximate harmonic mean function. Then, an approximate method for calculating the system ergodic capacity is presented. The inner products between singular vectors are approximately replaced by their expectations and distribution functions of singular values are derived. The numerical and simulation results demonstrate the efficiency of the analysis and the approximate method. Copyright ?? 2010 John Wiley & Sons, Ltd.#R##N##R##N#(The received signal-to-noise-ratio (SNR) at the destination node of the unicasting mode in multi-relay systems is analyzed and derived, which can be calculated with two SNRs related to the two phases as an approximate harmonic mean function. Then, an approximate method for calculating the system ergodic capacity is presented. The inner products between singular vectors are approximately replaced by their expectations and distribution functions of singular values are derived.)",Performance analysis of the unicasting mode in multi-relay systems
"['Hanan Saleet', 'Rami Langar', 'Otman A. Basir', 'Raouf Boutaba']","One of the major challenges for vehicular ad hoc networks (VANETs) is related to efficient location management issue. In this paper, we propose a new region-based location service management protocol (RLSMP) that uses mobility patterns as means to synthesize vehicle movement and thus can be used in VANETs applications. One of the key distinguishing features of our solution from existing literature is its scalability since it uses message aggregation in both updating and querying, and promises locality awareness as well as minimum signaling overhead. To evaluate the efficiency of our proposal, we compare our scheme with existing solutions using both analytical and simulation approaches. To achieve this, we develop analytical models to evaluate the location updates cost. Numerical and simulation results show that our protocol scales better than existing schemes, when increasing the size of VANET which enhances the feasibility of such large scale ad hoc networks.",Proposal and Analysis of Region-Based Location Service Management Protocol for VANETs
"['Nicol?˝ Cesa-Bianchi', 'Pierre Gaillard', 'G?≠bor Lugosi', 'Gilles Stoltz']","Mirror descent with an entropic regularizer is known to achieve shifting regret bounds that are logarithmic in the dimension. This is done using either a carefully designed projection or by a weight sharing technique. Via a novel unified analysis, we show that these two approaches deliver essentially equivalent bounds on a notion of regret generalizing shifting, adaptive, discounted, and other related regrets. Our analysis also captures and extends the generalized weight sharing technique of Bousquet and Warmuth, and can be refined in several ways, including improvements for small losses and adaptive tuning of parameters.",Mirror Descent Meets Fixed Share (and feels no regret)
"['Rafael Toledo-Moreo', 'Miguel Angel Zamora-Izquierdo', 'B. Ubeda-Miarro', 'Antonio Fernandez G??mez-Skarmeta']","User requirements for the performance of Global Navigation Satellite System (GNSS)-based road applications have been significantly increasing in recent years. Safety systems based on vehicle localization, electronic fee-collection systems, and traveler information services are just a few examples of interesting applications requiring onboard equipment (OBE) capable of offering a high available accurate position, even in unfriendly environments with low satellite visibility such as built-up areas or tunnels and at low cost. In addition to that, users and service providers demand from the OBEs not only accurate continuous positioning but integrity information of the reliability of this position as well. Specifically, in life-critical applications, high-integrity monitored positioning is absolutely required. This paper presents a solution based on the fusion of GNSS and inertial sensors (a Global Positioning System/satellite-based augmentation system/inertial navigation system integrated system) running an extended Kalman filter combined with an interactive multimodel method (IMM-EKF). The solution developed in this paper supplies continuous positioning in marketable conditions and a meaningful trust level of the given solution. A set of tests performed in controlled and real scenarios proves the suitability of the proposed IMM-EKF implementation as compared with low-cost GNSS-based solutions, dead reckoning systems, single-model EKF, and other filtering approaches of the current literature.",High-Integrity IMM-EKF-Based Road Vehicle Navigation With Low-Cost GPS/SBAS/INS
"['Anna Evlogimenou', 'Raouf Boutaba']","Until now, proposals for IP accounting management have not exploited the power of programmable networks. Thus, real-time charging and pricing schemes are still regarded as impractical, because the accounting management tasks are executed only at the edges. We introduce the notion of programmable accounting management which is more flexible, efficient and scalable. Specifically, we propose real-time charging and pricing mechanisms using programmable networks. Moreover, we propose a novel programmable accounting architecture for QoS-enabled virtual private networks (VPNs). This architecture gives the flexibility to the providers to employ real-time accounting, and offers an open accounting interface inside the network nodes. This entails that also the VPN subscribers can execute accounting tasks inside the network nodes. For example, they can keep their providers under surveillance, or they can apply their own accounting policies for their users.",Programmable accounting management for virtual private networks
"['Oier Lopez de Lacalle', 'Eneko Agirre']","Word Sense Disambiguation has been stuck for many years. In this paper we explore the use of large-scale crowdsourcing to cluster senses that are often confused by non-expert annotators. We show that we can increase performance at will: our in-domain experiment involving 45 highly polysemous nouns, verbs and adjective (9.8 senses on average), yields an average accuracy of 92.6 using a supervised classifier for an average polysemy of 6.1. Our proposal has the advantage of being cost-effective and being able to produce different levels of granularity. Our analysis shows that the error reduction with respect to finegrained senses is higher, and manual inspection show that the clusters are sensible when compared to those of OntoNotes and WordNet Supersenses.",A Methodology for Word Sense Disambiguation at 90% based on large-scale CrowdSourcing
"['El-Hadi Djermoune', 'Marc Tomczak']","We present a study of mode variance statistics for three SVD-based estimation methods in the case of a single-mode damped exponential. The methods considered are namely Kumaresan-Tufts, matrix pencil and Kung's direct data approximation. Through first-order perturbation analysis, we derive closed-form expressions of the variance of the complex mode, frequency and damping factor estimates. These expressions are used to compare the different methods and to determine the optimal prediction order for matrix pencil and direct data approximation methods. Application to the undamped case shows the coherence of the results with those already stated in the literature. It is also found that the variances converge linearly towards the Cramer-Rao bound. Finally, the theoretical results are verified using Monte Carlo simulations.",Perturbation Analysis of Subspace-Based Methods in Estimating a Damped Complex Exponential
"['Dirk Brockmann', 'Theo Geisel']","We present a phenomenological model for the generation of human visual scanpaths. Successions of saccadic eye movements are treated as realizations of a stochastic jump process in a random quenched salience ""eld. E$ciency of the process is de""ned in terms of convergence properties of the time-dependent probability of ""xating a region in the visual environment. Based on the assumption that the visual system minimizes the typical time needed to process a visual scene, our theory predicts that scanpaths are geometrically similar to a prominent class of random walks known as LeH vy #ights. The theory is well con""rmed by psychophysical experiments. ( 2000 Elsevier Science B.V. All rights reserved.",The ecology of gaze shifts
"['Yannis Drougas', 'Vana Kalogeraki']","In today's world, stream processing systems have become important, as applications like media broadcasting, sensor network monitoring and on-line data analysis increasingly rely on real-time stream processing. In this paper, we propose a distributed stream processing system that composes stream processing applications dynamically, while meeting their rate demands. Our system consists of the following components: (1) a distributed component discovery algorithm that discovers components available at nodes on demand, (2) resource monitoring techniques to maintain current resource availability information, (3) a scheduling algorithm that schedules application execution, and (4) a minimum cost composition algorithm that composes applications dynamically based on component and resource availability and scheduling demands. Our detailed experimental results, over the PlanetLab testbed, demonstrate the performance and efficiency of our approach.",RASC: Dynamic Rate Allocation for Distributed Stream Processing Applications
"['Jungwon Lee', 'Dimitris Toumpakaris', 'Edward W. Jang', 'Hui-Ling Lou']","The implementation of decision feedback equalization-based receivers for MIMO systems that employ hybrid ARQ (HARQ) is considered. It is shown how the theory of MIMO decision feedback equalization can be applied to derive optimal DFE architectures for the case of MIMO HARQ. Incremental structures are proposed that reduce the implementation complexity of the DFE and the memory requirements at the receiver without loss in performance. Moreover, sub-optimal, post-equalization combining architectures are examined that simplify the receiver at the cost of performance loss.",DFE-Based Receiver Implementation for MIMO Systems Employing Hybrid ARQ
"['Sargur N. Srihari', 'Sung-Hyuk Cha', 'Sangjik Lee']","We undertook a study to objectively validate the hypothesis that handwriting is individualistic. Handwriting samples of one thousand five hundred individuals, representative of the US population with respect to gender age, ethnic groups, etc., were obtained. Analyzing differences in handwriting was done by using computer algorithms for extracting features from scanned images of handwriting. Attributes characteristic of the handwriting were obtained, e.g., line separation, slant, character shapes, etc. These attributes, which are a subset of attributes used by expert document examiners, were used to quantitatively establish individuality by using machine learning approaches. Using global attributes of handwriting and very few characters in the writing, the ability to determine the writer with a high degree of confidence was established. The work is a step towards providing scientific support for admitting handwriting evidence in court. The mathematical approach and the resulting software also have the promise of aiding the expert document examiner.",Establishing handwriting individuality using pattern recognition techniques
"['Michael Shalmon', 'Michael A. Kaplan']","We present a complete analysis of delays for a tandem network of queues with deterministic service and multiple, interfering sources. The model is of a packet-based data collection system consisting of some finite, but arbitrary, number of stations connected in tandem by a unidirectional, asynchronous transmission network. Packets, or, more generally, tasks enter the system at every station, are handed from station to station in store-and-forward fashion, and exit at the downstream end; there are no intermediate departures. The stations are provided with infinite storage, and the lines between them operate concurrently and asynchronously. Intermediate sources are Poisson; the source at the head of the network can be somewhat more general than Poisson. Tasks from each source wait at each station downstream of their point of origin. We calculate the joint steady-state moment-generating functions for these waiting times, provided that the line capacities do not increase in the direction of flow; the solution contains as a special case the steady-state moment-generating function for end-to-end delay for each source.",A Tandem Network of Queues with Deterministic Service and Intermediate Arrivals
"['Jae-Kark Choi', 'Nan Hao', 'Sang-Jo Yoo']","For handover in IEEE 802.16e, scanning procedure by mobile station is defined to find the most reliable target base station. However, the conventional scanning schemes cannot guarantee a fast target base station decision because some of the neighbor base stations of the scanning list provided by a serving base station are not actually attachable by the mobile stations. In this paper, we propose a new group-based scanning scheme, in which grouping of mobile stations by signal strength reduces the number of channels to scan so that fast scanning is achieved. To enhance the performance of the group-based scanning scheme, a dynamic neighbor base station list (DNL) management method is proposed. Simulation results show that our proposed scanning scheme is more efficient than the conventional one in terms of scanning time and accuracy of scanning channel list.",Fast group scanning scheme with dynamic neighbor base station list in IEEE 802.16e networks
"['Samar Agnihotri', 'H. S. Jamadagni', 'Pavan Nuggehalli']","We provide a new unified framework, called ""multiple correlated informants - single recipient"" communication, to address the variations of the traditional Distributed Source Coding (DSC) problem. Different combinations of the assumptions about the communication scenarios and the objectives of communication result in different variations of the DSC problem. For each of these variations, the complexities of communication and computation of the optimal solution is determined by the combination of the underlying assumptions. In the proposed framework, we address the asymmetric, interactive, and lossless variant of the DSC problem, with various objectives of communication and provide optimal solutions for those. Also, we consider both, the worst-case and average-case scenarios.",Interactive Distributed Source Coding in Asymmetric Communication Scenarios
"['Kostyantyn M. Shchekotykhin', 'Dietmar Jannach', 'Gerhard Friedrich']","Web mining systems exploit the redundancy of data published on the Web to automatically extract information from existing Web documents. The first step in the Information Extraction process is thus to locate as many Web pages as possible that contain relevant information within a limited period of time, a task which is commonly accomplished by applying focused crawling techniques. The performance of such a crawler can be measured by its É??recallÉ?ù, i.e., the percentage of documents found and identified as relevant compared to the total number of existing documents. A higher recall value implies that more redundant data are available, which in turn leads to better results in the subsequent fact extraction phase of the Web mining process. In this paper, we propose xCrawl, a new focused crawling method which outperforms state-of-the-art approaches with respect to the recall values achievable within a given period of time. This method is based on a new combination of ideas and techniques used to identify and exploit the navigational structures of Web sites, such as hierarchies, lists, or maps. In addition, automatic query generation is applied to rapidly collect Web sources containing target documents. The proposed crawling technique was inspired by the requirements of a Web mining system developed to extract product and service descriptions given in tabular form and was evaluated in different application scenarios. Comparisons with existing focused crawling techniques reveal that the new crawling method leads to a significant increase in recall while maintaining precision.",xCrawl: a high-recall crawling method for Web mining
['Simon L. Peyton Jones'],"User-defined data types, pattern-matching, and recursion are ubiquitous features of Haskell programs. Sometimes a function is called with arguments that are statically known to be in constructor form, so that the work of pattern-matching is wasted. Even worse, the argument is sometimes freshly-allocated, only to be immediately decomposed by the function.   In this paper we describe a simple, modular transformation that specialises recursive functions according to their argument ""shapes"". We describe our implementation of this transformation in the Glasgow Haskell Compiler, and give measurements that demonstrate substantial performance improvements: a worthwhile 10% on average, with a factor of 10 in particular cases.",Call-pattern specialisation for Haskell programs
"['Rodrigo M. Santos', 'Sergio F. Ochoa']","VHF radio systems commonly used to support search and rescue activities after disasters limit the flow of information among response teams deployed in the field. It generates islands of information that jeopardizes the coordination and effectiveness of the response activities. This article proposes a communication model that uses opportunistic networks and real-time messages delivery to help address such limitation. Since the communication model is computable, it is possible to diagnose the flow of information expected for a particular work scenario. The diagnose results allow identifying elements that could help improve the information flow in such scenario. This proposal allows first responders to address the stated problem during the phases of preparedness, response and recovery from a disaster.",Disseminating shared information in disaster relief efforts: A communication computable model
"['Salman Abdul Baset', 'Henning Schulzrinne']","The presence of restrictive network address translators (NATs) and firewalls prevent nodes from directly exchanging packets and thereby pose a problem for peer-to-peer (p2p) communication systems. Skype, a popular p2p VoIP application, addresses this problem by using another Skype client (relay) with unrestricted connectivity to relay the signaling and media traffic between session endpoints. This distributed technique for addressing connectivity issues raises challenging questions about the reliability and latency of relayed calls, relay selection techniques, and the interference of relayed calls with the applications running on relays -- a phenomena we refer to as user annoyance.   We devise a framework to analyze reliability in peer-to-peer communication systems and present a simple model to estimate the number of relays needed for maintaining the desired reliability for the media sessions. We then analyze two techniques for improving the reliability of relayed calls. We present a distributed relay selection technique that leverages a two level hierarchical p2p network to find a relay in  O (1) hop. We augment our distributed relay selection technique to find a relay that minimizes call latency and user annoyance. Our results indicate that for Skype node lifetimes, at least three relays are needed to achieve a 99.9% success rate for call duration of 60 mins (95  th   percentile of Skype call durations).",Reliability and relay selection in peer-to-peer communication systems
"['Timothy Bourke', 'Marc Pouzet']","Zelus is a new programming language for modeling systems that mix discrete logical time and continuous time behaviors. From a user's perspective, its main originality is to extend an existing Lustre-like synchronous language with Ordinary Differential Equations (ODEs). The extension is conservative: any synchronous program expressed as data-flow equations and hierarchical automata can be composed arbitrarily with ODEs in the same source code. A dedicated type system and causality analysis ensure that all discrete changes are aligned with zero-crossing events so that no side effects or discontinuities occur during integration. Programs are statically scheduled and translated into sequential code that, by construction, runs in bounded time and space. Compilation is effected by source-to-source translation into a small synchronous subset which is processed by a standard synchronous compiler architecture. The resultant code is paired with an off-the-shelf numeric solver.   We show that it is possible to build a modeler for explicit hybrid systems a la Simulink/Stateflow on top of an existing synchronous language, using it both as a semantic basis and as a target for code generation.",Z??lus: a synchronous language with ODEs
"['Vasileios Mezaris', 'Ioannis Kompatsiaris', 'Michael G. Strintzis']","In this paper, a color image segmentation al- gorithm and an approach to large-format image segmen- tation are presented, both focused on breaking down im- ages to semantic objects for object-based multimedia ap- plications. The proposed color image segmentation algo- rithm performs the segmentation in the combined intensity- texture-position feature space in order to produce con- nected regions that correspond to the real-life objects shown in the image. A preprocessing stage of conditional image fil- tering and a modified K-Means-with-connectivity-constraint pixel classification algorithm are used to allow for seamless integration of the different pixel features. Unsupervised op- eration of the segmentation algorithm is enabled by means of an initial clustering procedure. The large-format image seg- mentation scheme employs the aforementioned segmenta- tion algorithm, providing an elegant framework for the fast segmentation of relatively large images. In this framework, the segmentation algorithm is applied to reduced versions of the original images, in order to speed-up the completion of the segmentation, resulting in a coarse-grained segmen- tation mask. The final fine-grained segmentation mask is produced with partial reclassification of the pixels of the original image to the already formed regions, using a Bayes classifier. As shown by experimental evaluation, this novel scheme provides fast segmentation with high perceptual seg- mentation quality.",Still Image Segmentation Tools for Object-based Multimedia Applications
['Alexander van Deursen'],"a b s t r a c t Purpose: Despite the amount of health information available online, there are several barriers that limit the Internet from being adopted as a source of health information. The purpose of this study was to identify individual skill-related problems that users experience when",Internet skill-related problems in accessing online health information
"['Mutlu Avci', 'Tulay Yildirim']","Pass transistor logic (PTL) circuits are known for their smaller silicon area usage, low power consumption and reduced delay advantages. The 123 decision diagram is a very effective PTL synthesis tool based on binary decisions. It realizes a logic function using NMOS pass transistors with CMOS restoring buffers. At the same time, layout of the circuit for a two metal process is obtained by this diagram. A special coding is required to apply the 123 decision diagram. Until now, the coding for the diagram has not been explained. In this paper, a very easy and effective coding method for circuit synthesis and simplification with 123 decision diagrams is proposed.",A coding method for 123 decision diagram pass transistor logic circuit synthesis
"['Yang Guo', 'Kyoungwon Suh', 'James F. Kurose', 'Donald F. Towsley']","Providing on-demand video streaming service over the Internet is a challenging task. In this paper, we propose DirectStream, a directory based peer-to-peer video streaming service that efficiently and cost-effectively provides video on-demand service with VCR operation support. We analytically and experimentally examine the system performance, and show that the proposed scheme can significantly reduce the workload posed on the server, and that it scales extremely well as the popularity of the video increases even if participating clients behave non-cooperatively. We propose a QoS parent selection algorithm to construct the appropriate peer-to-peer networks, and discuss how to provide continuous playback in the face of clients' early departures. Our study suggests that peer-to-peer networking is a promising technique to address scalability in on-demand streaming service.",A peer-to-peer on-demand streaming service and its performance evaluation
"['Thomas Feyer', 'Klaus-Dieter Schewe', 'Bernhard Thalheim']","Due to the development of the internet and cable nets information services are going to be widely used. On the basis of projects for the development of information services like regional information services or shopping services we develop a method for the creation of information systems and services to be used through different nets. The approach is based on the codesign of data, dialogues and presentations. The main concepts are information units and information containers. These basic concepts for information services are presented in this paper. Information units are defined by generalized views with enabled functions for retrieving, summarizing and restructuring of information. Information containers are used for transfering information to users according to their current needs and the corresponding dialogue step. Size and presentation of information containers depend on the restrictions of the users environment.",Conceptual Design and Development of Information Services
"['Dongjun Lee', 'Mark W. Spong']","We propose a novel control framework for bilateral teleoperation of a pair of multi-degree-of-freedom nonlinear robotic systems under constant communication delays. The proposed framework uses the simple proportional-derivative control, i.e., the master and slave robots are directly connected via spring and damper over the delayed communication channels. Using the controller passivity concept, the Lyapunov-Krasovskii technique, and Parseval's identity, we can passify the combination of the delayed communication and control blocks altogether robustly, as long as the delays are finite constants and an upper bound for the round-trip delay is known. Having explicit position feedback through the delayed P-action, the proposed framework enforces master-slave position coordination, which is often compromised in the conventional scattering-based teleoperation. The proposed control framework provides humans with extended physiological proprioception, so that s/he can affect and sense the remote slave environments mainly relying on her/his musculoskeletal systems. Simulation and experiments are performed to validate and highlight properties of the proposed control framework",Passive Bilateral Teleoperation With Constant Time Delay
"['Sheldon X.-D. Tan', 'Weikun Guo', 'Zhenyu Qi']","This paper provides a novel approach to exact symbolic analysis of very large analog circuits. The new method is based on determinant decision diagrams (DDDs) to represent symbolic product terms. But instead of constructing DDD graphs directly from a flat circuit matrix, the new method constructs DDD graphs in a hierarchical way based on hierarchically defined circuit structures. The resulting algorithm can analyze much larger analog circuits exactly than before. Theoretically, we show that exact symbolic expressions of a circuit are cancellation-free expressions when the circuit is analyzed hierarchically. Practically we propose a novel hierarchical DDD graph construction algorithm. Our experimental results show that very large analog circuits, which can't be analyzed exactly before like ?¨A725 and other unstructured circuits up to 100 nodes, can be analyzed by the new approach for the first time. The new approach significantly improves the exact symbolic capacity and promises huge potentials for the new applications of symbolic analysis in analog circuit design automation.",Hierarchical approach to exact symbolic analysis of large analog circuits
"['Flavia Cruz', 'Fernanda Araujo Bai?úo', 'Marta Mattoso', 'Gerson Zaverucha']","The performance of applications on Object Oriented Database Ma-nagement Systems (OODBs) is strongly affected by Distribution Design, which reduces irrelevant data accessed by applications and data exchange among sites. In an OO environment, the Distributed Design is a complex task, and an open research problem. In this work, we present a knowledge-based approach for the vertical fragmentation phase of the distributed design of object-oriented databases. In this approach, we show a Prolog implementation of a vertical fragmentation algorithm, and describe how it can be used as background knowledge for a knowledge discovery/revision process through In-ductive Logic Programming (ILP). The objective of the work is to extend our framework proposed to handle the class fragmentation problem, showing the viability of automatically improving the vertical fragmentation algorithm to produce more efficient fragmentation schemas, using a theory revision system. We do not intend to propose the best vertical fragmentation algorithm. We concentrate here on the process of revising a vertical fragmentation algorithm through knowledge discovery techniques, rather than only obtaining a final optimal algorithm.",Towards a Theory Revision Approach for the Vertical Fragmentation of Object Oriented Databases
"['Kyung Sup Kwak', 'Kwang Jae Lim']","Random multiple access and reservation multiple-access protocols which are currently employed in most satellite communication systems have some merits and demerits according to the type of users and traffic. A modified packet demand assignment multiple access (PDAMA) protocol with better characteristics is proposed and analyzed. Using simulation with approximate analysis, the performance of the modified PDAMA is compared with that of a modified random-access protocol as well as the original PDAMA in terms of delay and throughput. For small packet arrival rate, the performance of the proposed access protocol is shown to be similar to that of the modified random-access protocol, and better than that of the PDAMA protocol. While the packet arrival rate increases, the performance of the proposed access protocol is improved greatly compared with that of the PDAMA protocol. >",A modified PDAMA protocol for mobile satellite communication systems
"['Wenxin Liu', 'Jagannathan Sarangapani', 'Ganesh K. Venayagamoorthy', 'Donald C. Wunsch', 'David A. Cartes']",This paper presents a neural network (NN) based decentralized excitation controller design for large scale power systems. The proposed controller design considers not only the dynamics of generators but also the algebraic constraints of the power flow equations. The control signals are calculated using only local signals. The transient stability and the coordination of the subsystem controllers can be guaranteed. NNs are used to approximate the unknown/imprecise dynamics of the local power system and the interconnections. All signals in the closed loop system are guaranteed to be uniformly ultimately bounded (UUB). Simulation results with a 3-machine power system demonstrate the effectiveness of the proposed controller design.,Neural Network based Decentralized Excitation Control of Large Scale Power Systems
"['Linrui Li', 'Shu Wang']","In this paper we prove the inviscid limit for the 2D non-dissipative quasi-geostrophic equations, and study the global existence on the weak solution. Our proofs are based on the vanishing viscosity method.",On the Inviscid Limit for the 2D Non-dissipative Quasi-geostrophic Equations
"['Marek Domanski', 'Krzysztof Rakowski']","The problem of error accumulation in consecutive compression/decompression cycles of still images is discussed in this article. Considered are predictive and transform techniques together with color transformations. Theoretical analysis is provided for error accumulation in linear transformations followed by quantization. The conditions for lack of error accumulation are discussed. This theoretical analysis is applied to color transformations and compression cycles. Experimental analysis verifies theoretical results and deals with more complicated cases. The experimental analysis includes lossy and near lossless compression as well as color transformations themselves. Moreover, a new technique of near lossless compression with no error accumulation is proposed.",Error accumulation in multiple cycles of still image compression and color transformations
['Stephen M. Smith'],"This paper describes a real-time integrated motion segmentation and 3D reconstruction/interpretation system. The motion segmentation system detects and tracks all moving objects, and removes the corresponding features from the original 2D feature data set. The remaining features are passed on to a 3D reconstruction system. The resulting list of 3D feature data, which hopefully contains useful information about the static part of the world, is then interpreted to give some high level understanding of the environment.",Integrated real-time motion segmentation and 3D interpretation
"['Dirk Basten', 'Ali Sunyaev']",An analysis of 32 accuracy factors yields a list of useful guidelines for improving software development effort estimation accuracy.,Guidelines for Software Development Effort Estimation
"['Anindya Ghosh', 'Xavier Martin', 'Johannes M. Pennings', 'Filippo Carlo Wezel']","Organizations create high-impact inventions when they combine disparate strands of technology in their corporate research and development. We theorize that when undertaking complex inventive search characterized by high breadth, i.e., drawing on multiple diverse technology components, an organization's propensity toward high-impact inventions depends on its stock of experience with recombining such components and on the focus of its inventive search. Building on learning transfer theory, we argue that the complexity and causal ambiguity of higher-breadth projects is such that experience with similar inventive search will be a poorer guide, comparatively reducing their inventive impact; however, this negative effect can be attenuated by the degree of focus of an organization's contemporaneous inventive search. Using a longitudinal data set of patents from the photographic imaging industry, we find support for our predictions.",Ambition Is Nothing Without Focus: Compensating for Negative Transfer of Experience in R&D
"['Wenbo Xu', 'Guoping Zhang', 'Jianxi Huang']","The ability to extract the special land use type of environment, and associated temporal changes, has important societal and economic meaning. This paper uses the high spatial resolution of the image---QuickBird to extract greenhouse in agriculture of Hexian region, ANHUI province in China. The paper uses software package eCognition to process data and extract information. The software adopted object-oriented image segmentation and classification which is based on fuzzy logic. In this study the greenhouse is a special land cover type in agricultural land, we use not only image object's attributes, but also the relationship between networked image objects; it can perform sophisticated classification and get satisfied classification result, allows the integration of a broad spectrum of different object features, such as spectral values, shape and texture. The aim of this work was to develop an object-oriented segmentation and classification approach for extracting special land cover type.",An Object-Oriented Approach of Extracting Special Land use Classification by using Quick Bird Image
"['Qian Zhang', 'Zhu Ji', 'Wenwu Zhu', 'Ya-Qin Zhang']","Video communication over wireless links using handheld devices is a challenging task due to the time-varying characteristics of the wireless channels and limited battery resources. Rate-distortion (RD) analysis plays a key role in video coding and communication systems, and usually the RD relation does not assume any power constraint. We investigate the relations of rate, distortion and power consumption. Based on those relations, we propose a power-minimized bit-allocation scheme considering the processing power, for source coding and channel coding, jointly with the transmission power. The total bits are allocated between source and channel coders, according to wireless channel conditions and video quality requirements, to minimize the total power consumption for a single user and a group of users in a cell, respectively. Simulation results show that our proposed joint power-control and bit-allocation scheme achieves high power savings compared to the conventional scheme.",Power-minimized bit allocation for video communication over wireless channels
"['Sarang Lakare', 'Arie E. Kaufman']","OpenVL is a modular, extensible, and high performance library for handling volumetric datasets. It provides a standard, uniform, and easy to use API for accessing volumetric data. It allows the volumetric data to be laid out in different ways to optimize memory usage and speed. It supports reading/writing of volumetric data from/to files in different formats using plugins. It provides a framework for implementing various algorithms as plugins that can be easily incorporated into user applications. The plugins are implemented as shared libraries which can be dynamically loaded as needed. OpenVL is developed openly and is a free software available on the web.",OpenVL: the open volume library
"['Tyler H. Summers', 'Iman Shames', 'John Lygeros', 'Florian Dorfler']","We consider a network topology design problem in which an initial undirected graph underlying the network is given and the objective is to select a set of edges to add to the graph to optimize the coherence of the resulting network. We show that network coherence is a submodular function of the network topology. As a consequence, a simple greedy algorithm is guaranteed to produce near optimal edge set selections. We also show that fast rank one updates of the Laplacian pseudoinverse using generalizations of the Sherman-Morrison formula and an accelerated variant of the greedy algorithm can speed up the algorithm by several orders of magnitude in practice. These allow our algorithms to scale to network sizes far beyond those that can be handled by convex relaxation heuristics.",Topology design for optimal network coherence
"['Riccardo Petrolo', 'Valeria Loscri', 'Nathalie Mitton']","Smart City represents one of the most promising, prominent and challenging Internet of Things (IoT) applications. In the last few years, indeed, the smart city concept has played an important role in academic and industry fields, with the development and deployment of various middleware platforms and IoT-based infrastructures. However, this expansion has followed distinct approaches creating, therefore, a fragmented scenario, in which different IoT ecosystems are not able to communicate between them. To fill this gap, there is a need to re-visit the smart city IoT semantic and offer a global common approach. To this purpose, this paper browses the semantic annotation of the sensors in the cloud, and innovative services can be implemented and considered by bridging Cloud and Internet of Things. Things-like semantic will be considered to perform the aggregation of heterogeneous resources by defining the Cloud of Things (CoT) paradigm. We survey the smart city vision, providing information on the main requirements and highlighting the benefits of integrating different IoT ecosystems within the cloud under this new CoT vision. This paper also discusses relevant challenges in this research area.","Towards a smart city based on cloud of things, a survey on the smart city vision and paradigms"
"['Xiaolong Jin', 'Jiming Liu']","In this paper, we study the efficiency of emergent constraint satisfaction in small-world and random agent networks. We find that emergent constraint satisfaction in a small-world network is less efficient than in some other networks (e.g., regular networks). Further, we find that this finding holds in almost all random networks. Based on these observations, we study the relationship between the efficiency of emergent constraint satisfaction and the randomness of the corresponding network from which the constraint satisfaction problem is generated.",Efficiency of emergent constraint satisfaction in small-world and random agent networks
['Demetri Terzopoulos'],"This paper reviews recently developed physics-based surface modeling techniques for geometric design, medical image analysis, and human facial modeling. It briefly motivates the problems of interest in each application area, describes the models that the authors have developed to address them, presents sample results, and provides references to technical papers containing the full details.",Physics-based models for image analysis/synthesis and geometric design
"['Oleg Lobachev', 'Rita Loogen']","This paper discusses the pros and cons of using a functional language for implementing a computer algebra system. The contributions of the paper are twofold. Firstly, we discuss some language---centered design aspects of a computer algebra system -- the ""language unity"" concept. Secondly, we provide an implementation of a fast polynomial multiplication algorithm, which is one of the core elements of a computer algebra system. The goal of the paper is to test the feasibility of an implementation of (some elements of) a computer algebra system in a modern functional language.",Towards an Implementation of a Computer Algebra System in a Functional Language
['Michel Wermelinger'],"Dynamic reconfiguration is the ability to modify a parallel or distributed system while it is running. We adopt the framework developed by Kramer et al. (1985) at the system architecture level: changes must occur in a consistent state, which is brought about by ""freezing"" some system components. The goal is to reduce system disruption, i.e., to minimize: the part of the system to be ""frozen""; and the time taken by reconfiguration operations. Towards the first goal we take a connection based approach instead of a component based one. To reduce time, we refine the reconfiguration algorithm by executing changes in parallel as much as possible. Our model also handles hierarchic systems.",A hierarchic architecture model for dynamic reconfiguration
"['Rico Sennrich', 'Beat Kunz']",,Zmorge: A German Morphological Lexicon Extracted from Wiktionary
"['Mark S. Avnet', 'Annalisa L. Weigel']","OBJECTIVE: We propose a methodology for analyzing shared knowledge in engineering design teams. BACKGROUND: Whereas prior work has focused on shared knowledge in small teams at a specific point in time, the model presented here is both scalable and dynamic. METHOD: By quantifying team members' common views of design drivers, we build a network of shared mental models to reveal the structure of shared knowledge at a snapshot in time. Based on a structural comparison of networks at different points in time, a metric of change in shared knowledge is computed. RESULTS: Analysis of survey data from 12 conceptual space mission design sessions reveals a correlation between change in shared knowledge and each of several system attributes, including system development time, system mass, and technological maturity. CONCLUSION: From these results, we conclude that an early period of learning and consensus building could be beneficial to the design of engineered systems. APPLICATION: Although we do not examine team performance directly, we demonstrate that shared knowledge is related to the technical design and thus provide a foundation for improving design products by incorporating the knowledge and thoughts of the engineering design team into the process. Language: en",The Structural Approach to Shared Knowledge An Application to Engineering Design Teams
"['Marcio F. da S. Oliveira', 'Francisco Assis M. do Nascimento', 'Wolfgang Mueller', 'Fl?≠vio Rech Wagner']","In this paper, we present a design space exploration (DSE) method for embedded systems, which represents the design space as a categorical graph product, in order to overcome the challenge of performing multiple DSE activities, such as task mapping, processor allocation, and software binding. Moreover, the method adopts a Model-Driven Engineering (MDE) approach, defining a design space metamodel to represent the categorical graph product and other DSE concepts, such as solutions, costs, and DSE activities. Furthermore, exploiting the MDE approach, we use model-to-model transformation rules to implement the design constraints, which guide and prune the design space. The method is applied to the design of a real-life application, and experiments demonstrate its effectiveness.",Design space abstraction and metamodeling for embedded systems design space exploration
"['Dongrong Xu', 'Bradley S. Peterson']","Warping diffusion tensor (DT) fields accurately is much more complicated than that of conventional scalar images. It requires tensors be reoriented in the space to which the tensors are warped based on both the local deformation field and the orientation of the underlying fibers in the original image. Because DT images contain high dimensional information of both spatial orientation and magnitude, standard warping using backward mapping for regular intensity-based images cannot be applied to warp DT images. Therefore, all existing algorithms for warping tensors typically use forward mapping deformations; forward mapping, however, can also create artifacts by failing to define accurately the voxels in the template space where the local deformation is expanding. To overcome this disadvantage, we propose a novel method for the spatial normalization of DT fields that uses a bijection to warp DT datasets from one imaging space to another, without generating artifacts.",Improved Warping of Diffusion Tensor Fields Free of Artifacts
"['Arghya Sarkar', 'Samarjit Sengupta']","A novel digital signal processing algorithm for online estimation of the fundamental frequency of the distorted power system signals is presented. The basic algorithm relies on the development of an efficient variance reduction algorithm; and design of a new stable bandpass infinite impulse response (IIR) second-degree digital integrator (SDDI) with reduced approximation error. Compared with the well-established technique such as the enhanced-phase-locked-loop (EPLL) system, the proposed algorithm provides the following: 1) higher degree of immunity and insensitivity to harmonics and noise and 2) faster response during step frequency change. Structural simplicity, wide range of application, controls over speed and accuracy, and parameter robustness are other salient features of the method. The only limitation as compared with the EPLL system is its slower transient response during step change in signal magnitude. Based on simulation studies, performances of the proposed algorithm at different operating conditions have been presented, and its accuracy and response time have been compared with the EPLL systems.",Bandpass Second-Degree Digital-Integrator-Based Power System Frequency Estimation Under Nonsinusoidal Conditions
"['Bas Verhelst', 'R. Morren', 'Keith Michael Baker']","A test vector format standard can avoid the cumbersome process which is necessary, for instance, to take test data from a simulator to a tester. In the near future iP test view will be added to EDIF. This paper evaluates this new standard both from a software developer's standpoint and from a user's view.",Using EDIF for Transfer of Test Data: Practical Experience
"['Wei-Shi Zheng', 'Jian-Huang Lai', 'Pong Chi Yuen']","This paper addresses the dimension reduction problem in Fisherface for face recognition. When the number of training samples is less than the image dimension (total number of pixels), the within-class scatter matrix (Sw) in linear discriminant analysis (LDA) is singular, and principal component analysis (PCA) is suggested to employ in Fisherface for dimension reduction of Sw so that it becomes nonsingular. The popular method is to select the largest nonzero eigenvalues and the corresponding eigenvectors for LDA. To attenuate the illumination effect, some researchers suggested removing the three eigenvectors with the largest eigenvalues and the performance is improved. However, as far as we know, there is no systematic way to determine which eigenvalues should be used. Along this line, this paper proposes a theorem to interpret why PCA can be used in LDA and an automatic and systematic method to select the eigenvectors to be used in LDA using a genetic algorithm (GA). A GA-PCA is then developed. It is found that some small eigenvectors should also be used as part of the basis for dimension reduction. Using the GA-PCA to reduce the dimension, a GA-Fisher method is designed and developed. Compared with the traditional Fisherface method, the proposed GA-Fisher offers two additional advantages. First, optimal bases for dimensionality reduction are derived from GA-PCA. Second, the computational efficiency of LDA is improved by adding a whitening procedure after dimension reduction. The Face Recognition Technology (FERET) and Carnegie Mellon University Pose, Illumination, and Expression (CMU PIE) databases are used for evaluation. Experimental results show that almost 5% improvement compared with Fisherface can be obtained, and the results are encouraging.",GA-fisher: a new LDA-based face recognition algorithm with selection of principal components
"['Hina A. Khan', 'Marina Drosou', 'Mohamed A. Sharaf']","The explosion of big data emphasizes the need for scalable data diversification, especially for applications based on web, scientific, and business databases. However, achieving effective diversification in a multi-user environment is a rather challenging task due to the inherent high processing costs of current data diversification techniques. In this paper, we address the concurrent diversification of multiple search results using various approximation techniques that provide orders of magnitude reductions in processing cost, while maintaining comparable quality of diversification as compared to sequential methods. Our extensive experimental evaluation shows the scalability exhibited by our proposed methods under various workload settings.",Scalable diversification of multiple search results
"['Ming-yu Chen', 'Michael G. Christel', 'Alexander G. Hauptmann', 'Howard D. Wactlar']","The authors developed an extensible system for video exploitation that puts the user in control to better accommodate novel situations and source material. Visually dense displays of thumbnail imagery in storyboard views are used for shot-based video exploration and retrieval. The user can identify a need for a class of audiovisual detection, adeptly and fluently supply training material for that class, and iteratively evaluate and improve the resulting automatic classification produced via multiple modality active learning and SVM. By iteratively reviewing the output of the classifier and updating the positive and negative training samples with less effort than typical for relevance feedback systems, the user can play an active role in directing the classification process while still needing to truth only a very small percentage of the multimedia data set. Examples are given illustrating the iterative creation of a classifier for a concept of interest to be included in subsequent investigations, and for a concept typically deemed irrelevant to be weeded out in follow-up queries. Filtering and browsing tools making use of existing and iteratively added concepts put the user further in control of the multimedia browsing and retrieval process.",Putting active learning into multimedia applications: dynamic definition and refinement of concept classifiers
"['Fl?≠vio Jorge Ponzoni', 'Clayton Borges da Silva', 'Sandra Benfica dos Santos', 'Ot?≠vio Cristiano Montanher', 'Thiago Batista dos Santos']","Relationships between biophysical parameters and radiometric data have been tested and evaluated by several professionals using empirical and/or physical approaches. Remote sensing data collected from airborne or orbital platforms are, of course, influenced by different factors, such as illumination/observation geometry (data collection geometry), atmospheric effects, etc., rather than by target spectral properties. Besides that, the target topographic positioning actually defines the amount of incident energy, as well as the amount of energy that is reflected toward the sensor. The sum of both data collection geometry and topographic positioning defines the so-called É??local illuminationÉ?ù. The objective of this paper was to evaluate the influence of local illumination on empirical relationships between a biophysical variable (plant area index, PAI) and two vegetation indices calculated from Resourcesat/Linear Imaging Self-Scanner sensor (LISS-3) orbital data. Local illumination was expressed by the cosine factor (Fcos) and calculated from topographic and solar position data at three different dates. The study area was based on a typical Brazilian southeastern forest fragment located in the Augusto Ruschi municipal preservation park dispersed on roughhouse topography. PAI was estimated by hemispherical photographs taken under the forest canopy from sample points arbitrarily dispersed on the forest fragment. Results confirmed a stronger relationship between vegetation indices and local illumination conditions.",Local Illumination Influence on Vegetation Indices and Plant Area Index (PAI) Relationships
['John W. Lockwood'],"An instructional platform has been developed that allows rapid prototype of network packet processing functions in hardware. This platform, called the Field Programmable Port Extender (FPX), enables engineering students to rapidly prototype and implement components for use in an Internet router or firewall. Customized circuits allow networking equipment to increase the throughput and enhance functionality of packet processing operations. On the FPX, custom circuits are implemented as hardware modules. An infrastructure circuit on the FPX interconnects multiple modules and provides a common interface to shared resources. All logic on the FPX is implemented with Field Programmable Gate Arrays (FPGAs). A teaching methodology has been developed which minimizes the learning curve for hardware engineering students that want to develop network modules but are relatively unfamiliar with Asynchronous Transfer Mode (ATM) and/or the Internet Protocol (IP) suite. Library functions have been developed to implement low-level details of the networking protocols. The use of these libraries and the infrastructure logic on the FPX allows the students to focus their effort on the design of their own module and to leverage the work of others.",Platform and methodology for teaching design of hardware modules in Internet routers and firewalls
"['Christoph John', 'Ulrich Schwanecke', 'Holger Regenbrecht']","Enhancing desk-based computer environments with virtual reality technology requires natural interaction support, in particular head and hand tracking. Todays motion capture systems instrument users with intrusive hardware like optical markers or data gloves which limit the perceived realism of interactions with a virtual environment and constrain the free moving space of operators. Our work therefore focuses on the development of fault-tolerant techniques for fast, non-contact 3D hand motion capture, targeted to the application in standard office environments. This paper presents a table-top setup which utilizes vision based volumetric reconstruction and tracking of skin colored objects to integrate the users hands and face into virtual environments. The system is based on off-the-shelf hardware components and satisfies real-time constraints.",Real-time Volumetric Reconstruction and Tracking of Hands and Face as a User Interface for Virtual Environments
['Michael L. Bushnell'],"Ulysses  is a  VLSI CAD  environment which effectively addresses the problems associated with CAD tool integration. Specifically, Ulysses allows the integration of CAD tools into a design automation system, the codification of a design methodology, and the representation of a design space. Ulysses keeps track of the progress of a design and allows exploration of the design space. The environment employs artificial intelligence techniques, functions as an interactive  expert system , and interprets descriptions of design tasks encoded in the  scripts  language. An example of an integrated circuit layout design task is provided. The use of Ulysses in performing this task is discussed in detail.",VLSI CAD Tool Integration Using the Ulysses Environment
"['J. Nitzsche', 'T. van Lessen', 'Frank Leymann']","The Web Service Description Language (WSDL) provides means to describe functional aspects of a service in a service oriented architecture (SOA) based on Web service technology. In contrast to its predecessor (WSDL 1.1), WSDL 2.0 does not define a fixed set of operation types but provides for a generic mechanism to define an operation by means of message exchange patterns (MEPs). In this paper we compare the expressivity of MEPs in general with other work and formalisms in the field of service interaction. Furthermore, we identify new MEPs and extend the template used to define MEPs to allow expressing more complex patterns. We give a refined definition of MEPs based on a detailed discussion and discuss how WSDL and the MEPs in particular can be combined with the choreography approach.",WSDL 2.0 Message Exchange Patterns: Limitations and Opportunities
"['Efstratios Komaitis', 'Efstathios G. Vasiliou', 'Kremmydas Gf', 'Dimitrios G. Georgakopoulos', 'Constantinos A. Georgiou']","This paper describes the development of an automated Flow Injection analyzer for water toxicity assessment. The analyzer is validated by assessing the toxicity of heavy metal (Pb2+, Hg2+ and Cu2+) solutions. One hundred ?¨L of a Vibrio fischeri suspension are injected in a carrier solution containing different heavy metal concentrations. Biosensor cells are mixed with the toxic carrier solution in the mixing coil on the way to the detector. Response registered is % inhibition of biosensor bioluminescence due to heavy metal toxicity in comparison to that resulting by injecting the Vibrio fischeri suspension in deionised water. Carrier solutions of mercury showed higher toxicity than the other heavy metals, whereas all metals show concentration related levels of toxicity. The biosensorÉ??s response to carrier solutions of different pHs was tested. Vibrio fischeriÉ??s bioluminescence is promoted in the pH 5É??10 range. Experiments indicate that the whole cell biosensor, as applied in the automated fluidic system, responds to various toxic solutions.",Development of a Fully Automated Flow Injection Analyzer Implementing Bioluminescent Biosensors for Water Toxicity Assessment
"['Petros Spachos', 'Jieyu Lin', 'Hadi Bannazadeh', 'Alberto Leon-Garcia']","Software-Defined Infrastructure (SDI) provides a unified framework for managing heterogeneous virtualized resources in cloud infrastructures. In this paper, we demonstrate the design of our monitoring system called MonArch that tackles the above challenge in a smart room infrastructure. A realtime wireless ad-hoc sensor network system for carbon dioxide monitoring at a complex indoor environment is supported. The system aims to detect and monitor the level of carbon dioxide on a real-time basis and provide overall air quality alerts timely.",Smart room monitoring through wireless sensor networks in software defined infrastructures
"['Marcelo Cataldo', 'Sangeeth Nambiar']","SUMMARY#R##N##R##N#Geographically dispersed work is a fundamental trend that has shaped software development in the past few decades. Although researchers have shown that such a trend has its costs in terms of product quality, the past empirical work has neglected to consider the multidimensional nature of geographic dispersion and limited attention has been given to the nature of technical coupling in the context of distributed development. The work reported in this paper seeks to achieve a better understanding of how the quality of systems produced by distributed development projects is impacted by the various dimensions of geographic dispersion and by the forms of technical coupling. We examined 189 software projects from a multinational development organization and our results revealed the various dimensions of distribution that had an independent and complementary impact on the software quality. In particular, we found that projects with uneven distributions of developers across locations were more likely to exhibit higher levels of defects than projects with balanced distributions. Our results also showed that logical dependencies among architectural components are significantly more important for projects than syntactic dependencies. Moreover, considering technical coupling as conditional on project boundaries revealed that internal and external dependencies have a significant and independent impact on the software quality. Copyright ?? 2010 John Wiley & Sons, Ltd.",The impact of geographic distribution and the nature of technical coupling on the quality of global software development projects
"['Chengwei Lei', 'Jianhua Ruan']","Motivation: Recent advances in technology have dramatically increased the availability of proteinÉ??protein interaction (PPI) data and stimulated the development of many methods for improving the systems level understanding the cell. However, those efforts have been significantly hindered by the high level of noise, sparseness and highly skewed degree distribution of PPI networks. Here, we present a novel algorithm to reduce the noise present in PPI networks. The key idea of our algorithm is that two proteins sharing some higher-order topological similarities, measured by a novel random walk-based procedure, are likely interacting with each other and may belong to the same protein complex.#R##N##R##N#Results: Applying our algorithm to a yeast PPI network, we found that the edges in the reconstructed network have higher biological relevance than in the original network, assessed by multiple types of information, including gene ontology, gene expression, essentiality, conservation between species and known protein complexes. Comparison with existing methods shows that the network reconstructed by our method has the highest quality. Using two independent graph clustering algorithms, we found that the reconstructed network has resulted in significantly improved prediction accuracy of protein complexes. Furthermore, our method is applicable to PPI networks obtained with different experimental systems, such as affinity purification, yeast two-hybrid (Y2H) and protein-fragment complementation assay (PCA), and evidence shows that the predicted edges are likely bona fide physical interactions. Finally, an application to a human PPI network increased the coverage of the network by at least 100%.#R##N##R##N#Availability: www.cs.utsa.edu/~jruan/RWS/.#R##N##R##N#Contact: Jianhua.Ruan@utsa.edu#R##N##R##N#Supplementary information: Supplementary data are available at Bioinformatics online.",A novel link prediction algorithm for reconstructing proteinÉ??protein interaction networks by topological similarity
"['Yan Wang', 'Chuanjiang He']","We propose a novel image segmentation algorithm using piecewise smooth (PS) approximation to image. The proposed algorithm is inspired by four well-known active contour models, i.e., Chan and VeseÉ?? piecewise constant (PC)/smooth models, the region-scalable fitting model, and the local image fitting model. The four models share the same algorithm structure to find a PC/smooth approximation to the original image; the main difference is how to define the energy functional to be minimized and the PC/smooth function. In this article, pursuing the same idea we introduce different energy functional and PS function to search for the optimal PS approximation of the original image. The initial function with our model can be chosen as a constant function, which implies that the proposed algorithm is robust to initialization or even free of manual initialization. Experiments show that the proposed algorithm is very appropriate for a wider range of images, including images with intensity inhomogeneity and infrared ship images with low contrast and complex background.",Image segmentation algorithm by piecewise smooth approximation
['Ulrich Meyer'],"We study the average-case running-time of single-source shortest-path (SSSP) algorithms assuming arbitrary directed graphs with n nodes, m edges, and independent random edge weights uniformly distributed in [0,1]. We give the first label-setting and label-correcting algorithms that run in linear time O(n + m) on the average. In fact, the result for the label-setting version is even obtained for dependent edge weights. In case of independence, however, the linear-time bound holds with high probability, too.Furthermore, we propose a general method to construct graphs with random edge weights that cause large expected running times when input to many traditional SSSP algorithms. We use our method to prove lower bounds on the average-case complexity of the following algorithms: the ""Bellman-Ford algorithm"" [R. Bellman, Quart. Appl. Math. 16 (1958) 87-90, L.R. Ford, D.R. Fulkerson, 1963], ""Pallottino's Incremental Graph algorithm"" [S. Pallottino, Networks 14 (1984) 257-267], the ""Threshold approach"" [F. Glover, R. Glover, D. Klingman, Networks 14 (1984) 23-37, F. Glover, D. Klingman, N. Phillips, Oper. Res. 33 (1985) 65-73, F. Glover, D. Klingman, N. Phillips, R.F. Schneider, Management Sci. 31 (1985) 1106-1128], the ""Topological Ordering SSSP algorithm"" [A.V. Goldberg, T. Radzik, Appl. Math. Lett. 6 (1993) 3-6], the ""Approximate Bucket implementation"" of Dijkstra's algorithm [B.V. Cherkassky, A.V. Goldberg, T. Radzik, Math. Programming 73 (1996) 129-174], and the ""??-Stepping algorithm"" [U. Meyer, P. Sanders, 1998].",Average-case complexity of single-source shortest-paths algorithms: lower and upper bounds
"['Jamie Dinkelacker', 'Pankaj K. Garg', 'Rob Miller', 'Dean Nelson']","The success of several Open SourceÉ?õ software systems, e.g., Apache, Bind, Emacs, and Linux, has recently sparked interest in studying and emulating the software engineering principles underlying this innovative development and use model. Certain aspects of the Open Source development method, e.g., community building, open discussions for requirements and features, and evolvable and modular designs are having fundamental and far reaching consequences on general software engineering practices.To leverage such Open Source methods and tools, we have defined an innovative software engineering paradigm for large corporations:  Progressive Open Source (POS).  POS leverages the power of Open Source methods and tools for large corporations in a progressive manner: starting from completely within the corporation, to include partner businesses, and eventually complete Open Source. In this paper we present the design goals and principles for POS. We illustrate POS with two programs in HP: Corporate Source and the Collaborative Development Program (CDP). We present early results from both these programs suggesting the power and necessity of POS for all modern large corporations.",Progressive open source
"['Apostolos Apostolaras', 'Navid Nikaein', 'Raymond Knopp', 'Antonio Maria Cipriano', 'Thanasis Korakis', 'Iordanis Koutsopoulos', 'Leandros Tassiulas']","In this paper, we propose a novel architecture for next generation cellular networks that enables collaborative forwarding at Layer 2 among adjacent eNBs with the aid of enhanced user equipment (UE) devices, that act voluntarily as packet forwarders. Therefore, legacy UEs are leveraged as active network elements being capable of operating simultaneously over multiple base stations (eNBs). To this end, we introduce an evolved-UE (eUE) in order to enable reliable multi-hop operation through relaying and to achieve low-latency communication through efficient L2/MAC forwarding. Through extensive experimentation with OpenAirInterface emulation platform, we evaluated the performance and also validated the feasibility of the proposed architecture. Our results show that, in certain use cases corresponding to public safety and moving/small cell scenarios, the proposed architecture achieves significant reduction in latency (up to 16.94%) and improvement on packet loss rate (up to 59.25%), as the number of the employed eUEs increases with increasing BLER up to 20%. Moreover, the proposed architecture enables eUEs to increase the aggregated data rate in downlink by exploiting data connection to multiple eNBs at the expense of extra power consumption, which calls for the appropriate incentives to enable such a cooperation.",Evolved user equipment for collaborative wireless backhauling in next generation cellular networks
"['Danyang Chen', 'Yongbin Zhou', 'Yang Han', 'Rui Xue', 'Qing He']","Random extractors are proven to be important building blocks in constructing leakage-resilient cryptographic primitives. Nevertheless, recent efforts have shown that they are likely more leaky than other elementary components (e.g. block ciphers) in unprotected implementations of these primitives, in the context of side-channel attacks. In this context, to the adversary, the extractors themselves could become the point of interest. Therefore, this paper extends the problem of how leakage resilience of random extractors could be in the case of protected instantiations. Specifically, we investigate the feasibility of applying classical countermeasures to ameliorate leakage resilience of cryptographic components and/or primitives against side-channel attacks. Then we show how to evaluate the physical leakage resilience of these instantiations both theoretically and practically. The countermeasures considered are masking, shuffling, and the combination of both. Taking the leakage-resilient stream cipher presented at FOCS 2008 as a case of study, we not only examine the leakage resilience of the underlying extractor, but also discuss how leakages from the extractor and from the underlying pseudo-random generator respectively impact the leakage resilience of the stream cipher as a whole. On the one hand, our theoretical and experimental results, which are consistent, do justify some existing observations. On the other hand, which is more important, our results reveal some new observations that do contrast with these known ones, which explicitly indicates that previous observations are (mostly likely) incomplete. We argue that our work is of both obvious theoretical interest and important practical significance, and may help foster the further research on the design and implementation of random extractors in leakage-resilient cryptography.",On Hardening Leakage Resilience of Random Extractors for Instantiations of Leakage Resilient Cryptographic Primitives
"['Mikko I. Malinen', 'Pasi Fr??nti']","Traditional approach to clustering is to fit a model (partition or prototypes) for the given data. We propose a completely opposite approach by fitting the data into a given clustering model that is optimal for similar pathological data of equal size and dimensions. We then perform inverse transform from this synthetic data back to the original data while refining the optimal clustering structure during the process. The key idea is that we do not need to find optimal global allocation of the prototypes. Instead, we only need to perform local fine-tuning of the clustering prototypes during the transformation in order to preserve the already optimal clustering structure.",K-means: Clustering by Gradual Data Transformation
"['Muhammad Ismail', 'Mohamed Kashef', 'Erchin Serpedin', 'Khalid A. Qaraqe']","The high energy consumption of network operators and mobile users has raised environmental, financial, and quality-of-experience concerns. These concerns have renewed the research efforts in developing green communication strategies for energy efficient wireless network operation. Network operators employ dynamic planning to save energy at low call traffic load by switching off some of their base stations (BSs), and mobile users are served by the remaining active BSs. The existing research investigates energy efficiency of dynamic planning approaches only from the network operator perspective. Dynamic planning, if not carefully designed, can lead to higher energy consumption for the mobile users in the uplink, which in turn degrades the uplink service quality due to mobile terminals' battery depletion. In this article we propose a dynamic planning framework with balanced energy efficiency that accounts for the energy consumption of the mobile users in the uplink as well as that of the network operators in the downlink. We discuss the associated challenges and implementation issues. A dynamic planning approach based on a multi-time scale decision process is proposed to achieve the balanced energy efficiency framework. Numerical results demonstrate the improved energy efficiency performance for the uplink mobile users as compared with the traditional dynamic planning approach.",On balancing energy efficiency for network operators and mobile users in dynamic planning
['Bajis Dodin'],"A fundamental problem in PERT networks is to identify a project's critical paths and its critical activities. In this paper we define the criticality index of a path as the probability that the duration of the path is greater than or equal to the duration of every other path in the network and define the criticality index of an activity as the sum of the criticality indices of the paths containing that activity. The most critical path or K most critical paths in a PERT network could be found by enumerating all the paths and calculating the corresponding criticality indices, both of which are burdensome tasks. This paper uses stochastic dominance to develop a procedure to identify the K most critical paths without using this path enumeration. The procedure has been applied to various sized PERT networks generated at random, and the results are found to be very close to those obtained by extensive Monte Carlo sampling.",Determining the K Most Critical Paths in PERT Networks
"['Ali Assi', 'Mohamad Sawan', 'R. Raut']",A new CMOS transconductance (Gm) circuit with voltage-tunability and very wide bandwidth is proposed and analysed. The transconductance circuit is then used to realize a tunable VHF 2nd-order bandpass filter cell. The center frequency (f0) of the designed filter can be tuned by varying the transconductance value (gm) of the tunable Gm circuit. Simulation results indicate the excellent performances of both the transconductance circuit and the filter over a wideband range. The transconductance value can be tuned from 40 /spl mu/S to 950 /spl mu/S (490 /spl mu/S) and the filter center frequency (f0) in the range 30 MHz (4 MHz)-110 MHz (49 MHz) for /spl plusmn/2.5 V (/spl plusmn/1.5 V) supply voltages.,A new CMOS tunable transconductor dedicated to VHF continuous-time filters
"['Edgar Nett', 'Hermann Streich', 'Paolo Costa Bizzarri', 'Andrea Bondavalli', 'Fabio Tarini']","Real time applications with high dependability requirements demand fault tolerance strategies. While for small systems with static behaviour policies, worse case execution times can be used, this is not true for more complex systems, in which worst case execution times are partially unknown or differ drastically from their average execution time. In such cases often only a minimum of quality can be achieved. The paper proposes to combine fault tolerant policies described by the FERT (Fault tolerant Entity for Real Time) notation with the dynamic scheduling scheme TPS (TaskPair Scheduling). TPS alleviates FERT's precondition of completely known WCETs and provides a flexible implementation base to enable an easy mapping of FERT strategies to a runtime system. In a first step, a significant subset of FERT is investigated, which implies: the recovery block scheme, N Version programming, and imprecise computations. TPS is utilised to guarantee different levels of quality, tailored to the application and the required level of fault tolerance, while guaranteeing that a common deadline is met.",Adaptive software fault tolerance policies with dynamic real-time guarantees
"['Libo Zheng', 'John E. Fletcher', 'Barry W. Williams', 'Xiangning He']","A technique to improve the flux pattern within a five-phase induction machine is presented. The technique is developed through dual-plane vector control, with synchronized fluxes. By vector space decomposition, an analytical model and vector control of the machine are accomplished in two orthogonal vector planes,d 1 -q 1  and d s -q s  . The magnitude and rotating speed of the associated fluxes (fundamental and third harmonic) can be independently controlled in each vector plane. Synchronization control locks the relative position between the two fluxes. The resultant air-gap flux density is fully controlled, preventing iron saturation. This feature is especially important in reshaping the flux and back EMF waveform of the machine. A quasi-trapezoidal air-gap flux density distribution is achieved for better iron utilization and higher torque density. It is confirmed that compared with sinusoidal fluxing, the quasi-trapezoidal flux pattern will not lead to an oversized power inverter when improving machine torque density. The basic understanding and control scheme can be extended to a multiphase induction machine with a phase number greater than five.",Dual-Plane Vector Control of a Five-Phase Induction Machine for an Improved Flux Pattern
"['Stefan Schlobach', 'Marius Olsthoorn', 'Maarten de Rijke']","Open domain question answering (QA) systems have to bridge the potential vocabulary mismatch between a question and its candidate answers. One can view this as a recall problem and address it accordingly. Recall oriented strategies to QA may generate considerable amounts of noise. To combat this, many open domain QA systems contain an explicit filtering or re-ranking component, which often check whether the answer is of the correct semantic type. Particular classes of questions expect specific answer types to which all of their answers should belong. We compare two kinds of strategies for answer type checking for open domain QA. One is redundancy-based and builds on the intuition that the amount of implicit knowledge which connects an answer to a question can be estimated by exploiting the redundancy of information available on the web. The other is knowledge-intensive, and exploits structured and semi-structured data sources to determine, with high confidence, the semantic type of suggested answers.",Type checking in open-domain question answering
"['I-Wei Lai', 'Tsung-Han Yu', 'Tzi-Dar Chiueh']",This paper presents a low-complexity interpolation-based channel estimator suited for scattered-pilot OFDM systems in fast-fading channel. The proposed estimator consists of two interpolators: one RLS interpolator for estimating scattered subcarriers along the time direction and another raised-cosine (RC) interpolator for estimating data subcarriers along the frequency direction. A low-complexity architecture tailored for DVB-T/H is demonstrated as an exemplar. Simulation results show that the proposed channel estimator has comparable accuracy with almost order-of-magnitude lower complexity than conventional adaptive channel estimators.,Low-complexity adaptive channel estimation for OFDM system in fast-fading channel
"['Meng Guo', 'Thomas Bo Elmedyb', 'S??ren Holdt Jensen', 'Jesper Jensen']","In this paper, we analyze a general multiple-microphone and single-loudspeaker system, where an adaptive algorithm is used to cancel acoustic feedback/echo and a beamformer processes the feedback/echo canceled signals. This system can be viewed as part of a typical hearing aid system and/or a traditional acoustic echo cancelation system. We introduce and derive an approximation of a useful frequency domain measure - the power transfer function - and show how to predict the system stability bound, convergence rate and the steady-state behavior across time and frequency. Furthermore, we show how the derived expressions can be used to determine e.g. the step size parameter in the adaptive algorithms to achieve a desired system property e.g. convergence rate at a specific frequency.",Analysis of adaptive feedback and echo cancelation algorithms in a general multiple-microphone and single-loudspeaker system
"['Baihua Zheng', 'Wang-Chien Lee', 'Dik Lun Lee']","A continuous nearest neighbor (CNN) search, which retrieves the nearest neighbors corresponding to every point in a given query line segment, is important for location-based services such as vehicular navigation and tourist guides. It is infeasible to answer a CNN search by issuing a traditional nearest neighbor query at every point of the line segment due to the large number of queries generated and the overhead on bandwidth. Algorithms have been proposed recently to support CNN search in the traditional client- server systems but not in the environment of wireless data broadcast, where uplink communication channels from mobile devices to the server are not available. In this paper, we develop a generalized search algorithm for continuous k-nearest neighbors based on Hilbert Curve Index in wireless data broadcast systems. A performance evaluation is conducted to compare the proposed search algorithms with an algorithm based on R-tree Air Index. The result shows that the Hilbert Curve Index-based algorithm is more energy efficient than the R-tree-based algorithm.",On Searching Continuous k Nearest Neighbors in Wireless Data Broadcast Systems
"['Daniel Chern-Yeow Eng', 'Yoonsuck Choe']","Advancement in high-throughput microscopy technology such as the Knife-Edge Scanning Microscopy (KESM) is enabling the production of massive amounts of high-resolution volumetric data of biological microstructures. To fully utilize these data, they should be efficiently distributed to the scientific research community (e.g., through the Internet) and should be easily annotated and analyzed. Given the volumetric nature of the data, visualizing them in 3D is important. Since we cannot assume that every end user has high-end hardware, an approach that has minimal hardware requirement will be necessary. There are several prominent applications that facilitate the viewing of large collections of images over the web. Google Maps and Google Maps-like interfaces such as Brainmaps.org allow users to pan and zoom 2D images efficiently. However, they do not yet support the rendering of volumetric data in their standard web interface. Thus, we propose a new method of rendering volumetric data over the web that directly uses the raw image stack, without any computation on the data at all. The human visual system has the capability of viewing stereo images in 2D and turn that into a 3D perception. To generate stereo images, we will need to create the effects of depth and binocular disparity using 2D images. By using simple HTML and JavaScript that are computationally cheap, we can accomplish both tasks dynamically in a standard web browser, by overlaying the images with intervening semi-opaque layers. We expect the approach presented in this paper to be applicable to a broader domain, including geology and meteorology.",Stereo pseudo 3D rendering for web-based display of scientific volumetric data
"['Inna Pereverzeva', 'Elena Troubitsyna', 'Linas Laibinis']","Multi-agent systems (MAS) are increasingly used in critical applications. To ensure dependability of MAS, we need powerful development techniques that would allow us to master complexity inherent to MAS and formally verify correctness and safety of collaborative agent activities. In this paper we present a development of hospital MAS by refinement in Event-B. We demonstrate that Event-B allows the developers to rigorously specify complex agent interactions and verify their correctness and safety.",Formal Development of Critical Multi-agent Systems: A Refinement Approach
"['Jong Kook Kim', 'Hyun Wook Park']","Clustered microcalcifications on X-ray mammograms are an important sign for early detection of breast cancer. Texture-analysis methods can be applied to detect clustered microcalcifications in digitized mammograms. In this paper, a comparative study of texture-analysis methods is performed for the surrounding region-dependence method, which has been proposed by the authors, and conventional texture-analysis methods, such as the spatial gray level dependence method, the gray-level run-length method, and the gray-level difference method. Textural features extracted by these methods are exploited to classify regions of interest (ROI's) into positive ROI's containing clustered microcalcifications and negative ROI's containing normal tissues. A three-layer backpropagation neural network is used as a classifier. The results of the neural network for the texture-analysis methods are evaluated by using a receiver operating-characteristics (ROC) analysis. The surrounding region-dependence method is shown to be superior to the conventional texture-analysis methods with respect to classification accuracy and computational complexity.",Statistical textural features for detection of microcalcifications in digitized mammograms
"['Yi Deng', 'Jiacun Wang', 'MengChu Zhou']","To real-time system designers, end-to-end time delay between external inputs and outputs is among the most important constraints. To ensure these system-wide constraints are satisfied, each of the constituent components is subject to a set of derived intermediate constraints. Since the system-wide constraints allow many possibilities for the intermediate constraints based on design tradeoffs, an important issue is how to guarantee the consistency between system-wide constraints and intermediate component constraints. In this paper, we present a systematic method for the verification of consistency between a system's global timing constraints and intermediate component constraints. The essence of this technique is to construct a timing model for each component, based on component constraints. This model treats a component as a black box. When replacing each component with its timing model, we obtain a complete time Petri net model for system architecture, which allows us to verify the consistency between global and component constraints. The key contribution is twofold. First, our technique of verification is efficient by supporting incremental analysis and suppressing internal state space of components. Second, much of the verification process presented in this paper can be automated. We illustrate the consistency verification process through a flexible manufacturing system example.",Consistency verification in modeling of real-time systems
"['Daniel S. Roche', 'Adam J. Aviv', 'Seung Geol Choi']","We present a new secure cloud storage mechanism that combines three previously disjoint security properties: obliviousness, secure deletion, and history independence. The system maintains strong privacy guarantees against a cloudobservation attack, wherein an attacker learns all previous states of, and accesses to, the persistent cloud storage, as well as a catastrophic attack, where the decryption keys from erasable memory are also leaked. In the first scenario, the access pattern reveals nothing about the contents (obliviousness), and in both scenarios, no previously deleted data is recoverable (secure deletion), and the structure of the data leaks a bounded amount of information about previous states (history independence). To achieve these goals we developed a new oblivious-RAM with variable-size storage blocks (vORAM) and a new history independent, randomized data structure based on B-trees (HIRB tree) stored within the vORAM. We prove that the vORAM+HIRB achieves obliviousness and secure deletion. We also show that any such system must inevitably leak a bounded amount of history information, and prove that our vORAM+HIRB construction matches this lower bound, up to logarithmic factors. Our system also provides better utilization of the ORAM buckets and reduces the amount of local storage requirements by O(logn) as compared to prior work for storing map data structures. Finally, we have implemented and measured the performance of our system using Amazon Web Services for a sample password-management application that maintains the privacy of login records, addition and removal of accounts, and password changes. The empirical performance is comparable to current state of the art in ORAM technologies and greatly outperforms naive approaches that provide the same security properties.",Oblivious Secure Deletion with Bounded History Independence
['Vern Paxson'],"The large-scale behavior of routing in the Internet has gone virtually without any formal study, the exception being Chinoy's analysis of the dynamics of Internet routing information [Ch93]. We report on an analysis of 40,000 end-to-end route measurements conducted using repeated ""traceroutes"" between 37 Internet sites. We analyze the routing behavior for pathological conditions, routing stability, and routing symmetry. For pathologies, we characterize the prevalence of routing loops, erroneous routing, infrastructure failures, and temporary outages. We find that the likelihood of encountering a major routing pathology more than doubled between the end of 1994 and the end of 1995, rising from 1.5% to 3.4%. For routing stability, we define two separate types of stability, ""prevalence"" meaning the overall likelihood that a particular route is encountered, and ""persistence,"" the likelihood that a route remains unchanged over a long period of time. We find that Internet paths are heavily dominated by a single prevalent route, but that the time periods over which routes persist show wide variation, ranging from seconds up to days. About 2/3's of the Internet paths had routes persisting for either days or weeks. For routing symmetry, we look at the likelihood that a path through the Internet visits at least one different city in the two directions. At the end of 1995, this was the case half the time, and at least one different autonomous system was visited 30% of the time.",End-to-end routing behavior in the internet
['Hal Glicksman'],"Green is the center of the visible spectrum and the hue to which we are most sensitive. In RGB color, green is 60 percent of white. When we look through a prism at a white square, as Goethe did, we see white between yellow and cyan, just where green appears in the spectrum of Newton. Additional arguments were published previously and appear at www.csulb.edu/-percept, along with the Percept color chart of the hue/value relationships. A new argument, derived from the perception of leaves, is presented here. The Percept color chart transformed into a color wheel is also presented.?? (1998) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.",White is green
['Kal Toth'],"For software engineering (SE) and computer science (CS) programs to deliver on their promises, they must go beyond teaching students about principles, processes, models, and strategies and offer them realistic, practical experience as well. Although industry has been pressing to increase the emphasis on practical aspects, many CS programs continue to give students relatively simple problems focused on selected computing and software concepts and theories. Open source software offers CS and SE educators an opportunity to give their students practical, hands-on software engineering experience",Experiences with Open Source Software Engineering Tools
"['Yanbin Wang', 'Wei Zhang']","Novice driver training has become an important issue in China with the quick increase of vehicle population. However, the training school mainly provides only operation skill training, which seems insufficient for novice driver training. To help novice drivers develop higher-order perceptual and cognitive skills safely and quickly, a prototype model of driver training platform is proposed. The system allows many networked drivers to interact with other drivers as well as the environment. Some typical road hazards were designed for drivers to experience. Potential impact may include improved training efficiency and safety for higher-order perceptual and cognitive skills. It also provides a means of education to improve publicity of road hazards.",A Driver Training Platform Prototype Based on Distributed Simulation
['Ashley Montanaro'],"The d-dimensional pattern matching problem is to nd an occurrence of a pattern of length m m within a text of length n n, with n m. This task models various problems in text and image processing, among other application areas. This work describes a quantum algorithm which solves the pattern matching problem for random patterns and texts in time e O((n=m) d=2 2 O(d 3=2p log m) ). For large m this is super-polynomially faster than the best possible classical algorithm, which requires time e (( n=m) d +n d=2 ). The algorithm is based on the use of a quantum subroutine for nding hidden shifts in d dimensions, which is a variant of algorithms proposed by Kuperberg.",Quantum pattern matching fast on average
"['A. Mart??nez', 'Yannis A. Dimitriadis', 'Bartolom?? Rubia', 'E. G??mez', 'P. de la Fuente']","Studying and evaluating real experiences that promote active and collaborative learning is a crucial field in CSCL. Major issues that remain unsolved deal with the merging of qualitative and quantitative methods and data, especially in educational settings that involve both physical and computer-supported collaboration. In this paper we present a mixed evaluation method that combines traditional sources of data with computer logs, and integrates quantitative statistics, qualitative data analysis and social network analysis in an overall interpretative approach. Several computer tools have been developed to assist in this process, integrated with generic software for qualitative analysis. The evaluation method and tools have been incrementally applied and validated in the context of an educational and research project that has been going on during the last three years. The use of the method is illustrated in this paper by an example consisting of the evaluation of a particular category within this project. The proposed method and tools aim at giving an answer to the need of innovative techniques for the study of new forms of interaction emerging in CSCL; at increasing the efficiency of the traditionally demanding qualitative methods, so that they can be used by teachers in curriculum-based experiences; and at the definition of a set of guidelines for bridging different data sources and analysis perspectives.",Combining qualitative evaluation and social network analysis for the study of classroom social interactions
"['Nabil Charkani', 'Yannick Deville']","This paper deals with the separation of two convolutively mixed signals. The proposed approach uses a recurrent structure adapted by a generic rule involving arbitrary separating functions. These functions should ideally be set so as to minimize the asymptotic error variance of the structure. However, these optimal functions are often unknown in practice. The proposed alternative is based on a self-adaptive (sub-)optimization of the separating functions, performed by estimating the projection of the optimal functions on a predefined set of elementary functions. The equilibrium and stability conditions of this rule and its asymptotic error variance are studied. Simulations are performed for real mixtures of speech signals. They show that the proposed approach yields much better performance than classical rules.",A convolutive source separation method with self-optimizing non-linearities
"['Qiong Luo', 'Sailesh Krishnamurthy', 'C. Mohan', 'Hamid Pirahesh', 'Honguk Woo', 'Bruce G. Lindsay', 'Jeffrey F. Naughton']","While scaling up to the enormous and growing Internet population with unpredictable usage patterns, E-commerce applications face severe challenges in cost and manageability, especially for database servers that are deployed as those applications' backends in a multi-tier configuration. Middle-tier database caching is one solution to this problem. In this paper, we present a simple extension to the existing federated features in DB2 UDB, which enables a regular DB2 instance to become a DBCache without any application modification. On deployment of a DBCache at an application server, arbitrary SQL statements generated from the unchanged application that are intended for a backend database server, can be answered: at the cache, at the backend database server, or at both locations in a distributed manner. The factors that determine the distribution of workload include the SQL statement type, the cache content, the application requirement on data freshness, and cost-based optimization at the cache. We have developed a research prototype of DBCache, and conducted an extensive set of experiments with an E-Commerce benchmark to show the benefits of this approach and illustrate tradeoffs in caching considerations.",Middle-tier database caching for e-business
"['Jo?úo Manuel Freitas Xavier', 'Victor Barroso', 'Jos?? M. F. Moura']","We present a closed-form algorithm for blind identification of multiple-input/multiple-output (MIMO) finite-impulse response (FIR) systems driven by digital sources. The algorithm is based on second-order statistics and yields an asymptotically exact estimate of the MIMO channel. We assign distinct spectral signatures to each user through transmitter correlative filters, and exploit this spectral asymmetry to derive the closed-form solution. Simulation results illustrate the good performance of the proposed approach. We compare the mean-square error (MSE) of the MIMO channel estimate against the Cramer-Rao bound, and assess the algorithm capability in rejecting inter-user crosstalk interference.",Closed-form blind identification of MIMO channels
"['Nicola Barbieri', 'Giuseppe Manco', 'Ettore Ritacco']",* Preface* The Recommendation Process* Probabilistic Models for Collaborative Filtering* Bayesian Modeling* Exploiting Probabilistic Models* Contextual Information* Social Recommender Systems* Conclusions* Bibliography* Authors' Biographies,Probabilistic Approaches to Recommendations
"['Savina Bansal', 'P. Kumar', 'Kuldip Singh']","Scheduling precedence constrained task graphs, with or without duplication, is one of the most challenging NP-complete problems in parallel and distributed computing systems. Duplication heuristics are more effective, in general, for fine grain task graphs and for networks with high communication latencies. However, most of the available duplication algorithms are designed under the assumption of unbounded availability of fully connected processors, and lie in a high complexity range. Low complexity optimal duplication algorithms work under restricted cost and/or shape parameters for the task graphs. Further, the required number of processors grows in proportion to the task-graph size significantly. An improved duplication strategy is proposed that works for arbitrary task graphs, with a limited number of interconnection-constrained processors. Unlike most other algorithms that replicate all possible parents/ancestors of a given task, the proposed algorithm tends to avoid redundant duplications and duplicates the nodes selectively, only if it helps in improving the performance. This results in lower duplications and also lower time and space complexity. Simulation results are presented for clique and an interconnection-constrained network topology with random and regular benchmark task graph suites, representing a variety of parallel numerical applications. Performance, in terms of normalized schedule length and efficiency, is compared with some of the well-known and recently proposed algorithms. The suggested algorithm turns out to be most efficient, as it generates better or comparable schedules with remarkably less processor consumption.",An improved duplication strategy for scheduling precedence constrained graphs in multiprocessor systems
['Dae-Kyoo Kim'],"In this paper, we describe an approach to checking conformance of UML class diagrams to design patterns. The technique provides a set of checks that evaluate syntactic and semantic pattern conformance. Syntactic pattern conformance is concerned with structural conformance of a class diagram to the structural properties of a pattern. Semantic pattern conformance is concerned with conformance of invariants and pre- and post-conditions in a class diagram to semantic pattern properties. A class diagram is said to conform a pattern when it acquires both syntactic and semantic conformance.",Evaluating conformance of UML models to design patterns
"['Zhengrong Yao', 'Haibo Li']","This paper addresses an important issue, how to evaluate a vision-based face tracking system? Although nowadays it is getting popular to employ a magnetic sensor to evaluate the performance of such systems. The related issues such as condition and limitation of usage are often omitted. In this paper we studied this accepted evaluation methodology together with another evaluation method, Peak Signal to Noise (PSNR) commonly used in image coding community. The condition of proper usage of magnetic sensor as evaluating system is discussed. Our theoretical analysis and experiments with real video sequences show that we have to be very careful to select the so-called ""ground truth"". We believe that to help further development of face tracking techniques, a valid performance evaluation is necessary, both the evaluating system and the tracking system have to be jointly considered to decide if the evaluating method is valid. The experimental results give us further hints about the tracking performance when using different tracking scheme.",Is A Magnetic Sensor Capable of Evaluating A Vision-Based Face Tracking System?
"['Robert P. Dick', 'Niraj K. Jha']","In this paper we present a hardware-software co-synthesis algorithm, called COWLS, which targets embedded system composed of servers and low-power clients which communicate with each other through a channel of limited bandwidth, e.g., a wireless link. Clients may be mobile. COWLS allows both hard and soft real-time constraints. It simultaneously optimizes the price of the client-server system, the power consumption of the client, and the response times of tasks which have only soft deadlines, while meeting all the hard deadlines. It produces numerous solutions which trade off different architectural features, e.g., price, power consumption, and response time, of an embedded client-server system. As far as we know, this is the first co-synthesis algorithm of its kind. We present experimental results of synthesizing a low power, client-server camera system as well as several randomized examples.",COWLS: hardware-software co-synthesis of distributed wireless low-power embedded client-server systems
"['Hongda Mao', 'Megan Gribble', 'Arkady M. Pertsov', 'Pengcheng Shi']","Embryonic heart morphogenesis (EHM) is a complex and dynamic process where the heart transforms from a single tube into a four-chambered pump. This process is of great biological and clinical interest but is still poorly understood for two main reasons. On the one hand, the existing imaging modalities for investigating EHM suffered from either limited penetration depth or limited spatial resolution. On the other hand, current works typically adopted manual segmentation, which was tedious, subjective, and time consuming considering the complexity of developing heart geometry and the large size of images. In this paper, we propose to utilize confocal microscopy imaging with tissue optical immersion clearing technique to image the heart at different stages of development for EHM study. The imaging method is able to produce high spatial resolution images and achieve large penetration depth at the same time. Furthermore, we propose a novel convex active contour model for automatic image segmentation. The model has the ability to deal with intensity fall-off in depth which is characterized by confocal microscopy images. We acquired the images of embryonic quail hearts from day 6 to day 14 of incubation for EHM study. The experimental results were promising and provided us with an insight view of early heart growth pattern and also paved the road for data-driven heart growth modeling.",Embryonic Heart Morphogenesis from Confocal Microscopy Imaging and Automatic Segmentation
"['Arunesh Mishra', 'Vivek Shrivastava', 'Dheeraj Agrawal', 'Suman Banerjee', 'Samrat Ganguly']","Wireless 802.11 hotspots have grown in an uncoordinated fashion with highly variable deployment densities. Such uncoordinated deployments, coupled with the difficulty of implementing coordination protocols, has often led to conflicting configurations (e.g., in choice of transmission power and channel of operation) among the corresponding Access Points (APs). Overall, such conflicts cause both unpredictable network performance and unfairness among clients of neighboring hotspots. In this paper, we focus on the fairness problem for uncoordinated deployments. We study this problem from the channel assignment perspective. Our solution is based on the notion of channel-hopping, and meets all the important design considerations for control methods in uncoordinated deployments - distributed in nature, minimal to zero coordination among APs belonging to different hotspots, simple to implement, and interoperable with existing standards. In particular, we propose a specific algorithm called MAXchop, which works efficiently when using only non-overlapping wireless channels, but is particularly effective in exploiting partially-overlapped channels that have been proposed in recent literature. We also evaluate how our channel assignment approach complements previously proposed carrier sensing techniques in providing further performance improvements. Through extensive simulations on real hotspot topologies and evaluation of a full implementation of this technique, we demonstrate the efficacy of these techniques for not only fairness, but also the aggregate throughput, metrics.We believe that this is the first work that brings into focus the fairness properties of channel hopping techniques and we hope that the insights from this research will be applied to other domains where a fair division of a system's resources is an important consideration.",Distributed channel management in uncoordinated wireless environments
"['Thomas Guionnet', 'Christine Guillemot']","This paper addresses the issue of robust decoding of arithmetic codes. We first analyze dependencies between the variables involved in arithmetic coding by means of the Bayesian formalism. This provides a suitable framework for designing a soft decoding algorithm that provides high error-resilience. It also provides a natural setting for ""soft synchronization"", i.e., to introduce anchors favoring the likelihood of ""synchronized"" paths. In order to maintain the complexity of the estimation within a realistic range, a simple, yet efficient, pruning method is described. Models and algorithms are then applied to context-based arithmetic coding widely used in practical systems (e.g. JPEG-2000). Experimentation results with both theoretical sources and with real images coded with JPEG-2000 reveal very good error resilience performances.",Robust decoding of arithmetic codes for image transmission over error-prone channels
"['Giovanni Sabato', 'Mehdi Molkaraie']","The performance of the generalized belief propagation algorithm for computing the noiseless capacity of finite-sized two-dimensional and three-dimensional run-length limited constraints is investigated. For each constraint, a method is proposed to choose a set of clusters. Simulation results for different sizes of channels with different constraints are reported. Convergence to the Shannon capacity is also discussed.",Generalized belief propagation algorithm for the capacity of multi-dimensional run-length limited constraints
['See-Kiong Ng'],"The current emergence of wireless and pervasive computing technologies presents an invaluable opportunity for digital convergence in the traditionally pen-and-paper based life sciences laboratories. We envision a bio-laboratory of the future where smart computing technologies are embedded into the traditional laboratory environment to simplify laboratory work and improve the quality and productivity of scientists in their experimentation activities. By such unprecedented transportation of computing beyond the desk-top PCs of ""dry"" labs right onto the lab benches of ""wet"" labs where biological data are generated, we envisage that the current success of computing in biology can thus be furthered beyond traditional bioinformatics.",Smart bio-laboratories of the future
"['Kyu-Han Kim', 'Alexander W. Min', 'Dhruv Gupta', 'Prasant Mohapatra', 'Jatinder Pal Singh']","Mobile data usage over cellular networks has been dramatically increasing over the past years. Wi-Fi based wireless networks offer a high-bandwidth alternative for offloading such data traffic. However, intermittent connectivity, and battery power drain in mobile devices, inhibits always-on connectivity even in areas with good Wi-Fi coverage. This paper presents WiFisense, a system that employs user mobility information retrieved from low-power sensors (e.g., accelerometer) in smartphones, and further includes adaptive Wi-Fi sensing algorithms, to conserve battery power while improving Wi-Fi usage. We implement the proposed system in Android-based smartphones and evaluate the implementation in both indoor and outdoor Wi-Fi networks. Our evaluation results show that WiFisense saves energy consumption for scans by up to 79% and achieves considerable increase in Wi-Fi usage for various scenarios.",Improving energy efficiency of Wi-Fi sensing on smartphones
"['Mahdi Jafari', 'Soheil Mohajer', 'Christina Fragouli', 'Suhas N. Diggavi']","The min-cut value towards a single receiver in a network with unit capacity edges can be achieved by routing a single bit. The multicast theorem in network coding shows that, the common min-cut value towards N É?ù 1 receivers can also be achieved using packets of length logN bits, if the operations the intermediate nodes perform are deterministically known at the receivers. We here calculate the capacity in the case where these operations are unknown, and characterize how the capacity depends on the min-cut value and the packet length.",On the capacity of non-coherent network coding
"['Antal van den Bosch', 'Sander Canisius']","In performing morpho-phonological sequence processing tasks, such as letter-phoneme conversion or morphological analysis, it is typically not enough to base the output sequence on local decisions that map local-context input windows to single output tokens. We present a global sequence-processing method that repairs inconsistent local decisions. The approach is based on local predictions of overlapping trigrams of output tokens, which open up a space of possible sequences; a data-driven constraint satisfaction inference step then searches for the optimal output sequence. We demonstrate significant improvements in terms of word accuracy on English and Dutch letter-phoneme conversion and morphological segmentation, and we provide qualitative analyses of error types prevented by the constraint satisfaction inference method.",Improved morpho-phonological sequence processing with constraint satisfaction inference
"['Chao Gu', 'Detong Zhu']","In this paper, we propose a nonmonotone sequential quadratic programming-filter method for solving nonlinear equality constrained optimization. This new method has more flexibility for the acceptance of the trial step and requires less computational costs compared with the monotone methods. Under reasonable conditions, we give the global convergence properties. Further, the second-order correction step and nonmonotone reduction conditions are used to overcome Maratos effect so that quadratic local convergence is achieved. The numerical experiments are reported to show the effectiveness of the proposed algorithm.",A nonmonotone SQP-filter method for equality constrained optimization
"['Jerzy Brzeziè?ski', 'Anna Kobusiè?ska', 'Michaè? Szychowiak']","In the mobile environment, weak consistency replication of shared data is the key to obtaining high data availability, good access performance, and good scalability. Therefore new class of consistency models, called session guarantees, recommended for mobile environment, has been introduced. Session guarantees, called also client-centric consistency models, have been proposed to define required properties of the system regarding consistency from the client's point of view. Unfortunately, none of proposed consistency protocols providing session guarantees is resistant to server failures. Therefore, in this paper checkpointing and rollback-recovery protocol rVsMW, which preserves monotonic writes session guarantee is presented. The recovery protocol is integrated with the underlying consistency protocol by integrating operations of taking checkpoints with coherence operations of VsSG protocol.",Checkpointing and rollback-recovery protocol for mobile systems with MW session guarantee
"['Ferdian Thung', 'Tegawend?? Fran??ois D Assise Bissyande', 'David Lo', 'Lingxiao Jiang']","Social coding enables a different experience of software development as the activities and interests of one developer are easily advertised to other developers. Developers can thus track the activities relevant to various projects in one umbrella site. Such a major change in collaborative software development makes an investigation of networkings on social coding sites valuable. Furthermore, project hosting platforms promoting this development paradigm have been thriving, among which GitHub has arguably gained the most momentum. In this paper, we contribute to the body of knowledge on social coding by investigating the network structure of social coding in GitHub. We collect 100,000 projects and 30,000 developers from GitHub, construct developer-developer and project-project relationship graphs, and compute various characteristics of the graphs. We then identify influential developers and projects on this sub network of GitHub by using PageRank. Understanding how developers and projects are actually related to each other on a social coding site is the first step towards building tool supports to aid social programmers in performing their tasks more efficiently.",Network Structure of Social Coding in GitHub
"['Francis Avnaim', 'Jean Daniel Boissonnat', 'Bernard Faverjon']","A general and simple algorithm is presented which computes the set FP of all free configurations for a polygonal object I (with m edges) which is free to translate and/or to rotate but not to intersect another polygonal object E. The worst-case time complexity of the algorithm is O(m/sup 3/n/sup 3/ log mn), which is close to optimal. FP is a three-dimensional curved object which can be used to find free motions within the same time bounds. Two types of motion have been studied in some detail. Motion in contact, where I remains in contact with E, is performed by moving along the faces of the boundary of FP. By partitioning FP into prisms, it is possible to compute motions when I never makes contact with E. In this case, the theoretical complexity does not exceed O(m/sup 6/n/sup 6/ alpha (mn)) but it is expected to be much smaller in practice. In both cases, pseudo-optimal motions can be obtained with a complexity increased by a factor log mn. >",A practical exact motion planning algorithm for polygonal objects amidst polygonal obstacles
"['Erik Meijering', 'Wiro J. Niessen', 'Joachim Weickert', 'Max A. Viergever']","Three-dimensional rotational angiography (3DRA) is a new and promising technique for obtaining high-resolution isotropic 3D images of vascular structures. However, due to the relatively high noise level and the presence of other background structures in clinical 3DRA images, noise reduction is inevitable. In this paper, we evaluate a number of linear and nonlinear diffusion techniques for this purpose. Specifically, we analyze the effects of these techniques on the threshold- based visualization and quantification of vascular anomalies in 3DRA images. The results of in- vitro experiments indicate that edge-enhancing anisotropic diffusion filtering is most suitable: the increase in the user-dependency of visualizations and quantifications is considerably less with this technique compared to linear filtering techniques, and it is better at reducing noise near edges than isotropic nonlinear diffusion. However, in view of the memory and computation-time requirements of this technique, the latter scheme may be considered a useful alternative. KeywordsÉ??Three-dimensional rotational angiography, noise reduction, linear diffusion, isotropic nonlinear diffusion, edge-enhancing anisotropic diffusion, in-vitro evaluation.",Diffusion-Enhanced Visualization and Quantification of Vascular Anomalies in Three-Dimensional Rotational Angiography: Results of an In-Vitro Evaluation
"['Abhishek Nagar', 'Anil K. Jain']","Many transformation functions have been proposed for generating revocable or non-invertible biometric templates. However, their security analysis either ignores the distribution of biometric features or uses inefficient feature matching. This usually leads to unrealistic estimates of security. In this paper we introduce a new measure of non-invertibility, called the Coverage-Effort (CE) curve which measures the number of guesses (Effort) required by an adversary to recover a certain fraction (Coverage) of the original biometric data. In addition to utilizing the feature distribution, the CE curve allows estimation of security against partial recovery of biometric features. We analyze the CE curves obtained using different instances of a mixture of Gaussians based feature transform for fingerprint templates. Our analysis shows that knowledge of the fingerprint minutiae distribution reduces the effort required to obtain a specified coverage.",On the security of non-invertible fingerprint template transforms
"['Martin Levesque', 'Halima Elbiaze']","Burst contention is a well-known challenging problem in Optical Burst Switching (OBS) networks. Contention resolution approaches are always reactive and attempt to minimize the BLR based on local information available at the core node. On the other hand, a proactive approach that avoids burst losses before they occur is desirable. To reduce the probability of burst contention, a more robust routing algorithm than the shortest path is needed. This paper proposes a new routing mechanism for JET-based OBS networks, called Graphical Probabilistic Routing Model (GPRM) that selects less utilized links, on a hop-by-hop basis by using a bayesian network. We assume no wavelength conversion and no buffering to be available at the core nodes of the OBS network. We simulate the proposed approach under dynamic load to demonstrate that it reduces the BLR Burst Loss Ratio compared to static approaches by using Network Simulator 2 (ns-2) on NSFnet network topology and with realistic traffic matrix. Simulation results clearly show that the proposed approach outperforms static approaches in terms of BLR.",Graphical Probabilistic Routing Model for OBS Networks with Realistic Traffic Scenario
"['Damien Josset', 'Jacques Pelon', 'Yongxiang Hu']","A-Train platforms offer the possibility of measuring the same physical parameters using active and passive instruments, to improve our understanding of geophysical processes in the Earth system. In this letter, a new calibration approach is developed using active [Cloud-Aerosol Lidar and Infrared Pathfinder Satellite Observation (CALIPSO) lidar and CloudSat radar] and passive [Advanced Microwave Scanning Radiometer for the Earth Observing System (AMSR-E)] instruments. The parameters of an existing oceanic surface model are first adjusted to give consistent sea surface scattering properties for CALIPSO and CloudSat observations. Revisiting the lidar/radar data analysis procedure using this model, as well as sea surface wind speed, the temperature and water vapor products of the microwave radiometer (AMSR-E) allowed one to refine the calibration factors for both lidar and radar observations in a coherent approach. This study also improves other applications such as the retrieval of atmospheric attenuation from aerosols at optical wavelengths.",Multi-Instrument Calibration Method Based on a Multiwavelength Ocean Surface Model
"['Le-Nam Tran', 'Een-Kee Hong']","Dirty paper coding (DPC) scheme is the capacity achieving transmission technique in multiuser MIMO downlink channels. As a suboptimal solution to DPC, successive zero-forcing DPC (SZF-DPC) has been proposed recently. The zero-interference constraint in designing the precoding matrices limits the number of supportable users. In this correspondence, we propose three low-complexity suboptimal user scheduling algorithms to exploit the multiuser diversity gain in SZF-DPC as the number of users grows. The first algorithm greedily maximizes the true sum rate. The second algorithm is based on eigenvalues. The third algorithm relies on the diagonal elements of the effective channel matrix since eigenvalues and diagonal entries of a Hermitian matrix have a strong relationship. Simulation results show that the proposed scheduling algorithms can obtain a significant fraction of sum rate of the optimal solution. Furthermore, the performance analysis is provided to prove that the proposed user selection algorithms can achieve the same asymptotic sum rate as that of DPC.",Multiuser Diversity for Successive Zero-Forcing Dirty Paper Coding: Greedy Scheduling Algorithms and Asymptotic Performance Analysis
"['Fuminori Yamasaki', 'Koh Hosoda', 'Minoru Asada']","This paper presents a framework of the energy consumption based control system for humanoid walking. Unlike the existing two approaches (One is based on precise control and powerful actuators (full control and wide applicability), and the other is passive dynamic walk (no control, therefore limited applicability)), the method aims at less energy consumption with more applicability. Knee stretching posture contributed to the former in general and a search algorithm based on the computer simulation enabled to find a feasible control to the given task.",An energy consumption based control for humanoid walking
"['Bang Chul Jung', 'Hu Jin', 'Dan Keun Sung', 'Sae-Young Chung']","In orthogonal code hopping multiplexing (OCHM) systems, hopping pattern (HP) collisions may degrade the system performance. Previous studies on the effect of HP collisions in OCHM systems were mainly based on computer simulations and there was no rigorous mathematical analysis of bit error rate (BER) performance. The HP collisions in OCHM systems differ from the hits in frequency-hopping (FH) systems or intracell interference in DS-CDMA systems because it can be effectively controlled through synergy and perforation techniques. In this paper, we introduce a received signal model for OCHM systems and analyze the BER performance for OCHM systems. Through the analysis of the BER performance, OCHM systems can be characterized more clearly and the allocated power at base station can be estimated. Furthermore, the user capacity is analyzed for a given channel coding scheme.",Performance Analysis of Orthogonal Code Hopping Multiplexing Systems
"['Abhik Sarkar', 'Frank Mueller', 'Harini Ramaprasad']","Locking cache lines in hard real-time systems is a common means of achieving predictability of cache access behavior and tightening as well as reducing worst case execution time, especially in a multitasking environment. However, cache locking poses a challenge for multi-core hard real-time systems since theoretically optimal scheduling techniques on multi-core architectures assume zero cost for task migration. Tasks with locked cache lines need to proactively migrate these lines before the next invocation of the task. Otherwise, cache locking on multi-core architectures becomes useless as predictability is compromised.   This paper proposes hardware-based push-assisted cache migration as a means to retain locks on cache lines across migrations. We extend the push-assisted migration model with several cache migration techniques to efficiently retain locked cache lines on a bus-based chip multi-processor architecture. We also provide deterministic migration delay bounds that help the scheduler decide which migration technique(s) to utilize to relocate a single or multiple tasks. This information also allows the scheduler to determine feasibility of task migrations, which is critical for the safety of any hard real-time system. Such proactive migration of locked cache lines in multi-cores is unprecedented to our knowledge.",Predictable task migration for locked caches in multi-core systems
"['Alok Desai', 'Dah-Jye Lee', 'Craig Wilson']","A feature descriptor that is robust to a number of image deformations is a basic requirement for vision based applications. Most feature descriptors work well in image deformations such as compression artifacts, illumination changes, and blurring. To develop a feature descriptor that works well apart from these image deformations like transformations caused by long baseline is a challenging task. This paper introduces a compact and efficient binary feature descriptor called PRObabilistic (PRO). A method for removing non-affine features from the initial feature list is developed, which results in further improved performance with the PRO descriptor when dealing with many deformations including long baseline between images. Feature matching accuracy using only affine features is compared with accuracy using both affine and non-affine features on benchmark datasets to demonstrate the advantages of using affine feature point for PRO descriptor.",Using affine features for an efficient binary feature descriptor
['Minoru Asogawa'],"For machines to perform classification tasks, such as speech and character recognition, appropriately handling deformed patterns is a key to achieving high performance. The authors presents a new type of classification system, an Adaptive Input Field Neural Network (AIFNN), which includes a simple pre-trained neural network and an elastic input field attached to an input layer. By using an iterative method, AIFNN can determine an optimal affine translation for an elastic input field to compensate for the original deformations. The convergence of the AIFNN algorithm is shown. AIFNN is applied for handwritten numerals recognition. Consequently, 10.83% of originally misclassified patterns are correctly categorized and total performance is improved, without modifying the neural network.",Adaptive Elastic Input Field for Recognition Improvement
"['Vittorio Miori', 'Dario Russo', 'Cesare Concordia']","The key idea underlying many Ambient Intelligence (AmI) projects and applications is context awareness, which is based mainly on their capacity to identify users and their locations. The actual computing capacity should remain in the background, in the periphery of our awareness, and should only move to the center if and when necessary. Computing thus becomes É??invisibleÉ??, as it is embedded in the environment and everyday objects. The research project described herein aims to realize an Ambient Intelligence-based environment able to improve users' quality of life by learning their habits and anticipating their needs. This environment is part of an adaptive, context-aware framework designed to make today's incompatible heterogeneous domotic systems fully interoperable, not only for connecting sensors and actuators, but for providing comprehensive connections of devices to users. The solution is a middleware architecture based on open and widely recognized standards capable of abstracting the peculiarities of underlying heterogeneous technologies and enabling them to co-exist and interwork, without however eliminating their differences. At the highest level of this infrastructure, the Ambient Intelligence framework, integrated with the domotic sensors, can enable the system to recognize any unusual or dangerous situations and anticipate health problems or special user needs in a technological living environment, such as a house or a public space.",Meeting People's Needs in a Fully Interoperable Domotic Environment
"['Lourens J. Waldorp', 'Hilde M. Huizenga', 'Raoul P. P. P. Grasman', 'K.B.E. B??cker', 'J.C. de Munck', 'P.C.M. Molenaar']","In electromagnetic source analysis, it is necessary to determine how many sources are required to describe the electroencephalogram or magnetoencephalogram adequately. Model selection procedures (MSPs) or goodness of fit procedures give an estimate of the required number of sources. Existing and new MSPs are evaluated in different source and noise settings: two sources which are close or distant and noise which is uncorrelated or correlated. The commonly used MSP residual variance is seen to be ineffective, that is it often selects too many sources. Alternatives like the adjusted Hotelling's test, Bayes information criterion and the Wald test on source amplitudes are seen to be effective. The adjusted Hotelling's test is recommended if a conservative approach is taken and MSPs such as Bayes information criterion or the Wald test on source amplitudes are recommended if a more liberal approach is desirable. The MSPs are applied to empirical data (visual evoked fields).",Model selection in electromagnetic source analysis with an application to VEFs
"['Hany Elgala', 'Raed Mesleh', 'Harald Haas']","The nonlinear characteristic of an LED (light emitting diode) imposes limitations on the performance of indoor optical wireless (OW) systems when using intensity modulation in combination with OFDM (orthogonal frequency division multiplexing). First, the impact of the nonlinear characteristic on bit-error performance is analyzed using a commercially available LED (OSRAM, SFH 4230). Second, the paper proposes a predistorter to overcome the nonlinearities. Key features of the predistorter reside in the use of the LED inverse characteristics as nonlinear distortion compensator. A DC biased optical OFDM (DCO-OFDM) system is considered and the performance without compensation and after compensation is analyzed via simulations in an AWGN (additive white Gaussian noise) environment. In this context, the bit-error performance is determined for different bias points and power back-off values applied to the OFDM signal modulating the LED intensity. It is shown that LED nonlinearity can significantly degrade the performance. However, it is demonstrated that this degradation can greatly be mitigated by using the proposed predistortion technique.",Predistortion in Optical Wireless Transmission Using OFDM
"['Jung W. Suh', 'Christopher L. Wyatt']","Computed tomography (CT) colonography is a minimally invasive screening technique for colorectal polyps, in which X-ray CT images of the distended colon are acquired, usually in the prone and supine positions of a single patient. Registration of segmented colon images from both positions will be useful for computer-assisted polyp detection. We have previously presented algorithms for registration of the prone and supine colons when both are well distended and there is a single connected lumen. However, due to inadequate bowel preparation or peristalsis, there may be collapsed segments in one or both of the colon images resulting in a topological change in the images. Such changes make deformable registration of the colon images difficult, and at present, there are no registration algorithms that can accommodate them. In this paper, we present an algorithm that can perform volume registration of prone/supine colon images in the presence of a topological change. For this purpose, 3-D volume images are embedded as a manifold in a 4-D space, and the manifold is evolved for nonrigid registration. Experiments using data from 24 patients show that the proposed method achieves good registration results in both the shape alignment of topologically different colon images from a single patient and the polyp location estimation between supine and prone colon images.",Registration Under Topological Change for CT Colonography
['Alexander N. Lozhkin'],"Often, to increase the efficiency of high-power amplifiers (HPAs), these amplifiers are driven into a nonlinear region. The nonlinearities in HPAs warp the signal constellation, increasing out-of-band radiation and causing both EVM and BER degradation. Digital predistortion (DPD) is one of the most promising linearization techniques that could lead to more efficient and cost-effective HPAs. The DPD approach works in many practical cases; however, as the bandwidth of the transmitted signal increases, the memory effect in HPAs arises. The performance of DPD algorithms that do not take the memory effect into account severely deteriorate as the bandwidth of the input signal increases. On the other hand, the DPD algorithms that do take the memory effect into account require computational complexity significantly higher than the complexity of DPD for memoryless HPA. In this paper, we propose an exact solution (turbo linearizer) that minimizes the in-band distortion caused by HPA nonlinearity with memory. In contrast to the conventional techniques, the proposed approach places the linearizer on the receiver side. The simulation results show that under certain conditions, the proposed 'turbo scheme' placed on the receiver side provides better BER performance for a given complexity than conventional DPD placed in a transmitter, especially at higher signal-to-noise ratios.",Turbo Linearizer for High Power Amplifier
"['Bhaskar Chatterjee', 'Manoj Sachdev', 'Steven L. C. Hsu', 'Ram Krishnamurthy', 'Shekhar Borkar']","This paper compares the effectiveness of different leakage control techniques in deep submicron (DSM) bulk CMOS technologies. Simulations show that the 3-5/spl times/ increase in I/sub OFF///spl mu/m per generation is offsetting the savings in switching energy obtained from technology scaling. We compare both the transistor I/sub OFF/ reduction and I/sub ON/ degradation due to each technique for the 130 nm-70 nm technologies. Our results indicate that the effectiveness of leakage control techniques and the associated energy vs. delay tradeoffs depend on the ratio of switching to leakage energies for a given technology. We use our findings to design a 70 nm low power word line driver scheme for a 256 entry, 64-bit register file (R-F). As a result, the leakage (total) energy of the word line drivers is reduced by 3/spl times/ (2.5/spl times/) and for the RF by up to 35% (25%) respectively.",Effectiveness and scaling trends of leakage control techniques for sub-130 nm CMOS technologies
"['Nicol?≠s J. Medrano-Marqu??s', 'G. Zatorre-Navarro', 'S. Celma-Pueyo']","This paper presents an adaptive processing system based on analog-digital circuits to extend the linear range of angular position sensors compensating the temperature-dependent drift. System-level simulations give a sensor output improvement higher than 70%, which drastically reduces deviations due to temperature. The building block architecture characteristics make these circuits suitable for implementing the ldquosmartnessrdquo part of smart sensors in embedded systems.",A Tunable Analog Conditioning Circuit Applied to Magnetoresistive Sensors
"['Bhargav Kanagal', 'Amol Deshpande']","In this paper, we address the problem of extending a relational database system to facilitate efficient real-time application of dynamic probabilistic models to streaming data. We use the recently proposed abstraction of model-based views for this purpose, by allowing users to declaratively specify the model to be applied, and by presenting the output of the models to the user as a probabilistic database view. We support declarative querying over such views using an extended version of SQL that allows for querying probabilistic data. Underneath we use particle filters, a class of sequential Monte Carlo algorithms, to represent the present and historical states of the model as sets of weighted samples (particles) that are kept up-to-date as new data arrives. We develop novel techniques to convert the queries on the model-based view directly into queries over particle tables, enabling highly efficient query processing. Finally, we present experimental evaluation of our prototype implementation over several synthetic and real datasets, that demonstrates the feasibility of online modeling of streaming data using our system and establishes the advantages of tight integration between dynamic probabilistic models and databases.","Online Filtering, Smoothing and Probabilistic Modeling of Streaming data"
"['Darrell Wesley Johnson', 'Jerrell R. Ballard', 'David L. Leese', 'Owen J. Eslinger']","The purpose of this study is to develop a stable apparent soil thermal diffusivity determination method for long- term scenarios that can be applied to thermal modeling. The apparent soil thermal diffusivities for three different data collections were calculated using two different analytical methods known as the amplitude and logarithmic methods. The three data collections were controlled and in-situ measurements gathered from a dirt-filled bucket and two separate arid, desert regions, respectively. Results for the data collected from the dirt- filled bucket show analytical stability for depths of 2 cm - 16 cm, while results for the first arid, desert region show an apparent possibility for analytical soil layer change detection. Results for the second arid, desert region showed relative analytical stability with some larger variations occurring during seasonal changes. As a whole, the results strongly suggest that a stable apparent soil thermal diffusivity determination method has been developed that can be used to study long-term scenarios and aid in the thermal modeling of regions similar to those in this study.",Apparent soil thermal diffusivity determinatior method for use in thermal modeling
"['Oliver Niggemann', 'Joachim Stroop']","In automotive software and system design, explicit system and especially software models have only recently found their way into the development process. This paper will try to give an overview for what such models have so-far been used and which advantages they brought to vehicle manufacturers and suppliers.   Another focus of this paper is the comparison to functional models which are already used in the automotive industry to define control algorithms and function implementation. In many cases too strong analogies have been seen between the existing functional control algorithm models and the new system models - leading to suboptimal development processes and tools. This paper will therefore try to outline differences between these model types.   Finally, a synthesis between functional, system, and software models will be sketched.",Models for model's sake: why explicit system models are also an end to themselves.
"['Anika Oellrich', 'Julius Jacobsen', 'Irene Papatheodorou', 'Damian Smedley']","Motivation: Large-scale phenotyping projects such as the Sanger Mouse Genetics project are ongoing efforts to help identify the influences of genes and their modification on phenotypes. GeneÉ??phenotype relations are crucial to the improvement of our understanding of human heritable diseases as well as the development of drugs. However, given that there are ~20 000 genes in higher vertebrate genomes and the experimental verification of geneÉ??phenotype relations requires a lot of resources, methods are needed that determine good candidates for testing.#R##N##R##N#Results: In this study, we applied an association rule mining approach to the identification of promising secondary phenotype candidates. The predictions rely on a large geneÉ??phenotype annotation set that is used to find occurrence patterns of phenotypes. Applying an association rule mining approach, we could identify 1967 secondary phenotype hypotheses that cover 244 genes and 136 phenotypes. Using two automated and one manual evaluation strategies, we demonstrate that the secondary phenotype candidates possess biological relevance to the genes they are predicted for. From the results we conclude that the predicted secondary phenotypes constitute good candidates to be experimentally tested and confirmed.#R##N##R##N#Availability: The secondary phenotype candidates can be browsed through at http://www.sanger.ac.uk/resources/databases/phenodigm/gene/secondaryphenotype/list.#R##N##R##N#Contact: ku.ca.regnas@5oa or ku.ca.regnas@5sd#R##N##R##N#Supplementary information: Supplementary data are available at Bioinformatics online.",Using association rule mining to determine promising secondary phenotyping hypotheses
"['Tammam Tillo', 'Gabriella Olmo']","We propose an algorithm that improves the performance of rate-distortion-based multiple description coding (RD-MDC). The gain is particularly significant in the high redundancy region, where RD-MDC suffers a major performance penalty with respect to MDC bounds. The improvement is obtained with negligible additional computational cost, by exploiting the coarse information also at the central decoder. The proposed method can be generalized to all MDC schemes that use scalar quantization, without modifying the quantizer structure. This feature guarantees the generation of descriptions that can be decoded without any modification of the decoder.",Improving the Performance of Multiple Description Coding Based on Scalar Quantization
['J. I. Ramos'],"A locally-analytical method for singularly perturbed two-point boundary-value problems with internal and boundary layers and with turning points is presented. The method is based on the linearization of ordinary differential equations in nonoverlapping intervals and results in linear constant-coefficients ordinary differential equations which can be integrated analytically, thus yielding piecewise analytical solutions. By imposing continuity conditions at the end points of each interval plus a smoothness condition at the common end point of two adjacent intervals, a global smooth solution is obtained. The accuracy of the globally smooth locally-analytical method is assessed by comparisons with exact and approximate solutions of several singularly perturbed problems with internal and boundary layers. It is shown that the smooth locally-analytical method is more precise than second-order accurate finite difference discretizations. It is also shown that the accuracy of the smooth locally-analytical method depends on the kind of nonlinearity and inhomogeneities of singularly perturbed ordinary differential equations, but is always higher than that of exponentially-fitted techniques based on the local solution of advection-diffusion operators.",A smooth locally-analytical technique for singularly perturbed two-point boundary-value problems
"['Honghao Gao', 'Nan Shi', 'Min Yan']","The tilings of the 2-dimensional sphere by congruent triangles have been extensively studied, and the edge-to-edge tilings have been completely classified. However, not much is known about the tilings by other congruent polygons. In this paper, we classify the simplest case, which is the edge-to-edge tilings of the 2-dimensional sphere by 12 congruent pentagons. We find one major class allowing two independent continuous parameters and four classes of isolated examples. The classification is done by first separately classifying the combinatorial, edge length, and angle aspects, and then combining the respective classifications together.",Spherical tiling by 12 congruent pentagons
"['Xin Li', 'Joseph S. Valacich', 'Traci J. Hess']","The concept of trust has emerged in the MIS field, with a focus on interpersonal or inter-organizational trust in the contexts of e-commerce and virtual teams. The inclusion of the trust concept within IS acceptance models provides further evidence that MIS researchers recognize the relevance of IS trust. While simple trust constructs have been incorporated into existing IS models, and trust models have been developed to address e-commerce issues, a comprehensive model of trust formation for a new information system has not been published. The goal of this research is to advance IS trust research by investigating the trust formation process with new IS. In this paper, two initial trust formation models, applicable to information systems, are reviewed and compared at both the conceptual and empirical levels. The first model was proposed by McKnight, Choudhury and Kacmar in an e-commerce context in 2002. The second model is based on the theory of reasoned action (TRA) and the theory of planned behavior (TPB), and was developed to predict people's trust in national identification (NID) systems. Based upon the conceptual comparison, the TRA/TPB-based trust model appears to be more powerful at predicting user trust in IS. An experimental study has been designed to compare these two models in the same context, predicting people's trust in NID systems. This empirical comparison provides more insights for understanding user trust towards IS.",Predicting user trust in information systems: a comparison of competing trust models
"['Ryan C. Hruska', 'Jessica J. Mitchell', 'Matthew O. Anderson', 'Nancy F. Glenn']","In the summer of 2010, an Unmanned Aerial Vehicle (UAV) hyperspectral calibration and characterization experiment of the Resonon PIKA II imaging spectrometer was conducted at the US Department of EnergyÉ??s Idaho National Laboratory (INL) UAV Research Park. The purpose of the experiment was to validate the radiometric calibration of the spectrometer and determine the georegistration accuracy achievable from the on-board global positioning system (GPS) and inertial navigation sensors (INS) under operational conditions. In order for low-cost hyperspectral systems to compete with larger systems flown on manned aircraft, they must be able to collect data suitable for quantitative scientific analysis. The results of the in-flight calibration experiment indicate an absolute average agreement of 96.3%, 93.7% and 85.7% for calibration tarps of 56%, 24%, and 2.5% reflectivity, respectively. The achieved planimetric accuracy was 4.6 m (based on RMSE) with a flying height of 344 m above ground level (AGL).",Radiometric and Geometric Analysis of Hyperspectral Imagery Acquired from an Unmanned Aerial Vehicle
"['Donald R. Haring', 'Dennis Ohori']","This paper presents a tabular method for synthesizing Boolean functions having four or less variables with multithreshold threshold elements. The method is similar to that used for conventional single-threshold threshold elements. All 2 2  4  functions of four variables are divided into 221 equivalence classes by variable complementations and/or permutations and/or function complementation. Each equivalence class is characterized by a subset of its corresponding Rademacher-Walsh coefficients, the size of the subset being determined by the number of thresholds required to realize that equivalence class. An arbitrary Boolean function of four or less variables is synthesized by systematically calculating subsets of its Rademacher-Walsh coefficients until, through simple equivalence operations, the equivalence class of the function is found in a table of the 221 equivalence classes. The table indicates a multithreshold realization of the given function. The table shows that any 4-variable function can be realized with at most five thresholds, or by a network of conventional, or single-threshold, threshold elements with at most three gates in which each gate has the identical weight vector for the four input variables.",A Tabular Method for the Synthesis of Multithreshold Threshold Elements
"['Xin Rong', 'Qiaozhu Mei']","The spreading of innovations among individuals and organizations in a social network has been extensively studied. Although the recent studies among the social computing and data mining communities have produced various insightful conclusions about the diffusion process of innovations by focusing on the properties and evolution of social network structures, less attention has been paid to the interrelationships among the multiple innovations being diffused, such as the competitive and collaborative relationships between innovations. In this paper, we take a formal quantitative approach to address how different pieces of innovations socialize with each other and how the interrelationships among innovations affect users' adoption behavior, which provides a novel perspective of understanding the diffusion of innovations. Networks of innovations are constructed by mining large scale text collections in an unsupervised fashion. We are particularly interested in the following questions: what are the meaningful metrics on the network of innovations? What effects do these metrics exert on the diffusion of innovations? Do these effects vary among users with different adoption preferences or communication styles? While existing studies primarily address social influence, we provide a detailed discussion of how innovations interrelate and influence the diffusion process.",Diffusion of innovations revisited: from social network to innovation network
"['Masayuki Murakami', 'Nakaji Honda', 'Junji Nishino']","This paper presents a hardware system that implements the active learning method (ALM), a methodology of soft computing. ALM has processing engines called IDS, which are tasked with extracting useful information from a system subject to modeling. In realizing ALM in hardware, it will be desirable in terms of processing nature, performance, and scalability to utilize dedicated hardware for IDS. This paper primarily describes the actual hardware design of an IDS module, and shows the findings of tests of an ALM hardware system that implemented this module.",A hardware design for a new learning system based on fuzzy concepts
"['Xiaoyan Yang', 'Cecilia M. Procopiuc', 'Divesh Srivastava']","Complex ad hoc join queries over enterprise databases are commonly used by business data analysts to understand and analyze a variety of enterprise-wide processes. However, effectively formulating such queries is a challenging task for human users, especially over databases that have large, heterogeneous schemas. In this paper, we propose a novel approach to automatically create join query recommendations based on input-output specifications (i.e.,input tables on which selection conditions are imposed, and output tables whose attribute values must be in the result of the query).The recommended join query graph includes (i) ""intermediate'' tables, and (ii) join conditions that connect the input and output tables via the intermediate tables. Our method is based on analyzing an existing query log over the enterprise database. Borrowing from program slicing techniques, which extract parts of a program that affect the value of a given variable, we first extract ""query slices'' from each query in the log. Given a user specification, we then re-combine appropriate slices to create a new join query graph, which connects the sets of input and output tables via the intermediate tables. We propose and study several quality measures to enable choosing a good join query graph among the many possibilities. Each measure expresses an intuitive notion that there should be sufficient evidence in the log to support our recommendation of the join query graph. We conduct an extensive study using the log of an actual enterprise database system to demonstrate the viability of our novel approach for recommending join queries.",Recommending Join Queries via Query Log Analysis
"['Mitre Costa Dourado', 'F?≠bio Protti', 'Jayme Luiz Szwarcfiter']","In 1923, Eduard Helly published his celebrated theorem, which originated the well known Helly property. Say that a family of subsets has the Helly property when every subfamily of it, formed by pairwise intersecting subsets, contains a common element. There are many generalizations of this property which are relevant to some parts of mathematics and several applications in computer science. In this work, we survey computational aspects of the Helly property. The main focus is algorithmic. That is, we describe algorithms for solving different problems arising from the basic Helly property. We also discuss the complexity of these problems, some of them leading to NP-hardness results.",Computational aspects of the Helly property: a survey
"['Tsz Yin Man', 'Philip K. T. Mok', 'Mansun Chan']",A high slew-rate amplifier with push-pull output driving capability is proposed to enable an ultra-low quiescent current (Iq ~ 1muA) low-dropout (LDO) regulator with improved transient responses. The proposed amplifier eliminates the tradeoff between small Iq and large slew-rate that is imposed by the tail-current in conventional amplifier design. Push-pull output stage is introduced to enhance the output driving ability. Small dropout voltage (Vbo) with large-size pass transistor and ultra-low Iq can thus be used to minimize power loss of LDO regulator without transient-response degradation. The proposed amplifier helps to improve stability of LDO regulators without using any on-chip and off-chip compensation capacitors. This is beneficial to chip-level power management requiring high-area efficiency. An LDO regulator with the proposed amplifier has been implemented in a 0.18- mum standard CMOS process and occupies 0.09 mm 2 . The LDO regulator can deliver 50-mA load current at 1-V input and ~ 100-mV V DO  . It only consumes 1.2 muA Iq and is able to recover within ~ 4 mus even under the worst case scenario.,A High Slew-Rate PushÉ??Pull Output Amplifier for Low-Quiescent Current Low-Dropout Regulators With Transient-Response Improvement
"['Hao Zou', 'Aakanksha Chowdhery', 'Sumanth Jagannathan', 'John M. Cioffi', 'J. Le Masson']","This paper investigates Orthogonal Frequency-Division Multiple-Access (OFDMA) resource-allocation schemes for two-hop relays in a home Powerline Communication (PLC) network. Unlike wireless channels, which can have noticeable path loss and shadow effects to bring a substantial relay gain, the powerline channel usually observes a quality of the relay channel on par with the direct link so that simple fixed relay configurations often lose performance instead of gaining any. Besides, the quasi-cyclostationary powerline channel benefits little from diversity combining, a scheme often found in cooperative wireless relay protocols to obtain performance improvements on fast fading channels. To address these special and challenging features of the powerline channel, Home-PLC relay protocols that jointly allocate subchannels and power to the source and relay nodes are proposed. Simulation results show that significant improvements of data rates can be achieved by jointly optimizing the subchannel and power allocation of Home-PLC relay networks.",Multi-User Joint Subchannel and Power Resource-Allocation for Powerline Relay Networks
"['Ricardo Gamelas Sousa', 'Beatriz Mora', 'Jaime S. Cardoso']","In this work we consider the problem of binary classification where the classifier may abstain instead of classifying each observation, leaving the critical items for human evaluation. This article motivates and presents a novel method to learn the reject region on complex data. Observations are replicated and then a single binary classifier determines the decision plane. The proposed method is an extension of a method available in the literature for the classification of ordinal data. Our method is compared with standard techniques on synthetic and real datasets, emphasizing the advantages of the proposed approach.",An Ordinal Data Method for the Classification with Reject Option
"['Noushin Ghaffari', 'Ivan Ivanov', 'Xiaoning Qian', 'Edward R. Dougherty']","Background#R##N#One of the most important goals of the mathematical modeling of gene regulatory networks is to alter their behavior toward desirable phenotypes. Therapeutic techniques are derived for intervention in terms of stationary control policies. In large networks, it becomes computationally burdensome to derive an optimal control policy. To overcome this problem, greedy intervention approaches based on the concept of the Mean First Passage Time or the steady-state probability mass of the network states were previously proposed. Another possible approach is to use reduction mappings to compress the network and develop control policies on its reduced version. However, such mappings lead to loss of information and require an induction step when designing the control policy for the original network.",A CoD-based stationary control policy for intervening in large gene regulatory networks
"['Jesper Jensen', 'S??ren Holdt Jensen', 'Egon Hansen']",A generalized sinusoidal model for speech signal processing is studied. The main feature of the model is that the amplitude of each sinusoidal component is allowed to vary exponentially with time. We propose to use the model in transitional speech segments such as speech onsets and voiced/unvoiced transitions. Computer simulations with natural speech signals indicate substantial better modeling performance in both transitional and voiced regions compared with the traditional constant-amplitude sinusoidal model.,Exponential sinusoidal modeling of transitional speech segments
"['Sangkyu Rho', 'March']","Distributed database design is a difficult and complex process involving two major, interrelated problems. First, data must be allocated to nodes in the network. Second given such an allocation data must be efficiently accessed, processed, and possibly communicated to meet the retrieval and update requirements of the users. Both of these problems can be formulated as constrained, integer, optimization problems; both of which are NP-hard. Genetic algorithms provide an efficient search method for problems of this type. We present a nested genetic algorithm that iteratively allocates data to nodes and determines where to perform access and processing operations to efficiently meet a specified set of retrieval and update requirements. >",A nested genetic algorithm for distributed database design
"['Adel El-Atawy', 'Taghrid Samak', 'Ehab Al-Shaer', 'Hong Li']","Packet classification plays a critical role in many of the current networking technologies, and efficient yet lightweight packet classification techniques are highly crucial for their successful deployment. Most of the current packet classification techniques exploit the characteristics of classification policies, without considering the traffic behavior in optimizing their search data structures. In this paper, we present novel techniques that utilize traffic characteristics coupled with careful analysis of the policy to obtain adaptive methods that can accommodate varying traffic statistics while maintaining a high throughput. The first technique uses segmentation of the traffic space to achieve disjoint subsets of traffic properties and build bounded depth Huffman trees using the statistics collected for these segments. The second technique simplifies the structure maintenance by keeping the segments ordered in a most-recently-used (MRU) list instead of a tree. The techniques are evaluated and their performance are compared. Moreover, attacks targeting the firewall performance are discussed and corresponding protection schemes are presented.",Using Online Traffic Statistical Matching for Optimizing Packet Filtering Performance
"['Heikki Mannila', 'Anne Patrikainen', 'Jouni K. Sepp??nen', 'Juha Kere']",ORF pairs in the same 1 291 005 0.052 0.063  100 kB 1 028 931 0.049 0.062 <0.0001,Long-range control of expression in yeast
"['Madhumita Bhattacharya', 'Jon Dron']","Recent developments in technology and access have offered the opportunity to improve online learning environments through increased communication, interactivity among participants, and incorporation of pedagogical models to evaluate the quality of learning. Resulting advantages of the different features for the distributed online learning environment have included: access to and from geographically isolated communities, multiple and collaborative participation among them, convenience of synchronous/asynchronous communication, interaction with and among individuals from diverse cultures, and ability to focus on participants' ideas, without knowledge of age, race, gender or background. We will focus on these features and discuss our research in enhancing and maintaining learning effectiveness using technology in online distributed learning environment.",In Search of Quality Learning Technologies for Online Distributed Classrooms
"['Ahmad A. Al-Yamani', 'Sadiq M. Sait', 'Habib Youssef', 'Hassan Barada']","In this paper, we present the parallelization of tabu search on a network of workstations using PVM. Two parallelization strategies are integrated: functional decomposition strategy and multi-search threads strategy. In addition, domain decomposition strategy is implemented probabilistically. The performance of each strategy is observed and analyzed. The goal of parallelization is to speedup the search in finding better quality solutions. Observations support that both parallelization strategies are beneficial, with functional decomposition producing slightly better results. Experiments were conducted for the VLSI cell placement, an NP-hard problem, and the objective was to achieve the best possible solution in terms of interconnection length, timing performance (circuit speed), and area. The multiobjective nature of this problem is addressed using a fuzzy goal-based cost computation.",Parallelizing Tabu Search on a Cluster of Heterogeneous Workstations
"['Lorenz Froihofer', 'Johannes Osrael', 'Karl M. Goeschka']","Data integrity is one of the dependability attributes in data-centric applications. However, applications exist, e.g., safety or mission critical systems, where availability is more important for dependability than strict data integrity. Consequently, in such systems availability can be increased by temporarily relaxing data integrity. Potential inconsistencies are accepted by constraint validation on replicated copies, which are potentially stale in the face of network partitions. Such consistency threats need to be bound and eventually resolved during reconciliation. The contribution of this paper is a solution approach to this trade-off between availability and integrity by means of explicit runtime-management of data integrity constraints and consistency threats as well as reconciliation support.",Trading Integrity for Availability by Means of Explicit Runtime Constraints
"['Konrad Groh', 'Sascha R??ck']","In highly flexible and complex handling systems the risk of collision of moving machine components is continuously increasing. When, for example, several robots share the same workspace and the environment in the workspace is changing due to the operation, the robot trajectories can no longer be planned in advance without the risk of collision. The objective of this paper is the collision-free motion planning of handling system manipulators from the start to the end position within a changing environment. For this purpose, the trajectories on the control system have to be planned during the operating time most efficiently depending on the current state of a varying environment. This paper describes a new approach for trajectory planning based on curve shortening flows combined with potential fields. The functional capability of the method will be demonstrated initially on a simple robot kinematics with two degrees of freedom.",A contribution to collision-free trajectory planning for handling systems in varying environments
"['St??phane S. Som??', 'Rachida Dssouli', 'Jean G. Vaucher']","Scenarios as partial behavior description, are used more and more to represent users requirements, and to conduct software engineering. The paper examines automatic generation of specifications from requirements. This is a crucial step when accuracy is desired in the requirement engineering process. Automatic construction of specifications from scenarios reduces to the merging of partial behaviors into global specifications, such that these specifications can reproduce them. The paper presents an incremental algorithm that synthesizes timed automata from scenarios with timing constraints. The algorithm is based on a formalism developed for scenarios. It uses operations semantics, and a mapping between concepts of scenarios, and those of the theory of timed automata.",From scenarios to timed automata: building specifications from users requirements
"['Sergey Bravyi', 'Barbara M. Terhal']","We study several problems related to properties of nonnegative matrices that arise at the boundary between quantum and classical probabilistic computation. Our results are twofold. First, we identify a large class of quantum Hamiltonians describing systems of qubits for which the adiabatic evolution can be efficiently simulated on a classical probabilistic computer. These are stoquastic local Hamiltonians with a É??frustration-freeÉ?ù ground-state. A Hamiltonian belongs to this class iff it can be represented as $H=\sum_{a}H_{a}$ where (1) every term $H_{a}$ acts nontrivially on a constant number of qubits, (2) every term $H_{a}$ has real nonpositive off-diagonal matrix elements in the standard basis, and (3) the ground-state of $H$ is a ground-state of every term $H_{a}$. Second, we generalize the Cook-Levin theorem proving NP-completeness of the satisfiability problem to the complexity class MA (Merlin-Arthur games)É??a probabilistic analogue of NP. Specifically, we construct a quantum version of the $k$-SAT problem which we call É??stoquastic $k$-SATÉ?ù such that stoquastic $k$-SAT is contained in MA for any constant $k$, and any promise problem in MA is Karp-reducible to stoquastic 6-SAT. This result provides the first nontrivial example of a MA-complete promise problem.",Complexity of Stoquastic Frustration-Free Hamiltonians
"['Koichi Ishida', 'Atit Tamtrakarn', 'Hiroki Ishikuro', 'Makoto Takamiya', 'Takayasu Sakurai']",SUMMARY An opamp design with outside-rail output relaxing a lowvoltage constraint on future scaled transistors is presented. The proposed opamp realizes 3-V output swing without gate-oxide stress although implemented in a 1.8-V 0.18-?Êm standard CMOS process. The 3-V-output operation is experimentally verified. The outside-rail output design with scaled transistors shows area advantage over un-scaled and inside-rail design while keeping signal-to-noise ratio and gain bandwidth constant. The chip area is estimated to be 47% of the conventional opamp using a 0.35?Êm CMOS and about an order of magnitude smaller compared with the conventional inside-rail 0.18-?Êm CMOS design due to reduced capacitor area. The proposed design could be extended to n-tuple VDD operation and applied to circuits with a feed back loop such as gain stage and filters. The extendibility of n-tuple VDD operation and its application are discussed with simulation results.,An Outside-Rail Opamp Design Relaxing Low-Voltage Constraint on Future Scaled Transistors
"['Rong Tong', 'Bin Ma', 'Haizhou Li', 'Eng Siong Chng']","This paper presents a strategy to optimize the phonotactic front-end for spoken language recognition. This is achieved by selecting a subset of phones from an existing phone recognizer's phone inventory such that only the phones that best discriminate each of the target languages are selected. Each such phone subset will be used to construct a target-oriented phone tokenizer (TOPT). In this study, we examine different approaches to construct such phone tokenizers for the front-end of a parallel phone recognizers followed  by  vector space modeling (PPR-VSM) system. We show that the target-oriented phone tokenizers derived from language-specific phone recognizers are more effective than the original parallel phone recognizers. Our experimental results also show that the target-oriented phone tokenizers derived from universal phone recognizers achieve better performance than those derived from language-specific phone recognizers. Using the proposed target-oriented phone tokenizers as the phonotactic front-end, the language recognition system performance is significantly improved without the need for additional training samples. We achieve an equal error rate (EER) of 1.27%, 1.42% and 2.73% on the NIST 1996, 2003 and 2007 LRE databases respectively for 30-s closed-set tests. This system is one of the subsystems in IIR's submission to NIST 2007 LRE.",A Target-Oriented Phonotactic Front-End for Spoken Language Recognition
"['Xinxing Xu', 'Wen Li', 'Dong Xu']","In this paper, we propose a new approach to improve face verification and person re-identification in the RGB images by leveraging a set of RGB-D data, in which we have additional depth images in the training data captured using depth cameras such as Kinect. In particular, we extract visual features and depth features from the RGB images and depth images, respectively. As the depth features are available only in the training data, we treat the depth features as privileged information, and we formulate this task as a distance metric learning with privileged information problem. Unlike the traditional face verification and person re-identification tasks that only use visual features, we further employ the extra depth features in the training data to improve the learning of distance metric in the training process. Based on the information-theoretic metric learning (ITML) method, we propose a new formulation called ITML with privileged information (ITML+) for this task. We also present an efficient algorithm based on the cyclic projection method for solving the proposed ITML+ formulation. Extensive experiments on the challenging faces data sets EUROCOM and CurtinFaces for face verification as well as the BIWI RGBD-ID data set for person re-identification demonstrate the effectiveness of our proposed approach.",Distance Metric Learning Using Privileged Information for Face Verification and Person Re-Identification
"['Jonas Schulte', 'Thomas Bopp', 'Robert Hinn', 'Thorsten Hampel']","Today's collaborative systems are usually designed as single, monolithic applications. Whenever additional functionality is needed, users are confronted with several independent applications, leading to discontinuances and media breaches in their workflows. It is therefore essential to integrate different applications into a common service-oriented infrastructure to enable such a collaborative system to offer various functionalities without unnecessary discontinuities. Atop on our now ten years of experience in the design of collaborative systems, we present a flexible and extensible service-oriented framework for collaborative systems. Our ""Wasabi"" framework builds upon modern enterprise architecture technologies.",Wasabi Framework An Open Service Infrastructure for Collaborative Work
"['John E. Moody', 'Lizhong Wu']","Our previous analysis of tick-by-tick interbank foreign exchange (FX) rates has suggested that the market is not efficient on short time scales. We find that the price changes show mean-reverting rather than random-walk behavior (Moody and Wu, 1994). The results of rescaled range and Hurst exponent analysis presented in the first part of this paper further confirms the mean-reverting attribute in the FX data. The second part of this paper reports on the highly significant correlations between Bid/Ask spreads, volatility and forecastability found in the FX data. These interactions show that higher volatility results in higher forecast error and increased risk for market makers, and that, to compensate for this increase in risk, market makers increase their Bid/Ask spreads.",Price behavior and Hurst exponents of tick-by-tick interbank foreign exchange rates
"['Jin Woo Kim', 'Chung Gu Kang', 'Byung-Jae Kwak', 'Dong Seung Kwon']","The paper considers a hybrid ARQ (HARQ) scheme for a linear pre-coding-based closed-loop multiple-input multiple-output (MIMO) system, in which a pre-coding matrix is dynamically selected for every retransmission by taking into account a symbol-level combining gain obtained from the previous receptions. A new retransmission structure with a sequence of multiple codebooks is proposed that are optimized for combining gain, rather than using a single identical codebook for every retransmission. Furthermore, a criterion for selecting a pre-coding matrix for each retransmission can be provided so as to minimize the retransmission error probability. Under the proposed multiple codebook structure for MIMO-HARQ systems, a sequence of optimum codebooks is constructed for the subsequent retransmissions. Simulation results are presented for showing that the proposed scheme achieves significant error performance advantages over conventional schemes with a single codebook and furthermore, a near-optimal performance is achieved as the codebook size chosen is large enough.",Design of a Codebook Structure for a Progressively Linear Pre-Coded Closed-Loop MIMO Hybrid ARQ System
"['Daniel E. Snyder', 'A.R. Wellens', 'Clifford E. Brown', 'Michael D. McNeese']","Three experimental paradigms used to study human-to-human and human-to-intelligent-machine relationships within a distributed decision-making environment are briefly described. Each approach focuses on different aspects of fusing multiple sources of knowledge into a coherent decision-making system. The first approach uses a computer-based team resource allocation problem to study dynamic team decision-making. The second approach uses the communication, command, and control (C3) interactive task for identifying emerging situations to study the effect of electronic media on group cohesion situation awareness and task performance. The third approach, cooperative systems methodology, proposes a methodology for integrating human decision-making with cooperative expert systems. >",Three paradigms for the study of multi-person and human-machine interaction
['Ernie Brickell'],"In this talk I will provide a description of recent uses Intel has made of cryptography in our platforms, including providing a hardware random number generator, using anonymous signatures, and improving performance of cryptographic algorithms. I will discuss how processor capabilities could be used more effectively by cryptographic algorithms. I will then discuss research questions in cryptographic protocols and platform security that are motivated by our goals.",Recent Advances and Existing Research Questions in Platform Security
"['P??ter Kacsuk', 'G?¨nter Haring', 'Szabolcs Ferenczi', 'Georg Pigel', 'G?≠bor D??zsa', 'Tibor Fadgyas']","A layered visual parallel programming approach based on object oriented model is presented, together with a methodology for constructing very high level parallel programs for massively parallel processors. After a concise description of the basic principles and the main features of the language, two examples are elaborated as case studies to demonstrate the feasibility and benefits of the proposed methodology and graphical notation.",Visual parallel programming in Monads-DPV
"['Sirpa Thessler', 'L. Kooistra', 'Frederick Teye', 'Hanna Huitu', 'A.K. Bregt']","Sensor technology, which benefits from high temporal measuring resolution, real-time data transfer and high spatial resolution of sensor data that shows in-field variations, has the potential to provide added value for crop production. The present paper explores how sensors and sensor networks have been utilised in the crop production process and what their added-value and the main bottlenecks are from the perspective of users. The focus is on sensor based applications and on requirements that users pose for them. Literature and two use cases were reviewed and applications were classified according to the crop production process: sensing of growth conditions, fertilising, irrigation, plant protection, harvesting and fleet control. The potential of sensor technology was widely acknowledged along the crop production chain. Users of the sensors require easy-to-use and reliable applications that are actionable in crop production at reasonable costs. The challenges are to develop sensor technology, data interoperability and management tools as well as data and measurement services in a way that requirements can be met, and potential benefits and added value can be realized in the farms in terms of higher yields, improved quality of yields, decreased input costs and production risks, and less work time and load.",Geosensors to Support Crop Production: Current Applications and User Requirements
"['Yong Gao', 'Joseph C. Culberson']","In this paper, we analyze the decision version of the NK landscape model from the perspective of threshold phenomena and phase transitions under two random distributions, the uniform probability model and the fixed ratio model. For the uniform probability model, we prove that the phase transition is easy in the sense that there is a polynomial algorithm that can solve a random instance of the problem with the probability asymptotic to 1 as the problem size tends to infinity. For the fixed ratio model, we establish several upper bounds for the solubility threshold, and prove that random instances with parameters above these upper bounds can be solved polynomially. This, together with our empirical study for random instances generated below and in the phase transition region, suggests that the phase transition of the fixed ratio model is also easy.",An analysis of phase transition in NK landscapes
"['Wei Peng', 'Tao Li']","IntClust is a software package for clustering gene-expression data with repeated measurements based on interval data analysis. By utilizing interval data for representing replicated microarray data, IntClust is able to take into account the scopes where replicate microarray data are distributed instead of simple data points. The soft ware package offers several transformation models for interval data representations, supports different extended distance similarity/distance measures for interval data analysis, provides some variations of modified K-means clustering, and presents three popular clustering quality evaluation measures. Our experiments show that IntClust improves the clustering performance of gene-expression microarray data over traditional approaches. The software package is available at http://cadse24.cs.fiu.edu/IntClust",IntClust: A Software Package for Clustering Replicated Microarray Data
"['Li Wang', 'J. Zhu']","Many image denoising methods can be characterized as minimizing É??loss + penalty,É?ù where the É??lossÉ?ù measures the fidelity of the denoised image to the data, and the É??penaltyÉ?ù measures the smoothness of the denoising function. In this paper, we propose two models that use the L 1 -norm of the pixel updates as the penalty. The L 1 -norm penalty has the advantage of changing only the noisy pixels, while leaving the non-noisy pixels untouched. We derive efficient algorithms that compute entire solution paths of these L 1 -norm penalized models, which facilitate the selection of a balance between the É??lossÉ?ù and the É??penalty.É?ù Copyright Springer Science+Business Media, LLC 2010",Image denoising via solution paths
"['Wilsin Gosti', 'Sunil P. Khatri', 'Alberto L. Sangiovanni-Vincentelli']","Timing closure problems occur when timing estimates computed during logic synthesis do not match with timing estimates computed from the layout of the circuit. In such a situation, logic synthesis and layout synthesis are iterated until the estimates match. The number of such iterations is becoming larger as technology scales. Timing closure problems occur mainly due to the difficulty in accurately predicting interconnect delay during logic synthesis. In this paper, we present an algorithm that integrates logic synthesis and global placement to address the timing closure problem. We introduce technology independent algorithms as well as technology dependent algorithms. Our technology independent algorithms are based on the notion of ""wire-planning"". All these algorithms interleave their logic operations with local and incremental/full global placement, in order to maintain a consistent placement while the algorithm is run. We show that by integrating logic synthesis and placement, we avoid the need to predict interconnect delay during logic synthesis. We demonstrate that our scheme significantly enhances the predictability of wire delays, thereby solving the timing closure problem. This is the main result of our paper. Our results also show that our algorithms result in a significant reduction in total circuit delay. In addition, our technology independent algorithms result in a significant circuit area reduction.",Addressing the timing closure problem by integrating logic optimization and placement
"['Thomas Kirkham', 'Ingo Dahn', 'David W. Chadwick', 'Marc Ericson C. Santos', 'Sandra Winfield']",Securing individual data objects using sticky policies in trusted networks is essential in user centric distributing computing applications. However aggregation of data objects presents a challenge in terms of sticky policy integrity for the new object. A possible solution is based on a mathematical merger of sticky polices associated with all aggregated data objects that respects all the individual policy rules in a new sticky policy for the data object. Whilst another approach is an aggregation using the policy enforcement framework of the trusted network to bypass the sticky rules. This process is enabled by the use of meta-polices that are introduced in this paper that explores the application both approaches in a employability demonstrator from the EU Framework 7 project TAS3 (Trusted Architecture for Securely Shared Services).,"Aggregating policies in user centric, real-time and distributed applications"
"['David J. Aldous', 'Shankar Bhamidi']","Consider the complete n-vertex graph whose edge-lengths are independent exponentially distributed random variables. Simultaneously for each pair of vertices, put a constant flow between them along the shortest path. Each edge gets some random total flow. In the nÉ??É??limit we find explicitly the empirical distribution of these edge-flows, suitably normalized. ?? 2010 Wiley Periodicals, Inc. Random Struct. Alg., 2010",Edge flows in the complete random-lengths network
"['Xiaodong Lu', 'Ivan Luque', 'Miho Kanda', 'Yanqing Jiang', 'Koichi Moriyama', 'Kinji Mori', 'Ryuji Takanuki', 'Yasushi Kuba']","Under a dynamic and heterogenous environment, the need for adaptability and rapid response time to information service systems has become increasingly important. To cope with the continuously changing conditions of service provision and utilization, a faded information field (FIF) has been proposed, which is an agent-based distributed information service system architecture. In the case of a mono-service request, the system is designed to improve users' access time and preserve load balancing through the information structure. However, with interdependent requests of multi-service increasing, adaptability and timeliness have to be assured by the system. In this paper, the relationship that exists among the correlated services and the users' preferences for a separate and integrated service is clarified. Based on these factors, the autonomous network-based heterogeneous information services integration technology to provide one-stop service for users multi-service requests is proposed. We proved the effectiveness of the proposed technology through the simulation and the results show that the integrated service can reduce the total users access time compared with the conventional system.",Autonomous network-based information services integration for high response in multi-agent information service systems
"['Martin Kruliè≠', 'Jakub Lokoéç', 'Christian Beecks', 'Tom?≠è≠ Skopal', 'Thomas Seidl']","The Signature Quadratic Form Distance on feature signatures represents a flexible distance-based similarity model for effective content-based multimedia retrieval. Although metric indexing approaches are able to speed up query processing by two orders of magnitude, their applicability to large-scale multimedia databases containing billions of images is still a challenging issue. In this paper, we propose the utilization of GPUs for efficient query processing with the Signature Quadratic Form Distance. We show how to process multiple distance computations in parallel and demonstrate efficient query processing by comparing many-core GPU with multi-core CPU implementations.",Processing the signature quadratic form distance on many-core GPU architectures
['Ingyu Lee'],"Consider a solution of systems of linear equations  Ax  =  b  when  A  is a symmetric positive definite matrix. We solve the equations using a direct method using Cholesky factorization,  A  =  LL   T   or a preconditioned iterative method. In this paper, we are proposing a new preconditioner based on the iterative recursion to improve the performance of conjugate gradient method. We demonstrate through experiments that our recursive iterative preconditioner improves convergence when it is used as a preconditioner for the conjugate gradient method. We conjecture that the improvements in the quality of preconditioning arise from the ability of our method to generate better approximations to the complete factor.",Towards a recursive iterative preconditioner
"['Emmanuel Roux', 'Frantz Thiessard', 'Annie Fourrier', 'Bernard B??gaud', 'Pascale Tubert-Bitter']","Pharmacovigilance aims at detecting the adverse effects of marketed drugs. It is generally based on the spontaneous reporting of events thought to be the adverse effects of drugs. Spontaneous Reporting Systems (SRSs) supply huge databases that pharmacovigilance experts cannot exhaustively exploit without data mining tools. Data mining methods; i.e., statistical association measures in conjunction with signal generation criteria, have been proposed in the literature but there is no consensus regarding their applicability and efficiency, especially since such methods are difficult to evaluate on the basis of actual data. The objective of this paper is to evaluate association measures on simulated datasets obtained with SRS modeling. We compared association measures using the percentage of false positive signals among a given number of the most highly ranked drug-event combinations according to the values of the association measures. By considering 150 drugs and 100 adverse events, these percentages of false positives, among the 500 most highly ranked drug-event couples, vary from 1.1% to 53.4% (averages over 1000 simulated datasets). As the measures led to very different results, we could identify which measures appeared to be the most relevant for pharmacovigilance.",Evaluation of statistical association measures for the automatic signal generation in pharmacovigilance
"['N. Dhananjay', 'Kai-Yew Lum', 'Jian-Xin Xu']","This brief discusses the convergence analysis of proportional navigation (PN) guidance law in the presence of delayed line-of-sight (LOS) rate information. The delay in the LOS rate is introduced by the missile guidance system that uses a low cost sensor to obtain LOS rate information by image processing techniques. A Lyapunov-like function is used to analyze the convergence of the delay differential equation (DDE) governing the evolution of the LOS rate. The time-to-go until which decreasing behaviour of the Lyapunov-like function can be guaranteed is obtained. Conditions on the delay for finite time convergence of the LOS rate are presented for the linearized engagement equation. It is observed that in the presence of line-of-sight rate delay, increasing the effective navigation constant of the PN guidance law deteriorates its performance. Numerical simulations are presented to validate the results.",Proportional Navigation With Delayed Line-of-Sight Rate
"['Patrick Adenis', 'Yicheng Wen', 'Asok Ray']","Probabilistic finite state automata (PFSA) have found their applications in diverse systems. This paper presents the construction of an inner-product space structure on a class of PFSA over the real field via an algebraic approach. The vector space is constructed in a stationary setting, which eliminates the need for an initial state in the specification of PFSA. This algebraic model formulation avoids any reference to the related notion of probability measures induced by a PFSA. A formal language-theoretic and symbolic modeling approach is adopted. Specifically, semantic models are constructed in the symbolic domain in an algebraic setting. Applicability of the theoretical formulation has been demonstrated on experimental data for robot motion recognition in a laboratory environment.",An inner product space on irreducible and synchronizable probabilistic finite state automata
"['Enric Cervera', 'Philippe Martinet']","Neither of the classical visual servoing approaches, position-based or image-based methods, are completely satisfactory. This paper presents a different approach with some advantages of both, i.e., the trajectory of the camera motion is predictable and the image features remain in the field of view of the camera. Our new approach is based on the computation of the pose of the object from the image, thus an appropriate calibration of the camera and a geometric model of the object are required. Experimental results on a real robotic platform are presented.",Visual servoing with indirect image control and a predictable camera trajectory
"['Yang Xiang', 'Alfredo Cuzzocrea', 'Michael Hobbs', 'Laurence T. Yang']",,"Advances in parallel, distributed, embedded, and ubiquitous systems"
"['Georg Grossmann', 'Michael Schrefl', 'Markus Stumptner']","Service composition is a recent field that has seen a flurry of different approaches proposed towards the goal of flexible distributed heterogeneous interoperation of software systems, usually based on the expectation that such systems must be derived from higher level models rather than be coded at low level.#R##N##R##N#We propose a conceptual modelling approach for the composition of service processes into a task workflow and its dynamic adaption to changes during runtime. The contribution of the paper is twofold. Firstly, our approach improves on existing approaches by the separation of configuration into service configuration and service instance configuration which reduces the reconfiguration cycles in case of changes and secondly, we describe a comprehensive modelling concept that combines existing dynamic composition techniques in a novel way.",A conceptual modeling approach for web service composition supporting service re-configuration
"['Dominik Gall', 'Riko Jacob', 'Andr??a W. Richa', 'Christian Scheideler', 'Stefan Schmid', 'Hanjo T??ubig']","Topological self-stabilization is an important concept to build robust open distributed systems (such as peer-to-peer systems) where nodes can organize themselves into meaningful network topologies. The goal is to devise distributed algorithms that converge quickly to such a desirable topology, independently of the initial network state. This paper proposes a new model to study the parallel convergence time. Our model sheds light on the achievable parallelism by avoiding bottlenecks of existing models that can yield a distorted picture. As a case study, we consider local graph linearizationÉ??i.e., how to build a sorted list of the nodes of a connected graph in a distributed and self-stabilizing manner. We propose two variants of a simple algorithm, and provide an extensive formal analysis of their worst-case and best-case parallel time complexities, as well as their performance under a greedy selection of the actions to be executed.",Time complexity of distributed topological self-stabilization: the case of graph linearization
"['Yindi Jing', 'Hamid Jafarkhani']","This paper considers the interference cancellation (IC) problem in multi-user wireless relay networks. First, it is shown that using distributed space-time coding (DSTC), the multiple antenna IC scheme previously proposed for systems with direct transmissions can be applied to relay networks. The ML decoding after full IC can be performed symbol by symbol. Then, by allowing IC at relays, a new degree of freedom in relay network design is discovered. With this new idea, the required number of antennas at the receiver for full IC can be reduced and a balance between diversity and delay can be obtained.",Interference Cancellation in Distributed Space-Time Coded Wireless Relay Networks
"['Deyun Gao', 'Jianfei Cai', 'Chang Wen Chen']","With the increasing popularity of using wireless local area networks (WLANs) for Internet access, the controlled channel access mechanism in IEEE 802.11e WLANs, i.e., HCF controlled channel access (HCCA), has received much more attention since its inherent centralized mechanism is more efficient in handling time-bounded multimedia traffic. So far, only a few research studies address the admission control problem of variable bit rate (VBR) traffic over HCCA. These existing studies consider each traffic flow individually and, thus, cannot exploit the statistical multiplexing gain among multiple VBR traffic flows. In this paper, we apply the existing statistical multiplexing framework to the studied admission control problem, with all the features of IEEE 802.11e HCCA being taken into consideration. Experimental results show that our proposed admission control scheme achieves significant improvement in network utilization while still satisfying all the quality-of-service (QoS) requirements.",Admission Control Based on Rate-Variance Envelop for VBR Traffic Over IEEE 802.11e HCCA WLANs
"['Zhang Yuemei', 'Li Yan-xi']","Disclosure quality is very important to listed companies and capital market. Based on factor analysis method this paper presents a disclosure quality index that is well verified by the assessment results of Chinese Shenzhen Stock Exchange. The index focuses on three aspects: corporate governance, firm structure characteristics and financial situation. The index has three advantages compared with previous researches: number of factors is less than other measurement methods; all weights of the variables are objective and avoid subjective errors; the results may assist in determining the quality disclosure and explaining the variation of current and prospective extent.",Information Disclosure Quality: Factors and Measurement
['Lech Polkowski'],"This work aims at presenting to a wider audience fundamental notions and ideas of rough mereology. We discuss various methods for constructing rough inclusions in data sets, then we show how to apply them to the task of knowledge granulation, and finally, we introduce granular reflections of data sets with examples of classifiers built on them.",Rough mereology in analysis of vagueness
"['Joseph Kairouz', 'A. Lam', 'Alfred S. Malowany', 'Franco A. Carnevale', 'Ron D. Gottesman']","A vital sign monitoring system was developed for the Patient Data Management System (PDMS) of the Pediatric intensive care unit (PICU) at the Montreal Children's Hospital. Fourteen bedside monitors were linked via a local area network (LAN) to a personal computer acting as a medical, nursing and administrative decision support tool. This paper discusses the implementation of the Expert Monitoring System (EMS) module of the PDMS and some evaluation results for the expert system. >",A vital sign monitoring system for a pediatric intensive care unit
"['Miroslaw Galicki', 'D. Uciè?ski']","An approach to planning time-optimal collision-free motions of robotic manipulators is presented. It is based on using a negative formulation of the Pontryagin Maximum Principle which handles efficiently various control and/or state constraints imposed on the manipulator motions, which arise naturally out of manipulator joint limits and obstacle avoidance. This approach becomes similar to that described by Weinreb and Bryson, as well as by Bryson and Ho if no state inequality constraints are imposed. In contrast to the penalty function method, the proposed algorithm does not require an initial admissible solution (i.e. an initial admissible trajectory) and finds manipulator trajectories with a smaller cost value than the penalty function approach. A computer example involving a planar redundant manipulator of three revolute kinematic pairs is included. The numerical results are compared with those obtained using an exterior penalty function method.",Time-optimal motions of robotic manipulators
"['Tingting Han', 'Joost-Pieter Katoen', 'Alexandru Mereacre']","This paper proposes a technique to synthesize parametric rate values in continuous-time Markov chains that ensure the validity of bounded reachability properties. Rate expressions over variables indicate the average speed of state changes and are expressed using the polynomials over reals. The key contribution is an algorithm that approximates the set of parameter values for which the stochastic real-time system guarantees the validity of bounded reachability properties. This algorithm is based on discretizing parameter ranges together with a refinement technique. This paper describes the algorithm, analyzes its time complexity, and shows its applicability by deriving parameter constraints for a real-time storage system with probabilistic error checking facilities.",Approximate Parameter Synthesis for Probabilistic Time-Bounded Reachability
"['Paolo Atzeni', 'Luigi Bellomarini', 'Francesca Bugiotti', 'Fabrizio Celli', 'Giorgio Gianforme']","To support heterogeneity is a major requirement in current approaches to integration and transformation of data. This paper proposes a new approach to the translation of schema and data from one data model to another, and we illustrate its implementation in the tool MIDST-RT. We leverage on our previous work on MIDST, a platform conceived to perform translations in an off-line fashion. In such an approach, the source database (both schema and data) is imported into a repository, where it is stored in a universal model. Then, the translation is applied within the tool as a composition of elementary transformation steps, specified as Datalog programs. Finally, the result (again both schema and data) is exported into the operational system. Here we illustrate a new, lightweight approach where the database is not imported. MIDST-RT needs only to know the schema of the source database and the model of the target one, and generates views on the operational system that expose the underlying data according to the corresponding schema in the target model. Views are generated in an almost automatic way, on the basis of the Datalog rules for schema translation. The proposed solution can be applied to different scenarios, which include data and application migration, data interchange, and object-to-relational mapping between applications and databases.",A runtime approach to model-generic translation of schema and data
"['Yekutiel Avargel', 'Israel Cohen']","In this paper, we introduce an adaptive algorithm for nonlinear system identification in the short-time Fourier transform (STFT) domain. The adaptive scheme consists of a parallel combination of a linear component, represented by crossband filters between subbands, and a quadratic component, which is modeled by multiplicative cross-terms. We adaptively update the model parameters using the least-mean-square (LMS) algorithm, and derive explicit expressions for the transient and steady-state mean-square error (MSE) in frequency bins for white Gaussian inputs. We show that estimation of the nonlinear component improves the MSE performance only when the power ratio of nonlinear to linear components is relatively high. Furthermore, as the number of crossband filters increases, a lower steady-state MSE may be obtained at the expense of slower convergence. Experimental results support the theoretical derivations.",Adaptive Nonlinear System Identification in the Short-Time Fourier Transform Domain
"['Mauro Pezz??', 'Michal Young']","As software evolves from early architectural sketches to final code, a variety of representations are appropriate. Moreover, at most points in development, different portions of a software system are at different stages in development, and consequently in different representations. State-space analysis techniques (reachability analysis, model checking, simulation, etc.) have been developed for several representations of concurrent systems, but each tool or technique has typically been targeted to a single design or program notation.We describe an approach to constructing space analysis tools using a core set of basic representations and components. Such a tool generation approach differs from translation to a common formalism. We need not map every supported design formalism to a single internal form that completely captures the original semantics; rather, a shared ""inframodel"" represents only the essential information for interpretation by tool components that can be customized to reflect the semantics of each formalism. This results in more natural and compact internal representations, and more efficient analysis, than a purely translational approach.We illustrate the approach by applying the prototype tool to a small example problem, coordination of access to a coffee machine. The coffee machine is controlled by an Ada program, and the protocol of human users is modeled with Petri nets. Nets and process graph models are represented in the common internal form, and their composite behavior is analyzed by the prototype tool.",Generation of multi-formalism state-space analysis tools
"['Saugata Basu', 'Richard Pollack', 'Marie-Fran??oise Roy']","We consider a family ofspolynomials, P = {P1, ?,Ps}, inkvariables with coefficients in a real closed fieldR, each of degree at mostd, and an algebraic varietyVof real dimensionk? which is defined as the zero set of a polynomialQof degree at mostd. The number of semi-algebraically connected components of all non-empty sign conditions on P overVis bounded bysk?(O(d))k. In this paper we present a new algorithm to compute a set of points meeting every semi-algebraically connected component of each non-empty sign condition of P overV. Its complexity issk? + 1dO(k). This interpolates a sequence of results between the Ben-Or?Kozen?Reif algorithm which is the casek? = 0, in one variable, and the Basu?Pollack?Roy algorithm which is the casek? =k. It improves the results where the same problem was solved in timesk? + 1dO(k?k).",On computing a set of points meeting every cell defined by a family of polynomials on a variety
['Heather Brown'],"SUMMARY Interactive editing and layout of high quality multi-media documents is a demanding application that is limited by the processing power available from current workstations. This short paper takes a preliminary look at the opportunities for exploiting parallelism within the document layout process, and suggests that radically new ways of thinking may be needed to take advantage of the enormous parallel processing capabilities offered by a new generation of workstations based on configurable networks of Transputers.",Parallel processing and document layout
"['Farshad Rafii', 'Sam Perkins']","Today's global markets and multinational networks demand the simultaneous release of applications in many languages. Concurrent engineering offers a solution to this challenge, if the benefits of a potential market can be successfully balanced against the risk of untried technologies. The authors present a framework of options for doing so. >",Internationalizing software with concurrent engineering
"['Jochen H. Schiller', 'Georg Carle']","Implementations of communication protocols are typically based on highly specialized components, e.g., VLSI chips for ATM Adaptation Layer processing. The development of these specialized implementations is very time-consuming and, thus, justified only for high volumes. For a large number multimedia applications, it would be desirable to be able to provide quickly high-performance implementations of protocols with new functionality. Using a unifying design method and suitable design tools, this goal can be achieved today. Our solution is based on a standard language for protocol specification, a generic target architecture for protocol implementation, and customized/commercial tools for mapping of the protocol specification onto the target platform. Using this approach for a powerful ATM Adaptation Layer protocol featuring forward error correction and retransmission we demonstrate the applicability for complex protocols with stringent timing requirements. Additionally, our approach results in fairly small hardware solutions compared to common processors.",Semi-automated design of high-performance communication subsystems
"['Martin Goldstern', 'Saharon Shelah']","1. For many regular cardinals lambda (in particular, for all successors of singular strong limit cardinals, and for all successors of singular omega-limits), for all n in {2,3,4, ...} : #R##N#There is a linear order L such that L^n has no (incomparability-)antichain of cardinality lambda, while L^{n+1} has an antichain of cardinality lambda . #R##N#2. For any nondecreasing sequence (lambda2,lambda3, ...) of infinite cardinals it is consistent that there is a linear order L such that L^n has an antichain of cardinality lambda_n, but not one of cardinality lambda_n^+ .",ANTICHAINS IN PRODUCTS OF LINEAR ORDERS
"['C??line Rouveirol', 'Nicolas Stransky', 'Philippe Hup??', 'Philippe La Rosa', 'Eric Viara', 'Emmanuel Barillot', 'Fran??ois Radvanyi']","Motivation: The identification of recurrent genomic alterations can provide insight into the initiation and progression of genetic diseases, such as cancer. Array-CGH can identify chromosomal regions that have been gained or lost, with a resolution of É?¨1É??mb, for the cutting-edge techniques. The extraction of discrete profiles from raw array-CGH data has been studied extensively, but subsequent steps in the analysis require flexible, efficient algorithms, particularly if the number of available profiles exceeds a few tens or the number of array probes exceeds a few thousands.#R##N##R##N#Results: We propose two algorithms for computing minimal and minimal constrained regions of gain and loss from discretized CGH profiles. The second of these algorithms can handle additional constraints describing relevant regions of copy number change. We have validated these algorithms on two public array-CGH datasets.#R##N##R##N#Availability: From the authors, upon request.#R##N##R##N#Contact: celine@lri.fr#R##N##R##N#Supplementary information: Supplementary data are available at Bioinformatics online.",Computation of recurrent minimal genomic alterations from array-CGH data
"['Elisabetta Genovese', 'St??phane Roche', 'Claude Caron', 'Robert Feick']","The EcoGeo II project has, as its main goal, the establishment of an economic model to evaluate geographic information (GI). The first phase of the EcoGeo Project has provided a visual representation, called Socioscope, of the overall flows of geospatial data between the main private and public stakeholders of the geomatic sector in the province of Quebec (Canada).#R##N##R##N#From this foundation, EcoGeo Phase II was launched in 2008 with several goals. The first goal was to analyze the most important existing research and approaches to evaluate the economic value of the GI sector. The results show that the value chain concept is, in theory, one of the most suitable approaches that can be adapted to assess GI value, but also one of the most complex due to the number of variables involved with how GI is produced and used within and between organizations.#R##N##R##N#Our second goal was to define the basis or conventions for evaluating GI and, more specifically, to develop a list of parameters which need to be considered for evaluating GI. We defined a set of guidelines that we called the EcoGeo cookbook, which aims at identifying, listing and describing the most important variables and attributes relating to GI value which have been identified in literature. #R##N##R##N#These attributes relate to how GI is produced and used (value of the location attribute, time dependency, quality, etc.), the costs of the GI product (i.e. transaction costs) and the price definition (based on value pricing strategy). Caution must be used when evaluating intangible benefits, which are less easily estimated than tangible ones.#R##N##R##N#The final goal will be to implement such variables and attributes into the SocioscopeÉ??s database structure. This will also allow for the definition of a specific value chain for the GI sector in Quebec.",The EcoGeo Cookbook for the Assessment of Geographic Information Value
"['Chungsoo Lim', 'Soojeong Lee', 'Jae-Hun Choi', 'Joon-Hyuk Chang']","SUMMARY In this letter, we propose a simple but effective technique that improves statistical model-based voice activity detection (VAD) by both reducing computational complexity and increasing detection accuracy. The improvements are made by applying Taylor series approximations to the exponential and logarithmic functions in the VAD algorithm based on an in-depth analysis of the algorithm. Experiments performed on a smartphone as well as on a desktop computer with various background noises confirm the effectiveness of the proposed technique.",Efficient Implementation of Statistical Model-Based Voice Activity Detection Using Taylor Series Approximation
"['Gwanggil Jeon', 'Jongmin You', 'Jechang Jeong']","Video deinterlacing can be realized using time-space interpolation filters. To improve video deinterlacing quality with respect to missing pixels on moving diagonal lines, we developed a fuzzy concept that utilizes deinterlacing methods. The proposed algorithm consists of three parts. The first part is fuzzy rule-assisted edge-preserving-based deinterlacing (FED), which has an edge-preserving unit that utilizes fuzzy theory to find the most accurate edge direction and interpolates the missing pixels. Using the introduced gradients in the interpolation, the vertical resolution in the deinterlaced image is subjectively concealed. The second part is weighted fuzzy-reasoning-assisted deinterlacing (WFD), which works in the spatio-temporal domain. In this part, the computed weights are considered and multiplied by the candidate deinterlaced pixels, which successively build approximations of the deinterlaced sequence. The third part of the proposed algorithm is weighted fuzzy switching filtering, which analyzes the suitability of each method on system performance and uses a switching algorithm between FED and WFD. The outcome of image interpolation can be adjusted continuously by varying the setting of the membership function for fuzzy inference. Compared with conventional image interpolation methods, the algorithm presented in this paper provides improved edge quality in the deinterlaced image without the introduction of evident artifacts.",Weighted Fuzzy Reasoning Scheme for Interlaced to Progressive Conversion
"['Vincent Andrieu', 'Gildas Besancon', 'Ulysse Serres']","This paper is about necessary conditions for the existence of an observer in the case of nonlinear systems. Those conditions are first highlighted in terms of detectability, for observers ensuring asymptotic state reconstruction. They then take the form of stronger observability notions, for the case of tunable observers, that is observers with a tunable rate of state reconstruction.",Observability necessary conditions for the existence of observers
"['Amandeep S. Sidhu', 'Tharam S. Dillon', 'Farookh Khadeer Hussain']","Recent progress in proteomics, computational biology, and ontology development has presented an opportunity to investigate protein data sources from a unique perspective that is, examining protein data sources through structure and hierarchy of Protein Ontology (PO). Various data mining algorithms and mathematical models provide methods for analyzing protein data sources; however, there are two issues that need to be addressed: (1) the need for standards for defining protein data description and exchange and (2) eliminating errors which arise with the data integration methodologies for complex queries. Protein Ontology is designed to meet these needs by providing a structured protein data specification for Protein Data Representation. Protein Ontology is a standard for representing protein data in a way that helps in defining data integration and data mining models for Protein Structure and Function. We report here our development of PO; a semantic heterogeneity framework based on relationships between PO concepts; and analysis of resultant PO Data of Human Proteins. We also talk in this paper briefly about our ongoing work of designing a trustworthy framework around PO.",Accomplishments and Challenges of Protein Ontology
['Fernando Rosa-Velardo'],"Mobile Synchronizing Petri Nets (MSPN's) are a model for mobility and coordination based on coloured Petri Nets, in which systems are composed of a collection of (possibly mobile) hardware devices and mobile agents, both modelled homogenously. In this paper we approach their verification, for which we have chosen to code MSPN's into rewriting logic. In order to obtain a representation of MSPN systems by means of a rewrite theory, we develop a class of them, that we call @n-Abstract Petri nets (@n-APN's), which are easily representable in that framework. Moreover, the obtained representation provides a local mechanism for fresh name generation. Then we prove that, even if @n-APN's are a particular class of MSPN systems, they are strong enough to capture the behaviour of any MSPN system. We have chosen Maude to implement @n-APN's, as well as the translation from MSPN's to @n-APN's, for which we make intensive use of its reflective features.",Coding Mobile Synchronizing Petri Nets into Rewriting Logic
['Richard T. Snodgrass'],"Computer science is often divided into two camps, systems and theory, but of course the reality is more complicated and more interesting than that. One example is the area of ""experimental algorithmics,"" also termed ""empirical algorithmics."" This fascinating discipline marries algorithm analysis, which is often done with mathematical proofs, with experimentation with real programs running on real machines.",On experimental algorithmics: an interview with Catherine McGeoch and Bernard Moret
"['Fengjie Geng', 'Dan Liu', 'Deming Zhu']","The bifurcations of generic heteroclinic loop with one nonhyperbolic equilibrium p1 and one hyperbolic saddle p2 are investigated, where p1 is assumed to undergo transcritical bifurcation. Firstly, we discuss bifurcations of heteroclinic loop when transcritical bifurcation does not happen, the persistence of heteroclinic loop, the existence of homoclinic loop connecting p1 (resp. p2) and the coexistence of one homoclinic loop and one periodic orbit are established. Secondly, we analyze bifurcations of heteroclinic loop accompanied by transcritical bifurcation, namely, nonhyperbolic equilibrium p1 splits into two hyperbolic saddles and , a heteroclinic loop connecting and p2, homoclinic loop with (resp. p2) and heteroclinic orbit joining and (resp. and p2; p2 and ) are found. The results achieved here can be extended to higher dimensional systems.",BIFURCATIONS OF GENERIC HETEROCLINIC LOOP ACCOMPANIED BY TRANSCRITICAL BIFURCATION
"['Ué?ur G?¨d?¨kbay', 'B?¨lent ??zg?¨??']","Physically-based modeling remedies the problem of producing realistic animation by including forces, masses, strain energies, and other physical quantities. The behavior of physically-based models is governed by the laws of rigid and nonrigid dynamics expressed through a set of equations of motion. This paper discusses various formulations for animating deformable models. The formulations based on elasticity theory express the interactions between discrete deformable model points using the stiffness matrices. These matrices store the elastic properties of the models and they should be evolved in time according to changing elastic properties of the models. An alternative to these formulations seems to be external force formulations of different types. In these types of formulations, elastic properties of the materials are represented as external spring or other tensile forces as opposed to forming complicated stiffness matrices. >",Animating deformable models: different approaches
"['Alessandro Mei', 'Julinda Stefa']","In this paper, we consider security-related and energy efficiency issues in multihop wireless networks. We start our work from the observation, known in the literature, that shortest path routing creates congested areas in multihop wireless networks. These areas are critical-they generate both security and energy efficiency issues. We attack these problems and set out routing in outer space, a new routing mechanism that transforms any shortest path routing protocol (or approximated versions of it) into a new protocol that does not create congested areas, does not have the associated security-related issues, and does not encourage selfish positioning. Moreover, the network is more energy efficient than the same network using the original routing protocol (in spite of using more energy globally) and dies more gracefully. We also describe applications of our idea to mobility and to a security protocol for the detection of node replication attacks.",Routing in Outer Space: Fair Traffic Load in Multihop Wireless Networks
"['Tao Lu', 'Jianhua Ge', 'Ye Yang', 'Yang Gao']","Signal space cooperative (SSC) system can realize full-rate transmission while achieving maximum cooperative diversity gain at the same time by using the signal space diversity technique. In this letter, we investigate the bit error rate (BER) performance of the SSC system over independent and non-identically distributed (i.ni.d.) Nakagami-m fading channels. Particularly, an accurate analytical expression for the overall BER is derived. Moreover, a tight approximation for the BER is obtained to reveal that the achievable diversity order of the SSC system is {{m}_{SD}}+\min { {{m}_{SR}},{{m}_{RD}} }, where m_SD, m_SR, and m_RD are the Nakagami fading parameters for the source-destination, source-relay, and relay-destination channels, respectively. Simulation results are presented to verify our theoretical analysis.",On Bit Error Performance of Full-Rate Signal Space Cooperative Communication over Nakagami-m Fading Channels
"['Humberto Miguel Garay-Malpartida', 'J?Êao Marcelo Occhiucci', 'Juliano Alves', 'Jos?? E. Beliz?≠rio']","Motivation:In vitro studies have shown that the most remarkable catalytic features of caspases, a family of cysteineproteases, are their stringent specificity to Asp (D) in the S1 subsite and at least four amino acids to the left of scissile bound. However, there is little information about the substrate recognition patterns in vivo. The prediction and characterization of proteolytic cleavage sites in natural substrates could be useful for uncovering these structural relationships.#R##N##R##N#Results: PEST-like sequences rich in the amino acids Ser (S), Thr (T), Pro (P), Glu or Asp (E/D), including Asn (N) and Gln (Q) are adjacent structural/sequential elements in the majority of cleavage site regions of the natural caspase substrates described in the literature, supporting its possible implication in the substrate selection by caspases. We developed CaSPredictor, a software which incorporated a PEST-like index and the position-dependent amino acid matrices for prediction of caspase cleavage sites in individual proteins and protein datasets. The program predicted successfully 81% (111/137) of the cleavage sites in experimentally verified caspase substrates not annotated in its internal data file. Its accuracy and confidence was estimated as 80% using ROC methodology. The program was much more efficient in predicting caspase substrates when compared with PeptideCutter and PEPS software. Finally, the program detected potential cleavage sites in the primary sequences of 1644 proteins in a dataset containing 9986 protein entries.#R##N##R##N#Availability: Requests for software should be made to Dr Jose E. Belizario#R##N##R##N#Contact: jebeliza@usp.br#R##N##R##N#Supplementary information: Supplementary information is available for academic users at site http://icb.usp.br/~farmaco/Jose/CaSpredictorfiles",CaSPredictor: a new computer-based tool for caspase substrate prediction
"['Lars Bauer', 'Muhammad Shafique', 'J??rg Henkel']","Processors that deploy fine-grained reconfigurable fabrics to implement application-specific accelerators on-demand obtained significant attention within the last decade. They trade-off the flexibility of general-purpose processors with the performance of application-specific circuits without tailoring the processor towards a specific application domain like Application Specific Instruction Set Processors (ASIPs). Vast amounts of reconfigurable processors have been proposed, differing in multifarious architectural decisions. However, it has always been an open question, which of the proposed concepts is more efficient in certain application and/or parameter scenarios. Various reconfigurable processors were investigated in certain scenarios, but never before a systematic design space exploration across diverse reconfigurable processor concepts has been conducted with the aim to aid a designer of a reconfigurable processor.   We have developed a first-of-its-kind comprehensive design space exploration tool that allows to systematically explore diverse reconfigurable processors and architectural parameters. Our tool allows presenting the first cross-architectural design space exploration of multiple fine-grained reconfigurable processors on a fair comparable basis. After categorizing fine-grained reconfigurable processors and their relevant parameters, we present our tool and an in-depth analysis of reconfigurable processors within different relevant scenarios.",Cross-architectural design space exploration tool for reconfigurable processors
['Kathryn Leonard'],"We propose efficiency of representation as a criterion for evaluating shape models, then apply this criterion to compare the boundary curve representation with the medial axis. We estimate the a-entropy of two compact classes of curves. We then construct two adaptive encodings for noncompact classes of shapes, one using the boundary curve and the other using the medial axis, and determine precise conditions for when the medial axis is more efficient. Along the way we construct explicit near-optimal boundarybased approximations for compact classes of shapes and an explicit compression scheme for non-compact classes of shapes based on the medial axis. We end with an application of the criterion to shape data.",An Efficiency Criterion for 2D Shape Model Selection.
"['Mansour Shafaei', 'Ahmad Patooghy', 'Seyed Ghassem Miremadi']","This paper proposes a Numeral-Based Crosstalk Avoidance Coding (NB-CAC) to protect communication channels of Network-on-Chips (NoCs) against crosstalk faults. The NB-CAC scheme produces code words without bit patterns '101' and '010' to eliminate harmful transition patterns from NoC channels. This is done by the use of a new numeral system proposed in the paper. Using the proposed numeral system, the NB-CAC scheme 1) can be utilized in NoC channels with any arbitrary width, and 2) can be implemented with low area, power, and timing overheads. VHDL and SPICE simulations have been carried out for a wide range of channel widths to evaluate delay, area, and power consumption of the NB-CAC codecs. Results of simulations reveal that the NB-CAC scheme completely removes crosstalk faults from NoC channel. In addition, the NB-CAC scheme provides reductions of 17.3% in area and 31.9% in power-delay product with respect to Fibonacci-based coding which has been recently proposed in literature.",Numeral-Based Crosstalk Avoidance Coding to Reliable NoC Design
"['Hong-Kwang Jeff Kuo', 'Lidia Mangu', 'Ahmad Emami', 'Imed Zitouni', 'Young-Suk Lee']","We report word error rate improvements with syntactic features using a neural probabilistic language model through N-best re-scoring. The syntactic features we use include exposed head words and their non-terminal labels both before and after the predicted word. Neural network LMs generalize better to unseen events by modeling words and other context features in continuous space. They are suitable for incorporating many different types of features, including syntactic features, where there is no pre-defined back-off order. We choose an N-best re-scoring framework to be able to take full advantage of the complete parse tree of the entire sentence. Using syntactic features, along with morphological features, improves the word error rate (WER) by up to 5.5% relative, from 9.4% to 8.6%, on the latest GALE evaluation test set.",Syntactic features for Arabic speech recognition
"['Katsuya Tanaka', 'Hiroaki Higaki', 'Makoto Takizawa']","In distributed applications, multiple objects are cooperated to achieve some objective. The objects may suffer from certain kinds of faults. If some object o is faulty, it is rolled back to the checkpoint and objects which have received messages from o are also required to be rolled back. In this paper, we define influential messages whose receivers are required to be rolled back if the senders are rolled back in the object-based computation model. By using these influential messages, object-based checkpoints are defined to denote semantically consistent global states of the system which are nevertheless inconsistent with the traditional message-based definition. We show how the number of checkpoints can be reduced by taking only the object-based checkpoints.",Object-based checkpoints in distributed systems
"['Luc Devroye', 'James Allen Fill', 'Ralph Neininger']",The weak limit of the normalized number of comparisons needed by the Quicksort algorithm to sort  n  randomly permuted items is known to be determined implicitly by a distributional fixed-point equation. We give an algorithm for perfect random variate generation from this distribution.,Perfect simulation from the Quicksort limit distribution
"['Fakhrul Zaman Rokhani', 'Gerald E. Sobelman']","A comprehensive analysis of energy consumption for voltage-mode multilevel signals on a nanometer-technology bus is presented. A transition-dependent model is used which allows simplified calculation of the energy consumption. The accuracy of the approach is demonstrated using circuit simulations of three different electrical models of the bus, namely, lumped-C, distributed-RC, and distributed- RLC networks. We also verify that bus energy consumption is independent of driver resistance, as predicted by the model. Finally, we present a comparative analysis of power consumption for multilevel and binary buses.",Bus Energy Consumption for Multilevel Signals
"['Ullrich Hafner', 'J?¨rgen Albert', 'Stefan Frank', 'Michael A. Unger']","Weighted finite automata (WFA) exploit self-similarities within single pictures and also sequences of pictures to remove spatial and temporal redundancies. Their implementation then combines techniques from hierarchical methods related to quadtrees and from vector quantization to achieve performance results for low bit rates which can be put on a par with state-of-the-art codecs like embedded zero-tree wavelet coding. Due to their simple mathematical structure, WFA provide an ideal platform for efficient hybrid compression methods. Therefore, WFA were chosen as a starting point for a fractal-like video compression integrating a hierarchical motion compensation as well as an option to vary the compression quality between ""centers of interest"" and ""background"" in a flexible manner.",Weighted finite automata for video compression
"['Vivek Elangovan', 'Markus Dietl', 'Puneet Sareen']",A new method of designing a very high bandwidth semi-digital PLL with a large operating frequency range from 100MHz to 1GHz is proposed. The PLL is modelled in Z-domain. The simulation results is also matched with the modelling to ensure that the PLL is stable for very high bandwidth. The bandwidth achieved is (1 over 4) th  of the input reference frequency for the whole operating range mentioned.,Very high bandwidth semi-digital PLL with large operating frequency range
"['Maximilian Matthe', 'Nicola Michailow', 'Ivan Gaspar', 'Gerhard Fettweis']",,Influence of pulse shaping on bit error rate performance and out of band radiation of Generalized Frequency Division Multiplexing
"['Heng Cao', 'Zhengyang Ling', 'Jun Zhu', 'Yu Wang', 'Wei Wang']","This paper presents a design frame of a leg ex-oskeleton. The performance of an exoskeleton was analyzed by studying a model of a linear coupled 1-DOF human-exoskeleton system. The results showed that a low-impedance, anthropomorphic and well attached mechanical structure is benefit for the control of exoskeleton, and a direct force feedback control could also be implemented to further reduce the impedance. Three kinds of applied sensor interface were compared. A leg exoskeleton called ELEBOT was developed for validating control algorithm. The ELEBOT included four main parts: mechanical structure, hydraulic actuator, sensor and control system and power. The function of such a load-carrying robot is to reduce the injuries associated with the back and lower limbs after a long heavy walk. Experiment results with a simple force controller confirmed that ELEBOT could efficiently assist people walking with 30 kg payload now.",Design frame of a leg exoskeleton for load-carrying augmentation
"['Yanqin Bai', 'Feifei Wang', 'Xi Luo']","In this paper we propose a primal-dual interior-point algorithm for convex quadratic semidefinite optimization problem. The search direction of algorithm is defined in terms of a matrix function and the iteration is generated by full-Newton step. Furthermore, we derive the iteration bound for the algorithm with small-update method, namely, O(Vn log n/e), which is best-known bound so far.",A Polynomial-time Interior-point Algorithm for Convex Quadratic Semidefinite Optimization
"['Gang Qian', 'Rama Chellappa', 'Qinfen Zheng']","In this paper, the problem of simultaneous motion estimation of multiple independently moving objects is addressed. A novel Bayesian approach is designed for solving this problem using the sequential importance sampling (SIS) method. In the proposed algorithm, a balancing step is added into the SIS procedure to preserve samples of low weights so that all objects have enough samples to propagate empirical motion distributions. By using the proposed algorithm, the relative motions of all moving objects with respect to camera can be simultaneously estimated . This algorithm has been tested on both synthetic and real image sequences. Improved results have been achieved.",A bayesian approach to simultaneous motion estimation of multiple independently moving objects
"['Chang Ming Xing', 'Fang Ai Liu', 'Feng Juan Zhang']","Replication mechanism is the efficient method of improving grid performances and reducing client delay. Replica placement strategy is the key problem of replication mechanism. This paper introduces the architecture of education resource grid. According to the architecture, it presents three dynamic replica placement strategies. These strategies can optimize the system performance from different aspects and reduce client delay on the premise of keeping the storage space of replica. These strategies are proved to be effective by analyzing experiment data.",Replica Placement Strategy Research in Education Resource Grid
['Michael K. Papamichael'],"This paper presents a set of two FPGA-based Network-on-Chip (NoC) simulation engines that composed the winning design of the 2011 MEMOCODE Design Contest in the absolute performance class. Both simulation engines were developed in Bluespec System Verilog (BSV) and were implemented on a Xilinx ML605 FPGA development board. For smaller networks and simpler router configurations a direct-mapped approach was employed, where the network to be simulated was directly implemented on the FPGA. For larger networks, where a direct-mapped approach is not feasible due to FPGA resource limitations, a virtualized time-multiplexed approach was used. Compared to the provided software reference implementation, our direct-mapped approach achieves three orders of magnitude speedup, while our virtualized time-multiplexed approach achieves one to two orders of magnitude speedup, depending on the network and router configuration.",Fast scalable FPGA-based Network-on-Chip simulation models
"['Sofia Mysirlaki', 'Fotini Paraskeva']","Over the years, digital games have become one of adolescents' main interests and a controversial issue among researchers all over the world. However, it is claimed that digital games can be a promising educational tool utilizing both the attributes that make them effective in the learning process and the students' predisposition to deal with them. The present study explores the background of digital game use in a group of Greek adolescents (N=125) (12 to 16 years old) and the potential of using digital games as an educational tool, on a socio-cognitive learning theory basis. Gender differences were revealed; concerning the time spent playing digital games, the game preferences, and the identification with the hero of the game. There was no correlation of frequent digital game use with low academic performance, though frequent game use was partially correlated with low self-esteem and high computer self-efficacy.",Digital games: Developing the Issues of Socio-cognitive Learning Theory in an Attempt to Shift an Entertainment Gadget to an Educational Tool
"['David L. Jennings', 'Dennis W. Ruck']","This paper presents the results of experimentation with a simple ultrasonic lip motion detector or ""Ultrasonic Mike"" in automatic speech recognition. The device is tested in a speaker dependent isolated word recognition task with a vocabulary consisting of the spoken digits from zero to nine. The ""Ultrasonic Mike"" is used as input to an automatic lip reader. The automatic lip reader uses template matching and dynamic time warping to determine the best candidate for a given test utterance. The device is first tested as a stand alone automatic lip reader achieving accuracy as high as 89%. Next the automatic lip reader is combined with a conventional automatic speech recognizer. Classifier fusion is based on a pseudo probability mass function derived from the dynamic time warping distances. The combined system is tested with various levels of acoustic noise added. In a typical example, at 0 dB, the acoustic recognizer's accuracy was 78%, the lip reader accuracy was at 69%, but the combined accuracy was 93%. This experiment demonstrates that this simple ultrasonic lip motion detector, that has an output data rate 12500 times less than a typical video camera, can improve automatic speech recognition in noisy environments. This experiment also demonstrates an effective classifier fusion algorithm based on dynamic time warping distances.",Enhancing automatic speech recognition with an ultrasonic lip motion detector
"['Gyula D??m??t??r', 'Mikl??s I. B?≠n', 'L?≠szl?? L. Stach??']","By using the dynamically defined reaction path (DDRP) method and starting from various initial polygons, the intrinsic reaction coordonate (IRC) of the H 2 +HÉ??H+H 2  reaction has been calculated. The numerical stability of the method is illustrated by the evolution phases of the reaction path. Techniques and experiences on the parameter choice and effects of the parameter values on the stability and computer time consumption are discussed","Experiences and practical hints on using the DDRP method, illustrated by the example of the H 2 + H reaction"
"['Xinghuo Yu', 'Bin Wang', 'Xiangjun Li']","Variable Structure Systems (VSSs) have been studied extensively for over 60 years and widely used in practical applications. A particular interest in VSS is the so-called Sliding-Mode Control (SMC), which is simple in control design and robust in parameter variations and disturbances. Modern control systems nowadays are implemented through computers. This presents challenges for SMC based VSS because the digital nature of computer-control weakens the essential assumption of SMC, that is, the switching frequency should be unlimited in order to deliver effective disruptive control actions. Extensive research activities have been since undertaken in computer-controlled VSS over the last thirty years. This survey provides a comprehensive account of the key developments in this field and examines the key technical research challenges for the future developments.",Computer-Controlled Variable Structure Systems: The State-of-the-Art
"['Cristian Dima', 'Simon Lacroix']","Describes the design and implementation of an algorithm for improving the performance of stereo vision in environments presenting repetitive patterns or regions with relatively weak texture. The proposed algorithm makes use of the common assumption that the disparities corresponding to continuous surfaces in the world vary smoothly; we use this assumption to alleviate the correspondence problem for pixels that cannot be reliably matched by the stereo algorithm. Our approach can be described as a reliability based filtering of the disparity image followed by a recursive propagation step. It can be applied to the output of almost any ""standard"" stereo algorithm with minimal modifications, and is computationally efficient.",Using multiple disparity hypotheses for improved indoor stereo
"['Hamid Barghi', 'W. Cronenwett', 'Jon G. Bredeson']","We propose a new group of slotted ring protocols called REALNET. In REALNET networks the ring access scheme, traffic control and management, and slot allocation for voice and data cycles are made highly adaptive. We analyze, simulate and compare different REALNET networks which use different methods for their cycle alternation strategy. The results show that REALNET networks that use prediction satisfy strict voice delay requirements, allow small percentages of voice packet loss for a large number of active voice stations, as well as providing higher bandwidth utilization, and giving lower delays and higher throughputs for data packets when compared to other integrated services protocols. >",Real-time study of a high performance protocol for local/metropolitan ring networks
"['Grant J. Scott', 'Matthew N. Klaric', 'Curt H. Davis', 'Chi-Ren Shyu']","In this paper, we present a novel indexing structure that was developed to efficiently and accurately perform content-based shape retrieval of objects from a large-scale satellite imagery database. Our geospatial information retrieval and indexing system, GeoIRIS, contains 45 GB of high-resolution satellite imagery. Objects of multiple scales are automatically extracted from satellite imagery and then encoded into a bitmap shape representation. This shape encoding compresses the total size of the shape descriptors to approximately 0.34% of the imagery database size. We have developed the entropy-balanced bitmap (EBB) tree, which exploits the probabilistic nature of bit values in automatically derived shape classes. The efficiency of the shape representation coupled with the EBB tree allows us to index approximately 1.3 million objects for fast content-based retrieval of objects by shape.",Entropy-Balanced Bitmap Tree for Shape-Based Object Retrieval From Large-Scale Satellite Imagery Databases
"['Shayan Shahand', 'Jeroen van Duffelen', 'S??lvia Delgado Olabarriaga']","Summary#R##N#The sustainability of science gateways has been a topic of active discussion because they have been created and supported in the context of temporary research and infrastructure projects. As successful projects come to an end, it is necessary to find (new) models to secure continuous exploitation of products generated by these projects. Taking this step requires business considerations that are not trivial to do from the role of a researcher. This paper presents our experiences in adopting a methodology from lean business development, the Business Model Canvas (BMC). This methodology enables structured reflection upon the business model and facilitates exploring alternative ones (pivoting). We have applied the BMC to one of the science gateways designed, developed, and operated by the Academic Medical Center (AMC) e-Science group: the AMC Computational Neuroscience Gateway. The current gateway BMC is explained in the paper and used as basis for a reflection to improve its sustainability. Alternative business models are given as examples of BMC iteration or pivots. This exercise helped us to structure the various aspects to be considered when designing or reflecting upon the business model of our gateway. It also facilitated the visualization of the complete business picture and helps the reflection about improvements in the business model toward sustainability. We believe that this methodology could be valuable also for the reflection about sustainability of other science gateways that are growing from academic groups that do not have business training. Copyright ?? 2015É??John Wiley & Sons, Ltd.",Reflections on science gateways sustainability through the business model canvas: case study of a neuroscience gateway
"['Angel Valera', 'Francesc Benimeli', 'Jos?? Solaz', 'H. De Rosario', 'Anders Robertsson', 'Klas Nilsson', 'R. Zotovic', 'M. Mellado']","For the last years, automation is widely used to relieve humans from repetitive tasks, primarily and firstly within manufacturing. However, for products with less ideal (or hard to model) properties, and when forces depends on human interaction, auto mated testing has not been explored until now. This work presents the analysis of the (human-dependent) motions/forces based on a fully implemented test case for car-seat testing. For emulation of the corresponding mechanical wear, an experimental test bench was developed. A sensor mat with a pressure gauge net was used in the test bench to determine the relevant loads, and the corresponding movements performed by the humans when sitting in a car seat were acquired by means of a photogrammetry system. Finally, to automate the reproduction of such movements by means of a dummy held by a robot, several controllers have been developed to regulate the force applied by the dummy on the seat. Simplicity and force-control performance for the human replication was also investigated in this work, showing the benefit of freely programmable (open) force control. The developed system has many practical applications, as allowing the analysis of the wearing caused by these movements on the seat upholstery. Thus, force controlled testing of fabrics using robots is a viable option.",A Car-Seat Example of Automated Anthropomorphic Testing of Fabrics Using Force-Controlled Robot Motions
"['Xianyi Yang', 'Max Q.-H. Meng']","A neural network approach is proposed for real-time collision-free motion planning of holonomic and nonholonomic car-like robots in a nonstationary environment. This model is capable of planning real-time robot motion with sudden environmental changes, motion of a car with multiple targets, and motion of multiple robots. The proposed neural network model is biologically inspired, where the dynamics of each neuron in the topologically organized neural network is characterized by a shunting equation or an additive equation. There are only local connections among neurons. The real-time optimal robot motion is planned through the dynamic neural activity landscape of the neural network without explicitly searching over the free workspace nor the collision paths, without explicitly optimizing any cost functions, without any prior knowledge of the dynamic environment, without any learning process, and without any local collision checking procedures. Therefore it is computationally efficient. The stability of the neural network is guaranteed by Lyapunov stability analysis. The effectiveness and efficiency of the proposed approach are demonstrated through simulation studies.",Real-time motion planning of car-like robots
['Jeremy N??meth'],"By controlling the public spaces in which true human interaction occurs, planners and policymakers are challenging the fundamental desires of groups to forge social bonds and enhance their own sense familiarity with the environment. Security is not merely achieved through an understanding of territory but through acknowledgement and confrontation with individuals or groups maintaining diverse perspectives on the world.",Redefining security in public space: the case of LOVE park
"['Sonia L. Rueda', 'J. Rafael Sendra']",The linear complete differential resultant of a finite set of linear ordinary differential polynomials is defined. We study the computation by linear complete differential resultants of the implicit equation of a system of n linear differential polynomial parametric equations in n-1 differential parameters. We give necessary conditions to ensure properness of the system of differential polynomial parametric equations.,Linear complete differential resultants and the implicitization of linear DPPEs
"['Han-Yu Lin', 'Tzong-Sun Wu', 'Ting-Yu Huang', 'Yi-Shiung Yeh']","A proxy convertible authenticated encryption (CAE) scheme allows an original signer to delegate his signing power to a proxy signer such that the proxy signer can generate an authenticated ciphertext on behalf of the original signer. The generated authenticated ciphertext can only be decrypted and verified by the specific recipient instead of everyone else for the purpose of confidentiality. Integrating with self-certified public key systems, the proposed scheme can save more communication overheads and computation efforts, since it is not necessary to transmit and verify the public key certificate. That is, authenticating the public key can be combined with subsequent cryptographic operations such as the signature verification. In case of a later repudiation, the specific recipient has the ability to convert the signature into an ordinary one for convincing anyone of the signer's dishonesty.",Self-Certified Proxy Convertible Authenticated Encryption Scheme
"['Kalle Folkesson', 'Christer Svensson', 'Jan-Erik Eklund']","In communication applications, the requirements on A/D converters are high and increasing. To be able to design high-perfomance converters, it is important to understand the speed limitations. In this work, performance decrease caused by dynamic errors related to settling time of the switched circuits at high sampling frequencies is investigated.",Modeling of dynamic errors in algorithmic A/D converters
"['Freddy Chong Tat Chua', 'Ee-Peng Lim']","Bipartite networks are often used to capture the relationships between different classes of objects. To model the structure of bipartite networks, we propose a new hierarchical model based on a hierarchical random graph model originally designed for one-mode networks. The new model can better preserve the network fidelity as well as the assortative and disassortative structures of bipartite networks. We apply the proposed model on some paper-author networks in DBLP to find their optimal hierarchical structures. Using the optimal bipartite hierarchical structure, we regenerate networks that exhibit the similar network properties and degree distribution as the observed networks.",Modeling Bipartite Graphs Using Hierarchical Structures
"['Ram Dantu', 'Prakash Kolan', 'Robert G. Akl', 'K. A. Loper']","Security administration is an uphill task to implement in an enterprise network providing secured corporate services. With the slew of patches being released by network component vendors, system administrators require a barrage of tools for analyzing the risk due to vulnerabilities in those components. In addition, criticalities in patching some end hosts raises serious security issues about the network to which the end hosts are connected. In this context, it would be imperative to know the risk level of all critical resources keeping in view the everyday emerging new vulnerabilities. We hypothesize that sequence of network actions by attackers depends on their social and attack profile (behavioral resources such as skill level, time, and attitude). To estimate the types of attack behavior, we surveyed individuals for their ability and attack intent. Using the individuals' responses, we determined their behavioral resources and classified them as having opportunist, hacker, or explorer behavior. The profile behavioral resources can be used for determining risk by an attacker having that profile. Thus, suitable vulnerability analysis and risk management strategies can be formulated to efficiently curtail the risk from different types of attackers.",Classification of Attributes and Behavior in Risk Management Using Bayesian Networks
"['Hui-Liang Shen', 'Tong-sheng Mou', 'John H. Xin']","Two methods for colorimetric characterization of color scanner are proposed based on the measures of perceptual color difference error. The first method is used to minimize the total color differences between the actual and predicted color samples. The second one, which is a generalization of the existing cubic-root pre- processing technique, derives the mapping between the p'th root of scanner responses and Commission Internationale de l'Eclairage Lab (CIELAB) values. The experiment results indicate that the color accuracies of the proposed methods, especially the second one, are better than those of the traditional CIE XYZ (CIEXYZ)- space-based characterization methods. ?? 2006 SPIE and",Colorimetric characterization of scanner by measures of perceptual color error
"['Caroline Kulcs?≠r', 'Henri-Fran??ois Raynaud', 'Cyril Petit', 'Jean-Marc Conan']","Adaptive Optics (AO) systems enable to compensate the adverse effects of atmospheric turbulence on ground-based telescopes' images in real time, using a deformable mirror (DM) inserted in the telescope's optical path, and measurements provided by a wavefront sensor (WFS). This paper revisits minimum-variance (MV) control design for astronomical AO systems in a state-space framework. It presents a survey of the modeling and control issues arising in this multi-variable disturbance rejection problem. In a linear time-invariant framework, and under some mild assumptions, the optimal solution to MV control for AO systems is shown to be a discrete-time LQG controller. This result holds for a DM with instantaneous response, and for a fairly general class of DM's dynamics. The state-space approach is extended to Wide-field Adaptive Optics (WfAO) configurations involving several DMs and/or WFSs. Integral-action control used in existing AO systems is compared with the LQG controller. Experimental WfAO results obtained on a laboratory test bench are presented, showing significant improvement in performance. Finally, open issues and perspectives of applicative and/or theoretical interests are discussed.",Survey paper: Minimum variance prediction and control for adaptive optics
"['Markus Esch', 'Wei Tsang Ooi', 'Ingo Scholtes']","Massive Multiuser Virtual Environments (MMVEs) and the idea of a global scale 3D Web have grown popular in recent years. While commercial precursors of such environments for the most part rely on centralized client/server architectures, it is commonly accepted that a global scale virtual online world can only be realized in a distributed fashion. Within the HyperVerse project, we have developed and recently presented a two-tier Peer-to-Peer (P2P) architecture that incorporates a loosely structured P2P overlay of user peers and a highly structured overlay of server machines constituting a reliable backbone service. In such a distributed environment, an essential question is how avatars are tracked and interconnected in order to allow mutual rendering and interaction. We have previously proposed a hybrid avatar management scheme that utilizes the backbone service for avatar tracking if necessary, but handles tracking in a P2P fashion when peers can track each other to reduce the backbone load. This paper presents a detailed performance analysis of this algorithm under a realistic scenario, using traces from a large scale MMVE called Second Life. Moreover this paper presents and evaluates an optimization for the hybrid avatar tracking scheme that can be utilized under a weaker condition.",Evaluation of the HyperVerse Avatar Management Scheme Based on the Analysis of Second Life Traces
"['Fabio Roda', 'Leo Liberti', 'Franco Raimondi']","Recommender systems exploit a set of established user preferences to predict topics or products that a new user might like [2]. Recommender systems have become an important research area in the field of information retrieval. Many approaches have been developed in recent years and the interest is very high. However, despite all the efforts, recommender systems are still in need of further development and more advanced recommendation modelling methods, as these systems must take into account additional requirements on user preferences, such as geographic search and social networking. This fact, in particular, implies that the recommendation must be much more É??personalizedÉ?ù than it used to be.#R##N#In this paper, we describe the recommender system used in the É??DisMoiOuÉ?ù(É??TellMeWhereÉ?ù in French) on-line service (http://dismoiou.fr), which provides the user with advice on places that may be of interest to him/her; the definition of É??interestÉ?ù in this context is personalized taking into account the geographical position of the user (for example when the service is used with portable phones such as the Apple iPhone), his/her past ratings, and the#R##N#ratings of his/her neighbourhood in a known social network.#R##N#Using the accepted terminology [6], DisMoiOu is mainly a Collaborative Filtering System (CFS): it employs opinions collected from similar users to suggest likely places. By contrast with existing recommender systems, ours puts#R##N#together the use of a graph theoretical model [4] and that of combinatorial optimization methods [1]. Broadly speaking, we encode known relations between users and places and users and other users by means of weighted graphs. We then define essential components of the system by means of combinatorial optimization problems on a reformulation of these graphs, which are finally used#R##N#to derive a ranking on the recommendations associated to pairs (user,place). Preliminary computational results on the three classical evaluation parameters for recommender systems (accuracy, recall, precision [3]) show that our system performs well with respect to accuracy and recall, but precision results need to be improved.",Combinatorial optimization based recommender systems
"['Lei Hou', 'Haixin Duan', 'Jianping Wu']","In Peer-to-Peer (P2P) networks if adversaries such as Sybil attackers have got most identities in the network, they will control routing table or traffic. In this paper, we propose a framework based on two complementary techniques to defense malicious node after they transmit data to other malicious peers instead of honest peers. The first approach, based on behaviors of destination nodes, is used to count the local nodes? credit values in a period of credit construction time. The second approach is updating the global nodes? credit values between schedulers. We propose scheduler to maintain the trust relationship between global nodes. In order to assess the effectiveness of the above techniques, we extend Eclipse attack code in simulator p2pSim-0.3 and implement scheduler defense malicious attack behavior algorithm based on both credit and proximity. We adopt Chord protocol and Euclidean topology to implement scheduling algorithm, but the same methodology can be applied to other protocols and topologies as well.",Scheduling Peers Based on Credit Construction Period in Peer-to-Peer Networks
"['Ashraf Elnagar', 'Leena Lulu']","We introduce an effective computer aided learning visual tool (CALVT) to teach graph-based applications. We present the robot motion planning problem as an example of such applications. The proposed tool can be used to simulate and/or further to implement practical systems in different areas of computer science such as graphics, computational geometry, robotics and networking. In the robot motion planning example, CALVT enables users to setup the working environment by creating obstacles and a robot of different shapes, specifying starting and goal positions, and setting other path or environment parameters from a user-friendly interface. The path planning system involves several phases. Each of these modules is complex and therefore we provide the possibility of visualizing graphically the output of each phase. Based on our experience, this tool has been an effective one in classroom teaching. It not only cuts down, significantly, on the instructor's time and effort but also motivates senior/graduate students to pursue work in this specific area of research.",A visual tool for computer supported learning: The robot motion planning example
"['Bettina Klinz', 'Gerhard J. Woeginger']","We investigate the computational complexity of two special cases of the Steiner tree problem where the distance matrix is a Kalmanson matrix or a circulant matrix, respectively. For Kalmanson matrices we develop an efficient polynomial time algorithm that is based on dynamic programming. For circulant matrices we give an \(\mathcal{N}\mathcal{P}\)-hardness proof and thus establish computational intractability.",The Steiner tree problem in Kalmanson matrices and in circulant matrices
"['Jos?? M. Bad??a', 'Peter Benner', 'Rafael Mayo', 'Enrique S. Quintana-Ort??']","This paper describes the parallelization of the low-rank ADI iteration for the solution of large-scale, sparse Lyapunov equations. The only relevant operations involved in the method are matrix-vector products and the solution of linear systems. Experimental results on a cluster, using the SuperLU library, show the performance of this approach.",Solving Large Sparse Lyapunov Equations on Parallel Computers (Research Note)
"['Shi Qingsong', 'Chen Tianzhou', 'Hu Wei', 'Jolly Wang', 'Nick Bao']",Multicore has shown its merits of high performance and low power consumption compared with traditional single cores. It also puts a challenge to the universities in how to teach and train the students in this rapidly changing field. In this paper we present the online programming experience platform designed and implemented for multicore curriculum. Teaching effect indicates that this platform is beneficial for multi-core programming.,Online Programming Experience Platform for Multicore Curriculum
"['Mimi Kao', 'Nick Cercone', 'W. S. Luk']","When natural language front-ends are introduced to database management systems, generation of quality responses have proven problematical in situations when null values arise. In our work, in which we assume the database query language is SQL, we present methods for responding with appropriate answers to null value responses. To do so we use a knowledge base based on RM/T, an extended relational model proposed by E. F. Codd. The advantages of this approach are described. To demonstrate the utility of the knowledge base, a simple knowledge base is constructed. A detailed algorithm is given to provide additional information when a null answer is returned.",What do you mean É??nullÉ?ù? Turning null responses into quality responses
['Sung-eok Jeon'],"The generalized multiprotocol label switching (GMPLS) networks attain a hierarchical structure, andeach layer maintains an independent protection mechanism, resulting in redundant protection. The common pool method provides a basic approach to solve the redundant protection problem. Although the common pool method is simple and robust, however it can fail in some cases. This work first shows that none of the prior work satisfies both redundant protection and two links failure problem simultaneously. Moreover, this work also presents a new type of two links failure problem (i.e., the failure of two links at two different layers), which can happen frequently and the common poolmethod cannot cope with. To solve the proposed two links failure problem, while minimizing the cost of redundant protection problem, this work proposes a new protection scheme for hierarchical GMPLS networks.",Concurrent Failures and Redundant Protection Problem in Hierarchical GMPLS Networks
['Sven Beuchler'],"In this paper, a uniformly elliptic second order boundary value problem in 2-D discretized by the p-version of the finite element method is considered. An inexact Dirichlet-Dirichlet domain decomposition pre-conditioner for the system of linear algebraic equations is investigated. Two solvers for the problem in the sub-domains, a pre-conditioner for the Schur-complement and an extension operator operating from the edges of the elements into the interior are proposed as ingredients for the inexact DD-pre-conditioner. In the main part of the paper, several numerical experiments on a parallel computer are given.",A Domain Decomposition Preconditioner for p -FEM Discretizations of Two-dimensional Elliptic Problems
"['Cyrus Gerami', 'Narayan B. Mandayam', 'Larry J. Greenstein']","Since the FCC's approval of unlicensed use of TV white spaces, the issue of how to use these white spaces has led to innovative technologies such as cognitive radios as well as a variety of spectrum policy proposals. There have been proposals to devise alternate rules for spectrum usage citing the overly conservative restrictions on secondary transmissions to protect incumbents. In this paper, instead, we propose to utilize white spaces for a backhaul network for internet traffic based on existing restrictions. Using the available white spaces and backhaul traffic demands in New Jersey as a case study, we evaluate the feasibility of such backhauling and present a methodology that can be used for other areas as well. Using a basic design involving fixed towers and directional antennas, our results show that the TV white spaces can be an effective medium for radio backhaul as an alternative to the costly laying of optical fiber. We also show that meeting FCC requirements on sensing and avoiding harm to wireless microphones will have only a minor impact on capacity.",Backhauling in TV White Spaces
"['Tae-Jung Kim', 'Jan-Peter Muller']","This paper describes a new automated building extraction algorithm developed for high resolution stereo imagery. The problems of urban area imagery for stereo matching were investigated. Buildings were found to create isolated regions and many blunders. To overcome the problem of isolated regions, a pyramidal matching algorithm with automatic seed points using a tile-based control strategy was developed. To remove possible blunders and extract buildings from other background objects, a series of É??smartÉ?? operations using linear elements from buildings was applied. A quantitative analysis of the accuracy of the algorithm was assessed in comparison with a previous algorithm and shown to have an RMS error of 3.65 m in elevation compared to 13.46 m.",Automated urban area building extraction from high resolution stereo imagery
"['Pablo Andr??s Barrientos', 'Pablo Lopez']","The development of domain-specific languages (DSLs) is considered by many authors as a hard task. To simplify the design of DSLs we describe a design pattern based on the combinators technique, which can also provide guidelines for previous domain analysis phase because it is based on equational reasoning over the domain knowledge. Combinators is a common technique from functional programming to write programs. It was used many times to implement domain-specific embedded languages (DSELs) but that implementation approach is not the only one. In this paper we present the pattern together with the underlying and basic ideas behind it. We also show benefits of using it and illustrate the use of this pattern with some examples.",Developing DSLs using combinators. A design pattern
"['James W. Fonda', 'Sarangapani Jagannathan', 'Steve E. Watkins']","A novel fault diagnostics and prediction scheme in continuous-time is introduced for a class of nonlinear systems. The proposed method uses a novel neural network (NN) based robust integral sign of the error (RISE) observer, or estimator, allowing for semi-global asymptotic stability in the presence of NN approximation errors, disturbances and unmodeled dynamics. This is in comparison to typical results presented in the literature that show only boundedness in the presence of uncertainties. The output of the observer/estimator is compared with that of the nonlinear system and a residual is used for declaring the presence of a fault when the residual exceeds a user defined threshold. The NN weights are tuned online with no offline tuning phase. The output of the RISE observer is utilized for diagnostics. Additionally, a method for time-to-failure (TTF) prediction, a first step in prognostics, is developed by projecting the developed parameter-update law under the assumption that the nonlinear system satisfies a linear-in-the-parameters (LIP) assumption. The TTF method uses known critical values of a system to predict when an estimated parameter will reach a known failure threshold. The performance of the NN/RISE observer system is evaluated on a nonlinear system and a simply supported beam finite element analysis (FEA) simulation based on laboratory experiments. Results show that the proposed method provides as much as 25% increased accuracy while the TTF scheme renders a more accurate prediction.",Robust neural network RISE observer based fault diagnostics and prediction
"['Michael Shindler', 'Alex Wong', 'Adam Meyerson']","Clustering is a popular problem with many applications. We consider the k-means problem in the situation where the data is too large to be stored in main memory and must be accessed sequentially, such as from a disk, and where we must use as little memory as possible. Our algorithm is based on recent theoretical results, with significant improvements to make it practical. Our approach greatly simplifies a recently developed algorithm, both in design and in analysis, and eliminates large constant factors in the approximation guarantee, the memory requirements, and the running time. We then incorporate approximate nearest neighbor search to compute k-means in o(nk) (where n is the number of data points; note that computing the cost, given a solution, takes ??(nk) time). We show that our algorithm compares favorably to existing algorithms - both theoretically and experimentally, thus providing state-of-the-art performance in both theory and practice.",Fast and Accurate k-means For Large Datasets
"['Xing Qiu', 'Yuanhui Xiao', 'Alexander Y. Gordon', 'Andrei Yakovlev']",Background#R##N#The number of genes declared differentially expressed is a random variable and its variability can be assessed by resampling techniques. Another important stability indicator is the frequency with which a given gene is selected across subsamples. We have conducted studies to assess stability and some other properties of several gene selection procedures with biological and simulated data.,Assessing stability of gene selection in microarray data analysis.
"['Nan-Wei Gong', 'Mathew Laibowitz', 'Joseph A. Paradiso']","This paper describes the design and implementation of a dynamic privacy management system aimed at enabling tangible privacy control and feedback in a pervasive sensor network. Our work began with the development of a potentially invasive sensor network (with high resolution video, audio, and motion tracking capabilities) featuring different interactive applications that created incentive for accepting this network as an extension of people's daily social space. A user study was then conducted to evaluate several privacy management approaches - an active badge system for both online and on-site control, on/off power switches for physically disabling the hardware, and touch screen input control. Results from a user study indicated that an active badge for on-site privacy control is the most preferable method among all provided options. We present a set of results that yield insight into the privacy/benefit tradeoff from various sensing capabilities in pervasive sensor networks and how privacy settings and user behavior relate in these environments.",Dynamic privacy management in pervasive sensor networks
"['Jing Deng', 'Kang Li', 'Eileen Harkin-Jones', 'Minrui Fei', 'Shaoyuan Li']","This paper concerns the nonlinear system modelling using Radial Basis Function (RBF) neural networks. RBF neural models can be constructed through a subset selection procedure where the nonlinear parameters associated to the hidden nodes are fixed, thus only significant hidden nodes are selected for inclusion in the final model. However, due to existence of noise on data, this procedure often leads to an over-fitted model with unsatisfactory generalisation performance. Bayesian regularisation and leave-one-out cross validation can be incorporated to tackle this issue, but the algorithm stability is an issue that needs to be addressed. This paper proposes a new method which not only improves the compactness of the resultant RBF neural model, but also the accuracy of estimated model coefficients. This is achieved by effectively incorporating the A-optimality design criterion into a recently proposed two-stage subset selection, while the computational efficiency is still retained from the original two-stage selection method by introducing a residual matrix. Experimental results on two simulation benchmarks are included to illustrate the effectiveness of the proposed approach.",A novel two stage algorithm for construction of RBF neural models based on A-optimality criterion
"['William P. Evans', 'David Burnell']","Physical effects in deep submicron processes can affect reliability, performance, and even functionality in common circuit building blocks used in data converters. NBTI (Negative Bias Temperature Instability), STI (Shallow Trench Isolation) stress, and NWELL proximity effects will be reviewed and examples are given of circuit topologies and layout practices which can minimize the detrimental aspects these effects on commonly used blocks such as comparators, op-amps, and high speed flip-flops.",Deep submicron effects on data converter building blocks
"['Johannes Neugebauer', 'Markus Reiher']","Abstract#R##N##R##N#The mode-tracking principle [J. Chem. Phys. 2003, 118, 1634] for the direct quantum chemical calculation of preselected, characteristic molecular vibrations makes vibrational analyses of very large molecules feasible. This is demonstrated here for the [(Ph3PAu)6C]2+ complex, in which 18 phenyl groups in the ligand sphere are explicitly taken into account. We are aiming at the motion of the endohedral carbon atom, which is in an extraordinary bonding situation because it is surrounded by an octahedral core of gold atoms in this cluster. Secondary effects of the full ligand sphere on the vibrations of the [Au6C] core embedded in [(R3PAu)6C]2+ clusters are investigated. For this purpose, local vibrations of the octahedral core are generated, and their long-range couplings with the phosphine ligand sphere become visible in the mode-tracking iterations. The exact normal modes of these characteristic vibrations of the cluster are then obtained after convergence of the mode-tracking refinement. This protocol allows us to assess the coupling of the outer ligand sphere with the inner core of the cluster in terms of changes of the vibrational frequencies and of the collective motions of the atomic nuclei. The vibrational frequencies of the octahedral [Au6C] core split due to symmetry breaking in the C1-symmetric [(Ph3PAu)6C]2+ cluster. Our study demonstrates how effects of the periphery of a large molecule on local vibrations can be quantified. Furthermore, we predict the first set of characteristic vibrational frequencies obtained with first-principles methods for this gold cluster, whose vibrational spectra have not yet been recorded experimentally. ?? 2004 Wiley Periodicals, Inc. J Comput Chem 25: 587É??597, 2004",Vibrational center-ligand couplings in transition metal complexes.
"['Sriishan Sridhar', 'Glenn Healey']","The authors develop and analyze high-speed algorithms for the detection of point targets in infrared (IR) images with spatially varying clutter. Current target detection systems are effective in detecting bright targets in a uniform sky, but in areas of strong clutter are either unable to detect targets reliably or are limited by high false alarm rates. The authors assume that target and sensor models are available. Clutter is considered to be poorly characterized and spatially varying. Target detection algorithms are based on filtering to enhance the target signal relative to the background, followed by an adaptive threshold. Statistical analysis of the algorithms is provided to quantify algorithm performance. The system implements a spatially adaptive algorithm that maximizes probability of target detection while maintaining a fixed false alarm rate. The algorithms are robust in the presence of spatially varying clutter. The authors include experimental results to illustrate this. >",Point target detection in spatially varying clutter
"['Lei Wang', 'Yueming Cai', 'Junquan Hu', 'Weiwei Yang']",We analyze the impact of channel estimation errors on amplify-and-forward (AF) cooperative communication systems over Nakagami-m fading channels. We derive exact closed-form expression of the average symbol error rate (SER) with channel estimation errors by using the moment-generating function (MGF) based approach. Numerical results confirm that our theoretical analysis is very accurate.,Performance Analysis of Amplify-and-Forward Cooperative Communication Systems with Channel Estimation Errors over Nakagami-m Channels
"['Allan Borodin', 'Ran El-Yaniv', 'Vincent Gogan']","A novel algorithm for actively trading stocks is presented. While traditional expert advice and ""universal"" algorithms (as well as standard technical trading heuristics) attempt to predict winners or trends, our approach relies on predictable statistical relations between all pairs of stocks in the market. Our empirical results on historical markets provide strong evidence that this type of technical trading can ""beat the market"" and moreover, can beat the best stock in the market. In doing so we utilize a new idea for smoothing critical parameters in the context of expert learning.",Can We Learn to Beat the Best Stock
['Marcin Peczarski'],"We present the Gold Partition Conjecture which immediately implies the \(1/3\)É??\(2/3\) Conjecture and tight upper bound for sorting. We prove the Gold Partition Conjecture for posets of width two, semiorders and posets containing at most \(11\) elements. We prove that the fraction of partial orders on an \(n\)-element set satisfying our conjecture converges to \(1\) when \(n\) approaches infinity. We discuss properties of a hypothetical counterexample.",The Gold Partition Conjecture
"['Kenji Kashima', 'Yutaka Yamamoto', 'Masaaki Nagahara']","Discrete wavelet transform is usually executed by the so-called pyramid algorithm. It, however, requires a proper initialization, i.e., expansion coefficients with respect to the basis of one of the desired approximation subspaces. An interesting question here is how we can obtain such coefficients when only sampled values of signals are available. This letter provides a design method for a digital filter that (sub-)optimally gives such coefficients assuming certain a priori knowledge on the frequency characteristic of target functions. We then extend the result to the case of nonorthogonal wavelets. Examples show the effectiveness of the proposed method.",Optimal wavelet expansion via sampled-data control theory
"['Roshan K. Thomas', 'Ravi S. Sandhu']","Dissemination control (DCON) is emerging as one of the most important and challenging goals for information security. DCON is concerned with controlling information and digital objects even after they have been delivered to a legitimate recipient. The need for DCON arises in many different domains ranging from the dissemination of digital music and movies, eBooks, business proprietary and sensitive electronic documents as well as the propagation of mailing lists in relation to direct marketing. Our goal in this short paper is to present some of the multidimensional technical issues that need to be modeled and understood so as to provide a comprehensive set of DCON capabilities. It represents a first but necessary step in our ongoing work in formulating a family of DCON models.",Towards a multi-dimensional characterization of dissemination control
"['Salman Aslam', 'Christopher F. Barnes', 'Aaron F. Bobick']","In this paper, we discuss methods to enable robust surveillance on compressed video. We show that if the particular surveillance algorithm that is likely to be run on the compressed video is known apriori, then steps can be taken during the encoding process to facilitate the performance of the algorithm. We show that by performing signal processing on the input video signal before it is encoded, or by adaptively changing the parameters of the encoding process, we can make the resulting signal more robust to degradations in the encoding process. The result is better and more consistent tracking on the compressed video from high to low bitrates, but with some loss in PSNR. We demonstrate the validity of this approach for Mean Shift tracking running on MPEG-4 coded video.",Robust Surveillance on Compressed Video: Uniform Performance from High to Low Bitrates
"['David Rollinson', 'Austin Buchan', 'Howie Choset']","We present a comparison of methods to estimate the shape and orientation of a locomoting snake robot by fusing the robot's redundant internal proprioceptive sensors using and Extended Kalman Filter (EKF). All of the estimators used in this work represent the shape of the snake with gait parameters to reduce the complexity of the robot configuration space. The compared approaches for representing shape and pose of the snake robot differ primarily in the use of a body frame fixed to the pose of a single module versus one that is aligned with the virtual chassis. Additionally, we evaluate a state representation that explicitly tracks joint angles for improved estimates. For one particular gait, rolling, we present experimental data where motion capture data of the snake robot is used as ground truth to compare the accuracy of the state estimates from these techniques. We show that using the virtual chassis body frame, rather than a fixed body frame, results in improved accuracy of the snake robot's estimated pitch and roll. We also show that, in general, representing the robot's shape with gait parameters is sufficient to accurately estimated shape and pose, though it can be improved upon in specific cases by explicitly modeling joint angles.",State estimation for snake robots
"['Kostis Xenoulis', 'Nicholas Kalouptsidis']","A new tight upper bound on the maximum-likelihood (ML) word and bit-error decoding probabilities for specific codes over discrete channels is presented. It constitutes an enhanced version of the Gallager upper bound and its variations resulting from the Duman-Salehi second bounding technique. An efficient technique is developed that, in the case of symmetric channels, overcomes the difficulties associated with the direct computation of the proposed bound. Surprisingly, apart from the distance and input-output weight enumerating functions (IOWEFs), the bound depends also on the coset weight distribution of the code.",Improvement of Gallager Upper Bound and its Variations for Discrete Channels
"['Jayawan H B Wijekoon', 'Piotr Dudek']","This paper presents a novel analogue VLSI circuitry that reproduces spiking and bursting firing patterns of cortical neurons, using only 14 MOSFETs. The circuit provides a basic building block for the development of neuromorphic architectures. It enables implementation of many neurons in a single silicon chip while exhibiting flexibility in obtaining different types of adaptive and oscillatory neuron behaviours, by simply adjusting the biasing voltage. The simulation results, using a 0.35um CMOS technology, are presented.",Simple Analogue VLSI Circuit of a Cortical Neuron
"['Filiz Kalelioglu', 'Yasemin G?¨lbahar']","The aim of this research study was to explore the effect of instructional techniques on critical thinking and critical thinking dispositions in online discussion, based on triangulation design. Six Thinking Hats, Brainstorming, Role Playing, Socratic Seminar, and Anyone Here an Expert, were selected as an instructional techniques for online discussion. In the quantitative part, according to the results of ANOVA, except Socratic Seminar, there is no difference between groups in terms of scores of pre-tests and post-tests of critical thinking dispositions. In the qualitative part, according to the results of the analysis of critical thinking in online discussion, the Mixed Techniques group performed as having the best ability of critical thinking, the Anyone Here an Expert group was second and the Brainstorming group was third in terms of performing critical thinking ability in online discussion.",The Effect of Instructional Techniques on Critical Thinking and Critical Thinking Dispositions in Online Discussion
"['Thomas Gamer', 'Christoph P. Mayer', 'Martina Zitterbart']","Distributed denial-of-service attacks pose unpredictable threats to the Internet infrastructure and Internet-based business. Thus, many attack detection systems and anomaly detection methods were developed in the past. A realistic evaluation of these mechanisms and comparable results, however, are impossible up to now. Furthermore, an adaptation to new situations or an extension of existing systems in most cases is complex and time-consuming. Therefore, we developed a framework for attack detection which allows for an integration of various detection methods as lightweight modules. These modules can be combined easily and arbitrarily and thus, adapted to varying situations. Additionally, our framework can be applied in different runtime environments transparently. This enables an easy evaluation with meaningful and comparable results based on realistic large-scale scenarios, e.g. by using a network simulator.",Distack -- A Framework for Anomaly-Based Large-Scale Attack Detection
"['Richard Rzeszutek', 'Thomas F. El-Maraghi', 'Dimitrios Androutsos']","The Random Walks image segmentation algorithm provides a fast and effective method for supervised image segmentation. However, Random Walks does not work very well in the presence of noise or texture. Therefore, we propose an augmented version of Random Walks known as É??Scale-Space Random WalksÉ?ù (SSRW) that addresses these problems. Through a minor, though non-trivial, modification to the Random Walks algorithm, we show that the SSRW can produce more accurate segmentations in the presence of noise and texture then the original Random Walks can.",Scale-Space Random Walks
['John D. Musa'],"Software testing often results in delays to market and high cost without assuring product reliability. Software reliability engineering can be applied to carefully engineer testing to overcome these weaknesses. This application is often referred to as software-reliability-engineered testing. We present several case studies illustrating the application of software reliability engineering to testing. We take a quick overview of the field, to better understand the terms used and procedures discussed in the case studies.",Introduction to software reliability engineering and testing
